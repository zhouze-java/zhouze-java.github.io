<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Elasticsearch-94-search template搜索模板化]]></title>
    <url>%2F2019%2F01%2F21%2FElasticsearch-94-search-template%E6%90%9C%E7%B4%A2%E6%A8%A1%E6%9D%BF%E5%8C%96%2F</url>
    <content type="text"></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-93-深入剖析搜索结果的高亮显示]]></title>
    <url>%2F2019%2F01%2F18%2FElasticsearch-93-%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90%E6%90%9C%E7%B4%A2%E7%BB%93%E6%9E%9C%E7%9A%84%E9%AB%98%E4%BA%AE%E6%98%BE%E7%A4%BA%2F</url>
    <content type="text"><![CDATA[搜索结果高亮显示先来看一个最基本的高亮案例 首先创建一个索引1234567891011121314151617PUT /blog_website&#123; &quot;mappings&quot;: &#123; &quot;blogs&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; &#125;, &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; &#125; &#125; &#125; &#125;&#125; 加条数据进去12345PUT /blog_website/blogs/1&#123; &quot;title&quot;: &quot;我的第一篇博客&quot;, &quot;content&quot;: &quot;大家好，这是我写的第一篇博客，特别喜欢这个博客网站！！！&quot;&#125; 搜索12345678910111213GET /blog_website/blogs/_search &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;博客&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;title&quot;: &#123;&#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930&#123; &quot;took&quot;: 29, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.26742277, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;blog_website&quot;, &quot;_type&quot;: &quot;blogs&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.26742277, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;我的第一篇博客&quot;, &quot;content&quot;: &quot;大家好，这是我写的第一篇博客，特别喜欢这个博客网站！！！&quot; &#125;, &quot;highlight&quot;: &#123; &quot;title&quot;: [ &quot;我的第一篇&lt;em&gt;博客&lt;/em&gt;&quot; ] &#125; &#125; ] &#125;&#125; &lt;em&gt;&lt;/em&gt; 标签,会在网页中将内容变成红色,所以指定的field中,如果包含了那个搜索词的话,就会在那个field的文本中,对搜索词进行红色的高亮显示 多个filed高亮也是一样的12345678910111213141516171819202122232425GET /blog_website/blogs/_search &#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;博客&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;博客&quot; &#125; &#125; ] &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;title&quot;: &#123;&#125;, &quot;content&quot;: &#123;&#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233&#123; &quot;took&quot;: 8, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.6390219, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;blog_website&quot;, &quot;_type&quot;: &quot;blogs&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.6390219, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;我的第一篇博客&quot;, &quot;content&quot;: &quot;大家好，这是我写的第一篇博客，特别喜欢这个博客网站！！！&quot; &#125;, &quot;highlight&quot;: &#123; &quot;title&quot;: [ &quot;我的第一篇&lt;em&gt;博客&lt;/em&gt;&quot; ], &quot;content&quot;: [ &quot;大家好，这是我写的第一篇&lt;em&gt;博客&lt;/em&gt;，特别喜欢这个&lt;em&gt;博客&lt;/em&gt;网站！！！&quot; ] &#125; &#125; ] &#125;&#125; highlight中的field,必须跟query中的field是一一对应的 三种highlight介绍plain highlight默认的高亮就是用的这种,底层是lucene highlight posting highlight设置索引的 index_options = offset 后,高亮就是用的posting highlight 优点: 性能比plain highlight要高,因为不需要重新对高亮文本进行分词 对磁盘的消耗更少 将文本分割为句子,并且对句子进行高亮,效果更好 删除之前的索引,然后重新建123456789101112131415161718PUT /blog_website&#123; &quot;mappings&quot;: &#123; &quot;blogs&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; &#125;, &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;index_options&quot;: &quot;offsets&quot; &#125; &#125; &#125; &#125;&#125; content设置了 “index_options”: “offsets” ,然后还是把之前那条数据添加进去,对content搜索高亮12345678910111213GET /blog_website/blogs/_search &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;博客&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;content&quot;: &#123;&#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930&#123; &quot;took&quot;: 33, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.37159908, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;blog_website&quot;, &quot;_type&quot;: &quot;blogs&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.37159908, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;我的第一篇博客&quot;, &quot;content&quot;: &quot;大家好，这是我写的第一篇博客，特别喜欢这个博客网站！！！&quot; &#125;, &quot;highlight&quot;: &#123; &quot;content&quot;: [ &quot;大家好，这是我写的第一篇&lt;em&gt;博客&lt;/em&gt;，特别喜欢这个&lt;em&gt;博客&lt;/em&gt;网站！！！&quot; ] &#125; &#125; ] &#125;&#125; 其实效果跟plain是一样的 fast vector highlightindex-time时候term vector设置在mapping中,就会用fast vector highlight 在field值比较大(大于1MB)的情况下性能更高 还是将之前的删掉,重新建123456789101112131415161718PUT /blog_website&#123; &quot;mappings&quot;: &#123; &quot;blogs&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; &#125;, &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;term_vector&quot; : &quot;with_positions_offsets&quot; &#125; &#125; &#125; &#125;&#125; 在content里面设置了term_vector,就可以使用了 强制使用某种highlight比如,对于开启了term_vector的filed,强制使用plain highlight123456789101112131415GET /blog_website/blogs/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;博客&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;content&quot;:&#123; &quot;type&quot;:&quot;plain&quot; &#125; &#125; &#125;&#125; 自定义高亮html标签默认是&lt;em&gt;&lt;/em&gt;,可以通过设置pre_tags和post_tags来自定义html标签 1234567891011121314151617GET /blog_website/blogs/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;博客&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;pre_tags&quot;: [&quot;&lt;tag&gt;&quot;], &quot;post_tags&quot;: [&quot;&lt;/tag&gt;&quot;], &quot;fields&quot;: &#123; &quot;content&quot;:&#123; &quot;type&quot;:&quot;plain&quot; &#125; &#125; &#125;&#125; 高亮片段fragment的设置fragment_size先来添加一个比较长的数据12345PUT /blog_website/blogs/2&#123; &quot;title&quot;: &quot;我的第二篇博客&quot;, &quot;content&quot;: &quot;大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！&quot;&#125; 然后查询的时候设置 fragment_size123456789101112131415161718GET /blog_website/blogs/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;博客&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;pre_tags&quot;: [&quot;&lt;tag&gt;&quot;], &quot;post_tags&quot;: [&quot;&lt;/tag&gt;&quot;], &quot;fields&quot;: &#123; &quot;content&quot;:&#123; &quot;type&quot;:&quot;plain&quot;, &quot;fragment_size&quot;: 20 &#125; &#125; &#125;&#125; 返回值:12345678910111213141516171819202122232425262728293031323334&#123; &quot;took&quot;: 6, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.56305844, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;blog_website&quot;, &quot;_type&quot;: &quot;blogs&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.56305844, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;我的第二篇博客&quot;, &quot;content&quot;: &quot;大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！&quot; &#125;, &quot;highlight&quot;: &#123; &quot;content&quot;: [ &quot;大家好，这是我写的第二篇&lt;tag&gt;博客&lt;/tag&gt;，特别喜欢&quot;, &quot;这个&lt;tag&gt;博客&lt;/tag&gt;网站！！！大家好，这是我写&quot;, &quot;的第二篇&lt;tag&gt;博客&lt;/tag&gt;，特别喜欢这个&lt;tag&gt;博客&lt;/tag&gt;网站！！！大家好&quot;, &quot;，这是我写的第二篇&lt;tag&gt;博客&lt;/tag&gt;，特别喜欢这个&lt;tag&gt;博客&lt;/tag&gt;&quot;, &quot;网站！！！大家好，这是我写的第二篇&lt;tag&gt;博客&lt;/tag&gt;&quot; ] &#125; &#125; ] &#125;&#125; 设置了fragment_size是20,然后在highlight中,就会把content按长度是20的切割成多个 使用场景:有时候field长度太长,但是你的页面不可能全显示出来,可能只需要显示一段内容就好,然后就可以通过设置fragment_size(默认是100)来将field的值进行分割 number_of_fragmentsnumber_of_fragments: 用来指定显示多少个片段 比如上面,一共拆分出5个片段来,可能我们只需要三个,那就设置number_of_fragments为3,就可以了12345678910111213141516171819GET /blog_website/blogs/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;博客&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;pre_tags&quot;: [&quot;&lt;tag&gt;&quot;], &quot;post_tags&quot;: [&quot;&lt;/tag&gt;&quot;], &quot;fields&quot;: &#123; &quot;content&quot;:&#123; &quot;type&quot;:&quot;plain&quot;, &quot;fragment_size&quot;: 20, &quot;number_of_fragments&quot;: 3 &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526272829303132&#123; &quot;took&quot;: 4, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.56305844, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;blog_website&quot;, &quot;_type&quot;: &quot;blogs&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.56305844, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;我的第二篇博客&quot;, &quot;content&quot;: &quot;大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！&quot; &#125;, &quot;highlight&quot;: &#123; &quot;content&quot;: [ &quot;大家好，这是我写的第二篇&lt;tag&gt;博客&lt;/tag&gt;，特别喜欢&quot;, &quot;这个&lt;tag&gt;博客&lt;/tag&gt;网站！！！大家好，这是我写&quot;, &quot;的第二篇&lt;tag&gt;博客&lt;/tag&gt;，特别喜欢这个&lt;tag&gt;博客&lt;/tag&gt;网站！！！大家好&quot; ] &#125; &#125; ] &#125;&#125; 只显示了前3个片段]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-92-查询term vectors词条向量信息]]></title>
    <url>%2F2019%2F01%2F18%2FElasticsearch-92-%E6%9F%A5%E8%AF%A2term-vectors%E8%AF%8D%E6%9D%A1%E5%90%91%E9%87%8F%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[term vectors介绍可以理解为,关于 词 的一些统计信息. 可以查询到的信息比如有 词条的信息,比如position位置,start_offset开始的偏移值, end_offset结束的偏移值,词条的payLoads(主要用于自定义字段的权重) 词条统计,doc_freq, ttf term_freq 该词出现的次数 频率 字段统计,包含sum_doc_freq:该字段中词的数量(去掉重复的数目) sum_ttf:文档中词的数量(包含重复的数目)、doc_count:涉及的文档数等等 默认这些统计信息都是基于分片的,可以设置dfs为true,返回全部的分片的信息,但是会有一定的性能问题,不推荐使用,还可以通过参数对返回的字段进行过滤,只返回感兴趣的部分 可以通过两种方式查询到term vector的信息 index-time,创建索引的时候,在mapping里面配置一下,就直接生成这些term和field的统计信息了 query-time,不需要提前创建,直接查询的时候使用就好了,是现场计算返回的 index-time生成创建索引12345678910111213141516171819202122232425262728293031323334353637PUT /my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;text&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;term_vector&quot;: &quot;with_positions_offsets_payloads&quot;, &quot;store&quot; : true, &quot;analyzer&quot; : &quot;fulltext_analyzer&quot; &#125;, &quot;fullname&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot; : &quot;fulltext_analyzer&quot; &#125; &#125; &#125; &#125;, &quot;settings&quot; : &#123; &quot;index&quot; : &#123; &quot;number_of_shards&quot; : 1, &quot;number_of_replicas&quot; : 0 &#125;, &quot;analysis&quot;: &#123; &quot;analyzer&quot;: &#123; &quot;fulltext_analyzer&quot;: &#123; &quot;type&quot;: &quot;custom&quot;, &quot;tokenizer&quot;: &quot;whitespace&quot;, &quot;filter&quot;: [ &quot;lowercase&quot;, &quot;type_as_payload&quot; ] &#125; &#125; &#125; &#125;&#125; 最下面是创建了一个分词器,然后settings里面是设置了shard的数量,上面mappings里面设置了两个field,再看text里面,设置了term_vector, fullname是没有设置的 添加两条数据进去1234567891011PUT /my_index/my_type/1&#123; &quot;fullname&quot; : &quot;Leo Li&quot;, &quot;text&quot; : &quot;hello test test test &quot;&#125;PUT /my_index/my_type/2&#123; &quot;fullname&quot; : &quot;Leo Li&quot;, &quot;text&quot; : &quot;other hello test ...&quot;&#125; 查询term vectors的数据123456789GET /my_index/my_type/1/_termvectors&#123; &quot;fields&quot;: [&quot;text&quot;], &quot;offsets&quot;: true, &quot;payloads&quot;: true, &quot;positions&quot;: true, &quot;term_statistics&quot;: true, &quot;field_statistics&quot;: true&#125; 用_termvectors查询,就是查询id是1doc中. text这个field里面的词,下面offsets,payloads,这些,都是用来控制这些数据在返回值显示不显示 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;took&quot;: 9, &quot;term_vectors&quot;: &#123; &quot;text&quot;: &#123; &quot;field_statistics&quot;: &#123; &quot;sum_doc_freq&quot;: 6, // 所有的document的trem的 doc_freq加起来 &quot;doc_count&quot;: 2, // 有多少document包含这个field &quot;sum_ttf&quot;: 8 // 所有document的trem 的ttf加起来 &#125;, &quot;terms&quot;: &#123; // 查询的这个field的值的所有term &quot;hello&quot;: &#123; // term值 &quot;doc_freq&quot;: 2, // 有多少document包含这个term &quot;ttf&quot;: 2, // 这个term在所有document中出现的频率 &quot;term_freq&quot;: 1, // 这个term在当前document中出现了几次 &quot;tokens&quot;: [ // 一个trem 可能在这个doc中出现了好几次,每个都是一个token &#123; &quot;position&quot;: 0, // 位置 &quot;start_offset&quot;: 0, // 开始下标 &quot;end_offset&quot;: 5, // 结束下标 &quot;payload&quot;: &quot;d29yZA==&quot; &#125; ] &#125;, &quot;test&quot;: &#123; &quot;doc_freq&quot;: 2, &quot;ttf&quot;: 4, &quot;term_freq&quot;: 3, &quot;tokens&quot;: [ &#123; &quot;position&quot;: 1, &quot;start_offset&quot;: 6, &quot;end_offset&quot;: 10, &quot;payload&quot;: &quot;d29yZA==&quot; &#125;, &#123; &quot;position&quot;: 2, &quot;start_offset&quot;: 11, &quot;end_offset&quot;: 15, &quot;payload&quot;: &quot;d29yZA==&quot; &#125;, &#123; &quot;position&quot;: 3, &quot;start_offset&quot;: 16, &quot;end_offset&quot;: 20, &quot;payload&quot;: &quot;d29yZA==&quot; &#125; ] &#125; &#125; &#125; &#125;&#125; query-time查询 term vector上面我们创建索引的时候是没有对fullname去设置 term vector的,所以查询fullname的term vector就是query-time生成的 语法还和之前一样12345678GET /my_index/my_type/1/_termvectors&#123; &quot;fields&quot; : [&quot;fullname&quot;], &quot;offsets&quot; : true, &quot;positions&quot; : true, &quot;term_statistics&quot; : true, &quot;field_statistics&quot; : true&#125; 返回值也是和上面是相同的, 一般来说,如果条件允许,就用query-time的term vector就可以了 手动指定doc的term vector请求:12345678910111213GET /my_index/my_type/_termvectors&#123; &quot;doc&quot; : &#123; &quot;fullname&quot; : &quot;Leo Li&quot;, &quot;text&quot; : &quot;hello test test test&quot; &#125;, &quot;fields&quot; : [&quot;text&quot;], &quot;offsets&quot; : true, &quot;payloads&quot; : true, &quot;positions&quot; : true, &quot;term_statistics&quot; : true, &quot;field_statistics&quot; : true&#125; 这里是手动指定了一个doc, 实际上不是去查这个doc,而是指定你想要去安插的词条,比如上面这个请求,是查询的text这个field, 那么就是将doc里的text进行分词,然后对每个term,都去计算它现有的所有doc中的一些统计信息 这个还是挺有用的,可以手动指定要探查的term的数据情况 手动指定分词器 来生成term vector12345678910111213141516GET /my_index/my_type/_termvectors&#123; &quot;doc&quot; : &#123; &quot;fullname&quot; : &quot;Leo Li&quot;, &quot;text&quot; : &quot;hello test test test&quot; &#125;, &quot;fields&quot; : [&quot;text&quot;], &quot;offsets&quot; : true, &quot;payloads&quot; : true, &quot;positions&quot; : true, &quot;term_statistics&quot; : true, &quot;field_statistics&quot; : true, &quot;per_field_analyzer&quot; : &#123; &quot;text&quot;: &quot;standard&quot; &#125;&#125; 前面的还是一样,就是请求最后,加了一个指定的分词器 terms filter123456789101112131415161718GET /my_index/my_type/_termvectors&#123; &quot;doc&quot; : &#123; &quot;fullname&quot; : &quot;Leo Li&quot;, &quot;text&quot; : &quot;hello test test test&quot; &#125;, &quot;fields&quot; : [&quot;text&quot;], &quot;offsets&quot; : true, &quot;payloads&quot; : true, &quot;positions&quot; : true, &quot;term_statistics&quot; : true, &quot;field_statistics&quot; : true, &quot;filter&quot; : &#123; &quot;max_num_terms&quot; : 3, &quot;min_term_freq&quot; : 1, &quot;min_doc_freq&quot; : 1 &#125;&#125; 请求后加了一个filter参数,常用的有 max_num_terms 最大的词条数目 min_term_freq 最小的词频，比如忽略那些在字段中出现次数小于一定值的词条。 max_term_freq 最大的词频 min_doc_freq 最小的文档频率，比如忽略那些在文档中出现次数小于一定的值的词条 max_doc_freq 最大的文档频率 min_word_length 忽略的词的最小长度 max_word_length 忽略的词的最大长度 就是说,根据term统计信息,过滤出想要看到term vector统计结果比如说,可以过滤掉一些出现频率过低的term multi term vector请求体中,指定index和type,id12345678910111213141516171819GET _mtermvectors&#123; &quot;docs&quot;: [ &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;term_statistics&quot;: true &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;fields&quot;: [ &quot;text&quot; ] &#125; ]&#125; 请求体中,指定type和id1234567891011121314151617GET /my_index/_mtermvectors&#123; &quot;docs&quot;: [ &#123; &quot;_type&quot;: &quot;test&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;fields&quot;: [ &quot;text&quot; ], &quot;term_statistics&quot;: true &#125;, &#123; &quot;_type&quot;: &quot;test&quot;, &quot;_id&quot;: &quot;1&quot; &#125; ]&#125; 请求体中指定id123456789101112131415GET /my_index/my_type/_mtermvectors&#123; &quot;docs&quot;: [ &#123; &quot;_id&quot;: &quot;2&quot;, &quot;fields&quot;: [ &quot;text&quot; ], &quot;term_statistics&quot;: true &#125;, &#123; &quot;_id&quot;: &quot;1&quot; &#125; ]&#125; 123456789101112131415161718192021GET /_mtermvectors&#123; &quot;docs&quot;: [ &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;doc&quot; : &#123; &quot;fullname&quot; : &quot;Leo Li&quot;, &quot;text&quot; : &quot;hello test test test&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;doc&quot; : &#123; &quot;fullname&quot; : &quot;Leo Li&quot;, &quot;text&quot; : &quot;other hello test ...&quot; &#125; &#125; ]&#125; 跟multi-type搜索模式是类似的]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-91-祖孙三层数据关系建模及搜索]]></title>
    <url>%2F2019%2F01%2F18%2FElasticsearch-91-%E7%A5%96%E5%AD%99%E4%B8%89%E5%B1%82%E6%95%B0%E6%8D%AE%E5%85%B3%E7%B3%BB%E5%BB%BA%E6%A8%A1%E5%8F%8A%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[数据建模祖孙三层的数据建模12345678910111213141516PUT /company&#123; &quot;mappings&quot;: &#123; &quot;country&quot;: &#123;&#125;, &quot;rd_center&quot;: &#123; &quot;_parent&quot;: &#123; &quot;type&quot;: &quot;country&quot; &#125; &#125;, &quot;employee&quot;: &#123; &quot;_parent&quot;: &#123; &quot;type&quot;: &quot;rd_center&quot; &#125; &#125; &#125;&#125; 看下请求,company下面有三个type,依次是country,rd_center,employee, 和之前一样,也是一层一层的指定parent就好了. 索引建好后,先在country里面添加几条数据12345POST /company/country/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;1&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;中国&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;2&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;美国&quot; &#125; 然后再添加研发中心的数据,和之前添加的方式是一样的,指定parent id就好了1234567POST /company/rd_center/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;1&quot;, &quot;parent&quot;: &quot;1&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;北京研发总部&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;2&quot;, &quot;parent&quot;: &quot;1&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;上海研发中心&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;3&quot;, &quot;parent&quot;: &quot;2&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;硅谷人工智能实验室&quot; &#125; 最后,添加员工的数据123456PUT /company/employee/1?parent=1&amp;routing=1&#123; &quot;name&quot;: &quot;张三&quot;, &quot;dob&quot;: &quot;1970-10-24&quot;, &quot;hobby&quot;: &quot;爬山&quot;&#125; 这里就和上面有些不一样了, country添加的时候,是用的自己的id去路由的,rd_center添加的时候,指定了parent id,所以他也是用的country 的id去路由的, 但是employee在添加的时候如果也是仅仅指定一个parent,那就表示用的是rd_center的id去路由,这就可能导致,这三种数据不在一个shard上. 所以孙子辈儿的doc添加的时候除了指定parent还需要加一个routing,值是爷爷辈儿的数据的id 搜索案例需求: 搜索有爬山爱好的员工所在的国家 123456789101112131415161718GET /company/country/_search&#123; &quot;query&quot;: &#123; &quot;has_child&quot;: &#123; &quot;type&quot;: &quot;rd_center&quot;, &quot;query&quot;: &#123; &quot;has_child&quot;: &#123; &quot;type&quot;: &quot;employee&quot;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;hobby&quot;: &quot;爬山&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324&#123; &quot;took&quot;: 24, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;company&quot;, &quot;_type&quot;: &quot;country&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;中国&quot; &#125; &#125; ] &#125;&#125; 需要一级一级用has_child去找]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-90-父子关系类型的数据建模与搜索聚合]]></title>
    <url>%2F2019%2F01%2F17%2FElasticsearch-90-%E7%88%B6%E5%AD%90%E5%85%B3%E7%B3%BB%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%90%9C%E7%B4%A2%E8%81%9A%E5%90%88%2F</url>
    <content type="text"><![CDATA[parent child建模之前用的nested object的建模方式,有个不好的地方,就是采取类似冗余数据的方式,将多个数据都放在了一起,维护成本比较高 parent child建模方式,采用的是类似数据库三范式的建模,多个实体都分割开来,每个实体之间都通过一些关联的方式,进行了父子关系的关联,各种数据都不需要放在一起,父doc和子doc在更新的时候,都不会影响对方 要点父子关系元数据映射,是用来保证查询时候的高性能,但是有一个限制,就是父子数据必须存在于同一个shard中 数据存在同一个shard中,而且还有映射其关联关系的元数据,那么搜索父子关系数据的时候,不用跨分片,一个分片本地自己搞定,性能自然就高 建模案例背景:以研发中心员工管理为案例,一个公司下有多个研发中心,一个研发中心下有多个员工 首先需要手动创建索引,设置mapping1234567891011PUT /company&#123; &quot;mappings&quot;: &#123; &quot;rd_center&quot;:&#123;&#125;, &quot;employee&quot;:&#123; &quot;_parent&quot;: &#123; &quot;type&quot;: &quot;rd_center&quot; &#125; &#125; &#125;&#125; 就是创建了一个company的索引,然后创建了两个type,一个是研发中心rd_center,一个是员工employee,员工里面设置了一个_parent,指向了rd_center,这样就建立了研发中心和员工的父子关系 父子关系建模的核心,就是多个type之间有父子关系的话,通过设置_parent指定父type 索引建好之后,添加几个研发中心的数据1234567POST /company/rd_center/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;1&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;北京研发总部&quot;, &quot;city&quot;: &quot;北京&quot;, &quot;country&quot;: &quot;中国&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;2&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;上海研发中心&quot;, &quot;city&quot;: &quot;上海&quot;, &quot;country&quot;: &quot;中国&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;3&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;硅谷人工智能实验室&quot;, &quot;city&quot;: &quot;硅谷&quot;, &quot;country&quot;: &quot;美国&quot; &#125; 然后,再添加子type的数据123456PUT /company/employee/1?parent=1 &#123; &quot;name&quot;: &quot;张三&quot;, &quot;birthday&quot;: &quot;1970-10-24&quot;, &quot;hobby&quot;: &quot;爬山&quot;&#125; 这里在添加的时候跟了一个parent的参数,参数值是父doc的id 传了这个值以后就不会根据id是1的这个employee doc去路由了,而是根据id是1的这个父doc的路由规则去路由 parent-child关系,就确保了说,父doc和子doc都是保存在一个shard上的.内部原理还是doc routing,employee和rd_center的数据,都会用parent id作为routing,这样就会到一个shard _bulk批量添加几条数据进去1234567POST /company/employee/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 2, &quot;parent&quot;: &quot;1&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;李四&quot;, &quot;birthday&quot;: &quot;1982-05-16&quot;, &quot;hobby&quot;: &quot;游泳&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 3, &quot;parent&quot;: &quot;2&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;王二&quot;, &quot;birthday&quot;: &quot;1979-04-01&quot;, &quot;hobby&quot;: &quot;爬山&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 4, &quot;parent&quot;: &quot;3&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;赵五&quot;, &quot;birthday&quot;: &quot;1987-05-11&quot;, &quot;hobby&quot;: &quot;骑马&quot; &#125; 搜索 聚合案例建立好父子关系的数据模型之后,就要基于这个模型进行各种搜索和聚合了 搜索一需求: 搜索1980年以后出生的员工的研发中心 123456789101112131415GET /company/rd_center/_search&#123; &quot;query&quot;: &#123; &quot;has_child&quot;: &#123; &quot;type&quot;: &quot;employee&quot;, &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;birthday&quot;: &#123; &quot;gte&quot;: &quot;1980-01-01&quot; &#125; &#125; &#125; &#125; &#125;&#125; 返回值:12345678910111213141516171819202122232425262728293031323334353637&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;company&quot;, &quot;_type&quot;: &quot;rd_center&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;北京研发总部&quot;, &quot;city&quot;: &quot;北京&quot;, &quot;country&quot;: &quot;中国&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;company&quot;, &quot;_type&quot;: &quot;rd_center&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;硅谷人工智能实验室&quot;, &quot;city&quot;: &quot;硅谷&quot;, &quot;country&quot;: &quot;美国&quot; &#125; &#125; ] &#125;&#125; 就是搜索的父级类型,然后用has_child,下面设置child的type名称,然后查询就ok了 搜索二需求: 搜索有姓名叫张三的员工的研发中心12345678910111213GET /company/rd_center/_search&#123; &quot;query&quot;: &#123; &quot;has_child&quot;: &#123; &quot;type&quot;: &quot;employee&quot;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;张三&quot; &#125; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;company&quot;, &quot;_type&quot;: &quot;rd_center&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;北京研发总部&quot;, &quot;city&quot;: &quot;北京&quot;, &quot;country&quot;: &quot;中国&quot; &#125; &#125; ] &#125;&#125; 搜索三需求: 搜索至少两个员工以上的研发中心123456789101112GET /company/rd_center/_search&#123; &quot;query&quot;: &#123; &quot;has_child&quot;: &#123; &quot;type&quot;: &quot;employee&quot;, &quot;min_children&quot;: 2, &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526&#123; &quot;took&quot;: 4, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;company&quot;, &quot;_type&quot;: &quot;rd_center&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;北京研发总部&quot;, &quot;city&quot;: &quot;北京&quot;, &quot;country&quot;: &quot;中国&quot; &#125; &#125; ] &#125;&#125; 搜索四需求: 搜索在中国的研发中心的员工12345678910111213GET /company/employee/_search&#123; &quot;query&quot;: &#123; &quot;has_parent&quot;: &#123; &quot;parent_type&quot;: &quot;rd_center&quot;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;country.keyword&quot;: &quot;中国&quot; &#125; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&#123; &quot;took&quot;: 6, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;company&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1, &quot;_routing&quot;: &quot;2&quot;, &quot;_parent&quot;: &quot;2&quot;, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;王二&quot;, &quot;birthday&quot;: &quot;1979-04-01&quot;, &quot;hobby&quot;: &quot;爬山&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;company&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_routing&quot;: &quot;1&quot;, &quot;_parent&quot;: &quot;1&quot;, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;张三&quot;, &quot;birthday&quot;: &quot;1970-10-24&quot;, &quot;hobby&quot;: &quot;爬山&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;company&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_routing&quot;: &quot;1&quot;, &quot;_parent&quot;: &quot;1&quot;, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;李四&quot;, &quot;birthday&quot;: &quot;1982-05-16&quot;, &quot;hobby&quot;: &quot;游泳&quot; &#125; &#125; ] &#125;&#125; 聚合需求: 对每个国家的员工的兴趣爱好进行统计. 先对国家划分bucket,然后再进行兴趣爱好划分bucket 12345678910111213141516171819202122232425GET /company/rd_center/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_country&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;country.keyword&quot; &#125;, &quot;aggs&quot;: &#123; &quot;group_by_child_employee&quot;: &#123; &quot;children&quot;: &#123; &quot;type&quot;: &quot;employee&quot; &#125;, &quot;aggs&quot;: &#123; &quot;group_by_hobby&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;hobby.keyword&quot; &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&#123; &quot;took&quot;: 12, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_country&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;中国&quot;, &quot;doc_count&quot;: 2, &quot;group_by_child_employee&quot;: &#123; &quot;doc_count&quot;: 3, &quot;group_by_hobby&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;爬山&quot;, &quot;doc_count&quot;: 2 &#125;, &#123; &quot;key&quot;: &quot;游泳&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125; &#125; &#125;, &#123; &quot;key&quot;: &quot;美国&quot;, &quot;doc_count&quot;: 1, &quot;group_by_child_employee&quot;: &#123; &quot;doc_count&quot;: 1, &quot;group_by_hobby&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;骑马&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125; &#125; &#125; ] &#125; &#125;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-89-针对nested object的聚合分析案例]]></title>
    <url>%2F2019%2F01%2F17%2FElasticsearch-89-%E9%92%88%E5%AF%B9nested-object%E7%9A%84%E8%81%9A%E5%90%88%E5%88%86%E6%9E%90%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[对于nested object里面的数据,如何做聚合分析呢? 以上文的数据作为背景 来看两个案例 案例一需求: 按照评论日期进行划分bucket,拿到每个月评论的stars的平均值请求123456789101112131415161718192021222324252627GET /website/blogs/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;comments_path&quot;: &#123; &quot;nested&quot;: &#123; &quot;path&quot;: &quot;comments&quot; &#125;, &quot;aggs&quot;: &#123; &quot;group_by_comments_date&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;comments.date&quot;, &quot;interval&quot;: &quot;month&quot;, &quot;format&quot;: &quot;yyyy-MM&quot; &#125;, &quot;aggs&quot;: &#123; &quot;avg_stars&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;comments.stars&quot; &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;took&quot;: 7, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;comments_path&quot;: &#123; &quot;doc_count&quot;: 2, &quot;group_by_comments_date&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key_as_string&quot;: &quot;2016-09&quot;, &quot;key&quot;: 1472688000000, &quot;doc_count&quot;: 1, &quot;avg_stars&quot;: &#123; &quot;value&quot;: 4 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-10&quot;, &quot;key&quot;: 1475280000000, &quot;doc_count&quot;: 1, &quot;avg_stars&quot;: &#123; &quot;value&quot;: 5 &#125; &#125; ] &#125; &#125; &#125;&#125; 看一下请求,第一个aggs,设置nested的path,然后第二个aggs是根据日期按月划分bucket的,第三个aggs是用来计算平均数的 案例二需求: 根据评论的年龄来划分bucket,然后按博客的tags进行分组 这里,评论的年龄是comments里面的数据,但是tags是外面的数据,那么如何用nested object外面的数据来进行聚合呢,先看一下请求 12345678910111213141516171819202122232425262728293031GET /website/blogs/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;comments_path&quot;: &#123; &quot;nested&quot;: &#123; &quot;path&quot;: &quot;comments&quot; &#125;, &quot;aggs&quot;: &#123; &quot;group_by_age&quot;: &#123; &quot;histogram&quot;: &#123; &quot;field&quot;: &quot;comments.age&quot;, &quot;interval&quot;: 10 &#125;, &quot;aggs&quot;: &#123; &quot;reverse_path&quot;:&#123; &quot;reverse_nested&quot;: &#123;&#125;, &quot;aggs&quot;: &#123; &quot;group_by_tags&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;tags.keyword&quot; &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;comments_path&quot;: &#123; &quot;doc_count&quot;: 2, &quot;group_by_comments_age&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: 20, &quot;doc_count&quot;: 1, &quot;reverse_path&quot;: &#123; &quot;doc_count&quot;: 1, &quot;group_by_tags&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;投资&quot;, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: &quot;理财&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125; &#125; &#125;, &#123; &quot;key&quot;: 30, &quot;doc_count&quot;: 1, &quot;reverse_path&quot;: &#123; &quot;doc_count&quot;: 1, &quot;group_by_tags&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;投资&quot;, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: &quot;理财&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125; &#125; &#125; ] &#125; &#125; &#125;&#125; 首先,第一个aggs还是先设置path,然后下钻第二个aggs根据年龄去分段划分bucket,继续下钻,第三个aggs的时候用了一个reverse_nested:{},加上这个之后,就可以使用nested object外面的数据了,最后再一次下钻,按照tags分组,因为tags是分词的,所以用了tags.keyword]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-88-nested object数据结构]]></title>
    <url>%2F2019%2F01%2F17%2FElasticsearch-88-nested-object%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[普通object类型用冗余数据的方式来建模,其实用的就是object类型,来看一个数据12345678910111213141516171819202122PUT /website/blogs/6&#123; &quot;title&quot;: &quot;花无缺发表的一篇帖子&quot;, &quot;content&quot;: &quot;我是花无缺，大家要不要考虑一下投资房产和买股票的事情啊。。。&quot;, &quot;tags&quot;: [ &quot;投资&quot;, &quot;理财&quot; ], &quot;comments&quot;: [ &#123; &quot;name&quot;: &quot;小鱼儿&quot;, &quot;comment&quot;: &quot;什么股票啊？推荐一下呗&quot;, &quot;age&quot;: 28, &quot;stars&quot;: 4, &quot;date&quot;: &quot;2016-09-01&quot; &#125;, &#123; &quot;name&quot;: &quot;黄药师&quot;, &quot;comment&quot;: &quot;我喜欢投资房产，风，险大收益也大&quot;, &quot;age&quot;: 31, &quot;stars&quot;: 5, &quot;date&quot;: &quot;2016-10-22&quot; &#125; ]&#125; 就是一个帖子下的所有评论,这种类型的数据,就是object类型的,然后来做一个搜索 搜索案例需求: 搜索被年龄是28岁的黄药师评论过的博客 12345678910111213141516171819GET /website/blogs/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;comments.name&quot;: &quot;黄药师&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;comments.age&quot;: &quot;28&quot; &#125; &#125; ] &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445&#123; &quot;took&quot;: 21, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1.8022683, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blogs&quot;, &quot;_id&quot;: &quot;6&quot;, &quot;_score&quot;: 1.8022683, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;花无缺发表的一篇帖子&quot;, &quot;content&quot;: &quot;我是花无缺，大家要不要考虑一下投资房产和买股票的事情啊。。。&quot;, &quot;tags&quot;: [ &quot;投资&quot;, &quot;理财&quot; ], &quot;comments&quot;: [ &#123; &quot;name&quot;: &quot;小鱼儿&quot;, &quot;comment&quot;: &quot;什么股票啊？推荐一下呗&quot;, &quot;age&quot;: 28, &quot;stars&quot;: 4, &quot;date&quot;: &quot;2016-09-01&quot; &#125;, &#123; &quot;name&quot;: &quot;黄药师&quot;, &quot;comment&quot;: &quot;我喜欢投资房产，风，险大收益也大&quot;, &quot;age&quot;: 31, &quot;stars&quot;: 5, &quot;date&quot;: &quot;2016-10-22&quot; &#125; ] &#125; &#125; ] &#125;&#125; 注意,我们在添加数据的时候,黄药师的年龄是31的,但是搜索的时候还是搜索出来了,这个结果就不准确了,在之前有写过object类型的底层存储是怎样的结构, 会将一个数组中的数据,进行扁平化, 我们上面这条数据在底层的存储可能就是这样的 12345678910&#123; &quot;title&quot;: [ &quot;花无缺&quot;, &quot;发表&quot;, &quot;一篇&quot;, &quot;帖子&quot; ], &quot;content&quot;: [ &quot;我&quot;, &quot;是&quot;, &quot;花无缺&quot;, &quot;大家&quot;, &quot;要不要&quot;, &quot;考虑&quot;, &quot;一下&quot;, &quot;投资&quot;, &quot;房产&quot;, &quot;买&quot;, &quot;股票&quot;, &quot;事情&quot; ], &quot;tags&quot;: [ &quot;投资&quot;, &quot;理财&quot; ], &quot;comments.name&quot;: [ &quot;小鱼儿&quot;, &quot;黄药师&quot; ], &quot;comments.comment&quot;: [ &quot;什么&quot;, &quot;股票&quot;, &quot;推荐&quot;, &quot;我&quot;, &quot;喜欢&quot;, &quot;投资&quot;, &quot;房产&quot;, &quot;风险&quot;, &quot;收益&quot;, &quot;大&quot; ], &quot;comments.age&quot;: [ 28, 31 ], &quot;comments.stars&quot;: [ 4, 5 ], &quot;comments.date&quot;: [ 2016-09-01, 2016-10-22 ]&#125; 所以,我们的查询就会查询到这个document,name=黄药师,age=28,正好符合 那么怎么来解决这种问题呢? nested object类型通过使用nested object就可以解决上面出现的问题 首先,需要修改mapping,把之前的index删掉,重新建1DELETE /website 然后创建索引,手动设置mapping12345678910111213141516171819PUT /website&#123; &quot;mappings&quot;: &#123; &quot;blogs&quot;: &#123; &quot;properties&quot;: &#123; &quot;comments&quot;:&#123; &quot;type&quot;: &quot;nested&quot;, &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;comment&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;age&quot;: &#123; &quot;type&quot;: &quot;short&quot; &#125;, &quot;stars&quot;: &#123; &quot;type&quot;: &quot;short&quot; &#125;, &quot;date&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 就是把comments的类型设置为nested,然后里面再设置各个属性,创建完成之后把之前那个数据再加进去. 搜索案例再次按之前的需求搜索,看一下请求1234567891011121314151617181920212223242526272829303132333435GET website/blogs/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;花无缺&quot; &#125; &#125;, &#123; &quot;nested&quot;: &#123; &quot;path&quot;: &quot;comments&quot;, &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;comments.name&quot;: &quot;黄药师&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;comments.age&quot;: &quot;28&quot; &#125; &#125; ] &#125; &#125; &#125; &#125; ] &#125; &#125;&#125; 首先,就是一个bool组合查询,查的是外层的数据,然后里面套了一个nested, 然后里面又是一个bool组合查询 返回值:1234567891011121314&#123; &quot;took&quot;: 4, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 0, &quot;max_score&quot;: null, &quot;hits&quot;: [] &#125;&#125; 这样的话就是查询不到的了. nested object底层数据结构12345678910111213141516171819&#123; &quot;comments.name&quot;: [ &quot;小鱼儿&quot; ], &quot;comments.comment&quot;: [ &quot;什么&quot;, &quot;股票&quot;, &quot;推荐&quot; ], &quot;comments.age&quot;: [ 28 ], &quot;comments.stars&quot;: [ 4 ], &quot;comments.date&quot;: [ 2014-09-01 ]&#125;&#123; &quot;comments.name&quot;: [ &quot;黄药师&quot; ], &quot;comments.comment&quot;: [ &quot;我&quot;, &quot;喜欢&quot;, &quot;投资&quot;, &quot;房产&quot;, &quot;风险&quot;, &quot;收益&quot;, &quot;大&quot; ], &quot;comments.age&quot;: [ 31 ], &quot;comments.stars&quot;: [ 5 ], &quot;comments.date&quot;: [ 2014-10-22 ]&#125;&#123; &quot;title&quot;: [ &quot;花无缺&quot;, &quot;发表&quot;, &quot;一篇&quot;, &quot;帖子&quot; ], &quot;body&quot;: [ &quot;我&quot;, &quot;是&quot;, &quot;花无缺&quot;, &quot;大家&quot;, &quot;要不要&quot;, &quot;考虑&quot;, &quot;一下&quot;, &quot;投资&quot;, &quot;房产&quot;, &quot;买&quot;, &quot;股票&quot;, &quot;事情&quot; ], &quot;tags&quot;: [ &quot;投资&quot;, &quot;理财&quot; ]&#125; nested object的底层数据结构是这样的,所以搜 name=黄药师而且age=28 的就搜索不到了 补充查询请求的时候,nested 里面可以加一个参数,是score_mode,作用是就是,如果搜索命中了多个nested document,如何将多个nested document的分数合并为一个分数.默认是avg,也可以设置max min none]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-87-基于共享锁和排它锁实现悲观锁并发控制]]></title>
    <url>%2F2019%2F01%2F16%2FElasticsearch-87-%E5%9F%BA%E4%BA%8E%E5%85%B1%E4%BA%AB%E9%94%81%E5%92%8C%E6%8E%92%E5%AE%83%E9%94%81%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[共享锁和排它锁共享锁: 这份数据是共享的,然后多个线程过来,都可以获取同一个数据的共享锁,然后对这个数据执行读操作.排它锁: 是排他的操作,只能一个线程获取排他锁,然后执行增删改操作 读写锁分离 如果是要读取数据的话,那么任意多少个线程都可以进来然后读取数据,每个线程都可以上一个共享锁,但是这个时候如果有线程过来修改数据,会尝试上排他锁排它锁会跟共享锁互斥,也就是说,如果有人已经上了共享锁了,那么排它锁就不能上,就得等. 就是说如果有人在读数据,就不允许别人来修改数据,反之也是一样的. 如果有人在修改数据,就是加了排他锁,这时候其他线程过来也要修改数据,也会尝试加排它锁,此时就会失败,锁冲突,就必须等待重试,同时只能有一个线程修改数据,如果有线程过来读取数据,就是尝试加共享锁,此时也会失败,因为共享锁和排它锁是互斥的 如果有线程再修改数据,就不许别的线程再来修改,也不许别的线程读取数据 案例多个线程上共享锁先试一下共享锁,多个线程同时读取数据. 上共享锁也需要一个groovy脚本,和之前一样,脚本内容是1if (ctx._source.lock_type == &apos;exclusive&apos;) &#123; assert false &#125;; ctx._source.lock_count++ 就是如果锁已经存在了,判断这个锁的类型是排它锁的话,直接报错,是共享锁的话,就把锁的数量加1 假设,先有一个线程过来上了一个共享锁1234567891011POST /fs/lock/1/_update&#123; &quot;upsert&quot;: &#123; &quot;lock_type&quot;:&quot;shared&quot;, &quot;lock_count&quot;: 1 &#125;, &quot;script&quot;: &#123; &quot;lang&quot;: &quot;groovy&quot;, &quot;file&quot;: &quot;judge-lock-2&quot; &#125;&#125; 返回值:1234567891011&#123; &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;lock&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 6, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;lock_type&quot;: &quot;shared&quot;, &quot;lock_count&quot;: 1 &#125;&#125; 添加的数据,一个是锁的类型,一个是锁的数量,已经成功上了一个共享锁了 这时候又有一个线程过来上共享锁1234567891011POST /fs/lock/1/_update&#123; &quot;upsert&quot;: &#123; &quot;lock_type&quot;:&quot;shared&quot;, &quot;lock_count&quot;: 1 &#125;, &quot;script&quot;: &#123; &quot;lang&quot;: &quot;groovy&quot;, &quot;file&quot;: &quot;judge-lock-2&quot; &#125;&#125; 返回值:123456789101112&#123; &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;lock&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 7, &quot;result&quot;: &quot;updated&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;&#125; 执行还是成功的,然后查询一下这个锁1GET /fs/lock/1 返回值:1234567891011&#123; &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;lock&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 7, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;lock_type&quot;: &quot;shared&quot;, &quot;lock_count&quot;: 2 &#125;&#125; lock_count是2,就是加了两个共享锁. 就是有线程上了共享锁以后,其他线程还要再上,直接上就可以了,只是lock_count + 1 共享锁没有释放的情况下 上排他锁上排他锁的请求1234PUT /fs/lock/1/_create&#123; &quot;lock_type&quot;:&quot;exclusive&quot;&#125; 这里用的是_create请求,就是强制创建,要求lock必须不能存在,但是之前已经有了共享锁了,/fs/lock/1是存在的 所以这里会创建失败,报错12345678910111213141516171819&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;version_conflict_engine_exception&quot;, &quot;reason&quot;: &quot;[lock][1]: version conflict, document already exists (current version [7])&quot;, &quot;index_uuid&quot;: &quot;RWijfql1Qlqa_OPoor0Ubw&quot;, &quot;shard&quot;: &quot;3&quot;, &quot;index&quot;: &quot;fs&quot; &#125; ], &quot;type&quot;: &quot;version_conflict_engine_exception&quot;, &quot;reason&quot;: &quot;[lock][1]: version conflict, document already exists (current version [7])&quot;, &quot;index_uuid&quot;: &quot;RWijfql1Qlqa_OPoor0Ubw&quot;, &quot;shard&quot;: &quot;3&quot;, &quot;index&quot;: &quot;fs&quot; &#125;, &quot;status&quot;: 409&#125; 释放共享锁这里也是需要一个释放共享锁的groovy脚本1if(--ctx._source.lock_count == 0)&#123;ctx.op = &apos;delete&apos;&#125; 就是执行脚本的时候,先把lock_count-1 然后判断是不是0,是的话直接删除掉.1234567POST /fs/lock/1/_update&#123; &quot;script&quot;: &#123; &quot;lang&quot;: &quot;groovy&quot;, &quot;file&quot;: &quot;unlock-shared&quot; &#125;&#125; 之前上了两个锁,执行两次,lock_count就是0了,然后就被删除了,所有的共享锁就都被释放了 再上排它锁1234PUT /fs/lock/1/_create&#123; &quot;lock_type&quot;:&quot;exclusive&quot;&#125; 返回值:12345678910111213&#123; &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;lock&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 10, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true&#125; 创建成功了,这时候如果再有其他线程过来申请排它锁1234PUT /fs/lock/1/_create&#123; &quot;lock_type&quot;:&quot;exclusive&quot;&#125; 返回值:12345678910111213141516171819&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;version_conflict_engine_exception&quot;, &quot;reason&quot;: &quot;[lock][1]: version conflict, document already exists (current version [10])&quot;, &quot;index_uuid&quot;: &quot;RWijfql1Qlqa_OPoor0Ubw&quot;, &quot;shard&quot;: &quot;3&quot;, &quot;index&quot;: &quot;fs&quot; &#125; ], &quot;type&quot;: &quot;version_conflict_engine_exception&quot;, &quot;reason&quot;: &quot;[lock][1]: version conflict, document already exists (current version [10])&quot;, &quot;index_uuid&quot;: &quot;RWijfql1Qlqa_OPoor0Ubw&quot;, &quot;shard&quot;: &quot;3&quot;, &quot;index&quot;: &quot;fs&quot; &#125;, &quot;status&quot;: 409&#125; 还是报错的,上锁失败,如果再上一个共享锁呢1234567891011POST /fs/lock/1/_update&#123; &quot;upsert&quot;: &#123; &quot;lock_type&quot;:&quot;shared&quot;, &quot;lock_count&quot;: 1 &#125;, &quot;script&quot;: &#123; &quot;lang&quot;: &quot;groovy&quot;, &quot;file&quot;: &quot;judge-lock-2&quot; &#125;&#125; 返回值:123456789101112131415161718192021222324&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;remote_transport_exception&quot;, &quot;reason&quot;: &quot;[f57uV91][127.0.0.1:9300][indices:data/write/update[s]]&quot; &#125; ], &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;failed to execute script&quot;, &quot;caused_by&quot;: &#123; &quot;type&quot;: &quot;script_exception&quot;, &quot;reason&quot;: &quot;error evaluating judge-lock-2&quot;, &quot;caused_by&quot;: &#123; &quot;type&quot;: &quot;power_assertion_error&quot;, &quot;reason&quot;: &quot;assert false\n&quot; &#125;, &quot;script_stack&quot;: [], &quot;script&quot;: &quot;&quot;, &quot;lang&quot;: &quot;groovy&quot; &#125; &#125;, &quot;status&quot;: 400&#125; 也是报错,加不了的 释放排他锁直接把这个删除掉就好了1DELETE /fs/lock/1]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-86-基于document锁实现悲观锁并发控制]]></title>
    <url>%2F2019%2F01%2F16%2FElasticsearch-86-%E5%9F%BA%E4%BA%8Edocument%E9%94%81%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[document锁上文中的全局锁,一次性就锁上了整个index,对这个index的所有增删改操作都会被block住,如果上锁不频繁,还可以,如果上锁释放锁的操作很频繁,显然不适用 document锁,顾名思义,每次就锁需要操作的几个doc,就可以了,被锁的doc,其他线程就不能对这些doc执行增删改操作了,但是只是对一部分doc上了锁,其他线程对于其他的doc还是可以操作的 具体实现.document是用脚本进行上锁的 es的config/script目录下,写一个名为judge-lock的groovy脚本,内容是:1if ( ctx._source.process_id != process_id ) &#123; assert false &#125;; ctx.op = &apos;noop&apos;; 加锁:1234567891011POST /fs/lock/1/_update&#123; &quot;upsert&quot;: &#123; &quot;process_id&quot;: 123 &#125;, &quot;script&quot;: &#123; &quot;lang&quot;: &quot;groovy&quot;, &quot;file&quot;: &quot;judge-lock&quot;, &quot;params&quot;: &#123; &quot;process_id&quot;: 123 &#125; &#125;&#125; fs/lock: 就是说fs下的lock type,专门用于进行上锁 fs/lock/id: 比如上面的1,id就是要上锁的那个doc的id,代表了某个doc对应的lock, 其实也是一个doc _update: 请求里面还有个upsert,执行的是upsert操作 script: 要执行的脚本 script.param: 传入的参数 script.param.process_id: 是要执行增删改操作的进程的唯一id,比如说在java系统中,启动的时候,可以给每个线程都用uuid生成一个唯一id,进程也可以分配一个uuid,process_id+thread_id就代表了某个进程下的某个线程 process_id很重要,会在lock中设置对对应的doc加锁的进程的id,这样其他进程过来的时候,才知道这条数据已经被别人锁了 请求进来的话,走groovy脚本,参数就是process_id,看一下这个脚本,意思就是process_id和当前数据中的process_id不相同的话就assert false,抛出异常,如果是相同的,就ctx.op = ‘noop’,不做任何修改 如果一个document之前没有被锁,拿上面的请求举例,比如说/fs/lock/1之前不存在,也就是id是1的doc没有被别人上锁, 因为用的是upsert,那么执行index操作,创建一个/fs/lock/1,用params中的数据作为这个lock的数据,process_id被设置为123,脚本不执行,完成之后,就标识id是1的这个doc被process_id=123的进程锁上了 然后现在又有一个process_id是234的进程过来尝试加锁,请求如下1234567891011POST /fs/lock/1/_update&#123; &quot;upsert&quot;: &#123; &quot;process_id&quot;: 234 &#125;, &quot;script&quot;: &#123; &quot;lang&quot;: &quot;groovy&quot;, &quot;file&quot;: &quot;judge-lock&quot;, &quot;params&quot;: &#123; &quot;process_id&quot;: 234 &#125; &#125;&#125; 这时候,这个doc已经被上一个进程锁上了,/fs/lock/1 这条数据也已经存在了, 那么这个upsert就是执行script脚本了,比对process_id,发现两个id并不相同,也就是说这个doc已经被别的进程锁上了,就直接报错了,需要一直重试,直到上锁成功 那如果说,process_id=123的这个线程,又要做一些其他的操作, 也还是过来先上锁,在执行脚本的时候,发现两个process_id是相同的,此时就会返回ctx.op= ‘noop’,什么都不做,所以这个请求是不会被 block的 最后做完全部的操作后,就要释放这个进程上的所有锁 案例还是之前的文件系统,两个线程要同时去修改id是1的doc的文件名 第一个线程过来,尝试上锁..1234567891011POST /fs/lock/1/_update&#123; &quot;upsert&quot;: &#123;&quot;process_id&quot;: 123&#125;, &quot;script&quot;: &#123; &quot;lang&quot;: &quot;groovy&quot;, &quot;file&quot;: &quot;judge-lock&quot;, &quot;params&quot;: &#123; &quot;process_id&quot;: 123 &#125; &#125;&#125; 返回值:123456789101112&#123; &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;lock&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;&#125; process_id是123的进程上锁成功,看一下1GET /fs/lock/1 返回值:12345678910&#123; &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;lock&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;process_id&quot;: 123 &#125;&#125; 这个时候第二个线程过来尝试加锁1234567891011POST /fs/lock/1/_update&#123; &quot;upsert&quot;: &#123;&quot;process_id&quot;: 234&#125;, &quot;script&quot;: &#123; &quot;lang&quot;: &quot;groovy&quot;, &quot;file&quot;: &quot;judge-lock&quot;, &quot;params&quot;: &#123; &quot;process_id&quot;: 234 &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;remote_transport_exception&quot;, &quot;reason&quot;: &quot;[f57uV91][127.0.0.1:9300][indices:data/write/update[s]]&quot; &#125; ], &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;failed to execute script&quot;, &quot;caused_by&quot;: &#123; &quot;type&quot;: &quot;script_exception&quot;, &quot;reason&quot;: &quot;error evaluating judge-lock&quot;, &quot;caused_by&quot;: &#123; &quot;type&quot;: &quot;power_assertion_error&quot;, &quot;reason&quot;: &quot;assert false\n&quot; &#125;, &quot;script_stack&quot;: [], &quot;script&quot;: &quot;&quot;, &quot;lang&quot;: &quot;groovy&quot; &#125; &#125;, &quot;status&quot;: 400&#125; 直接就报错了,是加不了锁的 如果说还是第一个线程来获取锁的话会怎样呢1234567891011POST /fs/lock/1/_update&#123; &quot;upsert&quot;: &#123;&quot;process_id&quot;: 123&#125;, &quot;script&quot;: &#123; &quot;lang&quot;: &quot;groovy&quot;, &quot;file&quot;: &quot;judge-lock&quot;, &quot;params&quot;: &#123; &quot;process_id&quot;: 123 &#125; &#125;&#125; 返回值:123456789101112&#123; &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;lock&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;noop&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 0, &quot;successful&quot;: 0, &quot;failed&quot;: 0 &#125;&#125; 返回值就是noop,什么都不做,这时候,这个进程就可以对id是1的这个doc进行各种操作,比如修改文件名123456POST /fs/file/1/_update&#123; &quot;doc&quot;: &#123; &quot;name&quot;: &quot;README1.txt&quot; &#125;&#125; 返回值:123456789101112&#123; &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;file&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 6, &quot;result&quot;: &quot;updated&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;&#125; 更新成功了,最后一步释放掉锁1POST /fs/_refresh 手动refresh一下,数据写入os cache并被打开供搜索的过程,叫做refresh 查询这个线程id,持有的所有锁,用scroll查询,或者普通的搜索都可以12345678910GET /fs/lock/_search?scroll=1m&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;process_id&quot;: &#123; &quot;value&quot;: &quot;123&quot; &#125; &#125; &#125;&#125; 返回值:12345678910111213141516171819202122232425&#123; &quot;_scroll_id&quot;: &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAAyjFmY1N3VWOTF4U19HUlRRUzJIbzgxcmcAAAAAAAAMpBZmNTd1VjkxeFNfR1JUUVMySG84MXJnAAAAAAAADKUWZjU3dVY5MXhTX0dSVFFTMkhvODFyZwAAAAAAAAymFmY1N3VWOTF4U19HUlRRUzJIbzgxcmcAAAAAAAAMohZmNTd1VjkxeFNfR1JUUVMySG84MXJn&quot;, &quot;took&quot;: 14, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;lock&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;process_id&quot;: 123 &#125; &#125; ] &#125;&#125; 搜索到所有的要释放的锁之后, 删除掉,这里用一个_bulk批量操作12PUT /fs/lock/_bulk&#123;&quot;delete&quot;:&#123;&quot;_id&quot; : 1 &#125;&#125; 返回值:12345678910111213141516171819202122&#123; &quot;took&quot;: 26, &quot;errors&quot;: false, &quot;items&quot;: [ &#123; &quot;delete&quot;: &#123; &quot;found&quot;: true, &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;lock&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 2, &quot;result&quot;: &quot;deleted&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;status&quot;: 200 &#125; &#125; ]&#125; 删除成功,锁释放掉了,这时候,process_id=234的线程尝试上锁1234567891011POST /fs/lock/1/_update&#123; &quot;upsert&quot;: &#123;&quot;process_id&quot;: 234&#125;, &quot;script&quot;: &#123; &quot;lang&quot;: &quot;groovy&quot;, &quot;file&quot;: &quot;judge-lock&quot;, &quot;params&quot;: &#123; &quot;process_id&quot;: 234 &#125; &#125;&#125; 返回值:123456789101112&#123; &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;lock&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;&#125; 上锁成功,和前面的一样,做完各种操作释放掉就ok了]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-85-基于全局锁实现悲观锁并发控制]]></title>
    <url>%2F2019%2F01%2F15%2FElasticsearch-85-%E5%9F%BA%E4%BA%8E%E5%85%A8%E5%B1%80%E9%94%81%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[并发控制的几种方案结合之前的文件系统的案例, 假设有多个线程过来,要并发的给/workspace/projects/helloworld下的README.txt修改文件名,这个时候就要进行并发的控制,避免多线程的并发安全问题. 基于version的乐观锁,先获取要修改的信息的版本号,然后和自己的版本号对比,如果版本号不一样了,就意味着有别的线程已经修改过这个数据了,必须重新获取新的版本号再次尝试修改 然后是悲观锁,就是上来就先尝试给这个数据加锁,如果要是加锁成功了,这个时候这个数据只有当前这个线程可以操作,其他线程过来也尝试加锁,发现已经被上一个线程加锁了,就一直重新上锁,直到获得锁以后才可以对数据进行操作 全局锁是粒度最粗的 全局锁的实现就是直接锁掉整个要操作的index 首先,第一步,上锁12PUT /fs/lock/global/_create&#123;&#125; fs: 要上锁的index lock:就是指定一个对这个index上全局锁的一个type global: 就是这个全局锁对应的这个doc的id _create: 强制必须是创建,如果这个doc已经存在,就创建失败,报错 其实就是添加了一个空的document,语法是和添加document一样的 /index/type/id 创建完成后,如果另一个线程同时尝试上锁,也就是再执行一次这个请求12PUT /fs/lock/global/_create&#123;&#125; 返回值:12345678910111213141516171819&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;version_conflict_engine_exception&quot;, &quot;reason&quot;: &quot;[lock][global]: version conflict, document already exists (current version [1])&quot;, &quot;index_uuid&quot;: &quot;RWijfql1Qlqa_OPoor0Ubw&quot;, &quot;shard&quot;: &quot;2&quot;, &quot;index&quot;: &quot;fs&quot; &#125; ], &quot;type&quot;: &quot;version_conflict_engine_exception&quot;, &quot;reason&quot;: &quot;[lock][global]: version conflict, document already exists (current version [1])&quot;, &quot;index_uuid&quot;: &quot;RWijfql1Qlqa_OPoor0Ubw&quot;, &quot;shard&quot;: &quot;2&quot;, &quot;index&quot;: &quot;fs&quot; &#125;, &quot;status&quot;: 409&#125; 可以看到,锁已经存在的情况下,其他线程再尝试加锁就会报错,没获得锁的线程就一直重复尝试上锁,直到成功 此时,第一个成功上锁的线程就可以对document进行各种操作,比如更新文件名123456POST /fs/file/1/_update&#123; &quot;doc&quot;: &#123; &quot;name&quot;: &quot;README1.txt&quot; &#125;&#125; 返回值:123456789101112&#123; &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;file&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 2, &quot;result&quot;: &quot;updated&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;&#125; 修改完成,最后一步,释放锁,也就是删除/fs/lock/global这个document 1DELETE /fs/lock/global 第二个线程之前上锁失败,但是这时候第一个线程已经释放了锁,所以第二个线程上锁成功,然后他也可以对这个index里面的数据做各种操作,最后一步delete掉,释放锁就好了 全局锁的优缺点优点: 操作简单,使用容易,成本低缺点: 直接把整个index锁上了,这时候别的线程对index中所有的doc的操作都会被block住,导致整个系统的并发能力很低 适用场景上锁释放锁的操作不是很频繁,每次上锁之后,执行的操作耗时不会太长,可以使用这种方式, 方便]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-84-文件系统数据建模及搜索案例]]></title>
    <url>%2F2019%2F01%2F15%2FElasticsearch-84-%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E5%8F%8A%E6%90%9C%E7%B4%A2%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[数据建模对类似文件系统(filesystem)这种的 有多层级关系的数据进行建模,比如说路径这种多层级的数据 数据添加首先先自定义一个分词器.123456789101112PUT /fs&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;analyzer&quot;: &#123; &quot;paths&quot;: &#123; &quot;tokenizer&quot;: &quot;path_hierarchy&quot; &#125; &#125; &#125; &#125;&#125; ath_hierarchy tokenizer:举个例子,比如现在有一个路劲是a/b/c/d,经过path_hierarchy这种分词,会被拆分为a/b/c/d/a/b/c/a/b/a 分词器创建好之后,手动创建mapping 1234567891011121314151617PUT /fs/_mapping/file&#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;path&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;fields&quot;: &#123; &quot;tree&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;paths&quot; &#125; &#125; &#125; &#125;&#125; path这个field里面又创建了一个内置的filed,path.tree 索引创建完成,添加数据123456PUT /fs/file/1&#123; &quot;name&quot;: &quot;README.txt&quot;, &quot;path&quot;: &quot;/workspace/projects/helloworld&quot;, &quot;contents&quot;: &quot;这是我的第一个elasticsearch程序&quot;&#125; 需求一查找一份内容包括elasticsearch,在/workspace/projects/helloworld这个目录下的文件 搜索请求:1234567891011121314151617181920212223GET /fs/file/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;contents&quot;: &quot;elasticsearch&quot; &#125; &#125;, &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;path&quot;: &quot;/workspace/projects/helloworld&quot; &#125; &#125; &#125; &#125; ] &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526&#123; &quot;took&quot;: 1, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1.284885, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;file&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1.284885, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;README.txt&quot;, &quot;path&quot;: &quot;/workspace/projects/helloworld&quot;, &quot;contents&quot;: &quot;这是我的第一个elasticsearch程序&quot; &#125; &#125; ] &#125;&#125; 需求二搜索/workspace目录下,内容包含elasticsearch的所有的文件1234567891011121314151617181920212223GET /fs/file/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;contents&quot;: &quot;elasticsearch&quot; &#125; &#125;, &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;path.tree&quot;: &quot;/workspace&quot; &#125; &#125; &#125; &#125; ] &#125; &#125;&#125; 请求和上面基本是一样的,只是filter里面用之前创建的path.tree,因为path是不分词的]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-83-聚合分析案例]]></title>
    <url>%2F2019%2F01%2F15%2FElasticsearch-83-%E8%81%9A%E5%90%88%E5%88%86%E6%9E%90%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[需求对每个用户发表的博客进行分组 添加测试数据123456789101112131415161718192021222324252627282930313233PUT /website/users/3&#123; &quot;name&quot;: &quot;黄药师&quot;, &quot;email&quot;: &quot;huangyaoshi@sina.com&quot;, &quot;birthday&quot;: &quot;1970-10-24&quot;&#125;PUT /website/blogs/3&#123; &quot;title&quot;: &quot;我是黄药师&quot;, &quot;content&quot;: &quot;我是黄药师啊，各位同学们！！！&quot;, &quot;userInfo&quot;: &#123; &quot;userId&quot;: 1, &quot;username&quot;: &quot;黄药师&quot; &#125;&#125;PUT /website/users/2&#123; &quot;name&quot;: &quot;花无缺&quot;, &quot;email&quot;: &quot;huawuque@sina.com&quot;, &quot;birthday&quot;: &quot;1980-02-02&quot;&#125;PUT /website/blogs/4&#123; &quot;title&quot;: &quot;花无缺的身世揭秘&quot;, &quot;content&quot;: &quot;大家好，我是花无缺，所以我的身世是。。。&quot;, &quot;userInfo&quot;: &#123; &quot;userId&quot;: 2, &quot;username&quot;: &quot;花无缺&quot; &#125;&#125; 实现请求:123456789101112131415161718192021GET /website/blogs/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_username&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;userInfo.username.keyword&quot; &#125;, &quot;aggs&quot;: &#123; &quot;top_blogs&quot;: &#123; &quot;top_hits&quot;: &#123; &quot;_source&quot;: &#123; &quot;include&quot;: &quot;title&quot; &#125;, &quot;size&quot;: 5 &#125; &#125; &#125; &#125; &#125;&#125; 返回值:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485&#123; &quot;took&quot;: 32, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_username&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;小鱼儿&quot;, &quot;doc_count&quot;: 1, &quot;top_blogs&quot;: &#123; &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blogs&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;小鱼儿的第一篇博客&quot; &#125; &#125; ] &#125; &#125; &#125;, &#123; &quot;key&quot;: &quot;花无缺&quot;, &quot;doc_count&quot;: 1, &quot;top_blogs&quot;: &#123; &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blogs&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;花无缺的身世揭秘&quot; &#125; &#125; ] &#125; &#125; &#125;, &#123; &quot;key&quot;: &quot;黄药师&quot;, &quot;doc_count&quot;: 1, &quot;top_blogs&quot;: &#123; &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blogs&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;我是黄药师&quot; &#125; &#125; ] &#125; &#125; &#125; ] &#125; &#125;&#125; 根据用户的username进行分组,然后拿前五条数据,top_hits._source 是设置要显示的field.]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-82-应用层关联查询与通过冗余数据关联对比]]></title>
    <url>%2F2019%2F01%2F15%2FElasticsearch-82-%E5%BA%94%E7%94%A8%E5%B1%82%E5%85%B3%E8%81%94%E6%9F%A5%E8%AF%A2%E4%B8%8E%E9%80%9A%E8%BF%87%E5%86%97%E4%BD%99%E6%95%B0%E6%8D%AE%E5%85%B3%E8%81%94%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[关系型数据建模数据建模以一个博客网站作为案例背景,模拟用户发表各种博客,然后针对用户和博客之间的关系进行数据建模,同时针对建模好的数据执行各种搜索 + 聚合的操作 先以类似关系型数据库中的模型来构建数据,还是将有关联的数据分割为不同的实体 添加数据12345678910111213PUT /website/users/1 &#123; &quot;name&quot;: &quot;小鱼儿&quot;, &quot;email&quot;: &quot;xiaoyuer@sina.com&quot;, &quot;birthday&quot;: &quot;1980-01-01&quot;&#125;PUT /website/blogs/1&#123; &quot;title&quot;: &quot;我的第一篇博客&quot;, &quot;content&quot;: &quot;这是我的第一篇博客，开通啦！！！&quot;, &quot;userId&quot;: 1 &#125; 一个用户对应一个博客,一对多的关系 查询假设我们现在要查询小鱼儿发表的所有博客, 就应该先拿到小鱼儿的用户id,然后去博客表中去根据user_id去搜索,请求如下 先去搜索用户12345678910GET /website/users/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;name.keyword&quot;: &#123; &quot;value&quot;: &quot;小鱼儿&quot; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526&#123; &quot;took&quot;: 35, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.2876821, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;users&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.2876821, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;小鱼儿&quot;, &quot;email&quot;: &quot;xiaoyuer@sina.com&quot;, &quot;birthday&quot;: &quot;1980-01-01&quot; &#125; &#125; ] &#125;&#125; 拿到用户id之后,去博客表根据userID去搜索1234567891011121314GET /website/blogs/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;terms&quot;: &#123; &quot;userId&quot;: [ 1 ] &#125; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blogs&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;我的第一篇博客&quot;, &quot;content&quot;: &quot;这是我的第一篇博客，开通啦！！！&quot;, &quot;userId&quot;: 1 &#125; &#125; ] &#125;&#125; 这样就把小鱼儿所有的博客都搜出来了,这种搜索就属于是应用层的join关联,在应用层先查出一份数据,然后再查出一份数据,进行关联 这里思考一个问题.假设要查询的是1万个人的博客,那么第一次查询的时候会拿到1万个user_id,然后第二次查询要在terms中放入1万个user_id,这样的话性能是很差的 优缺点优点: 数据不冗余,维护方便缺点: 应用层的join,如果关联的数据过多,导致查询过大,性能差 document型数据建模数据建模下面使用冗余数据,采用document数据模型,进行数据建模,实现用户和博客的关联 添加数据12345678910111213141516PUT /website/users/1&#123; &quot;name&quot;: &quot;小鱼儿&quot;, &quot;email&quot;: &quot;xiaoyuer@sina.com&quot;, &quot;birthday&quot;: &quot;1980-01-01&quot;&#125;PUT /website/blogs/1&#123; &quot;title&quot;: &quot;小鱼儿的第一篇博客&quot;, &quot;content&quot;: &quot;大家好，我是小鱼儿。。。&quot;, &quot;userInfo&quot;: &#123; &quot;userId&quot;: 1, &quot;username&quot;: &quot;小鱼儿&quot; &#125;&#125; 冗余数据,就是说,将可能进行搜索的条件和要搜索的数据放在一个doc中 基于冗余用户数据搜索博客请求:12345678910GET /website/blogs/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;userInfo.username.keyword&quot;: &#123; &quot;value&quot;: &quot;小鱼儿&quot; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526272829&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.2876821, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blogs&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.2876821, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;小鱼儿的第一篇博客&quot;, &quot;content&quot;: &quot;大家好，我是小鱼儿。。。&quot;, &quot;userInfo&quot;: &#123; &quot;userId&quot;: 1, &quot;username&quot;: &quot;小鱼儿&quot; &#125; &#125; &#125; ] &#125;&#125; 这样的话就不需要再走应用层的join,先搜索一个数据,再去搜另一份数据,直接走一个有冗余的type即可,指定要搜索的条件,即可搜索出自己想要的数据 优缺点优点: 性能高,不需要执行多次搜索缺点: 数据冗余,维护成本高. 比如说一个username变了,要同时更新user type和blog type. 一般来说,NoSql类型的数据存储,都是冗余模式,所以一旦出现冗余数据的修改,必须记得将所有关联的数据全都更新]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-81-关系型数据库与document类型数据模型对比]]></title>
    <url>%2F2019%2F01%2F15%2FElasticsearch-81-%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8Edocument%E7%B1%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[Java类有如下两个类,部门类和员工类12345678public class Department &#123; private Integer deptId; private String name; private String desc; private List&lt;Employee&gt; employees;&#125; 123456789public class Employee &#123; private Integer empId; private String name; private Integer age; private String gender; private Department dept;&#125; 关系型数据库在关系型数据库中上面的java类可能对应的就是两张表1234department表: dept_idnamedesc 123456employee表:emp_id nameagegender dept_id 关系型数据库中,每个数据实体都拆分为一个独立的表,然后通过主外键将数据关联起来 document文档数据模型在非关系型数据库中,就是document文档数据模型,里面存放的数据可能就是这样的12345678910111213141516171819202122232425&#123; &quot;deptId&quot;: &quot;1&quot;, &quot;name&quot;: &quot;研发部门&quot;, &quot;desc&quot;: &quot;负责公司的所有研发项目&quot;, &quot;employees&quot;: [ &#123; &quot;empId&quot;: &quot;1&quot;, &quot;name&quot;: &quot;张三&quot;, &quot;age&quot;: 28, &quot;gender&quot;: &quot;男&quot; &#125;, &#123; &quot;empId&quot;: &quot;2&quot;, &quot;name&quot;: &quot;王兰&quot;, &quot;age&quot;: 25, &quot;gender&quot;: &quot;女&quot; &#125;, &#123; &quot;empId&quot;: &quot;3&quot;, &quot;name&quot;: &quot;李四&quot;, &quot;age&quot;: 34, &quot;gender&quot;: &quot;男&quot; &#125; ]&#125; 这种document数据模型,类似于面向对象的数据模型,将所有由关联关系的数据,放到一个doc json类型的数据中,整个数据的关系还有完整的数据 都放在了一起]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-80-海量bucket的优化机制]]></title>
    <url>%2F2019%2F01%2F15%2FElasticsearch-80-%E6%B5%B7%E9%87%8Fbucket%E7%9A%84%E4%BC%98%E5%8C%96%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[概述先来看一个需求 现在有一堆数据,是每个演员的每个电影的评论,现在我们要取到10个演员的评论数量排名前5的电影 看一下请求体12345678910111213141516171819&#123; &quot;aggs&quot; : &#123; &quot;actors&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;actors&quot;, &quot;size&quot; : 10, &quot;collect_mode&quot; : &quot;breadth_first&quot; &#125;, &quot;aggs&quot; : &#123; &quot;costars&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;films&quot;, &quot;size&quot; : 5 &#125; &#125; &#125; &#125; &#125;&#125; 第一个聚合,根据演员分组,就能拿到每个演员的评论的数量, 然后 size是10 取前10个,collect_mode先不管,然后是第二个按电影分组,拿到了每个演员每个电影的评论数量,取前5的电影,这个需求就ok了 接下来看一下深度优先和广度优先的两种聚合方式 深度优先的聚合方式假设每个演员对应的电影数据如下12 actor1 actor2 .... actorfilm1 film2 film3 film1 film2 film3 ...film 比如说,现在有10万个actor,其实最后只需要10个就好了 但是我们使用的是深度优先的方式,它已经构建了一颗完整的树出来了,假设每个actor有10个电影, 那就是10万actor + 100万film的数据量的树. 最后再裁剪掉10万个actor中的99990个actor和99990 10 个film,剩下10个actor, 每个actor再裁剪掉5个film,剩下的数据就是10 5 = 50个, 从110万的数据裁剪成5个. 先构建了大量的数据,最后裁掉99.99%的数据,浪费了 广度优先的方式执行先构建actor的数,如下1actor1 actor2 actor3 ..... n个actor 假设10万个actor,但是现在不去构建他下面的film数据, 数据量也就是10万个,然后裁掉99990个 剩下10个,这个时候再去构建film,裁剪掉5个 数据就是从10万到剩下的50个,比上面的深度优先性能高了10倍. 再看上面那个请求体,其中collect_mode参数值为breadth_first,就是表示广度优先]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-79-fielddata filter的细粒度内存加载控制以及预加载机制]]></title>
    <url>%2F2019%2F01%2F15%2FElasticsearch-79-fielddata-filter%E7%9A%84%E7%BB%86%E7%B2%92%E5%BA%A6%E5%86%85%E5%AD%98%E5%8A%A0%E8%BD%BD%E6%8E%A7%E5%88%B6%E4%BB%A5%E5%8F%8A%E9%A2%84%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[filter的细粒度内存加载控制12345678910111213141516POST /test_index/_mapping/my_type&#123; &quot;properties&quot;: &#123; &quot;my_field&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fielddata&quot;: &#123; &quot;filter&quot;: &#123; &quot;frequency&quot;: &#123; &quot;min&quot;: 0.01, &quot;min_segment_size&quot;: 500 &#125; &#125; &#125; &#125; &#125;&#125; 主要设置的就是两个值,一个是min,一个是min_segment_size min: 仅仅加载至少1%的doc中出现过的term对应的fielddata比如说,有一个词 hello, 总共有1000个doc,hello必须在10个doc中出现,那么这个hello对应的fielddata才会加载到内存中来 min_segment_size: 少于500个doc的segment不加载fielddata加载fielddata的时候,也是按照segment去进行加载的,某个segment里面的doc数量少于500个的话,这个segment的fielddata就不会加载 这个一般不用去设置,了解一下就好了 fielddata预加载机制如果真的要对分词的field执行聚合,那么每次都在query-time现场生成fielddata并加载到内存中来,这样的话速度是比较慢的, 我们可以预先生成加载fielddata到内存中来 fielddata预加载1234567891011POST /test_index/_mapping/test_type&#123; &quot;properties&quot;: &#123; &quot;test_field&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;fielddata&quot;: &#123; &quot;loading&quot; : &quot;eager&quot; &#125; &#125; &#125;&#125; 这样可以将fielddata的生成变为index-time,建立倒排索引的时候,就同步生成fielddata并且加载到内存中,这样的话对分词field的聚合性能会大幅度增强 序号标记预加载global ordinal 原理: 假设有如下几个doc,分别是1234doc1: status1doc2: status2doc3: status2doc4: status1 在这样有很多重复值的情况,会进行global ordinal标记 status1 –&gt; 0status2 –&gt; 1 标记完成后,如下1234doc1: 0doc2: 1doc3: 1doc4: 0 建立fielddata也会是这个样子的,这样的好处就是减少重复字符串的出现次数,减少内存的消耗 设置的语法如下:1234567891011POST /test_index/_mapping/test_type&#123; &quot;properties&quot;: &#123; &quot;test_field&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;fielddata&quot;: &#123; &quot;loading&quot; : &quot;eager_global_ordinals&quot; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-78-fielddata内存控制及circuit breaker短路器]]></title>
    <url>%2F2019%2F01%2F14%2FElasticsearch-78-fielddata%E5%86%85%E5%AD%98%E6%8E%A7%E5%88%B6%E5%8F%8Acircuit-breaker%E7%9F%AD%E8%B7%AF%E5%99%A8%2F</url>
    <content type="text"><![CDATA[fielddata核心原理fielddata加载到内存的过程是lazy加载的,对一个分词field执行聚合时,才会加载,field执行聚合时,才会加载(query-time),而且是field-level加载的一个index的一个field,所有doc都会被加载,而不是少数doc fielddata内存限制1indices.fielddata.cache.size: 20% 在elasticsearch安装目录的config目录下的elasticsearch.yml里面配置 通过这个配置可以限制fielddata占用的内存,超出限制的话,就清除内存中已有的fielddata数据, 默认是无限制的,如果限制的话,可能会导致大量的evict和reload,大量的IO性能损耗,以及内存碎片和GC,不建议使用 监控fielddata内存使用情况可以通过以下几个请求去查看1GET /_stats/fielddata?fields=* 1GET /_nodes/stats/indices/fielddata?fields=* 1GET /_nodes/stats/indices/fielddata?level=indices&amp;fields=* circuit breaker如果一次query 加载的fielddata超过总内存,就会oom(内存溢出) circuit breaker(短路器)会估算query要加载的fielddata的大小,如果超出总内存,就短路,query直接失败 短路器的配置也是在elasticsearch.yml里面去配置的 123456# fielddata的内存限制,默认60%indices.breaker.fielddata.limit：# 执行聚合的内存限制,默认40%indices.breaker.request.limit：# 综合上面两个,限制在70%以内indices.breaker.total.limit：]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-77-fielddata原理]]></title>
    <url>%2F2019%2F01%2F14%2FElasticsearch-77-fielddata%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[对于不分词的field执行聚合操作.先来看一个请求12345678910GET /test_index/test_type/_search&#123; &quot;aggs&quot;: &#123; &quot;group_by_test_field&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;test_field&quot; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [test_field] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.&quot; &#125; ], &quot;type&quot;: &quot;search_phase_execution_exception&quot;, &quot;reason&quot;: &quot;all shards failed&quot;, &quot;phase&quot;: &quot;query&quot;, &quot;grouped&quot;: true, &quot;failed_shards&quot;: [ &#123; &quot;shard&quot;: 0, &quot;index&quot;: &quot;test_index&quot;, &quot;node&quot;: &quot;f57uV91xS_GRTQS2Ho81rg&quot;, &quot;reason&quot;: &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [test_field] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.&quot; &#125; &#125; ], &quot;caused_by&quot;: &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [test_field] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.&quot; &#125; &#125;, &quot;status&quot;: 400&#125; 请求是根据test_field去做聚合分析的,执行的时候报错,大概意思就是说,你必须打开fielddata,然后将正排索引数据加载到内存中,才可以对分词的field执行聚合操作,而且会消耗很大的内存 所以,对于分词的field要做聚合操作的话,需要设置fielddata=true 设置fielddata为true直接设置就可以了,不需要删除索引,重新建立123456789POST /test_index/_mapping/test_type&#123; &quot;properties&quot;: &#123; &quot;test_field&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;fielddata&quot;: true &#125; &#125;&#125; 执行完毕,查询看一下1GET /test_index/_mapping/test_type 返回值:1234567891011121314151617181920&#123; &quot;test_index&quot;: &#123; &quot;mappings&quot;: &#123; &quot;test_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;test_field&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125;, &quot;fielddata&quot;: true &#125; &#125; &#125; &#125; &#125;&#125; 设置好了之后,再执行最开始那个聚合请求1234567891011GET /test_index/test_type/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_test_field&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;test_field&quot; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526&#123; &quot;took&quot;: 26, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_test_field&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;test&quot;, &quot;doc_count&quot;: 3 &#125; ] &#125; &#125;&#125; 已经是可以执行的了 使用内置field执行聚合操作看下这个索引的_mapping, test_field中有一个内置的field是test_field.keyword,这个内置的field是不会进行分词的,所以,也可以的在不设置fielddata=true的情况下,使用内置的keyword 来进行聚合操作 1234567891011GET /test_index/test_type/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_test_field&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;test_field.keyword&quot; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526&#123; &quot;took&quot;: 8, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_test_field&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;test&quot;, &quot;doc_count&quot;: 3 &#125; ] &#125; &#125;&#125; 这里需要注意一下,内置的keyword这个field还有一个ignore_above属性,默认是256,前面有说过,是只保留field值的前256个,所以要控制field值在256以内,或者也可以自己设置ignore_above的值 分词field+fileddata工作原理如果某个field设置的是不分词的,那么在index-time的时候,就会自动生成doc_value,针对这些不分词的field执行聚合操作的时候,自动就会用doc value来执行 对于分词的field,是没有doc_value的. 在index-time的时候,如果某个field是分词的,那么是不会给它建立doc_value的,因为分词后,占用的空间过于大,所以默认是不支持分词field进行聚合的 如果要对分词的field执行聚合的话,是必须打开和使用fielddata,完全存在纯内存中, 结构和doc_value是类似的,但是只会将fielddata加载到内存中来,然后基于内存中的fielddata执行分词field的聚合操作,最开始的那个请求的报错信息中也说了,可能会占用大量的内存 那为什么fielddata必须存在内存? 因为分词的字符串,需要按照term进行聚合,需要执行更加复杂的算法和操作,如果基于磁盘和os cache,那么性能会很差]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-76-基于doc value正排索引的聚合内部原理]]></title>
    <url>%2F2019%2F01%2F14%2FElasticsearch-76-%E5%9F%BA%E4%BA%8Edoc-value%E6%AD%A3%E6%8E%92%E7%B4%A2%E5%BC%95%E7%9A%84%E8%81%9A%E5%90%88%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[概述 聚合分析的内部原理是什么 aggs term avg max等执行一个聚合操作的时候,内部原理是怎样的 用了什么样的数据结构去执行聚合 是不是用的倒排索引 聚合分析原理先来看一个搜索 + 聚合的请求123456789101112131415GET /test_index/test_type/_search &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;search_field&quot;: &quot;苹果&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;group_by_agg_field&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125; &#125; &#125;&#125; 采用倒排索引实现聚合的弊端首先,es内部绝对不是用倒排索引来实现聚合分析的,先来看上面的搜索,比如说现在倒排索引中有如下数据 term doc list 苹果 doc1,doc2 小米 doc3,doc4 这时候,我们的query,只要搜索到”苹果”,拿到doc list,就停止了, 并不需要去扫描整个倒排索引.所以效率还是很高的. 然后接下来就是进行聚合,在颜色的倒排索引中去搜索,如果要找到所有的颜色的话,就必须要遍历所有的倒排索引,因为是要做分组的嘛,这个性能就十分辣鸡了 color doc list 红色 doc1 蓝色 doc2 红色 doc3 蓝色 doc4 … … 使用倒排索引和正排索引(doc_value)来实现首先还是和之前的query一样,从倒排索引中去搜索,拿到搜索结果之后,进行聚合,只要根据搜索的结果,一次在正排索引中去搜索,做聚合操作,比如正排索引如下: doc color doc1 红色 doc2 蓝色 doc3 红色 doc4 蓝色 … … 比如搜索结果是doc1,doc2, 这时候,去正排索引中去找这两个doc,找到之后就停了,不会再继续往下找了 假设正排索引中有100万条数据,搜索结果有100条,去正排索引中去找这100条doc,比如说找到第1000条的时候,结果中的100条doc都找到了,那就没必要再去往下找了,就停了 doc_value原理在PUT/POST数据的时候,就会生成doc_value数据,也就是正排索引 和倒排索引类似,doc_value也会写入到磁盘,先是os cache进行缓存,以提升访问doc value的性能,如果os cache内存大小不足够放的下整个doc_value,就会将doc_value的数据写入磁盘文件中 es官方建议是,es大量是基于os cache来进行缓存和提升性能的,不建议用jvm内存来进行缓存,那样会导致一定的gc开销和oom问题,给jvm更少的内存,给os cache更大的内存 ,一般64G的服务器,给jvm最多16G,剩下的给os cache,os cache可以提升doc value和倒排索引的缓存和查询效率 column压缩举个例子:有如下三个doc123doc1: 550doc2: 550doc3: 500 会把相同的值合并掉,如doc1,doc2的值相同,就只保留一个550的标识就可以了 所有值相同,直接保留单值 少于256个值,使用table encoding模式(一种压缩方式) 大于256个值,看有没有最大公约数,有就除以最大公约数,然后保留这个最大公约数,如果没有最大公约数,采用offset结合压缩的方式 如何禁用有些情况下,我们可能并不需要用到doc_value,这时候就可以通过下面的请求,把doc_value禁用掉,还可以减少磁盘占用12345678910111213PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;my_field&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &quot;doc_values&quot;: false &#125; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-75-percentiles百分比算法及优化]]></title>
    <url>%2F2019%2F01%2F12%2FElasticsearch-75-percentiles%E7%99%BE%E5%88%86%E6%AF%94%E7%AE%97%E6%B3%95%E5%8F%8A%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[需求背景有一个网站,记录了每次请求访问的耗时,需要统计tp50,tp90,tp99. tp50:50%的请求的耗时最长在多长时间 tp90:90%的请求的耗时最长在多长时间 tp99:99%的请求的耗时最长在多长时间 准备数据1DELETE /website 创建索引123456789101112131415161718PUT /website&#123; &quot;mappings&quot;: &#123; &quot;logs&quot;: &#123; &quot;properties&quot;: &#123; &quot;latency&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;province&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;timestamp&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125; &#125; &#125; &#125;&#125; 添加数据12345678910111213141516171819202122232425POST /website/logs/_bulk&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;latency&quot; : 105, &quot;province&quot; : &quot;江苏&quot;, &quot;timestamp&quot; : &quot;2016-10-28&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;latency&quot; : 83, &quot;province&quot; : &quot;江苏&quot;, &quot;timestamp&quot; : &quot;2016-10-29&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;latency&quot; : 92, &quot;province&quot; : &quot;江苏&quot;, &quot;timestamp&quot; : &quot;2016-10-29&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;latency&quot; : 112, &quot;province&quot; : &quot;江苏&quot;, &quot;timestamp&quot; : &quot;2016-10-28&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;latency&quot; : 68, &quot;province&quot; : &quot;江苏&quot;, &quot;timestamp&quot; : &quot;2016-10-28&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;latency&quot; : 76, &quot;province&quot; : &quot;江苏&quot;, &quot;timestamp&quot; : &quot;2016-10-29&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;latency&quot; : 101, &quot;province&quot; : &quot;新疆&quot;, &quot;timestamp&quot; : &quot;2016-10-28&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;latency&quot; : 275, &quot;province&quot; : &quot;新疆&quot;, &quot;timestamp&quot; : &quot;2016-10-29&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;latency&quot; : 166, &quot;province&quot; : &quot;新疆&quot;, &quot;timestamp&quot; : &quot;2016-10-29&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;latency&quot; : 654, &quot;province&quot; : &quot;新疆&quot;, &quot;timestamp&quot; : &quot;2016-10-28&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;latency&quot; : 389, &quot;province&quot; : &quot;新疆&quot;, &quot;timestamp&quot; : &quot;2016-10-28&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;latency&quot; : 302, &quot;province&quot; : &quot;新疆&quot;, &quot;timestamp&quot; : &quot;2016-10-29&quot; &#125; pencentiles12345678910111213141516GET /website/logs/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;latency_percentiles&quot;: &#123; &quot;percentiles&quot;: &#123; &quot;field&quot;: &quot;latency&quot;, &quot;percents&quot;: [ 50, 90, 99 ] &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223&#123; &quot;took&quot;: 10, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 12, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;latency_percentiles&quot;: &#123; &quot;values&quot;: &#123; &quot;50.0&quot;: 108.5, &quot;90.0&quot;: 380.3, &quot;99.0&quot;: 624.8500000000001 &#125; &#125; &#125;&#125; 从返回值来看, tp50是108.5,tp90是380.3,tp99是624.85 这个结果是es算的,他并不是完全准确的 案例先根据地区分组,然后计算上面的数据 请求:1234567891011121314151617181920212223GET /website/logs/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_province&quot;:&#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;province&quot; &#125;, &quot;aggs&quot;: &#123; &quot;latency_percentiles&quot;:&#123; &quot;percentiles&quot;: &#123; &quot;field&quot;: &quot;latency&quot;, &quot;percents&quot;: [ 50, 95, 99 ] &#125; &#125; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526272829303132333435363738394041424344&#123; &quot;took&quot;: 7, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 12, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_province&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;新疆&quot;, &quot;doc_count&quot;: 6, &quot;latency_percentiles&quot;: &#123; &quot;values&quot;: &#123; &quot;50.0&quot;: 288.5, &quot;95.0&quot;: 587.75, &quot;99.0&quot;: 640.75 &#125; &#125; &#125;, &#123; &quot;key&quot;: &quot;江苏&quot;, &quot;doc_count&quot;: 6, &quot;latency_percentiles&quot;: &#123; &quot;values&quot;: &#123; &quot;50.0&quot;: 87.5, &quot;95.0&quot;: 110.25, &quot;99.0&quot;: 111.65 &#125; &#125; &#125; ] &#125; &#125;&#125; percentiles rank比如现在要计算网站访问时延SLA统计SLA: 就是提供的服务标准,比如确保所有的请求都要在200ms以内 需求: 在200ms以内的有百分之多少,在1000ms以内的有百分之多少 这个时候就需要用到percentiles ranks了,percentiles ranks其实比pencentile还要常用 请求:12345678910111213141516171819202122GET /website/logs/_search &#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_province&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;province&quot; &#125;, &quot;aggs&quot;: &#123; &quot;latency_percentile_ranks&quot;: &#123; &quot;percentile_ranks&quot;: &#123; &quot;field&quot;: &quot;latency&quot;, &quot;values&quot;: [ 200, 1000 ] &#125; &#125; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142&#123; &quot;took&quot;: 7, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 12, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_province&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;新疆&quot;, &quot;doc_count&quot;: 6, &quot;latency_percentile_ranks&quot;: &#123; &quot;values&quot;: &#123; &quot;200.0&quot;: 29.40613026819923, &quot;1000.0&quot;: 100 &#125; &#125; &#125;, &#123; &quot;key&quot;: &quot;江苏&quot;, &quot;doc_count&quot;: 6, &quot;latency_percentile_ranks&quot;: &#123; &quot;values&quot;: &#123; &quot;200.0&quot;: 100, &quot;1000.0&quot;: 100 &#125; &#125; &#125; ] &#125; &#125;&#125; percentile的优化percentile是采用TDigest算法的,用很多节点来执行百分比的计算,近似估计,所以会有一定的误差,用到的节点越多,就越精准 compression参数默认值是100, 限制节点的数量最多 compression * 20 个去计算 设置的越大,占用的内存就越多,就越精准,但是性能也会越差, 一个节点占用32字节, 100node 20 32 = 64KB 所以,如果你想要percentile算法越精准,compression可以设置的越大]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-74-cartinality metric去重算法及性能优化]]></title>
    <url>%2F2019%2F01%2F11%2FElasticsearch-74-cartinality-metric%E5%8E%BB%E9%87%8D%E7%AE%97%E6%B3%95%E5%8F%8A%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[cartinality metriccartinality metric:对每个bucket中的指定的field进行去重,取去重之后的count,类似于count(distinct) 案例需求: 每月销售品牌的数量统计12345678910111213141516171819GET tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;month&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;sold_date&quot;, &quot;interval&quot;: &quot;month&quot; &#125;, &quot;aggs&quot;: &#123; &quot;distinct_brand&quot;: &#123; &quot;cardinality&quot;: &#123; &quot;field&quot;: &quot;brand&quot; &#125; &#125; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100&#123; &quot;took&quot;: 35, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;month&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key_as_string&quot;: &quot;2016-05-01T00:00:00.000Z&quot;, &quot;key&quot;: 1462060800000, &quot;doc_count&quot;: 1, &quot;distinct_brand&quot;: &#123; &quot;value&quot;: 1 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-06-01T00:00:00.000Z&quot;, &quot;key&quot;: 1464739200000, &quot;doc_count&quot;: 0, &quot;distinct_brand&quot;: &#123; &quot;value&quot;: 0 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-07-01T00:00:00.000Z&quot;, &quot;key&quot;: 1467331200000, &quot;doc_count&quot;: 1, &quot;distinct_brand&quot;: &#123; &quot;value&quot;: 1 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-08-01T00:00:00.000Z&quot;, &quot;key&quot;: 1470009600000, &quot;doc_count&quot;: 1, &quot;distinct_brand&quot;: &#123; &quot;value&quot;: 1 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-09-01T00:00:00.000Z&quot;, &quot;key&quot;: 1472688000000, &quot;doc_count&quot;: 0, &quot;distinct_brand&quot;: &#123; &quot;value&quot;: 0 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-10-01T00:00:00.000Z&quot;, &quot;key&quot;: 1475280000000, &quot;doc_count&quot;: 1, &quot;distinct_brand&quot;: &#123; &quot;value&quot;: 1 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-11-01T00:00:00.000Z&quot;, &quot;key&quot;: 1477958400000, &quot;doc_count&quot;: 2, &quot;distinct_brand&quot;: &#123; &quot;value&quot;: 1 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-12-01T00:00:00.000Z&quot;, &quot;key&quot;: 1480550400000, &quot;doc_count&quot;: 0, &quot;distinct_brand&quot;: &#123; &quot;value&quot;: 0 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-01-01T00:00:00.000Z&quot;, &quot;key&quot;: 1483228800000, &quot;doc_count&quot;: 1, &quot;distinct_brand&quot;: &#123; &quot;value&quot;: 1 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-02-01T00:00:00.000Z&quot;, &quot;key&quot;: 1485907200000, &quot;doc_count&quot;: 1, &quot;distinct_brand&quot;: &#123; &quot;value&quot;: 1 &#125; &#125; ] &#125; &#125;&#125; 先根据月进行分组,然后用cardinality对品牌去重,就ok了 优化准确率和内存开销precision_threshold先来看个请求123456789101112GET /tvs/sales/_search&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;distinct_brand&quot; : &#123; &quot;cardinality&quot; : &#123; &quot;field&quot; : &quot;brand&quot;, &quot;precision_threshold&quot; : 100 &#125; &#125; &#125;&#125; 请求中有一个参数 precision_threshold,值是100(默认的就是100),这个搜索请求是根据brand来去重的,precision_threshold的作用就是 如果brand的unique value(去重后的唯一值)在100个以内的话,几乎是100%的准确率的 precision_threshold:在多少个unique value以内,cardinality会保证几乎100%准确 内存开销cardinality算法会占用 precision_threshold 8byte 的内存消耗, 比如我们上面这个请求中设置的是100,那么他的内存消耗就是 100 8byte = 800byte ,占用的内存很小. 而且搜索结果中unique value 的数量如果的确在设置的precision_threshold值以内的话,是可以确保100%准确的 官方的说明是如果设置precision_threshold的值是100,实际的unique value有百万的话,错误率是5%以内 HyperLogLog++ (HLL)算法性能优化cardinality运算的底层算法是HLL算法,如果要优化的话也是去优化HLL算法的性能 会对所有的uqniue value取hash值,通过hash值近似去求distcint count,所以会有一定的误差 默认情况下,发送一个cardinality请求的时候,会动态地对所有的field value取hash值, 我们可以在建立索引的时候,就对字段去设置他的hash值, 就不用去动态的取了 示例1234567891011121314151617PUT /tvs/&#123; &quot;mappings&quot;: &#123; &quot;sales&quot;: &#123; &quot;properties&quot;: &#123; &quot;brand&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;hash&quot;: &#123; &quot;type&quot;: &quot;murmur3&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 索引建立好之后,查询的时候,就需要用brand.hash这个field123456789101112GET /tvs/sales/_search&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;distinct_brand&quot; : &#123; &quot;cardinality&quot; : &#123; &quot;field&quot; : &quot;brand.hash&quot;, &quot;precision_threshold&quot; : 100 &#125; &#125; &#125;&#125; 这种方式其实性能也不会提升多少,可用可不用]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-73-易并行聚合算法 三角选择原则 近似聚合算法]]></title>
    <url>%2F2019%2F01%2F11%2FElasticsearch-73-%E6%98%93%E5%B9%B6%E8%A1%8C%E8%81%9A%E5%90%88%E7%AE%97%E6%B3%95-%E4%B8%89%E8%A7%92%E9%80%89%E6%8B%A9%E5%8E%9F%E5%88%99-%E8%BF%91%E4%BC%BC%E8%81%9A%E5%90%88%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[易并行聚合算法有些聚合分析的算法是很容易并行的,比如说max函数,如图 比如说,现在一共有三个shard,一个查询最大值的搜索请求过来了,那么coordinate node(协调节点)会把请求打到这三个shard上 这个时候,3个shard把他们各自的最大值返回给coordinate node,然后coordinate node再取到这三个里面的最大值,返回给客户端就可以了 有些聚合分析的算法是不好并行的,比如说count(distinct) 找去重后的数据量,并不是说在所有shard上去重之后返回给coordinate node的,因为数据可能会有很多 如图,还是3个shard,每个shard去重后有100W条数据返回给coordinate node,这时候,就要占用大量的内存,最少也需要100W条数据的内存, 他不像上面的max一样,只要保留一个long值的内存,每个shard返回数据的时候对比一下,大于就替换这种, 他需要占用大量的内存, 那么还用这种算法就不是很合适了 es会采用近似聚合的方式,就是采用在每个node上进行近估计的方式,得到最终的结论,但是这个结果跟实际是有一定的偏差的,比如说count(distinct)去重后,是有100W的数据,但是es估计的值是105万或者95万,就有5%左右的错误率 三角选择原则 精准 实时 大数据 在这三个中只能选择两个,比如 精准+实时:没有大数据,适合数据量很小的情况,一般就是单机跑,随便怎么玩儿 精准+大数据:比如hadoop,批处理,非实时,可以处理海量数据,保证精准,但是可能会跑个几分钟几小时的 大数据+实时:比如es,不精准,近似估计,可能会有百分之几的错误率 近似聚合算法如果采取近似估计的算法,延迟大概在100ms左右,0.5%的错误率如果采用100%精准的算法,延时一般在几秒到几十秒甚至几十分钟几小时, 0%的错误率]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-72-聚合分析 自定义排序]]></title>
    <url>%2F2019%2F01%2F11%2FElasticsearch-72-%E8%81%9A%E5%90%88%E5%88%86%E6%9E%90-%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[之前全部的排序都是按照每个bucket的doc_count降序来排的,那么如何自定义排序呢 自定义排序需求: 按每种颜色的平均销售额降序排序123456789101112131415161718192021GET /tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_color&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot;, &quot;order&quot;: &#123; &quot;avg_price&quot;: &quot;desc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; 返回值:12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_color&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;红色&quot;, &quot;doc_count&quot;: 4, &quot;avg_price&quot;: &#123; &quot;value&quot;: 3250 &#125; &#125;, &#123; &quot;key&quot;: &quot;绿色&quot;, &quot;doc_count&quot;: 2, &quot;avg_price&quot;: &#123; &quot;value&quot;: 2100 &#125; &#125;, &#123; &quot;key&quot;: &quot;蓝色&quot;, &quot;doc_count&quot;: 2, &quot;avg_price&quot;: &#123; &quot;value&quot;: 2000 &#125; &#125; ] &#125; &#125;&#125; 已经是按照请求中指定的avg_price降序排列的了 下钻分析时,深层的metric排序需求: 颜色 + 品牌下钻分析,按最深层的metric排序 12345678910111213141516171819202122232425262728GET /tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_color&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125;, &quot;aggs&quot;: &#123; &quot;group_by_brand&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;brand&quot;, &quot;order&quot;: &#123; &quot;avg_price&quot;: &quot;desc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 返回值:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394&#123; &quot;took&quot;: 1, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_color&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;红色&quot;, &quot;doc_count&quot;: 4, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;三星&quot;, &quot;doc_count&quot;: 1, &quot;avg_price&quot;: &#123; &quot;value&quot;: 8000 &#125; &#125;, &#123; &quot;key&quot;: &quot;长虹&quot;, &quot;doc_count&quot;: 3, &quot;avg_price&quot;: &#123; &quot;value&quot;: 1666.6666666666667 &#125; &#125; ] &#125; &#125;, &#123; &quot;key&quot;: &quot;绿色&quot;, &quot;doc_count&quot;: 2, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;小米&quot;, &quot;doc_count&quot;: 1, &quot;avg_price&quot;: &#123; &quot;value&quot;: 3000 &#125; &#125;, &#123; &quot;key&quot;: &quot;TCL&quot;, &quot;doc_count&quot;: 1, &quot;avg_price&quot;: &#123; &quot;value&quot;: 1200 &#125; &#125; ] &#125; &#125;, &#123; &quot;key&quot;: &quot;蓝色&quot;, &quot;doc_count&quot;: 2, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;小米&quot;, &quot;doc_count&quot;: 1, &quot;avg_price&quot;: &#123; &quot;value&quot;: 2500 &#125; &#125;, &#123; &quot;key&quot;: &quot;TCL&quot;, &quot;doc_count&quot;: 1, &quot;avg_price&quot;: &#123; &quot;value&quot;: 1500 &#125; &#125; ] &#125; &#125; ] &#125; &#125;&#125; 看下请求和返回值,请求中,排序是加到了下面一层group_by_brand中, 然后搜索结果中,也是group_by_brand下面的bucket按照我们指定的字段排序了,在上层的group_by_color中,还是按照doc_count去排序的]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-71-过滤filter 聚合结合使用]]></title>
    <url>%2F2019%2F01%2F11%2FElasticsearch-71-%E8%BF%87%E6%BB%A4filter-%E8%81%9A%E5%90%88%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[filter过滤+聚合分析需求: 统计价格大于1200的电视的平均价格 请求:12345678910111213141516171819202122GET tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;price&quot;: &#123; &quot;gte&quot;: 1200 &#125; &#125; &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;avg_of_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125;&#125; 返回值:12345678910111213141516171819&#123; &quot;took&quot;: 15, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 7, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;avg_of_price&quot;: &#123; &quot;value&quot;: 2885.714285714286 &#125; &#125;&#125; 跟搜索聚合的结合使用其实是一样的,就是把match换成了filter bucket filter先来看一个请求:123456789101112131415161718192021222324252627282930313233343536373839404142434445GET tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;brand&quot;: &#123; &quot;value&quot;: &quot;长虹&quot; &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;recent_150d&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;sold_date&quot;: &#123; &quot;gte&quot;: &quot;now-150d&quot; &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;recent_150d_avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125;, &quot;recent_140d&quot;:&#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;sold_date&quot;: &#123; &quot;gte&quot;: &quot;now-140d&quot; &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;recent_140d_avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; 先是query去搜索数据,过滤出只是长虹牌的数据,然后下面的aggs就是针对搜索结果的聚合, 然后每一个聚合分析里面有一个filter和aggs, filter呢,是用来过滤数据的,他是只针对这一个聚合去过滤的,然后filter下面aggs是再对filter过滤后的数据进行聚合分析. 每组聚合分析里面的数据,都是在query的结果上去过滤的,互不影响]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-70-搜索 聚合分析结合使用]]></title>
    <url>%2F2019%2F01%2F10%2FElasticsearch-70-%E6%90%9C%E7%B4%A2-%E8%81%9A%E5%90%88%E5%88%86%E6%9E%90%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[之前的几个案例都是全部使用的聚合分析,接下来呢,使用搜索和聚合分析结合起来使用 案例需求: 统计指定品牌下每个颜色的销量 (小米) 请求:123456789101112131415161718GET tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;brand&quot;: &#123; &quot;value&quot;: &quot;小米&quot; &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;group_by_color&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930&#123; &quot;took&quot;: 35, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_color&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;绿色&quot;, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: &quot;蓝色&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125; &#125;&#125; es的aggregation scope:任何的聚合,都必须在搜索出来的结果数据中执行,搜索结果,就是聚合分析操作的scope global bucket如果我们需要聚合分析两组数据,一组是根据搜索出来的结果集进行聚合分析,一组是根据全部的数据进行聚合分析,这时候就需要用到global bucket了,先看一个案例 需求: 分析单个品牌和所有品牌的销售平均价格 请求:12345678910111213141516171819202122232425262728GET tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;brand&quot;: &#123; &quot;value&quot;: &quot;长虹&quot; &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;single_brand_avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;all&quot;:&#123; &quot;global&quot;: &#123;&#125;, &quot;aggs&quot;: &#123; &quot;all_brand_avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; 第一个aggs中single_brand_avg_price先计算了搜索返回结果的平均价格,然后在下面用了global,然后又一个aggs,计算全部的平均价格,global就是将所有的数据纳入聚合的scope,而不管之前的query 返回值:12345678910111213141516171819202122232425&#123; &quot;took&quot;: 33, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;all&quot;: &#123; &quot;doc_count&quot;: 8, &quot;all_brand_avg_price&quot;: &#123; &quot;value&quot;: 2650 &#125; &#125;, &quot;single_brand_avg_price&quot;: &#123; &quot;value&quot;: 1666.6666666666667 &#125; &#125;&#125; 返回值中single_brand_avg_price.value就是针对query执行的聚合结果, all.all_brand_avg_price是针对所有数据执行的聚合结果]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-69-深入聚合分析数据II]]></title>
    <url>%2F2019%2F01%2F09%2FElasticsearch-69-%E6%B7%B1%E5%85%A5%E8%81%9A%E5%90%88%E5%88%86%E6%9E%90%E6%95%B0%E6%8D%AEII%2F</url>
    <content type="text"><![CDATA[常用的几种metric操作上文中,用了avg和count这两个操作,一般来说,常用的metric操作就是以下几种 count: 计算数量,用terms操作来分组的话,就会自动有一个doc_count,就相当于是count avg: 求一个bucket内,指定field数据的平均值 max: 求一个bucket内,指定field数据的最大值 min: 求一个bucket内,指定field数据的最小值 sum: 求一个bucket内,指定field数据的和 示例需求: 统计每种颜色的电视的数量和价格的平均值,最大值,最小值,总和 请求体:123456789101112131415161718192021222324252627282930313233GET tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;color&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125;, &quot;aggs&quot;: &#123; &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;max_price&quot;:&#123; &quot;max&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;min_price&quot;:&#123; &quot;min&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;sum_price&quot;:&#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; 返回值:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970&#123; &quot;took&quot;: 6, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;color&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;红色&quot;, &quot;doc_count&quot;: 4, &quot;max_price&quot;: &#123; &quot;value&quot;: 8000 &#125;, &quot;min_price&quot;: &#123; &quot;value&quot;: 1000 &#125;, &quot;avg_price&quot;: &#123; &quot;value&quot;: 3250 &#125;, &quot;sum_price&quot;: &#123; &quot;value&quot;: 13000 &#125; &#125;, &#123; &quot;key&quot;: &quot;绿色&quot;, &quot;doc_count&quot;: 2, &quot;max_price&quot;: &#123; &quot;value&quot;: 3000 &#125;, &quot;min_price&quot;: &#123; &quot;value&quot;: 1200 &#125;, &quot;avg_price&quot;: &#123; &quot;value&quot;: 2100 &#125;, &quot;sum_price&quot;: &#123; &quot;value&quot;: 4200 &#125; &#125;, &#123; &quot;key&quot;: &quot;蓝色&quot;, &quot;doc_count&quot;: 2, &quot;max_price&quot;: &#123; &quot;value&quot;: 2500 &#125;, &quot;min_price&quot;: &#123; &quot;value&quot;: 1500 &#125;, &quot;avg_price&quot;: &#123; &quot;value&quot;: 2000 &#125;, &quot;sum_price&quot;: &#123; &quot;value&quot;: 4000 &#125; &#125; ] &#125; &#125;&#125; histogram上面的请求都是用的terms来分组的, terms其实就是把field的值相同的数据分到了一个bucket里面,而histogram呢是可以根据某一范围区间去划分的 比如现在有一个需求,按照价格区间来统计销量和销售额12345678910111213141516171819GET /tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_price&quot;: &#123; &quot;histogram&quot;: &#123; &quot;field&quot;: &quot;price&quot;, &quot;interval&quot;: 2000 &#125;, &quot;aggs&quot;: &#123; &quot;sum_price&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; 返回值:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&#123; &quot;took&quot;: 7, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_price&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: 0, &quot;doc_count&quot;: 3, &quot;sum_price&quot;: &#123; &quot;value&quot;: 3700 &#125; &#125;, &#123; &quot;key&quot;: 2000, &quot;doc_count&quot;: 4, &quot;sum_price&quot;: &#123; &quot;value&quot;: 9500 &#125; &#125;, &#123; &quot;key&quot;: 4000, &quot;doc_count&quot;: 0, &quot;sum_price&quot;: &#123; &quot;value&quot;: 0 &#125; &#125;, &#123; &quot;key&quot;: 6000, &quot;doc_count&quot;: 0, &quot;sum_price&quot;: &#123; &quot;value&quot;: 0 &#125; &#125;, &#123; &quot;key&quot;: 8000, &quot;doc_count&quot;: 1, &quot;sum_price&quot;: &#123; &quot;value&quot;: 8000 &#125; &#125; ] &#125; &#125;&#125; 详细看一下请求体中的1234&quot;histogram&quot;: &#123; &quot;field&quot;: &quot;price&quot;, &quot;interval&quot;: 2000&#125; 这个部分,histogram和term类似也是进行bucket分组操作的, 里面的field就是按照哪个field进行分组,interval划分范围,比如我们请求中的是2000,那会就会划分0-2000,2000-4000,4000-6000….等等区间,然后根据price的值,去决定分到哪个bucket中,bucket有了之后,对它进行metric操作,和之前是一样的 date histogram需求: 统计每个月的电视销量 date histogram,可以按照我们指定的某一个date类型的field,以及日期interval,按照一定的日期间隔,去划分bucket 请求:123456789101112131415161718GET tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;sale&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;sold_date&quot;, &quot;interval&quot;: &quot;month&quot;, &quot;format&quot;: &quot;yyyy-MM-dd&quot;, &quot;min_doc_count&quot;: 0, &quot;extended_bounds&quot;:&#123; &quot;min&quot;: &quot;2016-01-01&quot;, &quot;max&quot;: &quot;2017-12-31&quot; &#125; &#125; &#125; &#125;&#125; 看一下请求,interval是month,就是按照月去划分,比如说2017-01-01~2017-01-31就是一个bucket, 然后 去扫描每个数据的date_field的值,判断落在哪个bucket中 min_doc_count:设置为0,意思就是说,即使某个interval区间中,一条数据都没有,那么这个区间也还是要返回的,不然默认是会过滤掉这个区间的 extended_bounds:划分bucket的时候,会限定这个起始日期和截止日期 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140&#123; &quot;took&quot;: 28, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;sale&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key_as_string&quot;: &quot;2016-01-01&quot;, &quot;key&quot;: 1451606400000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-02-01&quot;, &quot;key&quot;: 1454284800000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-03-01&quot;, &quot;key&quot;: 1456790400000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-04-01&quot;, &quot;key&quot;: 1459468800000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-05-01&quot;, &quot;key&quot;: 1462060800000, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-06-01&quot;, &quot;key&quot;: 1464739200000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-07-01&quot;, &quot;key&quot;: 1467331200000, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-08-01&quot;, &quot;key&quot;: 1470009600000, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-09-01&quot;, &quot;key&quot;: 1472688000000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-10-01&quot;, &quot;key&quot;: 1475280000000, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-11-01&quot;, &quot;key&quot;: 1477958400000, &quot;doc_count&quot;: 2 &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-12-01&quot;, &quot;key&quot;: 1480550400000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-01-01&quot;, &quot;key&quot;: 1483228800000, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-02-01&quot;, &quot;key&quot;: 1485907200000, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-03-01&quot;, &quot;key&quot;: 1488326400000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-04-01&quot;, &quot;key&quot;: 1491004800000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-05-01&quot;, &quot;key&quot;: 1493596800000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-06-01&quot;, &quot;key&quot;: 1496275200000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-07-01&quot;, &quot;key&quot;: 1498867200000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-08-01&quot;, &quot;key&quot;: 1501545600000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-09-01&quot;, &quot;key&quot;: 1504224000000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-10-01&quot;, &quot;key&quot;: 1506816000000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-11-01&quot;, &quot;key&quot;: 1509494400000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-12-01&quot;, &quot;key&quot;: 1512086400000, &quot;doc_count&quot;: 0 &#125; ] &#125; &#125;&#125; 返回值中,key_as_string 就是日期,key是13位的时间戳,doc_count就是统计的数量 案例需求: 统计每个季度每个品牌的销售额 请求:12345678910111213141516171819202122232425262728293031323334353637GET /tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_quarter&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;sold_date&quot;, &quot;interval&quot;: &quot;quarter&quot;, &quot;format&quot;: &quot;yyyy-MM-dd&quot;, &quot;min_doc_count&quot;: 0, &quot;extended_bounds&quot;:&#123; &quot;min&quot;:&quot;2016-01-01&quot;, &quot;max&quot;:&quot;2017-12-31&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;group_by_brand&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;brand&quot; &#125;, &quot;aggs&quot;: &#123; &quot;sum_of_price&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125;, &quot;total_sum_price&quot;:&#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; 请求中,先按照季度来分组,分好之后下面的下钻分析中,第一个group_by_brand 按照品牌分组, 第二个是total_sum_price 计算每二个季度的销售额, 然后group_by_brand下面继续下钻分析,统计每个品牌的销售额 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163&#123; &quot;took&quot;: 4, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_quarter&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key_as_string&quot;: &quot;2016-01-01&quot;, &quot;key&quot;: 1451606400000, &quot;doc_count&quot;: 0, &quot;total_sum_price&quot;: &#123; &quot;value&quot;: 0 &#125;, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [] &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-04-01&quot;, &quot;key&quot;: 1459468800000, &quot;doc_count&quot;: 1, &quot;total_sum_price&quot;: &#123; &quot;value&quot;: 3000 &#125;, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;小米&quot;, &quot;doc_count&quot;: 1, &quot;sum_of_price&quot;: &#123; &quot;value&quot;: 3000 &#125; &#125; ] &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-07-01&quot;, &quot;key&quot;: 1467331200000, &quot;doc_count&quot;: 2, &quot;total_sum_price&quot;: &#123; &quot;value&quot;: 2700 &#125;, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;TCL&quot;, &quot;doc_count&quot;: 2, &quot;sum_of_price&quot;: &#123; &quot;value&quot;: 2700 &#125; &#125; ] &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-10-01&quot;, &quot;key&quot;: 1475280000000, &quot;doc_count&quot;: 3, &quot;total_sum_price&quot;: &#123; &quot;value&quot;: 5000 &#125;, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;长虹&quot;, &quot;doc_count&quot;: 3, &quot;sum_of_price&quot;: &#123; &quot;value&quot;: 5000 &#125; &#125; ] &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-01-01&quot;, &quot;key&quot;: 1483228800000, &quot;doc_count&quot;: 2, &quot;total_sum_price&quot;: &#123; &quot;value&quot;: 10500 &#125;, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;三星&quot;, &quot;doc_count&quot;: 1, &quot;sum_of_price&quot;: &#123; &quot;value&quot;: 8000 &#125; &#125;, &#123; &quot;key&quot;: &quot;小米&quot;, &quot;doc_count&quot;: 1, &quot;sum_of_price&quot;: &#123; &quot;value&quot;: 2500 &#125; &#125; ] &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-04-01&quot;, &quot;key&quot;: 1491004800000, &quot;doc_count&quot;: 0, &quot;total_sum_price&quot;: &#123; &quot;value&quot;: 0 &#125;, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [] &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-07-01&quot;, &quot;key&quot;: 1498867200000, &quot;doc_count&quot;: 0, &quot;total_sum_price&quot;: &#123; &quot;value&quot;: 0 &#125;, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [] &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-10-01&quot;, &quot;key&quot;: 1506816000000, &quot;doc_count&quot;: 0, &quot;total_sum_price&quot;: &#123; &quot;value&quot;: 0 &#125;, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [] &#125; &#125; ] &#125; &#125;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-68-深入聚合分析数据I]]></title>
    <url>%2F2019%2F01%2F07%2FElasticsearch-68-%E6%B7%B1%E5%85%A5%E8%81%9A%E5%90%88%E5%88%86%E6%9E%90%E6%95%B0%E6%8D%AEI%2F</url>
    <content type="text"><![CDATA[背景本文,将以一个家电卖场中的电视销售数据为背景,进行各种各样角度的聚合分析 准备数据创建索引tvs123456789101112131415161718192021PUT /tvs&#123; &quot;mappings&quot;: &#123; &quot;sales&quot;: &#123; &quot;properties&quot;: &#123; &quot;price&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;color&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;brand&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;sold_date&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125; &#125; &#125; &#125;&#125; 添加测试数据1234567891011121314151617POST /tvs/sales/_bulk&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 1000, &quot;color&quot; : &quot;红色&quot;, &quot;brand&quot; : &quot;长虹&quot;, &quot;sold_date&quot; : &quot;2016-10-28&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 2000, &quot;color&quot; : &quot;红色&quot;, &quot;brand&quot; : &quot;长虹&quot;, &quot;sold_date&quot; : &quot;2016-11-05&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 3000, &quot;color&quot; : &quot;绿色&quot;, &quot;brand&quot; : &quot;小米&quot;, &quot;sold_date&quot; : &quot;2016-05-18&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 1500, &quot;color&quot; : &quot;蓝色&quot;, &quot;brand&quot; : &quot;TCL&quot;, &quot;sold_date&quot; : &quot;2016-07-02&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 1200, &quot;color&quot; : &quot;绿色&quot;, &quot;brand&quot; : &quot;TCL&quot;, &quot;sold_date&quot; : &quot;2016-08-19&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 2000, &quot;color&quot; : &quot;红色&quot;, &quot;brand&quot; : &quot;长虹&quot;, &quot;sold_date&quot; : &quot;2016-11-05&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 8000, &quot;color&quot; : &quot;红色&quot;, &quot;brand&quot; : &quot;三星&quot;, &quot;sold_date&quot; : &quot;2017-01-01&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 2500, &quot;color&quot; : &quot;蓝色&quot;, &quot;brand&quot; : &quot;小米&quot;, &quot;sold_date&quot; : &quot;2017-02-12&quot; &#125; 根据颜色分类统计销量请求体1234567891011GET /tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;popular_colors&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125; &#125; &#125;&#125; 请求体重的各种参数: size:设置为0的话,只获取聚合结果,不会把原始数据返回回来 aggs:固定语法,要对一份数据执行分组聚合操作 popular_colors:需要对每个aggs取一个名字,名字是自定义的 terms:表示要根据字段的值进行分组 field:要根据那个字段进行分组 上面请求的返回值:12345678910111213141516171819202122232425262728293031323334&#123; &quot;took&quot;: 8, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;popular_colors&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;红色&quot;, &quot;doc_count&quot;: 4 &#125;, &#123; &quot;key&quot;: &quot;绿色&quot;, &quot;doc_count&quot;: 2 &#125;, &#123; &quot;key&quot;: &quot;蓝色&quot;, &quot;doc_count&quot;: 2 &#125; ] &#125; &#125;&#125; 返回结果中的数据 hits.hits: 我们指定了size是0,所以这里就是空的,否则会把执行聚合的原始数据返回回来 aggregations: 聚合结果 popular_color: 在查询时候指定的那个名称 buckets: 根据我们指定的field划分出来的buckets key: 每个bucket对应的那个值 doc_count: 这个bucket分组内,有多少个数据 默认是按照doc_count降序排序的 统计每种颜色的平均价格请求体:123456789101112131415161718GET /tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;colors&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125;, &quot;aggs&quot;: &#123; &quot;price_avg&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; 还是和上面一样,按照color去分bucket,可以拿到每个color bucket中的数量(doc_count),这仅仅是一个bucket操作,doc_count的统计其实只是es的bucket操作默认执行的一个内置metric 上面请求中的计算平均值,就是对bucket执行的一个metric聚合统计操作 看一下请求体,在一个aggs执行的bucket操作(terms),同级下又加入了一个aggs,这第二个aggs内部,同样取了个名字,执行一个metric操作 avg,对之前的每个bucket中的数据的指定field, 求一个平均值 请求中的1234567&quot;aggs&quot;: &#123; &quot;price_avg&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; 就是一个metric操作,对分组后的每个bucket都要执行的一个操作 请求的返回值:12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;took&quot;: 1, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;colors&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;红色&quot;, &quot;doc_count&quot;: 4, &quot;price_avg&quot;: &#123; &quot;value&quot;: 3250 &#125; &#125;, &#123; &quot;key&quot;: &quot;绿色&quot;, &quot;doc_count&quot;: 2, &quot;price_avg&quot;: &#123; &quot;value&quot;: 2100 &#125; &#125;, &#123; &quot;key&quot;: &quot;蓝色&quot;, &quot;doc_count&quot;: 2, &quot;price_avg&quot;: &#123; &quot;value&quot;: 2000 &#125; &#125; ] &#125; &#125;&#125; 再来看一下返回值,buckets中除了key和doc_count还有 avg_price: 我们在发送请求时候,自己取的名字 value: metric计算的结果,每个bucket中的数据的price字段求平均值后的结果 这段请求,如果转成sql的话,就是1select avg(price) from tvs.sales group by color 下钻分析需求: 从颜色到品牌进行下钻分析, 分析每种颜色的平均价格,以及每个颜色中的每个品牌的平均价格 下钻的意思是,已经分了一个组了,然后还要对这个分组内的数据,再分组,比如上面这个案例中,颜色分组之后,还可以对品牌进行分组,最后对每个最小粒度的分组执行聚合分析的操作,就是下钻分析 搜索请求:123456789101112131415161718192021222324252627282930GET /tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_color&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125;, &quot;aggs&quot;: &#123; &quot;color_avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;group_by_brand&quot;:&#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;brand&quot; &#125;, &quot;aggs&quot;: &#123; &quot;brand_avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103&#123; &quot;took&quot;: 10, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_color&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;红色&quot;, &quot;doc_count&quot;: 4, &quot;color_avg_price&quot;: &#123; &quot;value&quot;: 3250 &#125;, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;长虹&quot;, &quot;doc_count&quot;: 3, &quot;brand_avg_price&quot;: &#123; &quot;value&quot;: 1666.6666666666667 &#125; &#125;, &#123; &quot;key&quot;: &quot;三星&quot;, &quot;doc_count&quot;: 1, &quot;brand_avg_price&quot;: &#123; &quot;value&quot;: 8000 &#125; &#125; ] &#125; &#125;, &#123; &quot;key&quot;: &quot;绿色&quot;, &quot;doc_count&quot;: 2, &quot;color_avg_price&quot;: &#123; &quot;value&quot;: 2100 &#125;, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;TCL&quot;, &quot;doc_count&quot;: 1, &quot;brand_avg_price&quot;: &#123; &quot;value&quot;: 1200 &#125; &#125;, &#123; &quot;key&quot;: &quot;小米&quot;, &quot;doc_count&quot;: 1, &quot;brand_avg_price&quot;: &#123; &quot;value&quot;: 3000 &#125; &#125; ] &#125; &#125;, &#123; &quot;key&quot;: &quot;蓝色&quot;, &quot;doc_count&quot;: 2, &quot;color_avg_price&quot;: &#123; &quot;value&quot;: 2000 &#125;, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;TCL&quot;, &quot;doc_count&quot;: 1, &quot;brand_avg_price&quot;: &#123; &quot;value&quot;: 1500 &#125; &#125;, &#123; &quot;key&quot;: &quot;小米&quot;, &quot;doc_count&quot;: 1, &quot;brand_avg_price&quot;: &#123; &quot;value&quot;: 2500 &#125; &#125; ] &#125; &#125; ] &#125; &#125;&#125; 先看一下搜索请求,就是在计算完按颜色分组之后的平均值后,又分了一次组group_by_brand,按的是品牌,然后分组之后,再计算按颜色品牌的平均值 再看返回结果,结构基本和搜索请求是相同的,先是按颜色的分组,然后下面又套了一个按品牌的分组]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-67-聚合分析bucket和metric]]></title>
    <url>%2F2019%2F01%2F07%2FElasticsearch-67-%E8%81%9A%E5%90%88%E5%88%86%E6%9E%90bucket%E5%92%8Cmetric%2F</url>
    <content type="text"><![CDATA[核心概念bucket是一个数据的分组,metric就是对一个bucket执行的某种聚合分析的操作,比如说,求平均值,最大值,最小值等 举个例子,有这么一组数据 city name 北京 小李 北京 小王 上海 小张 上海 小丽 上海 小陈 上面的数据可以通过城市划分出来两个bucket,一个是北京bucket一个是上海bucket 北京bucket包含了两个人:小李,小王上海bucket包含了三个人:小张,小丽,小陈 就是说,按照某个字段进行bucket划分,那个字段的值相同的那些数据,就会被划分到一个bucket中 metric呢就是对这些bucket进行的聚合分析的操作 比如有这么一个sql1select count(1) from access_log group_by user_id 在这个sql中bucket就是 group_by user_id 那些user_id相同的数据,就是一个bucketmetric就是count(1),计算每个bucket中的总数这个操作]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-66-修改IK分词器源码基于mysql热更新词库]]></title>
    <url>%2F2019%2F01%2F05%2FElasticsearch-66-%E4%BF%AE%E6%94%B9IK%E5%88%86%E8%AF%8D%E5%99%A8%E6%BA%90%E7%A0%81%E5%9F%BA%E4%BA%8Emysql%E7%83%AD%E6%9B%B4%E6%96%B0%E8%AF%8D%E5%BA%93%2F</url>
    <content type="text"><![CDATA[上文中,我们如果要配置一个自定义的词语,或者停用词的时候,必须要手动添加到ik分词器的配置中,然后重启es节点,这样就很坑了,而且如果es集群中有上百个节点的话,那一个个的修改要疯了 通过修改ik分词器的源码,可以使用mysql作为词库,有词语更新的话,直接添加到mysql的表中就好了,不需要再去重启. 热更新方案第一种:修改ik分词器源码,然后手动支持从mysql中每隔一定时间,自动加载新的词库第二种:基于ik分词器原生支持的热更新方案,部署一个web服务器,提供一个http接口,通过modified和tag两个http响应头,来提供词语的热更新 第一种方案是比较常用的, 第二种呢ik git官方社区都不建议采用 源码下载从github上把源码拉下来1git clone https://github.com/medcl/elasticsearch-analysis-ik.git 我们的es是5.2.0版本的,ik分词器也切换到5.2.0版本的分支上面1git checkout v5.2.0 切换完成后,直接用idea打开就好了 源码修改第一步,在pom中加入mysql的依赖123456&lt;!-- mysql --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;6.0.6&lt;/version&gt;&lt;/dependency&gt; 第二步,配置mysql的连接,在config目录下创建一个.properties文件123456789jdbc.url=jdbc:mysql://localhost:3306/my_ik_word?allowMultiQueries=true&amp;autoReconnect=true&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;serverTimezone=GMTjdbc.user=rootjdbc.password=123456# 更新词库的语句jdbc.reload.sql=select word from hot_words# 更新停用词的语句jdbc.reload.stopword.sql=select stop_word as word from hot_stop_words# 隔多少时间去更新一次jdbc.reload.interval=1000 第三步,新建一个线程,run方法中调用Dictionary类的reLoadMainDict()方法,就是让他去重新加载词典123456789101112public class HotDicReloadThread implements Runnable &#123; private static final Logger logger = ESLoggerFactory.getLogger(HotDicReloadThread.class.getName()); @Override public void run() &#123; while (true)&#123; logger.info(&quot;-------reload hot dic from mysql--------&quot;); Dictionary.getSingleton().reLoadMainDict(); &#125; &#125;&#125; 第四步,Dictionary类中,加入mysql驱动类12345678910// prop用来获取上面的properties配置文件private static Properties prop = new Properties();static &#123; try &#123; Class.forName(&quot;com.mysql.jdbc.Driver&quot;); &#125; catch (ClassNotFoundException e) &#123; logger.error(&quot;error&quot;, e); &#125;&#125; 第五步,initial()方法中,启动刚刚创建的线程123456789101112131415161718192021222324252627282930313233343536373839/** * 词典初始化 由于IK Analyzer的词典采用Dictionary类的静态方法进行词典初始化 * 只有当Dictionary类被实际调用时，才会开始载入词典， 这将延长首次分词操作的时间 该方法提供了一个在应用加载阶段就初始化字典的手段 * * @return Dictionary */public static synchronized Dictionary initial(Configuration cfg) &#123; if (singleton == null) &#123; synchronized (Dictionary.class) &#123; if (singleton == null) &#123; singleton = new Dictionary(cfg); singleton.loadMainDict(); singleton.loadSurnameDict(); singleton.loadQuantifierDict(); singleton.loadSuffixDict(); singleton.loadPrepDict(); singleton.loadStopWordDict(); // 执行更新词库的线程 new Thread(new HotDicReloadThread()).start(); if(cfg.isEnableRemoteDict())&#123; // 建立监控线程 for (String location : singleton.getRemoteExtDictionarys()) &#123; // 10 秒是初始延迟可以修改的 60是间隔时间 单位秒 pool.scheduleAtFixedRate(new Monitor(location), 10, 60, TimeUnit.SECONDS); &#125; for (String location : singleton.getRemoteExtStopWordDictionarys()) &#123; pool.scheduleAtFixedRate(new Monitor(location), 10, 60, TimeUnit.SECONDS); &#125; &#125; return singleton; &#125; &#125; &#125; return singleton;&#125; 第六步,新添加一个loadMainDict()方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * 从mysql中加载热更新词典 */private void loadMySqlExtDict()&#123; Connection connection = null; Statement statement = null; ResultSet resultSet = null; try &#123; Path file = PathUtils.get(getDictRoot(),&quot;jdbc-reload.properties&quot;); prop.load(new FileInputStream(file.toFile())); logger.info(&quot;-------jdbc-reload.properties-------&quot;); for (Object key : prop.keySet()) &#123; logger.info(&quot;key:&#123;&#125;&quot;, prop.getProperty(String.valueOf(key))); &#125; logger.info(&quot;------- query hot dict from mysql, sql:&#123;&#125;-------&quot;, prop.getProperty(&quot;jdbc.reload.sql&quot;)); // 建立mysql连接 connection = DriverManager.getConnection( prop.getProperty(&quot;jdbc.url&quot;), prop.getProperty(&quot;jdbc.user&quot;), prop.getProperty(&quot;jdbc.password&quot;) ); // 执行查询 statement = connection.createStatement(); resultSet = statement.executeQuery(prop.getProperty(&quot;jdbc.reload.sql&quot;)); // 循环输出查询啊结果,添加到Main.dict中去 while (resultSet.next()) &#123; String theWord = resultSet.getString(&quot;word&quot;); logger.info(&quot;------hot word from mysql:&#123;&#125;------&quot;, theWord); // 加到mainDict里面 _MainDict.fillSegment(theWord.trim().toCharArray()); &#125; &#125; catch (Exception e) &#123; logger.error(&quot;error:&#123;&#125;&quot;, e); &#125; finally &#123; try &#123; if (resultSet != null) &#123; resultSet.close(); &#125; if (statement != null) &#123; statement.close(); &#125; if (connection != null) &#123; connection.close(); &#125; &#125; catch (SQLException e)&#123; logger.error(&quot;error&quot;, e); &#125; &#125;&#125; 第七步,在loadMainDict()方法最后,调用上面添加的这个方法12// 加载mysql词典this.loadMySqlExtDict(); 第八步,新添加loadMySqlStopwordDict()方法,用来从mysql中获取停用词12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * 从mysql中加载停用词 */private void loadMySqlStopwordDict()&#123; Connection conn = null; Statement stmt = null; ResultSet rs = null; try &#123; Path file = PathUtils.get(getDictRoot(), &quot;jdbc-reload.properties&quot;); prop.load(new FileInputStream(file.toFile())); logger.info(&quot;-------jdbc-reload.properties-------&quot;); for(Object key : prop.keySet()) &#123; logger.info(&quot;-------key:&#123;&#125;&quot;, prop.getProperty(String.valueOf(key))); &#125; logger.info(&quot;-------query hot stopword dict from mysql, sql:&#123;&#125;&quot;,props.getProperty(&quot;jdbc.reload.stopword.sql&quot;)); conn = DriverManager.getConnection( prop.getProperty(&quot;jdbc.url&quot;), prop.getProperty(&quot;jdbc.user&quot;), prop.getProperty(&quot;jdbc.password&quot;)); stmt = conn.createStatement(); rs = stmt.executeQuery(prop.getProperty(&quot;jdbc.reload.stopword.sql&quot;)); while(rs.next()) &#123; String theWord = rs.getString(&quot;word&quot;); logger.info(&quot;------- hot stopword from mysql: &#123;&#125;&quot;, theWord); _StopWords.fillSegment(theWord.trim().toCharArray()); &#125; Thread.sleep(Integer.valueOf(String.valueOf(prop.get(&quot;jdbc.reload.interval&quot;)))); &#125; catch (Exception e) &#123; logger.error(&quot;error&quot;, e); &#125; finally &#123; try &#123; if(rs != null) &#123; rs.close(); &#125; if(stmt != null) &#123; stmt.close(); &#125; if(conn != null) &#123; conn.close(); &#125; &#125; catch (SQLException e)&#123; logger.error(&quot;error:&#123;&#125;&quot;, e); &#125; &#125;&#125; 第九步,在loadStopWordDict()方法最后,调用上面的更新停用词的方法12// 从mysql中加载停用词this.loadMySqlStopwordDict(); 至此,源码修改完毕,数据库的两个表如下 词库表12345CREATE TABLE `hot_words` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `word` varchar(50) COLLATE utf8_unicode_ci DEFAULT NULL COMMENT &apos;词语&apos;, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_unicode_ci; 停用词库表12345CREATE TABLE `hot_stop_words` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `stop_word` varchar(50) COLLATE utf8_unicode_ci DEFAULT NULL COMMENT &apos;停用词&apos;, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8 COLLATE=utf8_unicode_ci; 这些做完以后,maven打包项目1mvn clean package -DskipTests 打包完成后,在项目目录的target\releases 路径下面有个压缩包,解压到es\plugins\ik目录下,然后将mysql的驱动包丢进去, 之后重启es就完成了. 测试在停用词的表中加入 “我”,然后去kibana中测试一下12345GET /_analyze&#123; &quot;text&quot;: &quot;我的&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;&#125; 返回值:123&#123; &quot;tokens&quot;: []&#125; 我, 这个停用词已经生效了,直接被干掉了. 修改后的项目修改后的代码也传到我的github上去了,可以直接clone下来切换分支使用1git clone https://github.com/zhouze-java/elasticsearch-analysis-ik.git 切换分支1git checkout ik_zhouze 然后打包一下1mvn clean package -DskipTests 其他操作和上面一样.记得把mysql的驱动包丢进去]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-65-IK分词器配置文件详解]]></title>
    <url>%2F2019%2F01%2F05%2FElasticsearch-65-IK%E5%88%86%E8%AF%8D%E5%99%A8%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[ik配置文件ik配置文件地址: es目录/plugins/ik/config main.dic:ik原生内置的中文词库,总共有27万多条,只要是这些单词,都会被分在一起 quantifier.dic:放了一些单位相关的词 suffix.dic:放了一些后缀 surname.dic:中国的姓氏 stopword.dic:英文停用词 IKAnalyzer.cfg.xml:用来配置自定义词库地址 ik原生中最要的两个配置文件就是main.dic和stopword.dic 停用词一般就是像 a the at 等等这些单词,停用词在分词的时候会直接被干掉,不会建立倒排索引 自定义词库每年都会有一些流行语,比如像蓝瘦香菇,网红等等这些词在ik原生的词典里面一般是没有的我们可以添加到custom目录下面的mydict.dic添加完成后,重启es,我们自己添加的这些词语就会生效 也可以自己建立停用词库,比如 了,的,啥,么,我们可能并不想让这些词去建立索引,就可以补充在custom/ext_stopword.dic中,然后重启es就可以生效了 或者说这些自定义词库,停用词,都可以自己创建一个dic文件,然后在IKAnalyzer.cfg.xml配置好文件的路径就可以了]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-64-IK中文分词器的安装和基本使用]]></title>
    <url>%2F2019%2F01%2F04%2FElasticsearch-64-IK%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%99%A8%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[ik中文分词器安装从github上拉取ik分词器1git clone https://github.com/medcl/elasticsearch-analysis-ik 切换分支1git checkout tags/v5.2.0 编译1mvn package 将target/releases/elasticsearch-analysis-ik-5.2.0.zip拷贝到es/plugins/ik目录下 在es/plugins/ik下对elasticsearch-analysis-ik-5.2.0.zip进行解压缩,然后删除压缩包 最后重启es 或者说直接这里下载压缩包即可 基础知识ik分词器中,包含了两种analyzer,可以根据自己的需要自己选,一般用ik_max_word ik_max_word会将文本做最细粒度的拆分,比如说会将”中华人民共和国国歌”拆分为”中华人民共和国,中华人民,中华,华人,人民共和国,人民,人,民,共和国,共和,和,国国,国歌”,会穷尽各种可能的组合 ik_smart会做最粗粒度的拆分,比如将”中华人民共和国国歌”拆分为”中华人民共和国,国歌” 基本的使用创建索引的时候指定使用ik分词器12345678910111213PUT /my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;text&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; &#125; &#125; &#125; &#125;&#125; 创建完成后添加几条测试数据12345678910111213POST /my_index/my_type/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;1&quot;&#125; &#125;&#123; &quot;text&quot;: &quot;男子偷上万元发红包求交女友 被抓获时仍然单身&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;2&quot;&#125; &#125;&#123; &quot;text&quot;: &quot;16岁少女为结婚“变”22岁 7年后想离婚被法院拒绝&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;3&quot;&#125; &#125;&#123; &quot;text&quot;: &quot;深圳女孩骑车逆行撞奔驰 遭索赔被吓哭(图)&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;4&quot;&#125; &#125;&#123; &quot;text&quot;: &quot;女人对护肤品比对男票好？网友神怼&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;5&quot;&#125; &#125;&#123; &quot;text&quot;: &quot;为什么国内的街道招牌用的都是红黄配？&quot; &#125;``` 来看下这个分词器的效果 GET my_index/_analyze{ “text”: “男子偷上万元发红包求交女友 被抓获时仍然单身”, “analyzer”: “ik_max_word”}1返回值: { “tokens”: [ { “token”: “男子”, “start_offset”: 0, “end_offset”: 2, “type”: “CN_WORD”, “position”: 0 }, { “token”: “偷上”, “start_offset”: 2, “end_offset”: 4, “type”: “CN_WORD”, “position”: 1 }, { “token”: “上万”, “start_offset”: 3, “end_offset”: 5, “type”: “CN_WORD”, “position”: 2 }, { “token”: “万元”, “start_offset”: 4, “end_offset”: 6, “type”: “CN_WORD”, “position”: 3 }, { “token”: “万”, “start_offset”: 4, “end_offset”: 5, “type”: “CN_WORD”, “position”: 4 }, { “token”: “元”, “start_offset”: 5, “end_offset”: 6, “type”: “CN_CHAR”, “position”: 5 }, { “token”: “发红包”, “start_offset”: 6, “end_offset”: 9, “type”: “CN_WORD”, “position”: 6 }, { “token”: “发红”, “start_offset”: 6, “end_offset”: 8, “type”: “CN_WORD”, “position”: 7 }, { “token”: “发”, “start_offset”: 6, “end_offset”: 7, “type”: “CN_WORD”, “position”: 8 }, { “token”: “红包”, “start_offset”: 7, “end_offset”: 9, “type”: “CN_WORD”, “position”: 9 }, { “token”: “求”, “start_offset”: 9, “end_offset”: 10, “type”: “CN_CHAR”, “position”: 10 }, { “token”: “交”, “start_offset”: 10, “end_offset”: 11, “type”: “CN_CHAR”, “position”: 11 }, { “token”: “女友”, “start_offset”: 11, “end_offset”: 13, “type”: “CN_WORD”, “position”: 12 }, { “token”: “抓获”, “start_offset”: 15, “end_offset”: 17, “type”: “CN_WORD”, “position”: 13 }, { “token”: “获”, “start_offset”: 16, “end_offset”: 17, “type”: “CN_WORD”, “position”: 14 }, { “token”: “时”, “start_offset”: 17, “end_offset”: 18, “type”: “CN_CHAR”, “position”: 15 }, { “token”: “仍然”, “start_offset”: 18, “end_offset”: 20, “type”: “CN_WORD”, “position”: 16 }, { “token”: “单身”, “start_offset”: 20, “end_offset”: 22, “type”: “CN_WORD”, “position”: 17 } ]}123他会把所有可能的词语都拆分出来 然后我们来测试搜索一下 GET /my_index/my_type/_search{ “query”: { “match”: { “text”: “16岁少女结婚好还是单身好？” } }}1返回: { “took”: 11, “timed_out”: false, “_shards”: { “total”: 5, “successful”: 5, “failed”: 0 }, “hits”: { “total”: 3, “max_score”: 3.603062, “hits”: [ { “_index”: “my_index”, “_type”: “my_type”, “_id”: “2”, “_score”: 3.603062, “_source”: { “text”: “16岁少女为结婚“变”22岁 7年后想离婚被法院拒绝” } }, { “_index”: “my_index”, “_type”: “my_type”, “_id”: “4”, “_score”: 1.3862944, “_source”: { “text”: “女人对护肤品比对男票好？网友神怼” } }, { “_index”: “my_index”, “_type”: “my_type”, “_id”: “1”, “_score”: 0.2699054, “_source”: { “text”: “男子偷上万元发红包求交女友 被抓获时仍然单身” } } ] }}`]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-63-误拼写时的fuzzy模糊搜索]]></title>
    <url>%2F2019%2F01%2F04%2FElasticsearch-63-%E8%AF%AF%E6%8B%BC%E5%86%99%E6%97%B6%E7%9A%84fuzzy%E6%A8%A1%E7%B3%8A%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[fuzzy搜索我们在搜索的时候,可能会出现单词误拼写的情况,举个例子,有两个documentdocument1: hello worlddocument2: hello java这时候搜索请求误写成了hallo world, 我们期望的结果是全查询出来,但是hallo world是匹配不上doc2的,那么用fuzzy技术,可以将拼写错误的搜索文本进行纠正,纠正以后去尝试匹配索引中的数据. 准备数据删除之前用的my_index,然后执行以下添加1234567POST /my_index/my_type/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 1 &#125;&#125;&#123; &quot;text&quot;: &quot;Surprise me!&quot;&#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 2 &#125;&#125;&#123; &quot;text&quot;: &quot;That was surprising.&quot;&#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 3 &#125;&#125;&#123; &quot;text&quot;: &quot;I wasn&apos;t surprised.&quot;&#125; 示例如果我们的搜索不用fuzzy,就用之前的match直接搜索surprize, 这样呢是搜索不出来的,然后使用fuzzy搜索1234567891011GET my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;fuzzy&quot;: &#123; &quot;text&quot;: &#123; &quot;value&quot;: &quot;surprize&quot;, &quot;fuzziness&quot;: 2 &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233&#123; &quot;took&quot;: 50, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 0.22585157, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.22585157, &quot;_source&quot;: &#123; &quot;text&quot;: &quot;Surprise me!&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.1898702, &quot;_source&quot;: &#123; &quot;text&quot;: &quot;I wasn&apos;t surprised.&quot; &#125; &#125; ] &#125;&#125; fuzziness,这个参数的意思是,你的搜索文本最多可以纠正几个字母去跟你的数据匹配,不设置的话,默认就是2 上面的结果中返回了两条数据第一条中,只要将surprize中的z换成s就匹配到了,只要纠正一个字母就可以了,在我们设置的fuzziness范围内的第二条中,需要将surprize中的z换成s,然后末尾加个d,纠正了两次,也在fuzziness范围内的 没有查询出来的内容是surprising,这个需要把z变成s,去掉e,再加上ing,需要5次才可以匹配到,所以没返回, 但是将fuzziness设置成5 之后,依然没用,是因为es中有最大纠正次数的限制 改进上面这种搜索是不常用的,常用的会直接在match中设置一个 fuzziness属性,值为AUTO就可以了,如下123456789101112GET my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;text&quot;: &#123; &quot;query&quot;: &quot;SURPIZE ME&quot;, &quot;operator&quot;: &quot;and&quot;, &quot;fuzziness&quot;: &quot;AUTO&quot; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324&#123; &quot;took&quot;: 15, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.44248468, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.44248468, &quot;_source&quot;: &#123; &quot;text&quot;: &quot;Surprise me!&quot; &#125; &#125; ] &#125;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-62-用function_score自定义相关度分数算法]]></title>
    <url>%2F2019%2F01%2F04%2FElasticsearch-62-%E7%94%A8function-score%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9B%B8%E5%85%B3%E5%BA%A6%E5%88%86%E6%95%B0%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[function_score我们可以自定义一个function_score函数,自己将某个field的值,跟es内置算出来的分数进行运算,然后由自己制定的field来进行分数的增强 准备测试数据给所有帖子增加follower数量1234567891011POST /forum/article/_bulk&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;1&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;follower_num&quot; : 5&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;2&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;follower_num&quot; : 10&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;3&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;follower_num&quot; : 25&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;4&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;follower_num&quot; : 3&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;5&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;follower_num&quot; : 60&#125; &#125; 案例将对帖子搜索得到的分数,和follower_num进行运算,由follower_num在一定程度上增强帖子的分数 比如我们有一个这样的搜索请求123456789GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;java spark&quot;, &quot;fields&quot;: [&quot;content&quot;,&quot;title&quot;] &#125; &#125;&#125; 然后现在用function_score来对分数进行增强1234567891011121314151617181920GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;function_score&quot;: &#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;java spark&quot;, &quot;fields&quot;: [&quot;content&quot;,&quot;title&quot;] &#125; &#125;, &quot;field_value_factor&quot;: &#123; &quot;field&quot;: &quot;follower_num&quot;, &quot;modifier&quot;: &quot;log1p&quot;, &quot;factor&quot; : 0.5 &#125;, &quot;boost_mode&quot;: &quot;sum&quot;, &quot;max_boost&quot;: 2 &#125; &#125;&#125; 先看一下这段搜索请求,是在之前的query的下一层包了一层function_score. 参数详解field_value_factorfield_value_factor中,如果只有field,那么会将每个doc的分数都乘以follower_num,如果有的doc的follower_num是0,那么分数也会变为0,效果不好,因此一般还会设置一个modifier属性,加一个log1p函数,加了这个函数以后,公式会变为: new_score = old_score log(1 + number_of_votes),这样算出来的分数是比较合理的后面还有个factor参数,可以进一步影响分数,公式会变为new_score = old_score log(1 + factor * number_of_votes) boost_mode可以决定分数与指定字段是如何计算的,默认的是乘法(multiply), 也可以指定 sum,min,max,replace max_boost限制计算出来的分数不要超过max_boost指定的值,但是在新版的es中作用不大]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-61-常见相关度分数优化方法]]></title>
    <url>%2F2019%2F01%2F03%2FElasticsearch-61-%E5%B8%B8%E8%A7%81%E7%9B%B8%E5%85%B3%E5%BA%A6%E5%88%86%E6%95%B0%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[上文中,说了整个es的相关度评分的算法,本文将使用四种常见的方法来优化相关度分数 query-time boost就是之前说过的给某一个查询增加权重,语法如下12345678910111213141516171819202122GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;java spark&quot;, &quot;boost&quot;:2 &#125; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java spark&quot; &#125; &#125; ] &#125; &#125;&#125; 给哪个match增加boost,哪个match的权重就越高,相关度评分就越高 重构查询结构比如说一个搜索是这样的1234567891011121314151617181920212223242526272829GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;spark&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;solution&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;beginner&quot; &#125; &#125; ] &#125; &#125;&#125; 这样的话,这四个match的权重是一样的,如下这样重构1234567891011121314151617181920212223242526272829303132333435GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;spark&quot; &#125; &#125;, &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;solution&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;beginner&quot; &#125; &#125; ] &#125; &#125; ] &#125; &#125;&#125; 把后面的两个查询放到了一个bool中,这样的话, 下面这个bool中的两个match的权重和上面的一个match的权重是一样的 在新版的es中,这样重构查询对分数的影响越来越小了,一般不用也可以 negative boost假如说我们要搜索包含java,不包含spark的时候, 有spark的内容是不会出来的,那么如果我们想要的结果是包含java的排在前面,包含spark的尽量排在后面,而不是直接排除掉来看一下语法123456789101112131415161718GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;boosting&quot;: &#123; &quot;positive&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java&quot; &#125; &#125;, &quot;negative&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;spark&quot; &#125; &#125;, &quot;negative_boost&quot;: 0.2 &#125; &#125;&#125; 这样的话包含了negative term的doc,分数会乘以negative_boost,使得分数降低 constant_score如果我们就不需要相关度评分的话,直接使用constant_score就可以了 所有的doc的分数都是1,就没有评分的概念了123456789101112131415161718192021222324252627GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;constant_score&quot;: &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;java&quot; &#125; &#125; &#125; &#125;, &#123; &quot;constant_score&quot;: &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;java&quot; &#125; &#125; &#125; &#125; ] &#125; &#125;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-60-深入揭秘lucene的相关度分数算法]]></title>
    <url>%2F2019%2F01%2F03%2FElasticsearch-60-%E6%B7%B1%E5%85%A5%E6%8F%AD%E7%A7%98lucene%E7%9A%84%E7%9B%B8%E5%85%B3%E5%BA%A6%E5%88%86%E6%95%B0%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[上文中说了TF/IDF算法,那么在底层的Lucene中TF/IDF算法的完整的公式是什么 Boolean model就是我们之前有说过的,一个match会被转换为bool的组合查询,比如说123&quot;match&quot;:&#123; &quot;query&quot;:&quot;hello world&quot;&#125; 会被转换为:1234567891011121314&quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;hello&quot; &#125; &#125;, &#123; &quot;natch&quot;: &#123; &quot;title&quot;: &quot;world&quot; &#125; &#125; ]&#125; 普通multivalue搜索,转换为bool搜索就是boolean model lucene practical scoring functionpractical scoring function,来计算一个query对一个doc的分数的公式,该函数会使用一个公式来计算123456789score(q,d) = queryNorm(q) · coord(q,d) · ∑ ( tf(t in d) · idf(t)2 · t.getBoost() · norm(t,d) ) (t in q) score(q,d) is the relevance score of document d for query q.这个公式的最终结果,就是说是一个query(叫做q),对一个doc(叫做d)的最终的总评分 queryNormqueryNorm(q) is the query normalization factor (new).queryNorm,是用来让一个doc的分数处于一个合理的区间内,不要太离谱,举个例子,一个doc分数是10000,一个doc分数是0.1,相差太大,不是很好 coordcoord(q,d) is the coordination factor (new).简单来说,就是对更加匹配的doc,进行一些分数上的成倍的奖励 ∑ (t in q)∑ :是求和的意思query中每个term对doc的分数,进行求和,多个term对一个doc的分数,组成一个vector space,就在这一步,要进行TF/IDF算法 tf(t in d)tf(t in d) is the term frequency for term t in document d.就是计算单个term对于doc的分数 idf(t)idf(t) is the inverse document frequency for term t.进行idf计算 norm(t,d)norm(t,d) is the field-length norm, combined with the index-time field-level boost, if any. (new). queryNorm详解queryNorm = 1 / √sumOfSquaredWeightssumOfSquaredWeights = 所有term的IDF分数之和,开一个平方根,然后做一个平方根分之1主要是为了将分数进行规范化, 开平方根,首先数据就变小了, 然后还用1去除以这个平方根,分数就会很小,比如 1.几 或者零点几分数就不会出现几万,几十万,那样的离谱的分数 coord详解奖励那些匹配更多字符的doc更多的分数举个例子:document1 包含 hello → score: 1.5document2 包含 hello world → score: 3.0document3 包含 hello world java → score: 4.5 把计算出来的总分数 * 匹配上的term数量 / 总的term数量,让匹配不同term/query数量的doc,分数之间拉开差距 document1 包含 hello → score: 1.5 1 / 3 = 0.5document2 包含 hello world → score: 3.0 2 / 3 = 2.0document3 包含 hello world java → score: 4.5 * 3 / 3 = 4.5 field level boost就是之前说过的搜索的权重计算]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-59-深入揭秘TF&IDF算法以及向量空间模型算法]]></title>
    <url>%2F2019%2F01%2F03%2FElasticsearch-59-%E6%B7%B1%E5%85%A5%E6%8F%AD%E7%A7%98TF-IDF%E7%AE%97%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[一个搜索请求过来,是怎样进行打分的呢 boolean model首先一个搜索请求过来的时候,会先进行过滤,过滤出包含指定term的doc,这个过程就是boolean model 举个例子,查询的是hello world,首先会先过滤出来包含hello和包含world和包含hello world的数据,过滤出来的这些document是不会去打分数的.为了减少后续计算的document的数量,提升性能 TF/IDF算法第二步进行TF/IDF计算,之前有详细说过这部分的计算,但是这两个是计算的单个term在doc里面的分数. 比如有两个documentdoc1:java is my favourite programming language, hello world !!!doc2:hello java, you are very good, oh hello world!!!还是搜索hello world 先计算hello对于doc1的分数TF(term frequency)算法:找到hello在doc1中出现了几次,会根据出现的次数给个分数,一个term在一个doc中出现的次数越多,给的相关度评分就越高 IDF(inversed document frequency)算法:找到hello在所有的doc中出现的次数,一个term在所有的doc中,出现的次数越多,评分越低. 然后是length norm,hello搜索的那个field的长度,filed长度越长,给的相关度评分越低,field长度越短,给的相关度评分越高. 最后,会将hello这个term对doc1的分数综合TF IDF length norm,计算出来一个综合的分数 计算world的方法同样的 上面我们可以看出计算的只是单个term对于doc的分数,但是最后需要给这个query对于doc的总的分数,就是第三步vector space model vector space model计算多个term对于一个doc的总分数比如说上面的hello world,es会根据hello world在所有的doc中的评分情况,计算出一个总的 query vector(query向量) 比如说,hello这个term,给的基于所有doc的一个评分就是2,world这个term,给的基于所有doc的一个评分就是5,那么这个向量就是[2,5] 再举个例子,现在有3个doc,分别是doc1:包含hellodoc2:包含worlddoc3:包含hello,world查询的还是hello world,会拿每个term计算出一个分数来,hello有一个分数,world有一个分数,再拿所有的term的分数组成一个doc vector那么doc1就是[2,0],doc2是[0,5],doc3是[2,5] 把这几个分数画在一个坐标中就是:然后会取每个doc vector对于query vector的弧度,给出每个doc对于多个term的总分数,弧度越大,分数越低,弧度越小,分数越高 如果term不止两个,是多个的话,就无法用图表示了,就是线性代数来计算]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-58-通过ngram分词机制实现index-time搜索联想]]></title>
    <url>%2F2019%2F01%2F03%2FElasticsearch-58-%E9%80%9A%E8%BF%87ngram%E5%88%86%E8%AF%8D%E6%9C%BA%E5%88%B6%E5%AE%9E%E7%8E%B0index-time%E6%90%9C%E7%B4%A2%E8%81%94%E6%83%B3%2F</url>
    <content type="text"><![CDATA[什么是ngram举个例子,现在有一个quick这个单词,在ngram的长度是1的时候,quick这个单词会被拆分为ngram length = 1的情况下被拆分为q u i c kngram length = 2的情况下被拆分为qu ui ic ckngram length = 3的情况下被拆分为qui uic ickngram length = 4的情况下被拆分为quic uickngram length = 5的情况下被拆分为quick 如上,被拆分出来的每一个词就是一个ngram. edge ngram本文将使用edge ngram,实现搜索联想功能那什么是edge ngram? 举例,还是quick这个单词,使用edge ngram的话,会被拆分为qququiquicquick 举例说明假设有一个document的值是hello world ,然后劲歌edge ngram拆分hhehelhellhello wwoworworlworld 然后我们去搜索 hello w的时候,会用hello 和 w分别去匹配然后返回. 这歌搜索联想跟我们之前的搜索联想不同,这里搜索的时候,不用再根据一个前缀去扫描整个倒排索引了,而是拿前缀去倒排索引里面去匹配即可,类似于match这种全文检索 实战案例删除之前的my_index,然后重新创建索引,需要设置一下分词器123456789101112131415161718192021222324PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;filter&quot;: &#123; &quot;autocomplete_filter&quot;:&#123; &quot;type&quot;:&quot;edge_ngram&quot;, &quot;min_gram&quot;:1, &quot;max_gram&quot;:20 &#125; &#125;, &quot;analyzer&quot;: &#123; &quot;autocomplete&quot;:&#123; &quot;type&quot;:&quot;custom&quot;, &quot;tokenizer&quot;: &quot;standard&quot;, &quot;filter&quot;: [ &quot;lowercase&quot;, &quot;autocomplete_filter&quot; ] &#125; &#125; &#125; &#125;&#125; 创建完成之后,测试一下这个分词器12345GET /my_index/_analyze&#123; &quot;analyzer&quot;: &quot;autocomplete&quot;, &quot;text&quot;: &quot;quick brown&quot;&#125; 返回值:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;q&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 5, &quot;type&quot;: &quot;word&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;qu&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 5, &quot;type&quot;: &quot;word&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;qui&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 5, &quot;type&quot;: &quot;word&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;quic&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 5, &quot;type&quot;: &quot;word&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;quick&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 5, &quot;type&quot;: &quot;word&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;b&quot;, &quot;start_offset&quot;: 6, &quot;end_offset&quot;: 11, &quot;type&quot;: &quot;word&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;br&quot;, &quot;start_offset&quot;: 6, &quot;end_offset&quot;: 11, &quot;type&quot;: &quot;word&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;bro&quot;, &quot;start_offset&quot;: 6, &quot;end_offset&quot;: 11, &quot;type&quot;: &quot;word&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;brow&quot;, &quot;start_offset&quot;: 6, &quot;end_offset&quot;: 11, &quot;type&quot;: &quot;word&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;brown&quot;, &quot;start_offset&quot;: 6, &quot;end_offset&quot;: 11, &quot;type&quot;: &quot;word&quot;, &quot;position&quot;: 1 &#125; ]&#125; 分词器没问题以后,手动设定mapping映射12345678910PUT /my_index/_mapping/my_type&#123; &quot;properties&quot;: &#123; &quot;title&quot;:&#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;autocomplete&quot;, &quot;search_analyzer&quot;: &quot;standard&quot; &#125; &#125;&#125; 然后往里加几条测试数据1234567891011121314151617181920212223242526272829POST /my_index/my_type/1&#123; &quot;title&quot;:&quot;hello world&quot;&#125;POST /my_index/my_type/2&#123; &quot;title&quot;:&quot;hello we&quot;&#125;POST /my_index/my_type/3&#123; &quot;title&quot;:&quot;hello win&quot;&#125;POST /my_index/my_type/4&#123; &quot;title&quot;:&quot;hello wind&quot;&#125;POST /my_index/my_type/5&#123; &quot;title&quot;:&quot;hello dog&quot;&#125;POST /my_index/my_type/6&#123; &quot;title&quot;:&quot;hello cat&quot;&#125; 最后来搜索测试一下,搜索hello w12345678GET /my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;title&quot;: &quot;hello w&quot; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&#123; &quot;took&quot;: 29, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 4, &quot;max_score&quot;: 0.8361317, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.8361317, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;hello we&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.8361317, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;hello wind&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.8271048, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;hello world&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.797104, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;hello win&quot; &#125; &#125; ] &#125;&#125; 这里如果使用的是match的话,只有hello的也会查询出来,全文检索,分数比较低. 推荐使用match_phrase,要求每个term都有,而且position刚好靠着1位,符合我们的期望]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-57-搜索联想]]></title>
    <url>%2F2019%2F01%2F02%2FElasticsearch-57-%E6%90%9C%E7%B4%A2%E8%81%94%E6%83%B3%2F</url>
    <content type="text"><![CDATA[准备工作删除之前的my_index1DELETE /my_index 然后再添加几条测试用的数据1234567891011121314151617181920212223242526272829POST /my_index/my_type/1&#123; &quot;title&quot;:&quot;hello world&quot;&#125;POST /my_index/my_type/2&#123; &quot;title&quot;:&quot;hello we&quot;&#125;POST /my_index/my_type/3&#123; &quot;title&quot;:&quot;hello win&quot;&#125;POST /my_index/my_type/4&#123; &quot;title&quot;:&quot;hello wind&quot;&#125;POST /my_index/my_type/5&#123; &quot;title&quot;:&quot;hello dog&quot;&#125;POST /my_index/my_type/6&#123; &quot;title&quot;:&quot;hello cat&quot;&#125; 搜索联想比如说我们在Google的搜索框中输入了elasti,搜索框下面可能会出来elasticsearch, elasticsearch权威指南等这些信息. 我们可以用match_phrase_prefix来实现这个效果,原理和match_phrase类似,唯一的区别就是会把最后一个term作为前缀去搜索. 比如我们搜索hello w, hello就是去进行match,搜索对应的document,最后一个term是w,w就会被作为前缀,去扫描整个倒排索引,找到所有的w开头的document,然后找到既包含hello又包含w开头的document,根据你的slop去计算,看在slop范围内,能不能让hello w,正好跟document中的hello和w开头的单词的position相匹配 指定slop的话,也是只会将最后一个term作为前缀 max_expansions默认情况下,前缀搜索要扫描所有倒排索引中的document,按时这样性能太差了,可以使用max_expansions指定prefix最多匹配多少个document,超过这个数量就不继续匹配了,限定性能 语法123456789101112GET /my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase_prefix&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;hello w&quot;, &quot;slop&quot;:10, &quot;max_expansions&quot;: 10 &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 4, &quot;max_score&quot;: 1.8798604, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1.8798604, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;hello we&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 1.8798604, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;hello wind&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.51623213, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;hello world&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.51623213, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;hello win&quot; &#125; &#125; ] &#125;&#125; 查询出来的都是包含hello和w是前缀的数据 在实际中,这种搜索也是尽量不要用,因为最后一个前缀始终要去扫描大量的索引,性能可能会很差]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-56-前缀搜索 通配符搜索 正则搜索]]></title>
    <url>%2F2019%2F01%2F02%2FElasticsearch-56-%E5%89%8D%E7%BC%80%E6%90%9C%E7%B4%A2-%E9%80%9A%E9%85%8D%E7%AC%A6%E6%90%9C%E7%B4%A2-%E6%AD%A3%E5%88%99%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[准备工作首先我们先手动建立一个index,再添加几条数据.创建index123456789101112PUT /my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;:&#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125;&#125; 添加几条测试数据1234567891011121314POST /my_index/my_type/1&#123; &quot;title&quot;:&quot;C3D0-KD345&quot;&#125;POST /my_index/my_type/2&#123; &quot;title&quot;:&quot;C3K5-DFG65&quot;&#125;POST /my_index/my_type/3&#123; &quot;title&quot;:&quot;C4I8-UI365&quot;&#125; 前缀搜索C3D0-KD345C3K5-DFG65C4I8-UI365上面我们添加了title是这几个的数据,然后我现在要搜索以C3开头的数据,那么就是要搜索id是1和2的这两条数据,然后看下语法12345678910GET /my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;prefix&quot;: &#123; &quot;title&quot;: &#123; // 要搜索的filed &quot;value&quot;: &quot;C3&quot; // 前缀的值 &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233&#123; &quot;took&quot;: 10, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;C3K5-DFG65&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;C3D0-KD345&quot; &#125; &#125; ] &#125;&#125; 返回结果就是id是1和2的两个document 前缀搜索的原理看下上面的返回值两个结果的_score都是1,prefix query是不计算relevance score的,与prefix filter唯一的区别就是,filter会cache bitset前缀搜索会去扫描整个倒排索引,前缀越短,要处理的doc就越多,性能越差,应该尽可能的用长前缀搜索 举个例子,现在有3个document,内容分别是:C3-D0-KD345C3-K5-DFG65C4-I8-UI365注意,和上面添加的数据是不一样的. 如果我们使用match全文检索的话,每个字符串是都需要被分词.结果是c3d0kd345k5dfg65c4i8ui365被分成了这几个词,这时候我们去查询c3, 去扫描倒排索引,一旦扫描到c3,就可以停了,已经拿到了所有包含c3的document list了,所以说match的性能往往是很高的 如果不分词,去使用前缀搜索呢,还是查询C3,先找到了C3-D0-KD345,然后还是要继续往下搜,因为后面还可能有其他的前缀是c3的字符串,扫描到了一个前缀匹配的term,不能停,必须继续搜索,直到扫描完整个的倒排索引,才能结束.所以,前缀搜索的性能是很差的 那为什么不使用match搜索,而用前缀搜索呢,因为实际的场景中,可能有些场景是全文检索解决不了的.再举一个例子,比如说,有以下3个document,值分别是:C3D0-KD345C3K5-DFG65C4I8-UI365这时候,分词的结果可能就是c3d0kd345… 这种情况下,用c3 match扫描整个倒排索引,是找不到的. 只能用prefix 前缀搜索 通配符搜索? 代表任意一个字符* 代表0个或任意多个字符 比如,表达式是C?K*5,就是以C开头后面可以有任意一个字符,然后接着是K,最后以5结尾看下语法:12345678910GET /my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;wildcard&quot;: &#123; &quot;title&quot;: &#123; &quot;value&quot;: &quot;C?K*5&quot; &#125; &#125; &#125;&#125; 跟前缀搜索类似,功能更强大,但是性能一样很差,也是需要扫描整个倒排索引 正则搜索[0-9] 代表指定范围内的数字[a-z] 代表指定范围内的字母. 代表一个字符+ 代表前面的正则表达式可以出现一次或多次 比如正则表达式是 C[0-9].+,就是以C开头后面有一个0-9以内的数字,然后后面可以有多个字符 语法12345678GET /my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;regexp&quot;:&#123; &quot;title&quot;:&quot;C[0-9].+&quot; &#125; &#125;&#125; wildcard和regexp,与prefix原理一致,都会扫描整个索引,性能很差 在实际的应用中,这几种搜索能不用就尽量不用,因为性能很差]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-55-使用rescoring机制优化近似匹配的性能]]></title>
    <url>%2F2019%2F01%2F02%2FElasticsearch-55-%E4%BD%BF%E7%94%A8rescoring%E6%9C%BA%E5%88%B6%E4%BC%98%E5%8C%96%E8%BF%91%E4%BC%BC%E5%8C%B9%E9%85%8D%E7%9A%84%E6%80%A7%E8%83%BD%2F</url>
    <content type="text"><![CDATA[match 和 phrase match(proximity match)的区别match:只要简单的匹配到了一个term,就可以将term对应的doc返回. phrase match:首先扫描到所有term的document list,然后对每个document都计算term position,是否符合指定的范围,slop需要进行复杂的运算,来判断是否能通过slop移动,匹配一个document match query的性能比phrase match和proximity match要高很多,因为后两者都需要计算position的距离,match query比phrase match的性能要高10倍, 比proximity match的性能要高20倍.但是别太担心,因为es的性能一般都在毫秒级别,match query一般就在几毫秒,或者几十毫秒,而phrase match和proximity match 的性能在几十毫秒到几百毫秒之间,也是可以接受的 优化proximity match的性能主要思路就是:用match匹配先过滤出需要的数据,然后在用proximity match来根据距离提高doc的分数,同时proximity match只针对每个shard的分数排名的前n个document起作用,来重新调整他们的分数,这个过程称之为rescoring,重计分.因为一般的用户都会分页查询,只会看到前几页的数据,所以不需要对所有的结果都进行proximity match操作 举例比如说有一个查询,match出来了1000个document,默认的情况下,proximity match需要对全部的document都进行一次运算,判断slop移动是否能够匹配上,然后去贡献自己的分数,但是很多情况下,match出来的这1000个doc,用户是不会全部都看到的,可能最多只会看前五页,一页10条的话,我们只需要第前50个doc去进行slop移动匹配,去贡献自己的分数即可,不需要对全部的1000个doc都去进行计算和贡献分数. rescore重打分及其语法就上面的例子来说,match的1000个doc其实已经有了一个分数了,proximity match前50个doc进行rescore重打分即可,让前50个doc,term距离越近的,排在越前面 语法123456789101112131415161718192021GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java spark&quot; &#125; &#125;, &quot;rescore&quot;:&#123; &quot;window_size&quot;:50, &quot;query&quot;:&#123; &quot;rescore_query&quot;:&#123; &quot;match_phrase&quot;:&#123; &quot;content&quot;:&#123; &quot;query&quot;:&quot;java spark&quot;, &quot;slop&quot;:50 &#125; &#125; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-54-搜索实现召回率和精准度平衡]]></title>
    <url>%2F2018%2F12%2F27%2FElasticsearch-54-%E6%90%9C%E7%B4%A2%E5%AE%9E%E7%8E%B0%E5%8F%AC%E5%9B%9E%E7%8E%87%E5%92%8C%E7%B2%BE%E5%87%86%E5%BA%A6%E5%B9%B3%E8%A1%A1%2F</url>
    <content type="text"><![CDATA[首先需要了解两个概念,召回率和精准度 召回率比如搜索一个java spark,总共有100个document,能返回多少个document作为结果,这个就是召回率(recall) 精准度比如搜索一个java spark,能不能尽可能让包含java spark这个短语的,或者是java和spark离的很近的document,排在最前面,这个就是精准度(precision) 混合使用match和近似匹配直接使用match phrase短语搜索,会导致所有term都在document的filed中出现,而且距离要在slop规定的范围内,才能匹配的到 近似匹配的时候,召回率比较低,因为精准度太高了. 但是有时候我们可能希望是匹配到几个term中的部分,就可以作为返回结果返回回来,这样可以提高召回率,同时我们也希望用上match_phrase根据距离提升分数的功能,让几个term距离越近分数就越高,优先返回,也就是优先满足召回率 比如说,优先提升召回率就是:搜索java spark,包含java的也返回,包含spark的也返回,包含java和spark的也返回.同时兼顾精准度:就是说包含java和spark,同时java和spark离得越近的document排在前面 此时,我们可以使用bool组合match query和match_phrase query一起,来实现上述效果 实战案例构建一个搜索请求123456789101112131415161718192021222324GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java spark&quot; &#125; &#125; ], &quot;should&quot;: [ &#123; &quot;match_phrase&quot;: &#123; &quot;content&quot;: &#123; &quot;query&quot;: &quot;java spark&quot;, &quot;slop&quot;:50 &#125; &#125; &#125; ] &#125; &#125;&#125; 看一下上面这个请求,must里面可能返回的是包含java或spark或java spark,同时包含java spark的靠前,但是没法区分距离,也许距离很近但是排在了后面should里面呢,在slop以内,如果java spark能匹配上一个doc,那么就会对doc贡献自己的relevance score,如果java和spark靠的越近,那么分数就越高 先来试一下不加近似匹配的搜索1234567891011121314GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java spark&quot; &#125; &#125; ] &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263&#123; &quot;took&quot;: 31, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 0.68640786, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.68640786, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 50, &quot;title&quot;: &quot;this is java blog&quot;, &quot;content&quot;: &quot;i think java is the best programming language&quot;, &quot;sub_title&quot;: &quot;learned a lot of course&quot;, &quot;author_first_name&quot;: &quot;Smith&quot;, &quot;author_last_name&quot;: &quot;Williams&quot;, &quot;new_author_last_name&quot;: &quot;Williams&quot;, &quot;new_author_first_name&quot;: &quot;Smith&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;5&quot;, &quot;_score&quot;: 0.68324494, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot;: 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2018-12-03&quot;, &quot;tag&quot;: [ &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10, &quot;title&quot;: &quot;this is spark blog&quot;, &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java spark&quot;, &quot;sub_title&quot;: &quot;haha, hello world&quot;, &quot;author_first_name&quot;: &quot;Tonny&quot;, &quot;author_last_name&quot;: &quot;Peter Smith&quot;, &quot;new_author_last_name&quot;: &quot;Peter Smith&quot;, &quot;new_author_first_name&quot;: &quot;Tonny&quot; &#125; &#125; ] &#125;&#125; id是2的doc排在了id是5的前面.然后再加上近似匹配123456789101112131415161718192021222324GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java spark&quot; &#125; &#125; ], &quot;should&quot;: [ &#123; &quot;match_phrase&quot;: &#123; &quot;content&quot;: &#123; &quot;query&quot;: &quot;java spark&quot;, &quot;slop&quot;:50 &#125; &#125; &#125; ] &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1.258609, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;5&quot;, &quot;_score&quot;: 1.258609, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot;: 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2018-12-03&quot;, &quot;tag&quot;: [ &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10, &quot;title&quot;: &quot;this is spark blog&quot;, &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java spark&quot;, &quot;sub_title&quot;: &quot;haha, hello world&quot;, &quot;author_first_name&quot;: &quot;Tonny&quot;, &quot;author_last_name&quot;: &quot;Peter Smith&quot;, &quot;new_author_last_name&quot;: &quot;Peter Smith&quot;, &quot;new_author_first_name&quot;: &quot;Tonny&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.68640786, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 50, &quot;title&quot;: &quot;this is java blog&quot;, &quot;content&quot;: &quot;i think java is the best programming language&quot;, &quot;sub_title&quot;: &quot;learned a lot of course&quot;, &quot;author_first_name&quot;: &quot;Smith&quot;, &quot;author_last_name&quot;: &quot;Williams&quot;, &quot;new_author_last_name&quot;: &quot;Williams&quot;, &quot;new_author_first_name&quot;: &quot;Smith&quot; &#125; &#125; ] &#125;&#125; 可以看到id是5的doc排在了id是2的doc的前面]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-53-基于slop参数实现近似匹配]]></title>
    <url>%2F2018%2F12%2F27%2FElasticsearch-53-%E5%9F%BA%E4%BA%8Eslop%E5%8F%82%E6%95%B0%E5%AE%9E%E7%8E%B0%E8%BF%91%E4%BC%BC%E5%8C%B9%E9%85%8D%2F</url>
    <content type="text"><![CDATA[slop参数比如我们现在有一个搜索请求如下:1234567891011GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;java spark&quot;, &quot;slop&quot;:1 &#125; &#125; &#125;&#125; slop的作用是什么呢?query string 中的几个term,要经过几次移动才能与一个document匹配,移动的次数就是slop 举例说明现有一个document content的值是hello world, java is very good, spark is also very good.我们如果用之前说的match_phrase搜索java spark的话是搜索不到的 但是如果我们指定了slop,那么就允许java spark进行移动,来尝试与document进行匹配,比如就上面这个句子中要去匹配java spark如图,spark向后进行了三次移动后,就能匹配到了这个document了.slop的含义,不仅仅是说一个query string terms移动几次跟一个doc匹配上,而是说一个query string terms 最多可以移动几次去尝试跟一个doc匹配上就上面这个例子而言slop的值只要大于等于3 就可以匹配的到,如果设置的是2,是匹配不到的 再来看一个例子:1234567891011GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;content&quot;: &#123; &quot;query&quot;: &quot;spark data&quot;, &quot;slop&quot;: 3 &#125; &#125; &#125;&#125; 执行搜索,返回的这个document的content值是:spark is best big data solution based on scala ,an programming language similar to java spark搜索关键词是 spark data, content中spark 和 data中间有3个词, 所以也是只要移动3次就可以匹配的到,所以这个slop最小设置成3就可以匹配的到 那么如果是搜索的data spark 那要怎么移动呢1234567891011GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;content&quot;: &#123; &quot;query&quot;: &quot;data spark&quot;, &quot;slop&quot;: 5 &#125; &#125; &#125;&#125; 看下上面这个图,前两次移动是data和spark交换了位置,然后再进行3次移动后就匹配到了,所以这个请求的slop就是最小是5 slop搜索下,关键词离的越近,relevance score就会越高,再来看个案例.搜索关键词是java best1234567891011GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;content&quot;: &#123; &quot;query&quot;: &quot;java best&quot;, &quot;slop&quot;:15 &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 0.65380025, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.65380025, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 50, &quot;title&quot;: &quot;this is java blog&quot;, &quot;content&quot;: &quot;i think java is the best programming language&quot;, &quot;sub_title&quot;: &quot;learned a lot of course&quot;, &quot;author_first_name&quot;: &quot;Smith&quot;, &quot;author_last_name&quot;: &quot;Williams&quot;, &quot;new_author_last_name&quot;: &quot;Williams&quot;, &quot;new_author_first_name&quot;: &quot;Smith&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;5&quot;, &quot;_score&quot;: 0.07111243, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot;: 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2018-12-03&quot;, &quot;tag&quot;: [ &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10, &quot;title&quot;: &quot;this is spark blog&quot;, &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java spark&quot;, &quot;sub_title&quot;: &quot;haha, hello world&quot;, &quot;author_first_name&quot;: &quot;Tonny&quot;, &quot;author_last_name&quot;: &quot;Peter Smith&quot;, &quot;new_author_last_name&quot;: &quot;Peter Smith&quot;, &quot;new_author_first_name&quot;: &quot;Tonny&quot; &#125; &#125; ] &#125;&#125; 看先这两个的_score分数, 两个terms的距离越近,分数就越高 其实,加了slop的phrase match,就是proximity match,近似匹配]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-52-phrase match搜索及原理]]></title>
    <url>%2F2018%2F12%2F26%2FElasticsearch-52-phrase-match%E6%90%9C%E7%B4%A2%E5%8F%8A%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[近似匹配假设现在有两个document,他们的content的值分别是:java is my favourite programming language, and I also think spark is a very good big data system.java spark are very related, because scala is spark’s programming language and scala is also based on jvm like java. 用一个match query去搜索java spark12345&#123; &quot;match&quot;:&#123; &quot;content&quot;:&quot;java spark&quot; &#125;&#125; match query的话,只能搜索到包含java或者包含spark的document,但是不知道java和spark是不是离的很近 包含java或者包含spark的document都会被返回回来.我们其实并不知道哪个document中java和spark距离的比较近.如果我们是希望搜索java和spark,中间没有任何其他的字符,那么这时候用match匹配做全文检索肯定就不行了. 如果说我们要尽量让java和spark离的很近的document优先返回,要给他一个更高的relevance score,这就涉及到了proximity match 近似匹配. 如果现在有两个需求: java spark,要连在一起,中间没有任何字符 java spark,不需要连在一起,但是这两个单词靠的越近,doc的分数越高,排名越靠前 要实现上面两个需求,用match做全文检索是搞不定的. 必须得用proximity match,近似匹配 phrase match(短语匹配),proximity match(近似匹配)本文主要说的是 phrase match,就是仅仅搜索出java和spark靠在一起的那些doc,比如有个doc,是java use’d spark这样是不行的,必须是比如java spark are very good friends,是可以搜索出来的. phrase match: 就是将多个term作为一个短语,一起去搜索,只有包含这个短语的document才会作为返回结果. 案例先用match query全文检索搜索一下java spark12345678GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java spark&quot; &#125; &#125; &#125; 包含java和spark的都被返回来了,不是我们想要的结果,然后修改id是5的这个document的content,因为现在的数据没有符合我们要求的123456POST /forum/article/5/_update&#123; &quot;doc&quot;: &#123; &quot;content&quot;:&quot;spark is best big data solution based on scala ,an programming language similar to java spark&quot; &#125;&#125; 然后来用phrase match搜索12345678GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;content&quot;: &quot;java spark&quot; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;took&quot;: 17, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.5753642, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;5&quot;, &quot;_score&quot;: 0.5753642, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot;: 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2018-12-03&quot;, &quot;tag&quot;: [ &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10, &quot;title&quot;: &quot;this is spark blog&quot;, &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java spark&quot;, &quot;sub_title&quot;: &quot;haha, hello world&quot;, &quot;author_first_name&quot;: &quot;Tonny&quot;, &quot;author_last_name&quot;: &quot;Peter Smith&quot;, &quot;new_author_last_name&quot;: &quot;Peter Smith&quot;, &quot;new_author_first_name&quot;: &quot;Tonny&quot; &#125; &#125; ] &#125;&#125; 就是我们刚刚修改的那条数据,只有包含了java spark这个短语的document才返回了,其他的数据不会返回 原理term position比如现在有两个document的content值如下:document1: hello world, java sparkdocument2: hi, spark java 对上面的数据进行分词,然后会记录每个词在每个doc中出现的位置 word term position hello doc1(0) word doc1(1) java doc1(2) doc2(2) spark doc1(3) doc2(1) 我们可以用分词器来看一下12345GET _analyze&#123; &quot;text&quot;: &quot;hello world, java spark&quot;, &quot;analyzer&quot;: &quot;standard&quot;&#125; 返回值:1234567891011121314151617181920212223242526272829303132&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;hello&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 5, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;world&quot;, &quot;start_offset&quot;: 6, &quot;end_offset&quot;: 11, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;java&quot;, &quot;start_offset&quot;: 13, &quot;end_offset&quot;: 17, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 2 &#125;, &#123; &quot;token&quot;: &quot;spark&quot;, &quot;start_offset&quot;: 18, &quot;end_offset&quot;: 23, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 3 &#125; ]&#125; position就是每个词在句子中所在的位置 match_phrase搜索原理:用java spark在上面的两个document中进行搜索java对应的是doc1(2) doc2(2),spark对应的是doc1(3) doc2(1)要求一个doc,必须包含每个term,才能拿出来继续计算 首先看doc1:在document1中 spark的position比java的position大1,java的position是2,spark的position是3,满足条件然后看下doc2:在document2中 java position是2,spark position是1,spark position比java position小1,而不是大1,所以doc2不匹配]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-51-cross-fields搜索问题解决方案]]></title>
    <url>%2F2018%2F12%2F19%2FElasticsearch-51-cross-fields%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[解决方案一:使用copy_to用copy_to可以将多个field组合成一个field. 之前说的问题,其实就是出在了有多个field,那么我们只要把这些field合并成一个field即可,比如搜索一个人名,有first_name和last_name,将这两个field合并成一个full_name就可以解决了 示例首先,创建三个field: new_author_first_name , new_author_last_name , new_author_full_name12345678910111213141516PUT /forum/_mapping/article&#123; &quot;properties&quot;: &#123; &quot;new_author_first_name&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;copy_to&quot;: &quot;new_author_full_name&quot; &#125;, &quot;new_author_last_name&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;copy_to&quot;: &quot;new_author_full_name&quot; &#125;, &quot;new_author_full_name&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125; &#125;&#125; new_author_first_name和new_author_last_name都设置copy_to到new_author_full_name中去,用了这个copy_to语法之后,就可以将多个字段的值拷贝到一个字段中,并建立倒排索引 添加数据1234567891011POST /forum/article/_bulk&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;1&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;new_author_first_name&quot; : &quot;Peter&quot;, &quot;new_author_last_name&quot; : &quot;Smith&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;2&quot;&#125; &#125; &#123; &quot;doc&quot; : &#123;&quot;new_author_first_name&quot; : &quot;Smith&quot;, &quot;new_author_last_name&quot; : &quot;Williams&quot;&#125; &#125; &#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;3&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;new_author_first_name&quot; : &quot;Jack&quot;, &quot;new_author_last_name&quot; : &quot;Ma&quot;&#125; &#125; &#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;4&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;new_author_first_name&quot; : &quot;Robbin&quot;, &quot;new_author_last_name&quot; : &quot;Li&quot;&#125; &#125; &#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;5&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;new_author_first_name&quot; : &quot;Tonny&quot;, &quot;new_author_last_name&quot; : &quot;Peter Smith&quot;&#125; &#125; 添加完毕后,这时候可以去查询一下全部的数据,返现并没有new_author_full_name这个field,因为这个field就类似于之前有说过的 _all元数据,是不在_source中显示的 接着来查询名称是Peter Smith的数据12345678GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;new_author_full_name&quot;: &quot;Peter Smith&quot; &#125; &#125;&#125; 返回值:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 0.62191015, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.62191015, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 50, &quot;title&quot;: &quot;this is java blog&quot;, &quot;content&quot;: &quot;i think java is the best programming language&quot;, &quot;sub_title&quot;: &quot;learned a lot of course&quot;, &quot;author_first_name&quot;: &quot;Smith&quot;, &quot;author_last_name&quot;: &quot;Williams&quot;, &quot;new_author_last_name&quot;: &quot;Williams&quot;, &quot;new_author_first_name&quot;: &quot;Smith&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.51623213, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;hadoop&quot; ], &quot;tag_cnt&quot;: 2, &quot;view_cnt&quot;: 30, &quot;title&quot;: &quot;this is java and elasticsearch blog&quot;, &quot;content&quot;: &quot;i like to write best elasticsearch article&quot;, &quot;sub_title&quot;: &quot;learning more courses&quot;, &quot;author_first_name&quot;: &quot;Peter&quot;, &quot;author_last_name&quot;: &quot;Smith&quot;, &quot;new_author_last_name&quot;: &quot;Smith&quot;, &quot;new_author_first_name&quot;: &quot;Peter&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;5&quot;, &quot;_score&quot;: 0.5063205, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot;: 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2018-12-03&quot;, &quot;tag&quot;: [ &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10, &quot;title&quot;: &quot;this is spark blog&quot;, &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java&quot;, &quot;sub_title&quot;: &quot;haha, hello world&quot;, &quot;author_first_name&quot;: &quot;Tonny&quot;, &quot;author_last_name&quot;: &quot;Peter Smith&quot;, &quot;new_author_last_name&quot;: &quot;Peter Smith&quot;, &quot;new_author_first_name&quot;: &quot;Tonny&quot; &#125; &#125; ] &#125;&#125; 这里的搜索结果还是和之前一样的,因为es的算法的原因,没法实现这个场景,但是copy_to已经把前一节提到的问题解决了 之前的问题一被合并成一个field了,就不存在了,而且这里的查询可以使用minimum_should_match来去长尾,第三个问题Smith和Peter在一个field里面了,所以在所有document中出现的次数是均匀的,不会有极端的偏差 解决方案二:原生cross-fields语法1234567891011GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;Peter Smith&quot;, &quot;fields&quot;: [&quot;author_first_name&quot;,&quot;author_last_name&quot;], &quot;type&quot;: &quot;cross_fields&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125;&#125; 这种方法也可以解决上文提到的那三个问题 cross_fields是要求每个term都必须在任何一个field中出现 这就解决了第一个问题,举个例子:搜索条件还是Peter Smith,按照cross_fields来搜索的话要求Peter必须在author_first_name或author_last_name中出现要求Smith必须在author_first_name或author_last_name中出现Peter Smith可能是横跨在多个field中的,所以必须要求每个term都在某个field中出现,组合起来才能组成我们想要的标识,比如一个完整的人名 原来most-fields搜索的时候,可能像Smith Williams也可能会出现,因为most-fields要求只是任何一个field匹配了就可以,匹配的field越多,分数就越高 第二个问题是most-fields没办法去长尾的问题,用cross_fields的时候,每个term都要求出现,那长尾肯定被干掉了举个例子现在有一个搜索条件是java Hadoop spark 那么这三个term都必须在任何一个filed中出现了比如有的document中,只有一个field中包含一个java,那就被干掉了,作为长尾就没了. 第三个问题,在使用cross-fields查询的时候,es在计算IDF的时候会将每个query在每个filed中的IDF都取出来,取最小值,就不会出现极端情况下的最大值了 举个例子,还是查询Peter SmithSmith,在author_first_name这个field中,在所有document的这个field中,出现的频率很低,导致IDF分数很高;Smith在所有doc的author_last_name field中的频率算出一个IDF分数,因为一般来说last_name中的Smith频率都较高,所以IDF分数是正常的,不会太高;然后对于Smith来说,会取两个IDF分数中较小的那个分数.就不会出现IDF分过高的情况.]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-50-most_fields策略进行cross-fields搜索的弊端]]></title>
    <url>%2F2018%2F12%2F19%2FElasticsearch-50-most-fields%E7%AD%96%E7%95%A5%E8%BF%9B%E8%A1%8Ccross-fields%E6%90%9C%E7%B4%A2%E7%9A%84%E5%BC%8A%E7%AB%AF%2F</url>
    <content type="text"><![CDATA[cross-field搜索就是我们搜索一个唯一标识的时候跨越了多个field,比如一个人,标识是姓名,一个建筑的标识是地址. 姓名可以散落在多个field中,比如first_name和last_name中,地址可以散落在country,province,city中.跨多个field搜索一个标识,就是cross-fields搜索 这个情况下用most-fields搜索就比较合适了,因为best-fields是优先搜索单个field最匹配的结果,cross-fields本身就不是一个field的问题了 案例首先,准备数据1234567891011POST /forum/article/_bulk&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;1&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;author_first_name&quot; : &quot;Peter&quot;, &quot;author_last_name&quot; : &quot;Smith&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;2&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;author_first_name&quot; : &quot;Smith&quot;, &quot;author_last_name&quot; : &quot;Williams&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;3&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;author_first_name&quot; : &quot;Jack&quot;, &quot;author_last_name&quot; : &quot;Ma&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;4&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;author_first_name&quot; : &quot;Robbin&quot;, &quot;author_last_name&quot; : &quot;Li&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;5&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;author_first_name&quot; : &quot;Tonny&quot;, &quot;author_last_name&quot; : &quot;Peter Smith&quot;&#125; &#125; 然后来查询一下Peter Smith12345678910GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;Peter Smith&quot;, &quot;fields&quot;: [&quot;author_first_name&quot;,&quot;author_last_name&quot;], &quot;type&quot;: &quot;most_fields&quot; &#125; &#125;&#125; 返回值:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&#123; &quot;took&quot;: 28, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 0.6931472, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.6931472, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 50, &quot;title&quot;: &quot;this is java blog&quot;, &quot;content&quot;: &quot;i think java is the best programming language&quot;, &quot;sub_title&quot;: &quot;learned a lot of course&quot;, &quot;author_first_name&quot;: &quot;Smith&quot;, &quot;author_last_name&quot;: &quot;Williams&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.5753642, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;hadoop&quot; ], &quot;tag_cnt&quot;: 2, &quot;view_cnt&quot;: 30, &quot;title&quot;: &quot;this is java and elasticsearch blog&quot;, &quot;content&quot;: &quot;i like to write best elasticsearch article&quot;, &quot;sub_title&quot;: &quot;learning more courses&quot;, &quot;author_first_name&quot;: &quot;Peter&quot;, &quot;author_last_name&quot;: &quot;Smith&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;5&quot;, &quot;_score&quot;: 0.51623213, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot;: 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2018-12-03&quot;, &quot;tag&quot;: [ &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10, &quot;title&quot;: &quot;this is spark blog&quot;, &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java&quot;, &quot;sub_title&quot;: &quot;haha, hello world&quot;, &quot;author_first_name&quot;: &quot;Tonny&quot;, &quot;author_last_name&quot;: &quot;Peter Smith&quot; &#125; &#125; ] &#125;&#125; 来看一下返回值, id是2的document被排在了第一位,为什么?因为IDF分数高, document2的author_first_name 是Smith,在所有的doc中只出现过一次,出现的频率低再来看下document1 和 document5 这两个的author_last_name都出现了,所以导致document1的分数要比document2的分数要低 大概来说是这样的,es的算法很复杂,这些都是可能影响分数的. cross-fields问题 只是找到尽可能多的field匹配到的document,而不是某个field完全匹配的document most-fields没办法使用minimum_should_match去掉长尾数据,就是匹配特别少的结果 TF/IDF算法,比如上面搜索中的Peter Smith和Smith Williams,搜索Peter Smith的时候,由于first_name中很少有Smith的,所以在query中所有document中的频率很低,得到的分数很高,可能Smith Williams反而会排在Peter Smith的前面]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-49-实战案例-most-fields策略]]></title>
    <url>%2F2018%2F12%2F17%2FElasticsearch-49-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B-most-fields%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[对比之前我们写了best-fields策略,本文将使用most-fields来搜索,那么两者有什么区别呢?best-fields策略:主要是说,将某一个field匹配尽可能多的关键词document优先返回回来most-fields策略:主要是说将更多filed匹配到某个关键词的document优先返回回来 举例现在有两个document,如下:document1:1234&#123; &quot;title&quot;:&quot;China people&quot;, &quot;content&quot;:&quot;i am a good person&quot;&#125; document2:1234&#123; &quot;title&quot;:&quot;China person&quot;, &quot;content&quot;:&quot;i am a good people&quot;&#125; 一个搜索请求,搜索的关键字是China person,那么来看一下两种搜索策略的返回结果是怎样的 best-fields:会优先将document2返回回来,因为document2的title匹配了两个关键字most-fields:会优先将document1返回回来,因为document1匹配了两个field 实战案例先来准备数据,添加一个sub_title字段,手动创建mapping123456789101112131415POST /forum/_mapping/article&#123; &quot;properties&quot;: &#123; &quot;sub_title&quot;:&#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot;, &quot;fields&quot;: &#123; &quot;std&quot;:&#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;standard&quot; &#125; &#125; &#125; &#125;&#125; 添加数据1234567891011POST /forum/article/_bulk&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;1&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;sub_title&quot; : &quot;learning more courses&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;2&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;sub_title&quot; : &quot;learned a lot of course&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;3&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;sub_title&quot; : &quot;we have a lot of fun&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;4&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;sub_title&quot; : &quot;both of them are good&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;5&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;sub_title&quot; : &quot;haha, hello world&quot;&#125; &#125; 搜索查询sub_title中包含learning courses的document12345678GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;sub_title&quot;: &quot;learning courses&quot; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&#123; &quot;took&quot;: 4, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1.219939, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1.219939, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 50, &quot;title&quot;: &quot;this is java blog&quot;, &quot;content&quot;: &quot;i think java is the best programming language&quot;, &quot;sub_title&quot;: &quot;learned a lot of course&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.5063205, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;hadoop&quot; ], &quot;tag_cnt&quot;: 2, &quot;view_cnt&quot;: 30, &quot;title&quot;: &quot;this is java and elasticsearch blog&quot;, &quot;content&quot;: &quot;i like to write best elasticsearch article&quot;, &quot;sub_title&quot;: &quot;learning more courses&quot; &#125; &#125; ] &#125;&#125; 来看一下返回值,这里有个问题,为什么learned a lot of course 排在了 learning more courses 的前面? 在我们手动创建sub_title的mapping映射的时候,使用的是English分词器,所以会还原单词,将单词还原为其最基本的形态(stemmer),比如learning –&gt; learnlearned –&gt; learncourses –&gt; course 所以,我们的搜索条件也会变, learning courses –&gt; learn course,这时候去搜索对于这两个sub_title来说就是一样的,所以就会出现learned a lot of course 排在了 learning more courses 的前面这样的情况 most-fields搜索我们上面在sub_title中还创建了个子field sub_title.std,然后我们用这两个field来进行most-field搜索.请求12345678910GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;learning courses&quot;, &quot;type&quot;: &quot;most_fields&quot;, &quot;fields&quot;: [&quot;sub_title&quot;,&quot;sub_title.std&quot;] &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1.219939, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1.219939, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 50, &quot;title&quot;: &quot;this is java blog&quot;, &quot;content&quot;: &quot;i think java is the best programming language&quot;, &quot;sub_title&quot;: &quot;learned a lot of course&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1.012641, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;hadoop&quot; ], &quot;tag_cnt&quot;: 2, &quot;view_cnt&quot;: 30, &quot;title&quot;: &quot;this is java and elasticsearch blog&quot;, &quot;content&quot;: &quot;i like to write best elasticsearch article&quot;, &quot;sub_title&quot;: &quot;learning more courses&quot; &#125; &#125; ] &#125;&#125; 再来看一下返回值,依然是learned a lot of course排在了前面,但是learning more courses的分数有了大幅度的提高,可以对比一下第一个搜索时候的分数 区别和优缺点best-fields,是对多个field进行搜索,挑选某个filed匹配度最高的那个分数,同时在多个query最高分相同的情况下,在一定程度上考虑其他query的分数. 简单来说就是,对多个filed进行搜索,就想搜索到某一个field尽可能包含更多关键字的数据 优点:通过best_fields策略,以及综合考虑其他field,还有minimum_should_match支持,可以尽可能精准的将匹配的结果推送到最前面缺点:除了那些精准匹配的结果,其他差不多大的结果,排序结果不是太均匀,没有什么区分度了 most-fields,综合多个field一起进行搜索,尽可能多地让所有的field的query参与到总分数的计算中来,此时就会是个大杂烩,数显类似best_fields案例最开始的那个结果,结果不一定精准,某一个document的一个field包含更多的关键字,但是因为其他document有更多field匹配到了,所以排在前面, 所以就需要建立类似sub_title.std这样的field,尽可能让某一个field精准匹配query string,贡献更高的分数,将更精准匹配的数据排到前面 优点:将尽可能匹配更多的field的结果推送到前面,整个排序的结果都是比较均匀的缺点:可能那些精准匹配的结果无法排在最前面 实际的例子:wiki,明显的most_fields策略,搜索结果比较均匀,但是的确要翻好几页才能找到最匹配的结果]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-48-multi_match语法]]></title>
    <url>%2F2018%2F12%2F17%2FElasticsearch-48-multi-match%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[minimum_should_match作用我们先来看一个查询123456789101112131415161718192021222324252627GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;dis_max&quot;: &#123; &quot;queries&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;java beginner&quot;, &quot;minimum_should_match&quot;:&quot;50%&quot;, &quot;boost&quot;:2 &#125; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &#123; &quot;query&quot;: &quot;java beginner&quot;, &quot;minimum_should_match&quot;:&quot;30%&quot; &#125; &#125; &#125; ], &quot;tie_breaker&quot;: 0.3 &#125; &#125;&#125; 上面这个查询就是查询了title或者content中包含java beginner的内容,用了dis_max+tie_breaker查询,而且查询title的权重是2,还有一个搜索参数是 “minimum_should_match”,那么这个关键字是做什么用的呢 minimum_should_match: 去长尾,比如你搜素5个关键词,但是很多结果是只匹配一个关键词的,其实跟你想要的结果相差甚远,这些结果就是长尾minimum_should_match,可以控制搜索结果的精准度,只有匹配一定数量的关键词数据,才能返回 multi_match语法123456789101112GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;java beginner&quot;, &quot;type&quot;: &quot;best_fields&quot;, &quot;fields&quot;: [&quot;title^2&quot;,&quot;content&quot;], &quot;tie_breaker&quot;: 0.3, &quot;minimum_should_match&quot;:&quot;50%&quot; &#125; &#125;&#125; type: 默认就是best_fields查询title^2: 代表title的权重是2, 相当于上面的”boost”:2]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-47-实战案例-基于dis_max实现best fileds策略进行多字段搜索]]></title>
    <url>%2F2018%2F12%2F06%2FElasticsearch-47-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B-%E5%9F%BA%E4%BA%8Edis-max%E5%AE%9E%E7%8E%B0best-fileds%E7%AD%96%E7%95%A5%E8%BF%9B%E8%A1%8C%E5%A4%9A%E5%AD%97%E6%AE%B5%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[准备工作为帖子增加content字段1234567891011POST /forum/article/_bulk&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;1&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;content&quot; : &quot;i like to write best elasticsearch article&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;2&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;content&quot; : &quot;i think java is the best programming language&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;3&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;content&quot; : &quot;i am only an elasticsearch beginner&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;4&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;content&quot; : &quot;elasticsearch and hadoop are all very good solution, i am a beginner&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;5&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;content&quot; : &quot;spark is best big data solution based on scala ,an programming language similar to java&quot;&#125; &#125; 需求一搜索title或content中包含java或solution的帖子构建搜索条件12345678910111213141516171819GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;java solution&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java solution&quot; &#125; &#125; ] &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293&#123; &quot;took&quot;: 23, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 4, &quot;max_score&quot;: 0.8849759, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.8849759, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 50, &quot;title&quot;: &quot;this is java blog&quot;, &quot;content&quot;: &quot;i think java is the best programming language&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.7120095, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;QQPX-R-3956-#aD8&quot;, &quot;userID&quot;: 2, &quot;hidden&quot;: true, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 2, &quot;view_cnt&quot;: 80, &quot;title&quot;: &quot;this is java, elasticsearch, hadoop blog&quot;, &quot;content&quot;: &quot;elasticsearch and hadoop are all very good solution, i am a beginner&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;5&quot;, &quot;_score&quot;: 0.56008905, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot;: 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2018-12-03&quot;, &quot;tag&quot;: [ &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10, &quot;title&quot;: &quot;this is spark blog&quot;, &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.26742277, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;hadoop&quot; ], &quot;tag_cnt&quot;: 2, &quot;view_cnt&quot;: 30, &quot;title&quot;: &quot;this is java and elasticsearch blog&quot;, &quot;content&quot;: &quot;i like to write best elasticsearch article&quot; &#125; &#125; ] &#125;&#125; 我们来看一下返回值:排在第一位的是id是2的document,这个document中只有title包含了java,content也包含了java排在第二位的是id是4的document,这document中,是title中包含了java,content中包含了solution排在第三位的是id是5的document,这个document中,是content包含了java和solution 这样看来应该是id=5的document是相关度比id=4的高的,但是id=4的排在了前面,这是为什么呢? es的计算方式es在计算每个document的relevance score是每个query的分数的和,乘以matched query的数量,除以总query的数量对于每个query(就是上面should中的每个match),es都会计算一个数量, matched query 就是匹配到的条件的数量 我们来算一下id=4 的document的分数,查询中的两个条件{ “match”: { “title”: “java solution” }},针对document4 是有一个分数的,假设是1.1{ “match”: { “content”: “java solution” }}，针对document4,也是有一个分数的,假设是1.2query分数的和1.1 + 1.2 = 2.3,matched query的数量是2, 总共的query数量是2,所以计算出来就是2.3 * 2 / 2 = 2.3 我们再来算一下document 5 的分数,查询中的两个条件{ “match”: { “title”: “java solution” }},针对document5 是没有分数的,因为这个条件不匹配document5{ “match”: { “content”: “java solution” }}，针对document5,也是有一个分数的,假设是2.3这时候query分数的总和就是2.3,matched query的数量是1,总共的query数量是2,所以计算出来就是 2.3 * 1 / 2 = 1.15 2.3 &gt; 1.15 所以document4 排在了document5的前面 best fields策略 dis_maxbest fields策略: 就是说,搜索到的结果应该是某一个匹配到尽可能多的关键词的document被排在前面,而不是匹配到了少数的关键词还排在前面 搜索请求:12345678910111213141516171819GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;dis_max&quot;: &#123; &quot;queries&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;java solution&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java solution&quot; &#125; &#125; ] &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293&#123; &quot;took&quot;: 6, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 4, &quot;max_score&quot;: 0.68640786, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.68640786, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 50, &quot;title&quot;: &quot;this is java blog&quot;, &quot;content&quot;: &quot;i think java is the best programming language&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;5&quot;, &quot;_score&quot;: 0.56008905, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot;: 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2018-12-03&quot;, &quot;tag&quot;: [ &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10, &quot;title&quot;: &quot;this is spark blog&quot;, &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.5565415, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;QQPX-R-3956-#aD8&quot;, &quot;userID&quot;: 2, &quot;hidden&quot;: true, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 2, &quot;view_cnt&quot;: 80, &quot;title&quot;: &quot;this is java, elasticsearch, hadoop blog&quot;, &quot;content&quot;: &quot;elasticsearch and hadoop are all very good solution, i am a beginner&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.26742277, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;hadoop&quot; ], &quot;tag_cnt&quot;: 2, &quot;view_cnt&quot;: 30, &quot;title&quot;: &quot;this is java and elasticsearch blog&quot;, &quot;content&quot;: &quot;i like to write best elasticsearch article&quot; &#125; &#125; ] &#125;&#125; 可以看到,这次查询document5排在了document4的前面 dis_max语法,直接取多个query中,分数最高的那个query的分数即可,我们来分析一下:{ “match”: { “title”: “java solution” }},针对document4,是有一个分数的,比如1.1{ “match”: { “content”: “java solution” }},针对document4,也是有一个分数的,比如1.2取最大分数,1.2 { “match”: { “title”: “java solution” }},针对doc5,是没有分数的{ “match”: { “content”: “java solution” }}，针对doc5,是有一个分数的,比如2.3取最大分数,2.3 然后document4的分数 = 1.2 &lt; document5的分数 = 2.3,所以document5就可以排在更前面的地方,符合我们的需要 基于tie_breaker参数优化dis_max搜索效果场景搜索条件:搜索title或content中包含java beginner的帖子 假设我们现在有3个documentdocument1:title中包含java,content不包含 java beginner任何一个关键词document2:title中不包含任何一个关键词,content中包含beginnerdocument3:title中包含java,content中包含beginner 这时候执行搜索,可能出现的结果是document1和document2排在了document3的前面,而我们期望的是document3排在最前面 dis_max是只取一个query最大的分数,完全不考虑其他的query的分数 使用tie_breaker优化结果tie_breaker参数的意义在于,将其他的query分数,乘以tie_breaker,然后综合在一起计算,除了取最高分以外,还会考虑其他的query分数 tie_breaker的值在0-1之间 用法示例:1234567891011121314151617181920GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;dis_max&quot;: &#123; &quot;queries&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;java beginner&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java beginner&quot; &#125; &#125; ], &quot;tie_breaker&quot;:0.3 &#125; &#125;&#125; 跟queries是同级的, 可以去试一下加tie_breaker和不加时候查询的分数,对比一下就很清楚了,这里就不去演示了]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-46-多shard场景下relevance score不准确的问题]]></title>
    <url>%2F2018%2F12%2F05%2FElasticsearch-46-%E5%A4%9Ashard%E5%9C%BA%E6%99%AF%E4%B8%8Brelevance-score%E4%B8%8D%E5%87%86%E7%A1%AE%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[场景一个index的数据被分配到了多个shard上,每个shard都包含一部分这个index的数据如图所示,一个搜索请求条件是title中包含java,假如shard1 上面有10条符合条件的document,这个请求到达shard1上的时候,默认是在这个shard本地local去进行IDF计算在shard2中假如只有1个符合条件的数据,那shard2也会在local计算他的IDF,这时候这个分数就会算的很高 问题有时候导致出现的搜索结果,似乎不是你想要的结果,也许相关度高的document被排在了后面,很低的被排在了前面但是他的分数很高 解决方案生产环境生产环境中,数据量很大的话,在概率学的背景下,一般情况中es都是在多个shard中均匀的路由数据的,比如说有10个document,title都包含java,一共有5个shard,那么在概率学的背景下,如果负载均衡的话,其实每个shard都应该有2个document的title包含java如果说数据分布均匀的话,就没有上面说的问题了 测试环境测试环境下,可以将所有的primary shard个数设置为1,只有一个shard的话,所有的document都在这一个shard上面,就没有这个问题了 也可以在搜索时附带search_type=dfs_query_then_fetch参数计算一个doc的相关度分数的时候,就会将所有shard对的local IDF计算一下,获取出来,在本地进行global IDF分数的计算,会将所有shard的doc作为上下文来进行计算,也能确保准确性.但是production生产环境下,不推荐这个参数,因为性能很差]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-45-实战案例-基于boost的细粒度搜索条件权重控制]]></title>
    <url>%2F2018%2F12%2F05%2FElasticsearch-45-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B-%E5%9F%BA%E4%BA%8Eboost%E7%9A%84%E7%BB%86%E7%B2%92%E5%BA%A6%E6%90%9C%E7%B4%A2%E6%9D%A1%E4%BB%B6%E6%9D%83%E9%87%8D%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[场景我们来搜索一下标题必须包含 “blog” 的数据,然后可以包含 “java” “hadoop” “elasticsearch” “spark”的数据 实现组合搜索条件123456789101112131415161718192021222324252627282930313233343536GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;blog&quot; &#125; &#125; ], &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;java&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;elasticsearch&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;hadoop&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;spark&quot; &#125; &#125; ] &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107&#123; &quot;took&quot;: 43, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 5, &quot;max_score&quot;: 1.4930474, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 1.4930474, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;QQPX-R-3956-#aD8&quot;, &quot;userID&quot;: 2, &quot;hidden&quot;: true, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 2, &quot;view_cnt&quot;: 80, &quot;title&quot;: &quot;this is java, elasticsearch, hadoop blog&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.80226827, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;hadoop&quot; ], &quot;tag_cnt&quot;: 2, &quot;view_cnt&quot;: 30, &quot;title&quot;: &quot;this is java and elasticsearch blog&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;5&quot;, &quot;_score&quot;: 0.5753642, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot;: 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2018-12-03&quot;, &quot;tag&quot;: [ &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10, &quot;title&quot;: &quot;this is spark blog&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.5753642, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;JODL-X-1937-#pV7&quot;, &quot;userID&quot;: 2, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot;, &quot;tag&quot;: [ &quot;hadoop&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 100, &quot;title&quot;: &quot;this is elasticsearch blog&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.3971361, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 50, &quot;title&quot;: &quot;this is java blog&quot; &#125; &#125; ] &#125;&#125; 看一下返回结果, 5条数据都返回来了, 排在第一位的数据的title是 “this is java, elasticsearch, hadoop blog”,因为这条数据满足的条件最多,所以排在第一位 权重控制给每个条件一个权重值,boost, boost越大这个搜索条件的权重越大 现在 我们给”spark”,这个条件设置一个权重123456789101112131415161718192021222324252627282930313233343536373839GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;blog&quot; &#125; &#125; ], &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;java&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;elasticsearch&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;hadoop&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;spark&quot;, &quot;boost&quot;:5 &#125; &#125; &#125; ] &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107&#123; &quot;took&quot;: 8, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 5, &quot;max_score&quot;: 1.7260925, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;5&quot;, &quot;_score&quot;: 1.7260925, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot;: 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2018-12-03&quot;, &quot;tag&quot;: [ &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10, &quot;title&quot;: &quot;this is spark blog&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 1.4930474, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;QQPX-R-3956-#aD8&quot;, &quot;userID&quot;: 2, &quot;hidden&quot;: true, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 2, &quot;view_cnt&quot;: 80, &quot;title&quot;: &quot;this is java, elasticsearch, hadoop blog&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.80226827, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;hadoop&quot; ], &quot;tag_cnt&quot;: 2, &quot;view_cnt&quot;: 30, &quot;title&quot;: &quot;this is java and elasticsearch blog&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.5753642, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;JODL-X-1937-#pV7&quot;, &quot;userID&quot;: 2, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot;, &quot;tag&quot;: [ &quot;hadoop&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 100, &quot;title&quot;: &quot;this is elasticsearch blog&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.3971361, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 50, &quot;title&quot;: &quot;this is java blog&quot; &#125; &#125; ] &#125;&#125; 可以看到,搜索结果中包含spark的这个document排到了最前面, es在计算 relevance score 的时候,匹配权重更大的搜索条件的document,relevance score会更高,当然也会优先返回回来默认情况下,所有的搜索条件的权重都是1]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-44-基于term+bool实现的multi word搜索底层原理剖析]]></title>
    <url>%2F2018%2F12%2F04%2FElasticsearch-44-%E5%9F%BA%E4%BA%8Eterm-bool%E5%AE%9E%E7%8E%B0%E7%9A%84multi%20word%E6%90%9C%E7%B4%A2%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[在我们使用match这种查询的时候,在es底层其实会自动的转换成term+bool的这种查询 示例一原请求体:123&#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;java elasticsearch&quot;&#125;&#125; es转换后的请求体:12345678&#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;java&quot; &#125;&#125;, &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;elasticsearch&quot; &#125;&#125; ] &#125;&#125; 使用诸如上面的match query进行多值搜索的时候,es会在底层自动将这个match query转换为bool的语法 示例二原请求体:12345678&#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;java elasticsearch&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125;&#125; es转换后的请求体:12345678&#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;java&quot; &#125;&#125;, &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;elasticsearch&quot; &#125;&#125; ] &#125;&#125; 示例三原请求体:12345678&#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;java elasticsearch hadoop spark&quot;, &quot;minimum_should_match&quot;: &quot;75%&quot; &#125; &#125;&#125; es转换后的请求体:1234567891011&#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;java&quot; &#125;&#125;, &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;elasticsearch&quot; &#125;&#125;, &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;hadoop&quot; &#125;&#125;, &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;spark&quot; &#125;&#125; ], &quot;minimum_should_match&quot;: 3 &#125;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-43-实战案例-手动控制全文检索结果的精准度]]></title>
    <url>%2F2018%2F12%2F04%2FElasticsearch-43-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B-%E6%89%8B%E5%8A%A8%E6%8E%A7%E5%88%B6%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2%E7%BB%93%E6%9E%9C%E7%9A%84%E7%B2%BE%E5%87%86%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[准备工作为帖子增加标题字段1234567891011POST /forum/article/_bulk&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;1&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;title&quot; : &quot;this is java and elasticsearch blog&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;2&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;title&quot; : &quot;this is java blog&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;3&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;title&quot; : &quot;this is elasticsearch blog&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;4&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;title&quot; : &quot;this is java, elasticsearch, hadoop blog&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;5&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;title&quot; : &quot;this is spark blog&quot;&#125; &#125; 需求一搜索标题中包含java 或 elasticsearch的帖子12345678GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;java elasticsearch&quot; &#125; &#125;&#125; 就是只要标题中有java,elasticsearch其中的一个就可以作为返回结果,这个就和之前我们说的term query不一样了, term query是exact value,而这里的搜索是full text match query是负责全文检索的,当然如果要检索的field是not_analyzed不分词的,那么他的作用就和term query是一样的 需求二搜索标题中包含 java 和 elasticsearch的帖子1234567891011GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;:&#123; &quot;query&quot;: &quot;java elasticsearch&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125; &#125;&#125; 搜索结果精准度控制的第一步:灵活使用and关键字,如果希望所有的搜索关键字都要匹配,那么就用and,可以实现单纯match query无法实现的效果 需求三搜索包含java,elasticsearch,spark,hadoop,4个关键字中,至少3个的帖子12345678910111213141516GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;:&#123; &quot;query&quot;: &quot;java elasticsearch spark hadoop&quot;, &quot;minimum_should_match&quot;:&quot;75%&quot; &#125; &#125; &#125;&#125;``` 搜索结果精准度控制第二步:指定一些关键字中至少匹配到其中多少个关键字才能作为返回结果 #### bool组合搜索 GET forum/article/_search{ “query”: { “bool”: { “must”: [ { “match”: { “title”: “java” } } ], “must_not”: [ { “match”: { “title”: “spark” } } ], “should”: [ { “match”: { “title”: “hadoop” } }, { “match”: { “title”: “elasticsearch” } } ] } }}1234567891011121314151617看一下上面这个搜索请求, 就是搜索必须包含java,必须不包含spark,可以包含hadoop或elasticsearch的数据 #### bool组合多个搜索条件,计算relevance score的规则**must和should搜索对应的分数,加起来,除以must和should的总数** 在上面这个查询中: 排名第一:java,同时包含should中所有的关键字,hadoop,elasticsearch 排名第二:java,同时包含should中的elasticsearch 排名第三:java,不包含should中的任何关键字 should是会影响相关度分数的 must是确保说,谁必须有这个关键字,同时会根据这个must的条件去计算出document对这个搜索条件的relevance score 在满足must的基础之上,should中的条件,不匹配也可以,但是如果匹配的更多,那么document的relevance score就会更高 #### 用bool组合查询实现需求三搜索包含java,elasticsearch,spark,hadoop,4个关键字中,至少3个的帖子 GET /forum/article/_search{ “query”: { “bool”: { “should”: [ { “match”: { “title”: “java” } }, { “match”: { “title”: “elasticsearch” } }, { “match”: { “title”: “spark” } }, { “match”: { “title”: “hadoop” } } ], “minimum_number_should_match”: 3 } }}` 默认情况下should是可以不匹配任何一个的,但是如果没有must的话 should中必须匹配一个才可以,但是也可以通过我们上面请求用到的minimum_number_should_match 来控制必须满足几个才能作为返回结果 总结全文见检索的时候,进行多个值的检索,可以用match query 也可以空should搜过结果精准度控制: 用 and operator 或 minimum_number_should_match]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-42-实战案例-range filter进行范围过滤]]></title>
    <url>%2F2018%2F12%2F03%2FElasticsearch-42-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B-range-filter%E8%BF%9B%E8%A1%8C%E8%8C%83%E5%9B%B4%E8%BF%87%E6%BB%A4%2F</url>
    <content type="text"><![CDATA[准备工作为帖子增加浏览量的字段123456789POST /forum/article/_bulk&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;1&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;view_cnt&quot; : 30&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;2&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;view_cnt&quot; : 50&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;3&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;view_cnt&quot; : 100&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;4&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;view_cnt&quot; : 80&#125; &#125; 需求一搜索浏览量在30-60之间的帖子123456789101112131415GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;view_cnt&quot;: &#123; &quot;gt&quot;: 30, &quot;lt&quot;: 60 &#125; &#125; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526272829303132&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 50 &#125; &#125; ] &#125;&#125; lt: 小于 lte: 小于等于 gt: 大于 gte: 大于等于 需求二搜索发帖日期在最近1个月的帖子 先来添加一条最近一个月的帖子123POST /forum/article/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 5 &#125;&#125;&#123; &quot;articleID&quot; : &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot; : 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2018-12-03&quot;, &quot;tag&quot;: [&quot;elasticsearch&quot;], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10 &#125; 添加完成后搜索,一个月,也就是当前时间减去30天1234567891011121314GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;postDate&quot;: &#123; &quot;gt&quot;: &quot;now-30d&quot; &#125; &#125; &#125; &#125; &#125;&#125; 也可以是1234567891011121314GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;postDate&quot;: &#123; &quot;gt&quot;: &quot;2018-12-03||-30d&quot; &#125; &#125; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526272829303132&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;5&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot;: 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2018-12-03&quot;, &quot;tag&quot;: [ &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10 &#125; &#125; ] &#125;&#125; 总结 range 相当于sql中的between and 或者是 &gt;= , &lt;= range用来做范围过滤]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-41-实战案例-terms搜索多个值及搜索结果优化]]></title>
    <url>%2F2018%2F12%2F03%2FElasticsearch-41-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B-terms%E6%90%9C%E7%B4%A2%E5%A4%9A%E4%B8%AA%E5%80%BC%E5%8F%8A%E6%90%9C%E7%B4%A2%E7%BB%93%E6%9E%9C%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[之前的几个案例中都是用的term用来搜索. 本文使用terms来搜索数据 terms,就相当于sql中的in 准备工作为帖子添加tag字段123456789POST /forum/article/_bulk&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;1&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;tag&quot; : [&quot;java&quot;, &quot;hadoop&quot;]&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;2&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;tag&quot; : [&quot;java&quot;]&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;3&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;tag&quot; : [&quot;hadoop&quot;]&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;4&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;tag&quot; : [&quot;java&quot;, &quot;elasticsearch&quot;]&#125; &#125; terms搜索需求一搜索articleID为KDKE-B-9947-#kL5或QQPX-R-3956-#aD8的帖子 将需求转为sql就是:123SELECT * FROM forum.article whereid in (&apos;KDKE-B-9947-#kL5&apos;,&apos;QQPX-R-3956-#aD8&apos;) 然后在es中去构建搜索条件123456789101112131415GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;terms&quot;: &#123; &quot;articleID&quot;: [ &quot;KDKE-B-9947-#kL5&quot;, &quot;QQPX-R-3956-#aD8&quot; ] &#125; &#125; &#125; &#125;&#125; 需求二搜索tag中包含java的帖子1234567891011121314GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;terms&quot;: &#123; &quot;tag&quot;: [ &quot;java&quot; ] &#125; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ] &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;QQPX-R-3956-#aD8&quot;, &quot;userID&quot;: 2, &quot;hidden&quot;: true, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;elasticsearch&quot; ] &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;hadoop&quot; ] &#125; &#125; ] &#125;&#125; 从结果上看的话,tag的值中只要包含了java就被搜索出来了 优化搜索结果上面一个搜索中只要包含了java的数据都被搜索出来了,现在我们想搜索只包含java的数据 首先,我们需要修改一下数据,增加一个tag_cnt的字段123456789POST /forum/article/_bulk&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;1&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;tag_cnt&quot; : 2&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;2&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;tag_cnt&quot; : 1&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;3&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;tag_cnt&quot; : 1&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;4&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;tag_cnt&quot; : 2&#125; &#125; 执行完毕后,再次来构建搜索条件1234567891011121314151617181920212223GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;:[ &#123; &quot;term&quot;:&#123; &quot;tag_cnt&quot;:1 &#125; &#125;, &#123; &quot;terms&quot;:&#123; &quot;tag&quot;:[&quot;java&quot;] &#125; &#125; ] &#125; &#125; &#125; &#125;&#125; 返回值:12345678910111213141516171819202122232425262728293031&#123; &quot;took&quot;: 4, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1 &#125; &#125; ] &#125;&#125; 总结 掌握terms多值搜索 优化terms多值搜索结果 terms相当于sql中的in]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-40-实战案例-组合多个filter搜索]]></title>
    <url>%2F2018%2F12%2F03%2FElasticsearch-40-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B-%E7%BB%84%E5%90%88%E5%A4%9A%E4%B8%AAfilter%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[之前我们有写过用bool来组合多个query,同样的bool也可以组合多个filter来搜索 基于bool组合多个filter搜索数据需求一搜索发帖日期为2017-01-01,或者帖子ID为XHDK-A-1293-#fJ3的帖子,同时要求帖子的发帖日期绝对不为2017-01-02 这个需求如果写为SQL的话就是:1234SELECT * FROM forum.article where(postDate=&apos;2017-01-01&apos; or id =&apos;XHDK-A-1293-#fJ3&apos;)and postDate &lt;&gt; 2017-01-02 然后我们在es中组合一下搜索条件123456789101112131415161718192021222324GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;:[ &#123; &quot;term&quot;:&#123;&quot;postDate&quot;:&quot;2017-01-01&quot;&#125; &#125;, &#123; &quot;term&quot;:&#123;&quot;articleID&quot;:&quot;XHDK-A-1293-#fJ3&quot;&#125; &#125; ], &quot;must_not&quot;:[ &#123; &quot;term&quot;:&#123;&quot;postDate&quot;:&quot;2017-01-02&quot;&#125; &#125; ] &#125; &#125; &#125; &#125;&#125; 返回数据:123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;took&quot;: 61, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;JODL-X-1937-#pV7&quot;, &quot;userID&quot;: 2, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot; &#125; &#125; ] &#125;&#125; 需求二搜索帖子ID为XHDK-A-1293-#fJ3,或者是帖子ID为JODL-X-1937-#pV7而且发帖日期为2017-01-01的帖子 先来将需求转化为sql12345SELECT * FROM forum.article whereid = &apos;XHDK-A-1293-#fJ3&apos;or(id=&apos;JODL-X-1937-#pV7&apos; and postDate = &apos;2017-01-01&apos;) 然后在es中组合搜索条件 12345678910111213141516171819202122232425262728293031323334GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;:[ &#123; &quot;term&quot;:&#123; &quot;articleID&quot;:&quot;XHDK-A-1293-#fJ3&quot; &#125; &#125;, &#123; &quot;bool&quot;:&#123; &quot;must&quot;:[ &#123; &quot;term&quot;:&#123; &quot;articleID&quot;:&quot;JODL-X-1937-#pV7&quot; &#125; &#125;, &#123; &quot;term&quot;:&#123; &quot;postDate&quot;:&quot;2017-01-01&quot; &#125; &#125; ] &#125; &#125; ] &#125; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;took&quot;: 8, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;JODL-X-1937-#pV7&quot;, &quot;userID&quot;: 2, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot; &#125; &#125; ] &#125;&#125; 总结 should: 可以匹配其中任意一个 must: 必须匹配 must_not: 必须不匹配]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-39-filter原理深度剖析]]></title>
    <url>%2F2018%2F11%2F30%2FElasticsearch-39-filter%E5%8E%9F%E7%90%86%E6%B7%B1%E5%BA%A6%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[filter执行原理场景举个例子,假设有个字段是date类型的,在倒排索引中: word document1 document2 document3 2017-01-01 √ √ 2017-02-02 √ √ 2017-03-03 √ √ √ 在倒排索引中查找搜索串,获取document list这时候一个filter查询:2017-02-02,在倒排索引里面找,对应的document list是doc2,doc3 为每个在倒排索引中搜索到的结果构建一个bitset这点非常重要, 使用找到的document list构建一个bitset,一个二进制数组,数组每个元素都是0或1,用来标识一个doc对一个filter条件是否匹配,如果匹配就是1,不匹配就是0.上面的例子中,构建的bitset就是[0,1,1] 尽可能用简单的数据结构去实现复杂的功能,可以节省内存空间,提升性能 遍历每个过滤条件对应的bitset,优先从最稀疏的开始搜索,查找满足条件的所有document在一个search请求中,可以发出多个filter条件(这个后面再具体说),每个filter会对应一个bitset遍历每个filter条件对应的bitset,先从最稀疏的开始遍历. 怎么算稀疏呢?[0,0,0,1,0,0] – 比较稀疏[0,1,0,1,0,1]先遍历比较稀疏的bitset,可以过滤掉尽可能多的数据 比如现在有个请求 filter: postDate=2017-01,userID=1,然后构建的两个bitset分别是:[0,0,1,==1==,0,0][0,1,0,==1==,0,1]遍历玩两个bitset之后,找到匹配所有条件的document,就是第4个,这个时候就可以将符合结果document返回给客户端了 caching bitset 跟踪query对于在最近的256个query中超过一定次数的过滤条件,缓存其bitset.对于小segment(&lt;1000或&lt;3%)不缓存 举个例子,在最近的256次查询中,postDate=2017-02-02这个条件出现超过了一定的次数(不固定), 就会自动缓存这个filter对应的bitset filter对于小的segment中获取到的结果可以不缓存, segment中记录数小于1000的和segment大小小于index总大小的3%的 因为segment数据量很小的时候,扫描是很快的,而且我们之前有说过,segment会在后台自动合并的,小的segment很快会和其他小的segment合并,此时缓存也就没有什么意义了 大部分情况下 filter会在query之前执行filter先执行可以先过滤掉一部分数据,之前说过query是会计算相关度分数,然后去排序的,而filter是不计算分数,也不排序,所以先执行filter过滤掉尽可能多的数据 如果document有新增或修改,那么cached bitset会被自动更新举个例子,之前有个filter 过滤条件是postDate=2017-02-02,然后他的bitset是[0,0,0,1]这个时候如果新增了一条document进来 postDate也是 2017-02-02,id是5, 那么这个bitset会自动更新为[0,0,0,1,1]同理,如果id = 1的document的postDate更新为2017-02-02 那么bitset也会更新为[1,0,0,1,1] 以后只要是有相同的filter条件的，会直接来使用这个过滤条件对应的cached bitset]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-38-实战案例-term filter搜索]]></title>
    <url>%2F2018%2F11%2F30%2FElasticsearch-38-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B-term-filter%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[之前都是随便写的一些demo来测试es的api,本文及以后将会基于一个案例,来更加深入使用这些api,之后会再使用Java api来实现具体功能. 场景以一个IT论坛为背景,来置顶搜索需求,以及实现. 测试数据123456789POST /forum/article/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 1 &#125;&#125;&#123; &quot;articleID&quot; : &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot; : 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 2 &#125;&#125;&#123; &quot;articleID&quot; : &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot; : 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 3 &#125;&#125;&#123; &quot;articleID&quot; : &quot;JODL-X-1937-#pV7&quot;, &quot;userID&quot; : 2, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 4 &#125;&#125;&#123; &quot;articleID&quot; : &quot;QQPX-R-3956-#aD8&quot;, &quot;userID&quot; : 2, &quot;hidden&quot;: true, &quot;postDate&quot;: &quot;2017-01-02&quot; &#125; 使用_bulk api来添加数据,目前我们只添加这几个field,articleID,userId,hidden 执行完毕以后,我们来查看一下dynamic mapping给我建立的mapping1GET /forum/_mapping/article 返回值:12345678910111213141516171819202122232425262728&#123; &quot;forum&quot;: &#123; &quot;mappings&quot;: &#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;articleID&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; // 1 &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;hidden&quot;: &#123; &quot;type&quot;: &quot;boolean&quot; &#125;, &quot;postDate&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;userID&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125; &#125; &#125;&#125; 这里我们看1处,”articleID”的类型是text,里面还有一个”articleID.keyword”,这个东西是干嘛的呢? 在新版es中,type=text的时候,默认会设置两个field,一个是field本身,比如”articleID”,他是分词的,还有一个就是field.keyword,比如”articleID.keyword”,默认是不分词的, keyword里面还有一个属性是”ignore_above”:256,意思就是最多会保留256个字符 term filter的使用term filter/query: 对搜索文本不分词,直接拿去倒排索引中去匹配,你输入的是什么,就去匹配什么 需求1:根据用户id来搜索帖子123456789101112GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;userID&quot;: 1 &#125; &#125; &#125; &#125;&#125; 需求2:搜索没有隐藏的帖子123456789101112GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;hidden&quot;: false &#125; &#125; &#125; &#125;&#125; 需求3:根据发帖日期搜索帖子123456789101112GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;postDate&quot;: &quot;2017-01-01&quot; &#125; &#125; &#125; &#125;&#125; 需求4:根据帖子id搜索帖子123456789101112GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot; &#125; &#125; &#125; &#125;&#125; 返回值:1234567891011121314&#123; &quot;took&quot;: 5, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 0, &quot;max_score&quot;: null, &quot;hits&quot;: [] &#125;&#125; 这里可以看到,一条结果也没有,但是应该是有这个数据的,为什么呢? 在添加数据的时候,字符串是默认会去分词,然后建立倒排索引的,而term是不去分词的,所以是查不到的 我们可以用上面es自动建立的keyword来进行搜索123456789101112GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;articleID.keyword&quot;: &quot;XHDK-A-1293-#fJ3&quot; &#125; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot; &#125; &#125; ] &#125;&#125; 这样就可以搜索到了,但是同时也有一个问题,就是keyword只会保留256个字符,如果这个字段太长的话那就还是搜索不到的.这时候,我们最好重建索引,手动设置mapping 删除索引1DELETE /forum 手动创建索引,指定articleID不分词123456789101112PUT /forum&#123; &quot;mappings&quot;: &#123; &quot;article&quot;:&#123; &quot;properties&quot;: &#123; &quot;articleID&quot;:&#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125;&#125; 然后把上面的数据重新添加进去.现在,再用articleID来进行查询123456789101112GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot; &#125; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot; &#125; &#125; ] &#125;&#125; 这时候就可以查询的到了 总结 term filter:根据exact value来进行搜索,数字,Boolean,date类型的天然支持 text类型的field需要在建立的索引的时候指定not_analyzed(新版中可以直接指定type为keyword),才可以使用term]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-37-Java API document 增删改查]]></title>
    <url>%2F2018%2F11%2F29%2FElasticsearch-37-Java-API-document-%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5%2F</url>
    <content type="text"><![CDATA[前文都是讲的理论知识,用restful API来做的测试. 本文将使用java API来操作索引,document. 添加依赖123456789101112&lt;!-- es依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;transport&lt;/artifactId&gt; &lt;version&gt;$&#123;elasticsearch.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;$&#123;elasticsearch.version&#125;&lt;/version&gt;&lt;/dependency&gt; yml配置1234567elasticsearch: ip: 127.0.0.1 port: 9300 pool: 5# 集群名称 cluster: name: elasticsearch 配置client12345678910111213141516171819202122232425262728293031323334353637383940414243@Configuration@Slf4jpublic class ElasticsearchConfig &#123; /** * ip地址 */ @Value(&quot;$&#123;elasticsearch.ip&#125;&quot;) private String hostName; @Value(&quot;$&#123;elasticsearch.port&#125;&quot;) private int port; @Value(&quot;$&#123;elasticsearch.pool&#125;&quot;) private int poolSize; @Value(&quot;$&#123;elasticsearch.cluster.name&#125;&quot;) private String clusterName; @Bean public TransportClient init()&#123; TransportClient transportClient = null; try &#123; // 配置 Settings settings = Settings.builder() .put(&quot;cluster.name&quot;, clusterName) // 集群嗅探机制,找到es集群 .put(&quot;client.transport.sniff&quot;, true) // 增加线程池个数 .put(&quot;thread_pool.search.size&quot;, poolSize) .build(); transportClient = new PreBuiltTransportClient(settings) // 设置地址端口号 .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(hostName), port)); &#125; catch (Exception e)&#123; log.error(&quot;elasticsearch TransportClient init error,&#123;&#125;&quot;, e); &#125; return transportClient; &#125;&#125; 基础的配置已经完成了,接下来就是具体的方法 增删改查节点和索引Util工具类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471@Component@Slf4jpublic class ElasticsearchUtils &#123; @Autowired private TransportClient transportClient; private static TransportClient client; @PostConstruct public void init()&#123; client = this.transportClient; &#125; /** * 判断索引是否存在 * @param indexName 索引名称 * @return true/false */ public static boolean indexExist(String indexName)&#123; IndicesExistsResponse indicesExistsResponse = client.admin() .indices() .exists(new IndicesExistsRequest(indexName)) .actionGet(); if (indicesExistsResponse.isExists())&#123; log.info(&quot;Index [&apos;&#123;&#125;&apos;] is exists&quot;, indexName); &#125; else &#123; log.info(&quot;Index [&apos;&#123;&#125;&apos;] is not exists&quot;, indexName); &#125; return indicesExistsResponse.isExists(); &#125; /** * 创建索引 * @param indexName 索引名称 * @return isAcknowledged */ public static boolean createIndex(String indexName)&#123; if (!indexExist(indexName))&#123; log.info(&quot;Index is not exist&quot;); &#125; CreateIndexResponse response = client.admin() .indices() .prepareCreate(indexName) .execute() .actionGet(); return response.isAcknowledged(); &#125; /** * 删除索引 * @param indexName 索引名称 * @return isAcknowledged */ public static boolean deleteIndex(String indexName)&#123; if (!indexExist(indexName))&#123; log.info(&quot;Index is not exist&quot;); &#125; DeleteIndexResponse response = client.admin() .indices() .prepareDelete(indexName) .execute() .actionGet(); return response.isAcknowledged(); &#125; /** * 创建一个document,需要手动指定id * @param indexName 索引名称 * @param typeName 类型名称 * @param id id * @param xContentBuilder 数据(fields) * @return id */ public static String createDocument(String indexName, String typeName, String id, XContentBuilder xContentBuilder)&#123; IndexResponse response = client .prepareIndex(indexName, typeName, id) .setSource(xContentBuilder) .get(); log.info(&quot;add document response:&#123;&#125;&quot;, response.toString()); return response.getId(); &#125; /** * 创建一个document,不需要手动指定id * @param indexName 索引名称 * @param typeName 类型名称 * @param xContentBuilder 数据(fields) * @return id */ public static String createDocumentWithNoId(String indexName, String typeName, XContentBuilder xContentBuilder)&#123; IndexResponse response = client .prepareIndex(indexName, typeName) .setSource(xContentBuilder) .get(); log.info(&quot;add document response:&#123;&#125;&quot;, response.toString()); return response.getId(); &#125; /** * 更新document,partial update * @param indexName 索引名称 * @param typeName 类型名称 * @param id id * @param xContentBuilder 数据 * @return id */ public static String updateDocument(String indexName, String typeName, String id, XContentBuilder xContentBuilder)&#123; UpdateResponse updateResponse = client .prepareUpdate(indexName, typeName, id) .setDoc(xContentBuilder) .get(); log.info(&quot;update response:&#123;&#125;&quot;, updateResponse.toString()); return updateResponse.getId(); &#125; /** * 删除document * @param indexName 索引名称 * @param typeName 类型名称 * @param id id * @return id */ public static String deleteDocument(String indexName, String typeName, String id)&#123; DeleteResponse response = client .prepareDelete(indexName, typeName, id) .get(); log.info(&quot;delete response:&#123;&#125;&quot;, response.toString()); return response.getId(); &#125; /** * 根据id获取document * @param indexName 索引名称 * @param typeName 类型名称 * @param id id * @return _source数据 */ public static String getDocumentById(String indexName, String typeName, String id)&#123; GetResponse response = client .prepareGet(indexName, typeName, id) .get(); log.info(&quot;get response&quot;); return response.getSourceAsString(); &#125; /** * 只做查询,没有排序 * @param indexes 索引 * @param types 类型 * @param matchMap 搜索条件 * @param fields 要显示的fields,不传返回全部 * @return 结果集 */ public static List&lt;Map&lt;String,Object&gt;&gt; searchDocument(String indexes, String types, Map&lt;String,String&gt; matchMap, String fields)&#123; return searchDocument(indexes, types, 0, 0, matchMap, false, null, fields, null, null, null); &#125; /** * 查询/精准匹配,可以排序 * @param indexes 索引 * @param types 类型 * @param matchMap 查询条件 * @param fields 要显示的fields,不传返回全部 * @param matchPhrase true 使用短语精准匹配 * @param sortField 排序field * @param sortOrder 正序倒序(正序的话需要字段有正排索引) * @return 结果集 */ public static List&lt;Map&lt;String,Object&gt;&gt; searchDocument(String indexes, String types, Map&lt;String,String&gt; matchMap, String fields, boolean matchPhrase, String sortField, SortOrder sortOrder)&#123; return searchDocument(indexes, types, 0, 0, matchMap, matchPhrase, null, fields, sortField, sortOrder, null); &#125; /** * 查询/精准匹配,可以排序,高亮,文档大小限制 * @param indexes 索引 * @param types 类型 * @param matchMap 查询条件 * @param fields 要显示的fields,不传返回全部 * @param matchPhrase true 使用短语精准匹配 * @param sortField 排序field * @param sortOrder 正序倒序(正序的话需要字段有正排索引) * @param highlightField 高亮字段 * @param size 文档大小限制 * @return 结果集 */ public static List&lt;Map&lt;String,Object&gt;&gt; searchDocument(String indexes, String types, Map&lt;String,String&gt; matchMap, String fields, boolean matchPhrase, String sortField, SortOrder sortOrder, String highlightField, Integer size)&#123; return searchDocument(indexes, types, 0, 0, matchMap, matchPhrase, highlightField, fields, sortField, sortOrder, size); &#125; /** * 搜索document * @param indexes 索引名 * @param types 类型 * @param startTime 开始时间 * @param endTime 结束时间 * @param matchMap 查询条件(filed:value) * @param matchPhrase true 使用短语精准匹配 * @param highlightField 高亮显示的field * @param fields 要显示的fields,不传返回全部 * @param sortField 排序field * @param sortOrder 正序倒序(正序的话需要字段有正排索引) * @param size 文档大小限制 * @return 结果集 */ public static List&lt;Map&lt;String, Object&gt;&gt; searchDocument(String indexes, String types, long startTime, long endTime, Map&lt;String,String&gt; matchMap, boolean matchPhrase, String highlightField, String fields, String sortField, SortOrder sortOrder, Integer size)&#123; if (StringUtils.isEmpty(indexes))&#123; return null; &#125; // 构建查询的request body SearchRequestBuilder searchRequestBuilder = client.prepareSearch(indexes.split(&quot;,&quot;)); // 拆分type if (StringUtils.isNotEmpty(types))&#123; searchRequestBuilder.setTypes(types.split(&quot;,&quot;)); &#125; // 组合查询 bool BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery(); // 组装查询条件 boolQueryBuilder = boolQuery(boolQueryBuilder, startTime, endTime, matchMap, matchPhrase); // 设置高亮字段 searchRequestBuilder = setHighlightField(searchRequestBuilder, highlightField); // 搜索条件加到request中 searchRequestBuilder.setQuery(boolQueryBuilder); // 定制返回的fields if (StringUtils.isNotEmpty(fields))&#123; searchRequestBuilder.setFetchSource(fields.split(&quot;,&quot;), null); &#125; searchRequestBuilder.setFetchSource(true); // 设置排序 if (StringUtils.isNotEmpty(sortField))&#123; searchRequestBuilder.addSort(sortField, sortOrder); &#125; // 设置文档大小限制 if (size != null &amp;&amp; size &gt; 0)&#123; searchRequestBuilder.setSize(size); &#125; // 把请求体打印出来 log.info(&quot;查询请求体:&#123;&#125;&quot;, searchRequestBuilder); // 发送请求,执行查询 SearchResponse response = searchRequestBuilder .execute() .actionGet(); long totalHits = response.getHits().totalHits(); long length = response.getHits().getHits().length; log.info(&quot;共查询到[&#123;&#125;]条数据,处理数据条数[&#123;&#125;]&quot;, totalHits, length); if (response.status().getStatus() == 200)&#123; return setSearchResponse(response, highlightField); &#125; return null; &#125; /** * 分页查询 * @param indexes 索引 * @param types 类型 * @param pageNum 页码 * @param pageSize 每页显示数量 * @param startTime 开始时间 * @param endTime 结束时间 * @param fields 要显示的字段 * @param sortField 排序字段 * @param sortOrder 正序倒序(正序需要排序的字段有正排索引) * @param matchPhrase true 精准匹配 * @param highlightField 高亮子弹 * @param matchMap 查询条件 * @return PageVO */ public static PageVO searchDocumentPage(String indexes, String types, int pageNum, int pageSize, long startTime, long endTime, String fields, String sortField, SortOrder sortOrder, boolean matchPhrase, String highlightField, Map&lt;String,String&gt; matchMap)&#123; if (StringUtils.isEmpty(indexes))&#123; return null; &#125; SearchRequestBuilder searchRequestBuilder = client.prepareSearch(indexes.split(&quot;,&quot;)); if (StringUtils.isNotEmpty(types))&#123; searchRequestBuilder.setTypes(types.split(&quot;,&quot;)); &#125; searchRequestBuilder.setSearchType(SearchType.QUERY_THEN_FETCH); // 设置需要显示的字段 if (StringUtils.isNotEmpty(fields))&#123; searchRequestBuilder.setFetchSource(fields.split(&quot;,&quot;), null); &#125; // 设置排序字段 if (StringUtils.isNotEmpty(sortField))&#123; searchRequestBuilder.addSort(sortField, sortOrder); &#125; // 组合查询 bool BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery(); // 组装查询条件 boolQueryBuilder = boolQuery(boolQueryBuilder, startTime, endTime, matchMap, matchPhrase); // 设置高亮字段 searchRequestBuilder = setHighlightField(searchRequestBuilder, highlightField); // 搜索条件加到request中 searchRequestBuilder.setQuery(boolQueryBuilder); searchRequestBuilder.setQuery(QueryBuilders.matchAllQuery()); // 设置分页 searchRequestBuilder.setFrom(pageNum).setSize(pageSize); // 设置按照匹配度排序 searchRequestBuilder.setExplain(true); // 打印请求体 log.info(&quot;请求体:&#123;&#125;&quot;, searchRequestBuilder); // 发送请求,执行查询 SearchResponse response = searchRequestBuilder .execute() .actionGet(); long totalHits = response.getHits().totalHits(); long length = response.getHits().getHits().length; log.info(&quot;共查询到[&#123;&#125;]条数据,处理数据条数[&#123;&#125;]&quot;, totalHits, length); if (response.status().getStatus() == 200)&#123; // 解析查询对象 List&lt;Map&lt;String,Object&gt;&gt; rList = setSearchResponse(response, highlightField); return new PageVO(pageNum, pageSize, (int) totalHits, rList); &#125; return null; &#125; /** * 高亮结果集 特殊处理 * @param searchResponse 查询返回结果 * @param highlightField 高亮字段 * @return 结果 */ public static List&lt;Map&lt;String,Object&gt;&gt; setSearchResponse(SearchResponse searchResponse, String highlightField)&#123; List&lt;Map&lt;String,Object&gt;&gt; sourceList = new ArrayList&lt;&gt;(); StringBuilder stringBuilder = new StringBuilder(); // 循环查询结果 for (SearchHit searchHitFields : searchResponse.getHits().getHits()) &#123; // 把id放到_source里面去 searchHitFields.getSource().put(&quot;id&quot;, searchHitFields.getId()); // 有高亮字段的话做处理 if (StringUtils.isNotEmpty(highlightField))&#123; log.info(&quot;遍历高亮结果集,覆盖正常结果集...&#123;&#125;&quot;, searchHitFields.getSource()); Text[] texts = searchHitFields.getHighlightFields().get(highlightField).getFragments(); if (texts != null)&#123; for (Text text : texts) &#123; stringBuilder.append(text.toString()); &#125; // 遍历高亮结果集,覆盖正常结果集 searchHitFields.getSource().put(highlightField, stringBuilder.toString()); &#125; &#125; sourceList.add(searchHitFields.getSource()); &#125; return sourceList; &#125; /** * 封装 * @param boolQueryBuilder boolQueryBuilder * @param startTime 开始时间 * @param endTime 结束时间 * @param matchMap 查询条件 * @param matchPhrase true 使用精准匹配 * @return boolQueryBuilder */ public static BoolQueryBuilder boolQuery(BoolQueryBuilder boolQueryBuilder, long startTime, long endTime, Map&lt;String, String&gt; matchMap, boolean matchPhrase)&#123; // TODO 不清楚是做什么 if (startTime &gt; 0 &amp;&amp; endTime &gt; 0)&#123; boolQueryBuilder.must(QueryBuilders.rangeQuery(&quot;processTime&quot;) .format(&quot;epoch_millis&quot;) .from(startTime) .to(endTime) .includeLower(true) .includeUpper(true) ); &#125; // 搜索条件 if (!matchMap.isEmpty())&#123; for (Map.Entry&lt;String,String&gt; entry : matchMap.entrySet()) &#123; if (StringUtils.isNoneBlank(entry.getKey(),entry.getValue()))&#123; if (matchPhrase == Boolean.TRUE)&#123; // 精准匹配 boolQueryBuilder.must(QueryBuilders.matchPhraseQuery(entry.getKey(), entry.getValue())); &#125; else &#123; boolQueryBuilder.must(QueryBuilders.matchQuery(entry.getKey(), entry.getValue())); &#125; &#125; &#125; &#125; return boolQueryBuilder; &#125; /** * 封装设置高亮字段 * @param searchRequestBuilder searchRequestBuilder * @param highlightField 高亮字段 * @return searchRequestBuilder */ public static SearchRequestBuilder setHighlightField(SearchRequestBuilder searchRequestBuilder, String highlightField)&#123; // 高亮字段 if (StringUtils.isNotEmpty(highlightField))&#123; HighlightBuilder highlightBuilder = new HighlightBuilder(); // 设置前缀// highlightBuilder.preTags(&quot;&lt;span style=&apos;color:red&apos;&gt;&quot;); // 设置后缀// highlightBuilder.postTags(&quot;&lt;/span&gt;&quot;); // 设置高亮字段 highlightBuilder.field(highlightField); searchRequestBuilder.highlighter(highlightBuilder); &#125; return searchRequestBuilder; &#125;&#125; 分页model12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485/** * @author 周泽 * @date Create in 10:38 2018/11/29 * @Description 分页结果集 */@Getter@Setterpublic class PageVO &#123; /** * 当前页码 */ private Integer pageNum; /** * 一页显示数量 */ private Integer pageSize; /** * 总数量 */ private Integer total; /** * 结果集合 */ private List&lt;Map&lt;String,Object&gt;&gt; rList; /** * 共有多少页 */ private Integer pageCount; /** * 页码列表的开始索引(包含) */ private Integer beginPageIndex; /** * 码列表的结束索引(包含) */ private Integer endPageIndex; /** * 只接受前4个必要的属性，会自动的计算出其他3个属性的值 * @param pageNum 当前页码 * @param pageSize 每页显示条数 * @param total 总条数 * @param rList 结果集合 */ public PageVO(int pageNum, int pageSize, int total, List&lt;Map&lt;String, Object&gt;&gt; rList) &#123; this.pageNum = pageNum; this.pageSize = pageSize; this.total = total; this.rList = rList; // 计算总页码 pageCount = (total + pageSize - 1) / pageSize; // 计算 beginPageIndex 和 endPageIndex // &gt;&gt; 总页数不多于10页，则全部显示 if (pageCount &lt;= 10) &#123; beginPageIndex = 1; endPageIndex = pageCount; &#125; else &#123; // &gt;&gt; 总页数多于10页，则显示当前页附近的共10个页码 // 当前页附近的共10个页码（前4个 + 当前页 + 后5个） beginPageIndex = pageNum - 4; endPageIndex = pageNum + 5; // 当前面的页码不足4个时，则显示前10个页码 if (beginPageIndex &lt; 1) &#123; beginPageIndex = 1; endPageIndex = 10; &#125; // 当后面的页码不足5个时，则显示后10个页码 if (endPageIndex &gt; pageCount) &#123; endPageIndex = pageCount; beginPageIndex = pageCount - 10 + 1; &#125; &#125; &#125;&#125; 单元测试代码在源码里.源码地址]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-36-深度解析document增删改原理及优化过程]]></title>
    <url>%2F2018%2F11%2F27%2FElasticsearch-36-%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90document%E5%A2%9E%E5%88%A0%E6%94%B9%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BC%98%E5%8C%96%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[document写入原理在es底层,用的是Lucene,Lucene底层的index是分为多个segment的,每个segment都会存放部分数据 图中,客户端写入一个document的时候: 先写到了操作系统中的buffer缓存中 然后进行commit point buffer中的数据写入了新的index segment 然后写入操作系统的缓存中 缓存中的index segment被fsync强制刷新到磁盘上 同时新的index segment被打开,供搜索使用 将buffer缓存清空 更新删除原理如果是更新操作,实际上是将现有的document标记为deleted,然后将新的document写入新的index segment中,下次search过来的时候,也许会匹配到一个document的多个版本,但是之前的版本已经被标记为deleted了,所以会被过滤掉,不会作为搜索结果返回,删除操作同理. 每次commit point时,会有一个.del文件,标记了哪些segment中的哪些document被标记为deleted了搜索的时候回依次查询所有的segment,从旧的到新的,比如被修改过的document,在旧的segment中,会被标记为deleted,在新的segment中会有其新的数据 问题如果按照上面的流程的话,每次都必须等待fsync将segment刷入磁盘,才能将segment打开供search使用,这样的话,从一个document写入,到它可以被搜索,可能会超过1分钟,这就不是近实时的搜索了, 主要瓶颈在于fsync实际发生磁盘IO写数据进磁盘是很耗时的. 流程改进 数据写入buffer中 每隔一定的时间(默认是1s),buffer中的数据被写入新的segment文件,然后写入os cache中 只要segment写入到了os cache中了,那就直接打开index segment 供使用,不立即commit. 最后把buffer清空 数据写入os cache并被打开供搜索的过程,叫做refresh,默认是每隔1s refresh一次,就是说每隔一秒,就会将buffer中的数据写入一个新的index segment文件,先写入os cache中.所以es是近实时的,数据写入到可以搜索,默认是1秒. 我们也可以手动去设置refresh的间隔时间,比如时效性要求较低,写入数据一分钟后被搜索到就可以了123456PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;refresh_interval&quot;: &quot;60s&quot; &#125;&#125; 问题数据不及时写入到磁盘中,而是在缓存中,如果宕机的话,数据就会丢失,就不可靠了 再次优化写入流程 写入document的时候,数据同时写入buffer缓冲和translog日志文件 每隔1秒中,buffer中的数据被写入新的segment file,并进入os cache中,此时segment被打开并供search使用 buffer被清空 重复1-3,新的segment不断添加,buffer不断被清空,而translog中的数据不断累加 当translog长度达到一定的程度的时候,commit操作发生.5.1. buffer中所有数据写入一个新的segment中,并写入os cache 打开供使用.5.2. buffer被清空5.3. 一个commit point被写入磁盘,标明了所有的index segment5.4. os cache中的所有 数据被fsync强行刷到磁盘上去5.5. 现有的translog被清空,创建一个新的translog 基于translog和commit point进行数据恢复磁盘上存储的是上次commit point为止,所有的segment file,那么translog中存储的就是上一次flush(commit point)知道现在最近的数据变更记录如果说 os cache中已经囤积了一些数据,没有被刷到磁盘上,这个时候宕机了, 这时候机器重启,此时会将translog文件中的变更记录进行回放,重新执行之前的各种操作,等待下次commit即可 每次flush 会自动清空translog,默认每隔30分钟flush一次,或者当translog过大的时候,也会flush.我们也可以手动flush,POST /my_index/_flush,一般来说别手动flush,让它自动执行就可以了 translog,也是先放在缓存中的,每隔5秒被fsync一次到磁盘上.一般是在一次增删改操作之后. 如果说在一次增删改操作的时候正好要fsync translog到磁盘上,那么会等待primary shard和replica shard都成功之后,这次增删改操作才会成功 但是这种在一次增删改时强行fsync translog可能会导致部分操作比较耗时如果可以允许部分数据丢失,可以设置异步fsync translog 12345PUT /my_index/_settings&#123; &quot;index.translog.durability&quot;: &quot;async&quot;, &quot;index.translog.sync_interval&quot;: &quot;5s&quot;&#125; 终极优化上面说的,每秒生成一个segment文件,文件会越来越多,而且每次search都要搜索所有的segment,很耗时es会默认在后台执行合并的操作,在merge的时候,被标记为deleted的document 也会被彻底物理删除 每次merge的流程是 选择一些有相似大小的segment,merge成一个大的segment 将新的segment flush到磁盘上去 写一个新的commit point,包括了新的segment,并排除旧的那些segment 将新的segment打开供搜索 将旧的segment删除 也可以通过 POST /my_index/_optimize?max_num_segments=1 来手动合并,但是尽量不要手动执行]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-35-使用scroll+bulk+索引别名实现零停机重建索引]]></title>
    <url>%2F2018%2F11%2F27%2FElasticsearch-35-%E4%BD%BF%E7%94%A8scroll-bulk-%E7%B4%A2%E5%BC%95%E5%88%AB%E5%90%8D%E5%AE%9E%E7%8E%B0%E9%9B%B6%E5%81%9C%E6%9C%BA%E9%87%8D%E5%BB%BA%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[场景如果我们一开始新建了一个索引,并且依靠dynamic mapping,这个时候插入一条数据是2018-01-01这种格式的,这field就会被自动映射成了date类型,但是其实他应该是个string类型的,这时候应该怎么做呢? 解决方案一个field的设置是不能被修改的,如果要修改一个field,那么应该重新按照新的mapping来创建一个index,然后将旧的index中的数据查询出来,用_bulk api批量插入到新的索引中去 批量查询的时候,建议采用scroll api,采用多线程并发的方式来reindex数据. 案例我们先插入一条数据如下: 1234PUT /old_my_index/my_type/1&#123; &quot;title&quot;:&quot;2017-01-01&quot;&#125; 然后获取这个index的mapping 12345678910111213&#123; &quot;old_my_index&quot;: &#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125; &#125; &#125; &#125; &#125;&#125; 可以看到title已经被映射成了date类型,这时 如果我们在添加一个字符串的值是添加不进去的. 而且如果想修改这个field的类型也是不可能的 此时唯一的办法就是进行reindex,也就是说重新建立一个索引,将旧索引中的数据查询出来,导入新索引 这里可能会有一个问题,旧的索引名称是old_my_index,假如新的索引名称是new_my_index, 这时候已经有一个java应用在使用old_my_index在操作了, 那么这时候是不是要先停止应用,修改索引,然后重启呢? 这样的话会导致java应用停机,降低可用性 针对上面的问题呢,我们可以先给java应用一个旧索引的别名, java用的只是一个别名,指向旧的索引 1PUT /old_my_index/_alias/my_index 执行上面的代码,就是给了old_my_index一个别名(my_index),然后我们新建一个索引,将title这个field调整为string类型的 123456789101112PUT /new_my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;:&#123; &quot;properties&quot;: &#123; &quot;title&quot;:&#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; &#125;&#125; 新建完成以后,用scroll api从旧的索引中查询数据12345678GET old_my_index/my_type/_search?scroll=1m&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [&quot;_doc&quot;], &quot;size&quot;: 1&#125; 返回值:12345678910111213141516171819202122232425262728&#123; &quot;_scroll_id&quot;: &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAAHVFmY1N3VWOTF4U19HUlRRUzJIbzgxcmcAAAAAAAAB1xZmNTd1VjkxeFNfR1JUUVMySG84MXJnAAAAAAAAAdQWZjU3dVY5MXhTX0dSVFFTMkhvODFyZwAAAAAAAAHYFmY1N3VWOTF4U19HUlRRUzJIbzgxcmcAAAAAAAAB1hZmNTd1VjkxeFNfR1JUUVMySG84MXJn&quot;, &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: null, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;old_my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;2017-01-01&quot; &#125;, &quot;sort&quot;: [ 0 ] &#125; ] &#125;&#125; 查询出来以后用bulk api将scroll查出来的一批数据,批量写入新的索引. 123POST /_bulk&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;new_my_index&quot;,&quot;_type&quot;:&quot;my_type&quot;,&quot;_id&quot;:&quot;1&quot;&#125;&#125;&#123;&quot;title&quot;:&quot;2017-01-01&quot;&#125; 重复循环scroll查询和bulk批量插入,直到所有的数据都添加到了新的索引中 添加完成后,将别名切换到新的索引上去,这样的话java应用就直接通过别名使用新的索引中的数据了,不需要停机重启,高可用 1234567891011121314151617POST /_aliases&#123; &quot;actions&quot;: [ &#123; &quot;remove&quot;: &#123; // 把别名从旧的索引上先移除 &quot;index&quot;: &quot;old_my_index&quot;, &quot;alias&quot;: &quot;my_index&quot; &#125; &#125;, &#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;new_my_index&quot;, // 将别名指向新的索引 &quot;alias&quot;: &quot;my_index&quot; &#125; &#125; ]&#125; 总结总体来说,就是最开始就给索引一个别名去让客户端去使用,然后如果要切换索引的话,就先建一个索引,然后查询旧的索引数据,将数据插入到新的索引中,完成后将别名指向新的索引,就实现了零停机重建索引]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-34-定制自己的dynamic mapping策略]]></title>
    <url>%2F2018%2F11%2F26%2FElasticsearch-34-%E5%AE%9A%E5%88%B6%E8%87%AA%E5%B7%B1%E7%9A%84dynamic-mapping%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[定制dynamic策略true: 遇到陌生字段,就进行dynamic mappingfalse: 遇到陌生字段,就忽略strict: 遇到陌生字段,就报错 示例我们现在来新建一个index.1234567891011121314151617PUT /my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;dynamic&quot;:&quot;strict&quot;, // 1 &quot;properties&quot;: &#123; &quot;title&quot;:&#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;address&quot;:&#123; &quot;type&quot;: &quot;object&quot;, &quot;dynamic&quot;:&quot;true&quot; // 2 &#125; &#125; &#125; &#125;&#125; 1处,我们设置了这个my_type的dynamic是strict,就是遇到陌生的字段,就报错2处,设置了address这个object类型的filed的dynamic是true, 遇到陌生字段就进行dynamic mapping 先来放一条数据进去123456789PUT /my_index/my_type/1&#123; &quot;title&quot;:&quot;my title&quot;, &quot;content&quot;:&quot;test content&quot;, // 1 &quot;address&quot;:&#123; &quot;province&quot;:&quot;zhejiang&quot;, // 2 &quot;city&quot;:&quot;hangzhou&quot; // 3 &#125;&#125; 1处,content这个field 我们创建索引时,并没有设置设置这个content, dynamic是strict,遇到陌生字段应该报错2,3处的province和city,我们也没有设置, address的dynamic策略应该是遇到陌生字段就进行dynamic mapping 运行上面代码,返回值:12345678910111213&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;strict_dynamic_mapping_exception&quot;, &quot;reason&quot;: &quot;mapping set to strict, dynamic introduction of [content] within [my_type] is not allowed&quot; &#125; ], &quot;type&quot;: &quot;strict_dynamic_mapping_exception&quot;, &quot;reason&quot;: &quot;mapping set to strict, dynamic introduction of [content] within [my_type] is not allowed&quot; &#125;, &quot;status&quot;: 400&#125; 报错了因为我们在my_type中设置的dynamic是strict 然后把content这个field删掉12345678PUT /my_index/my_type/1&#123; &quot;title&quot;:&quot;my title&quot;, &quot;address&quot;:&#123; &quot;province&quot;:&quot;zhejiang&quot;, &quot;city&quot;:&quot;hangzhou&quot; &#125;&#125; 执行后,添加成功. 然后我们来查询一下这个type的mapping1GET /my_index/_mapping/my_type 返回值:12345678910111213141516171819202122232425262728293031323334353637&#123; &quot;my_index&quot;: &#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;dynamic&quot;: &quot;strict&quot;, &quot;properties&quot;: &#123; &quot;address&quot;: &#123; &quot;dynamic&quot;: &quot;true&quot;, &quot;properties&quot;: &#123; &quot;city&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;province&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125;, &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; &#125; &#125;&#125; 可以看到address中的province和city这两个字段已经被自动dynamic mapping了 定制dynamic mapping策略date_detectiones会默认按照一定的格式识别date类型,比如yyyy-MM-dd,但是如果某个field先过来一个2018-01-01的值,就会被自动dynamic mapping 成 date类型,后面如果再来一个”hello word”之类的值,就会报错.我们可以手动关闭某个type的date_detection,如果有需要,自己手动指定某个field为date类型.1234PUT /index/_mapping/type&#123; &quot;date_detection&quot;: false&#125; 定制自己的dynamic mapping template (type级别)首先我们需要在建索引的时候添加一个模板. 12345678910111213141516171819PUT /my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; // type名称 &quot;dynamic_templates&quot;:[ &#123; &quot;en&quot;:&#123; // 模板名称,自定义的 &quot;match&quot;:&quot;*_en&quot;, // 通配符匹配_en结尾的field &quot;match_mapping_type&quot;:&quot;string&quot;, &quot;mapping&quot;:&#123; &quot;type&quot;:&quot;string&quot;, &quot;analyzer&quot;:&quot;english&quot; // english分词器 &#125; &#125; &#125; ] &#125; &#125;&#125; 上面这段代码就是说 field名称是_en结尾的话,就是string类型的,分词器是english分词器 我们来添加两条数据,然后查询测试一下 123456789PUT /my_index/my_type/1&#123; &quot;title&quot;: &quot;this is my first article&quot;&#125;PUT /my_index/my_type/2&#123; &quot;title_en&quot;: &quot;this is my first article&quot;&#125; 分别用title 和 title_en去匹配 is这个词.会发现 用title_en是匹配不到的 title没有匹配到任何的dynamic模板,默认就是standard分词器,不会过滤停用词,is会进入倒排索引,用is来搜索是可以搜索到的title_en匹配到了dynamic模板,就是english分词器,会过滤停用词,is这种停用词就会被过滤掉,用is来搜索就搜索不到了 定制自己的dynamic mapping template (index级别)例: 1234567891011PUT /my_index&#123; &quot;mappings&quot;: &#123; &quot;_default_&quot;: &#123; &quot;_all&quot;: &#123; &quot;enabled&quot;: false &#125; &#125;, &quot;blog&quot;: &#123; &quot;_all&quot;: &#123; &quot;enabled&quot;: true &#125; &#125; &#125;&#125; 就是说默认的type的_all是禁用的, blog这个type的_all是启用的.]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-33-_mapping root object深入剖析]]></title>
    <url>%2F2018%2F11%2F26%2FElasticsearch-33-mapping-root-object%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[root object就是某个type对应的mapping json,包括了properties,metadata(_id,_source,_type), settings(analyzer),其他settings(比如include_in_all) 12345678PUT /index&#123; &quot;mappings&quot;: &#123; &quot;type&quot;: &#123; // 这里面的json就是这个type的 root object &#125; &#125;&#125; properties主要包括了各个field的数据类型,分不分词,用哪个分词器等. _source_source的好处: 查询的时候,可以直接拿到完整的document,不需要先拿到document id,再发送一起请求拿document. partial update是基于_source实现的. reindex时,直接基于_source实现,不需要从数据库(或者其他外部存储)查询数据再修改. 可以基于_source定制返回field. debug query更容易,因为可以直接看到_source. 如果不需要用到上面这些的话,可以禁用_source1234PUT /index/_mapping/type&#123; &quot;_source&quot;: &#123;&quot;enabled&quot;: false&#125;&#125; _all_all我们之前有详细介绍过,就是将所有field打包在一起,作为一个_all field,建立索引,没指定任何field进行搜索的时候,就是使用_all field在搜索.当然如果不需要用到的话也可以设置关闭1234PUT /index/_mapping/type&#123; &quot;_all&quot;: &#123;&quot;enabled&quot;: false&#125;&#125; 也可以在field级别设置include_in_all field,设置是否将filed的值包含在_all中123456789PUT /index/_mapping/type&#123; &quot;properties&quot;: &#123; &quot;field&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;include_in_all&quot;: false &#125; &#125;&#125; 标识性metadata包括_index,_type,_id]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-32-type底层数据结构]]></title>
    <url>%2F2018%2F11%2F26%2FElasticsearch-32-type%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[type的底层数据结构type是一个index中用来区分类似的数据的,类似的数据有可能有不同的field,而且有不同的属性来控制索引的建立 es是基于Lucene的,在es中每个field都有自己的数据类型,比如date,text等,但在底层的Lucene建立索引的时候,全部是opaque bytes类型,不区分类型的. Lucene是没有type的概念的,在document中,实际上是将type作为document的一个field类存储,即_type,es通过_type来进行过滤和筛选 一个index中的多个type,实际上是放在一起存储的,因此一个index下,不能有多个type重名. 举例说明现在,在ecommerce这个index下,有两个type,一个是elactronic_goods,另一个是fresh_goods,如下:1234567891011121314151617181920212223242526272829303132&#123; &quot;ecommerce&quot;: &#123; &quot;mappings&quot;: &#123; &quot;elactronic_goods&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &#125;, &quot;price&quot;: &#123; &quot;type&quot;: &quot;double&quot; &#125;, &quot;service_period&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125; &#125; &#125;, &quot;fresh_goods&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &#125;, &quot;price&quot;: &#123; &quot;type&quot;: &quot;double&quot; &#125;, &quot;eat_period&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125; &#125; &#125; &#125; &#125;&#125; 我们现在有两个document,分别是两个type下的数据,如下123456// type是elactronic_goods&#123; &quot;name&quot;: &quot;geli kongtiao&quot;, &quot;price&quot;: 1999.0, &quot;service_period&quot;: &quot;one year&quot;&#125; 123456// type是fresh_goods&#123; &quot;name&quot;: &quot;aozhou dalongxia&quot;, &quot;price&quot;: 199.0, &quot;eat_period&quot;: &quot;one week&quot;&#125; 这个index的底层存储是这样的:12345678910111213141516171819202122&#123; &quot;ecommerce&quot;: &#123; &quot;mappings&quot;: &#123; &quot;_type&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125; &quot;price&quot;: &#123; &quot;type&quot;: &quot;double&quot; &#125; &quot;service_period&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125; &quot;eat_period&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125; &#125; &#125;&#125; 可以看到type被当做了一个属性放到了document中, elactronic_goods,fresh_goods这两个type中不同的属性也被放到了一起 这两个document在底层的存储是:1234567&#123; &quot;_type&quot;: &quot;elactronic_goods&quot;, &quot;name&quot;: &quot;geli kongtiao&quot;, &quot;price&quot;: 1999.0, &quot;service_period&quot;: &quot;one year&quot;, &quot;eat_period&quot;: &quot;&quot;&#125; 1234567&#123; &quot;_type&quot;: &quot;fresh_goods&quot;, &quot;name&quot;: &quot;aozhou dalongxia&quot;, &quot;price&quot;: 199.0, &quot;service_period&quot;: &quot;&quot;, &quot;eat_period&quot;: &quot;one week&quot;&#125; 所以说,将类似结构的type放在一个index下,这些type应该有多个field是相同的. 假如说,两个type的field完全不同,放在一个index下,那么每条数据的很多field在底层的Lucene中是空置,会有严重的性能问题]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-31-手动创建索引以及定制分词器]]></title>
    <url>%2F2018%2F11%2F24%2FElasticsearch-31-%E6%89%8B%E5%8A%A8%E5%88%9B%E5%BB%BA%E7%B4%A2%E5%BC%95%E4%BB%A5%E5%8F%8A%E5%AE%9A%E5%88%B6%E5%88%86%E8%AF%8D%E5%99%A8%2F</url>
    <content type="text"><![CDATA[索引创建索引语法:1234567891011121314PUT /index&#123; &quot;settings&quot;:&#123; // any settings... &#125;, &quot;mappings&quot;:&#123; type1:&#123; // any settings... &#125;, type2:&#123; // any settings... &#125; &#125;&#125; 示例: 12345678910111213141516PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1, // primary shard的数量 &quot;number_of_replicas&quot;: 0 // replica shard 的数量 &#125;, &quot;mappings&quot;: &#123; &quot;my_type&quot;:&#123; &quot;properties&quot;: &#123; &quot;field1&quot;:&#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; &#125;&#125; 修改索引语法:1234PUT /index/_settings&#123; // any settings&#125; 示例:1234PUT /my_index/_settings&#123; &quot;number_of_replicas&quot;: 1 // 修改replica shard 的数量&#125; 删除索引1234DELETE /index DELETE /index1,index2DELETE /index_* // 通配符删除DELETE /_all // 删除全部 在elasticsearch.yml中设置action.destructive_requires_name: true,以后就不能使用 _all删除全部了 分词器修改分词器之前我们说过,es默认的分词器就是standard,他做了以下几件事:standard tokenizer:以单词边界进行切分standard token filter:什么都不做lowercase token filter:将所有字母转换为小写stop token filer(默认被禁用):移除停用词,比如a the it等等 我们先来新建一个索引,并启用english stop token filer12345678910111213PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; // 分词器相关 &quot;analyzer&quot;: &#123; // 分词器 &quot;es_std&quot;:&#123; // 自定义名称 &quot;type&quot;:&quot;standard&quot;, // 分词器类型 &quot;stopwords&quot;:&quot;_english_&quot; &#125; &#125; &#125; &#125;&#125; 执行成功后我们用之前说的测试分词器的方法来测试一下 12345GET /my_index/_analyze&#123; &quot;analyzer&quot;: &quot;es_std&quot;, // 我们上面定义的分词器名称 &quot;text&quot;: &quot;a dog is in the house&quot;&#125; 返回结果:123456789101112131415161718&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;dog&quot;, &quot;start_offset&quot;: 2, &quot;end_offset&quot;: 5, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;house&quot;, &quot;start_offset&quot;: 16, &quot;end_offset&quot;: 21, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 5 &#125; ]&#125; 可以看到停用词已经被去掉了 定制自己的分词器我们先把创建的这个索引删除掉1DELETE /my_index 然后手动定制分词器12345678910111213141516171819202122232425262728PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;char_filter&quot;: &#123; // 字符转换 &quot;&amp;_to_and&quot;:&#123; &quot;type&quot;:&quot;mapping&quot;, &quot;mappings&quot;:[&quot;&amp; =&gt; and &quot;] //&amp; 转成 and &#125; &#125;, &quot;filter&quot;: &#123; &quot;my_stop_words&quot;:&#123; // 自定义停用词过滤 &quot;type&quot;:&quot;stop&quot;, &quot;stopwords&quot;:[&quot;the&quot;,&quot;a&quot;] // 要过滤的词 &#125; &#125;, &quot;analyzer&quot;: &#123; &quot;my_analyzer&quot;:&#123; // 自定义名称 &quot;type&quot;:&quot;custom&quot;, &quot;char_filter&quot;:[&quot;html_strip&quot;,&quot;&amp;_to_and&quot;], // html脚本过滤和上面定义的&amp;_to_and &quot;tokenizer&quot;:&quot;standard&quot;, &quot;filter&quot;:[&quot;lowercase&quot;,&quot;my_stop_words&quot;] // 大小写转换 和 上面定义的停用词过滤 &#125; &#125; &#125; &#125;&#125; 执行完毕后来测试一下 12345GET /my_index/_analyze&#123; &quot;analyzer&quot;: &quot;my_analyzer&quot;, &quot;text&quot;: &quot;tom&amp;jerry are a friend in the house, &lt;a&gt;, HAHA!!&quot;&#125; 返回值:12345678910111213141516171819202122232425262728293031323334353637383940414243444546&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;tomandjerry&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 9, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;are&quot;, &quot;start_offset&quot;: 10, &quot;end_offset&quot;: 13, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;friend&quot;, &quot;start_offset&quot;: 16, &quot;end_offset&quot;: 22, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 3 &#125;, &#123; &quot;token&quot;: &quot;in&quot;, &quot;start_offset&quot;: 23, &quot;end_offset&quot;: 25, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 4 &#125;, &#123; &quot;token&quot;: &quot;house&quot;, &quot;start_offset&quot;: 30, &quot;end_offset&quot;: 35, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 6 &#125;, &#123; &quot;token&quot;: &quot;haha&quot;, &quot;start_offset&quot;: 42, &quot;end_offset&quot;: 46, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 7 &#125; ]&#125; a the 这两个停用词被去掉了,&amp;也转为and了,a标签被过滤掉,最后的大写也转成了小写 使用自定义分词器上面我们自定义的分词器已经可以使用了,那么如何让type中的某个filed来使用我们自定义的分词器123456789PUT /my_index/_mapping/my_type &#123; &quot;properties&quot;: &#123; &quot;content&quot;:&#123; // field名称 &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;my_analyzer&quot; // 分词器名称 &#125; &#125;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-30-scroll滚动查询]]></title>
    <url>%2F2018%2F11%2F24%2FElasticsearch-30-scroll%E6%BB%9A%E5%8A%A8%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[scroll查询如果我们要一次性查询10万条数据,那么性能会很差,此时一般会采用scroll滚动查询,一批一批的查,直到所有的数据都查询处理完成. 使用scroll滚动搜索,可以先搜索一批数据,然后下次再搜索一批数据,以此类推,直到搜索出全部的数据来 scroll搜索会在第一次搜索的时候,保存一个当前识图的快照,之后只会基于该旧的视图快照提供数据搜索,如果这个期间数据变更是不会让用户看到的. scroll搜索一般不会用_score相关分数去排序, 采用基于 _doc进行排序,性能比较高. 每次发送scroll请求,我们还需要指定一个scroll参数,指定一个时间窗口,每次搜索请求只要在这个时间窗口内能完成就可以了 示例在test_index/test_type下一共有5条数据,然后使用scroll滚动搜索,每次查询2条12345678910GET test_index/test_type/_search?scroll=1m&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &quot;_doc&quot; ], &quot;size&quot;: 2&#125; 返回值:1234567891011121314151617181920212223242526272829303132333435363738394041&#123; &quot;_scroll_id&quot;: &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAAJcFmY1N3VWOTF4U19HUlRRUzJIbzgxcmcAAAAAAAACWBZmNTd1VjkxeFNfR1JUUVMySG84MXJnAAAAAAAAAlsWZjU3dVY5MXhTX0dSVFFTMkhvODFyZwAAAAAAAAJaFmY1N3VWOTF4U19HUlRRUzJIbzgxcmcAAAAAAAACWRZmNTd1VjkxeFNfR1JUUVMySG84MXJn&quot;, &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 5, &quot;max_score&quot;: null, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;AWccvc7blcpuqacodv57&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;test_content&quot;: &quot;test1&quot;, &quot;test_title&quot;: &quot;test2&quot; &#125;, &quot;sort&quot;: [ 0 ] &#125;, &#123; &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;AWcctb8Zlcpuqacodv55&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;test_content&quot;: &quot;test1&quot; &#125;, &quot;sort&quot;: [ 0 ] &#125; ] &#125;&#125; 可以看到,返回值中有一个scroll_id,下次请求时要把这个scroll_id传过去,而且要在上次查询传过去的时间窗口scroll=1m,这个时间内进行第二次查询 GET /_search/scroll { &quot;scroll&quot;:&quot;1m&quot;, &quot;scroll_id&quot;:&quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAAJcFmY1N3VWOTF4U19HUlRRUzJIbzgxcmcAAAAAAAACWBZmNTd1VjkxeFNfR1JUUVMySG84MXJnAAAAAAAAAlsWZjU3dVY5MXhTX0dSVFFTMkhvODFyZwAAAAAAAAJaFmY1N3VWOTF4U19HUlRRUzJIbzgxcmcAAAAAAAACWRZmNTd1VjkxeFNfR1JUUVMySG84MXJn&quot; } scroll查询看起来挺像分页的,但是其实使用场景不一样,分页主要是用来一页一页搜索,给用户看的,scroll查询主要是用来一批一批检索数据的,让系统进行处理]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-29-搜索原理内核解析]]></title>
    <url>%2F2018%2F11%2F24%2FElasticsearch-29-%E6%90%9C%E7%B4%A2%E5%8E%9F%E7%90%86%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[query phase假设我们有一个index里面的数据分布在3个primary shard上(对应的replica也有),现在总共有7个shard,我们现在要搜索这个index中的数据的第10000条到10010条.如图所示 请求发送给某一个shard时,这个shard就是coordinate node, coordinate node会构建一个 priority queue,队列长度是查询时的from和size的和,默认是0 + 10 = 10; 我们要查询的是10000-10010条数据,所以请求的from = 9999,size = 10,这个时候coordinate node会在它本地建立一个长度是 9999 + 10 = 10009 的 priority queue, 然后coordinate node将请求打到其他的shard上去 接收到请求的每个shard,也会在本地建立一个 from + size大小的priority queue,每个shard将自己下标是0 - 10009的数据放到这个队列中, 也就是10010条数据,返回给coordinate node. coordinate node 将返回的所有数据进行合并,合并成一份from * size大小的priority queue,全局排序后,放到自己队列中去 最后在自己的队列中取出当前要获取的那一页的数据. 这里也可以看出我们之前提到过的deep paging问题,就是说,from * size分页太深,那么每个shard都要返回大量的数据给coordinate node,消耗大量的带宽,内存, CPU fetch phase在上面的query phase的工作处理完成之后,coordinate node 在priority queue里面找到了需要的数据, 但是其实这个队列时存的document的id, 这个时候,coordinate node就发送mget请求(批量查询)到所有shard上去获取对应的document 然后各个shard将document返回给coordinate node, coordinate node将合并后的document结果返回给客户端 bouncing results问题比如说有两个document,field值相同;但是分布在不同的shard上面,在不同的shard上可能排序也不相同, 每次请求轮询打到不同的shard上去,页面上看到的搜索结果的排序可能都不一样, 这就是 bouncing results,也就是跳跃的结果. preferencepreference 决定了哪些shard会执行搜索请求. bouncing results问题解决将preference设置为一个字符串,比如说user_id,让每个user每次搜索的时候,都使用同一个shard去执行,就不会看到bouncing results了]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-28-doc values初步了解]]></title>
    <url>%2F2018%2F11%2F23%2FElasticsearch-28-doc-values%E5%88%9D%E6%AD%A5%E4%BA%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[doc value搜索的时候,要依靠倒排索引去搜索,但是在排序的时候需要依靠正排索引,找到每个document的每个field,然后进行排序,所谓的正排索引,其实就是doc values es在建立索引的时候,一方面会建立倒排索引,以供搜索使用;一方面还会建立正排索引,也就是doc values,以供排序,聚合,过滤等操作使用 doc values是被保存在磁盘上的,此时如果内存足够,os会自动将其缓存在内存中,性能还是很高的,如果内存不够,os会将其写入到磁盘上 举例比如有两个document,数据如下document1: { “name”: “jack”, “age”: 27 }document2: { “name”: “tom”, “age”: 30 } 在es建立正排索引的时候就是这样子的 document name age document1 jack 27 document2 ton 30 这样在排序的时候es直接拿到正排索引里面的某一列去排序就好了]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-27-搜索相关度TF&IDF算法]]></title>
    <url>%2F2018%2F11%2F23%2FElasticsearch-27-%E6%90%9C%E7%B4%A2%E7%9B%B8%E5%85%B3%E5%BA%A6TF-IDF%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[算法介绍relevance score算法,简单来说就是计算出一个索引中的文本,与搜索文本,他们之间的关联匹配程度 Elasticsearch使用的是 term frequency/inverse document frequency算法，简称为TF/IDF算法 TF算法(Term frequency)Term frequency:搜索文本中的各个词条在field文本中出现了多少次,出现次数越多就越相关 举个例子:搜索请求是: hello worlddocument1:hello you, and world is very gooddocument2:hello, how are you hello和world这两个词在document1中出现了两次,document2中出现了一次,所以document更相关 IDF算法(inverse document frequency)inverse document frequency: 搜索文本中的各个词条在整个索引的所有document中出现了多少次,出现的次数越多,就越不相关 举例:搜索请求是:hello worlddocument1:hello, today is very gooddocument2:hi world, how are you 看起来hello和world是每个document都出先一次,但是这个应该是document2更相关 比如说在index中现在有一万条document,hello这个单词在所有的document中出现了1000次,world这个单词在所有的document中出现了100次,所以document2就更相关 Field-length normField-length norm: field的值长度越长,相关度越弱 举例:搜索请求:hello worlddocument1: { “title”: “hello article”, “content”: “babaaba…..(1万个单词)” }document2: { “title”: “my article”, “content”: “blablabala…. (1万个单词),hi world” }这个时候hello 和 world这两个词在整个index中出现的次数是一样多的,但是document1更相关,因为title这个filed中的数据短 查询_score是如何被计算出来的语法:12345678GET /index/type/_search?explain&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;field&quot;: &quot;text&quot; &#125; &#125;&#125; 分析一个document是如何被匹配上的语法:12345678GET /index/type/id/_explain&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;field&quot;: &quot;text&quot; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-26-字符串排序问题及解决方案]]></title>
    <url>%2F2018%2F11%2F23%2FElasticsearch-26-%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%8E%92%E5%BA%8F%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[字符串排序问题如果对一个string类型的field进行排序,结果往往不准确,因为string类型的field要进行分词,分词后是多个单词,再排序就不是我们想要的结果了 如何解决通常解决方式是,将一个string类型的field建立两次索引,一个分词用来进行搜索,一个不分词用来排序 示例我们之前建立过一个website的索引,先把它删除掉1DELETE /website 然后重新建立索引并手动创建mapping.12345678910111213141516171819202122232425262728PUT /website&#123; &quot;mappings&quot;: &#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; // 这里是重点,title里面在建立一个 string类型的field &quot;raw&quot;:&#123; // 名称 &quot;type&quot;: &quot;string&quot;, // 数据类型,不分词只能是string &quot;index&quot;: &quot;not_analyzed&quot; // 指定不分词 &#125; &#125;, &quot;fielddata&quot;: true // 建立正排索引,这个后面详细说 &#125;, &quot;content&quot;:&#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;post_date&quot;:&#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;author_id&quot;:&#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125; &#125;&#125; 建立好之后,往里面添加点数据 1234567891011121314151617181920212223PUT /website/article/1&#123; &quot;title&quot;: &quot;second article&quot;, &quot;content&quot;: &quot;this is my second article&quot;, &quot;post_date&quot;: &quot;2017-02-01&quot;, &quot;author_id&quot;: 110&#125;PUT /website/article/2&#123; &quot;title&quot;: &quot;first article&quot;, &quot;content&quot;: &quot;this is my frist article&quot;, &quot;post_date&quot;: &quot;2017-01-01&quot;, &quot;author_id&quot;: 110&#125;PUT /website/article/3&#123; &quot;title&quot;: &quot;third article&quot;, &quot;content&quot;: &quot;this is my third article&quot;, &quot;post_date&quot;: &quot;2017-03-01&quot;, &quot;author_id&quot;: 110&#125; 数据添加完成,我们来查询按照title排序一下12345678910111213GET /website/article/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;title&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ]&#125; 返回结果:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&#123; &quot;took&quot;: 1, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: null, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;third article&quot;, &quot;content&quot;: &quot;this is my third article&quot;, &quot;post_date&quot;: &quot;2017-03-01&quot;, &quot;author_id&quot;: 110 &#125;, &quot;sort&quot;: [ &quot;third&quot; ] &#125;, &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;second article&quot;, &quot;content&quot;: &quot;this is my second article&quot;, &quot;post_date&quot;: &quot;2017-02-01&quot;, &quot;author_id&quot;: 110 &#125;, &quot;sort&quot;: [ &quot;second&quot; ] &#125;, &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;first article&quot;, &quot;content&quot;: &quot;this is my frist article&quot;, &quot;post_date&quot;: &quot;2017-01-01&quot;, &quot;author_id&quot;: 110 &#125;, &quot;sort&quot;: [ &quot;first&quot; ] &#125; ] &#125;&#125; 可以看到 返回值中的sort这一列,是按照分词之后进行排序的,然后用我们上面创建出来title.raw来进行排序看下效果12345678910111213GET /website/article/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;title.raw&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ]&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&#123; &quot;took&quot;: 1, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: null, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;third article&quot;, &quot;content&quot;: &quot;this is my third article&quot;, &quot;post_date&quot;: &quot;2017-03-01&quot;, &quot;author_id&quot;: 110 &#125;, &quot;sort&quot;: [ &quot;third article&quot; ] &#125;, &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;second article&quot;, &quot;content&quot;: &quot;this is my second article&quot;, &quot;post_date&quot;: &quot;2017-02-01&quot;, &quot;author_id&quot;: 110 &#125;, &quot;sort&quot;: [ &quot;second article&quot; ] &#125;, &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;first article&quot;, &quot;content&quot;: &quot;this is my frist article&quot;, &quot;post_date&quot;: &quot;2017-01-01&quot;, &quot;author_id&quot;: 110 &#125;, &quot;sort&quot;: [ &quot;first article&quot; ] &#125; ] &#125;&#125; 再来看一下返回值中的sort ,这样排序就没有分词而是直接去排序的]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-25-Query DSL常用查询]]></title>
    <url>%2F2018%2F11%2F23%2FElasticsearch-25-Query-DSL%E5%B8%B8%E7%94%A8%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[Query DSL的常用的几种查询语法match all查询查询全部123456GET /index/type/_search&#123; &quot;query&quot;:&#123; &quot;match_all&quot;:&#123;&#125; &#125;&#125; match查询指定field搜索条件查询, 搜索的关键词会被分词12345678GET /_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;field&quot;: &quot;text&quot; &#125; &#125;&#125; multi match搜索条件在多个field上进行查询, 搜索条件也会被分词123456789GET /_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;text&quot;, &quot;fields&quot;: [&quot;field1&quot;,&quot;field2&quot;] &#125; &#125;&#125; range query在区间范围内查询1234567891011GET /_search&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;field&quot;: &#123; &quot;gte&quot;: 0, &quot;lte&quot;: 10 &#125; &#125; &#125;&#125; term query搜索条件不去进行分词查询,同样的,只能查询设置为不分词的field12345678GET /_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;field&quot;: &quot;text&quot; &#125; &#125;&#125; terms query一个field 去匹配多个值1234567891011GET /_search&#123; &quot;query&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: [ &quot;text1&quot;, &quot;text2&quot; ] &#125; &#125;&#125; filter之前有说过filter是不计算相关度分数的,就是直接把符合条件的数据筛选出来如果只用filter过滤的话 需要加”constant_score” 例:123456789101112131415GET _search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;field&quot;: &#123; &quot;gte&quot;: 10, &quot;lte&quot;: 20 &#125; &#125; &#125; &#125; &#125;&#125; 定位搜索不合法的原因语法:1234GET index/type/_validate/query?explain&#123; // 搜索条件&#125; 示例我们先来写一个错误的查询来试一下12345678GET test_index/test_type/_validate/query?explain&#123; &quot;query&quot;: &#123; &quot;math&quot;: &#123; // 应该是match 写成了 math &quot;test_field&quot;: &quot;text&quot; &#125; &#125;&#125; 返回值:1234&#123; &quot;valid&quot;: false, &quot;error&quot;: &quot;org.elasticsearch.common.ParsingException: no [query] registered for [math]&quot;&#125; 再来试一个正确的12345678GET test_index/test_type/_validate/query?explain&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;test_field&quot;: &quot;text&quot; &#125; &#125;&#125; 返回值:123456789101112131415&#123; &quot;valid&quot;: true, &quot;_shards&quot;: &#123; &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;explanations&quot;: [ &#123; &quot;index&quot;: &quot;test_index&quot;, &quot;valid&quot;: true, &quot;explanation&quot;: &quot;+test_field:text #(#_type:test_type)&quot; &#125; ]&#125; 一般用在那种特别复杂庞大的搜索下，比如你一下子写了上百行的搜索，这个时候可以先用validate api去验证一下，搜索是否合法 定制排序规则默认情况下,es是按照_score去排序的,然后某些情况下,可能没有有用的 _score,比如说filter ,那么我们如何使用自己的排序规则呢? 语法:12345678910111213GET /index/type/_search&#123; &quot;query&quot;:&#123; ... &#125;, &quot;sort&quot;:[ &#123; &quot;field&quot;:&#123; &quot;order&quot;:&quot;desc&quot; &#125; &#125; ]&#125; 就是在query后面加一个sort来进行排序,指定用哪一个field,和升序还是降序]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-24-search api和Query DSL基本语法]]></title>
    <url>%2F2018%2F11%2F22%2FElasticsearch-24-search-api%E5%92%8CQuery-DSL%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[search api 的基本语法12GET /_search&#125;&#123; 12345GET /_search&#123; &quot;from&quot;:0, &quot;size&quot;:10&#125; 1GET /_search?from=0&amp;size=10 可以直接将参数拼接在url请求上,也可以放在request body中 在HTTP协议中,一般不允许GET请求带上reques body,但是因为GET请求更加适合描述查询数据的操作,因此还是这么用了,很多浏览器,或者是服务器,也都支持GET+request body模式,如果遇到不支持的场景，也可以用POST请求12345POST /_search&#123; &quot;from&quot;:0, &quot;size&quot;:10&#125; Query DSL基本语法:123456789101112131415&#123; QUERY_NAME: &#123; ARGUMENT: VALUE, ARGUMENT: VALUE,... &#125;&#125;&#123; QUERY_NAME: &#123; FIELD_NAME: &#123; ARGUMENT: VALUE, ARGUMENT: VALUE,... &#125; &#125;&#125; 示例12345678GET /test_index/test_type/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; // 查询条件 &quot;test_field&quot;: &quot;test&quot; &#125; &#125;&#125; 组合多个搜索条件示例我们先来添加几个document 用来进行搜索123456789101112131415161718192021PUT /query_index/query_type/1&#123; &quot;title&quot;: &quot;my elasticsearch article&quot;, &quot;content&quot;: &quot;es is very bad&quot;, &quot;author_id&quot;: 110&#125;PUT /query_index/query_type/2&#123; &quot;title&quot;: &quot;my hadoop article&quot;, &quot;content&quot;: &quot;hadoop is very bad&quot;, &quot;author_id&quot;: 111&#125;PUT /query_index/query_type/3&#123; &quot;title&quot;: &quot;my elasticsearch article&quot;, &quot;content&quot;: &quot;es is very goods&quot;, &quot;author_id&quot;: 111&#125; 然后我们制定一个搜索条件,比如 我们要查询 title必须包含 elasticsearch ,content 可以包含 elasticsearch 也可以不包含,author_id必须不为111 我们先来看一下数据: title必须包含 elasticsearch : id是2和3的数据都符合 content 可以包含 elasticsearch 也可以不包含: 2和3中都没有包含, author_id必须不为111: 3的id是111根据这几个条件来看搜索结果就是id为1的那一条数据,然后我们来组合搜索条件进行搜索 12345678910111213141516171819202122232425262728GET /query_index/query_type/_search&#123; &quot;query&quot;: &#123; // 查询 &quot;bool&quot;: &#123; // 组合查询条件 &quot;must&quot;: [ // 必须符合的条件 &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;elasticsearch&quot; &#125; &#125; ], &quot;should&quot;: [ // 可以符合,也可以不符合的条件 &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;elasticsearch&quot; &#125; &#125; ], &quot;must_not&quot;: [ // 必须不符合的条件 &#123; &quot;match&quot;: &#123; &quot;author_id&quot;: 111 &#125; &#125; ] &#125; &#125;&#125; 执行后的结果: 1234567891011121314151617181920212223242526&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.25316024, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;query_index&quot;, &quot;_type&quot;: &quot;query_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.25316024, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;my elasticsearch article&quot;, &quot;content&quot;: &quot;es is very bad&quot;, &quot;author_id&quot;: 110 &#125; &#125; ] &#125;&#125; 只返回了id是1的数据 query 与 filter示例我们现在有三条数据,如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&#123; &quot;_index&quot;: &quot;company&quot;, &quot;_type&quot;: &quot;emp&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;address&quot;: &#123; &quot;country&quot;: &quot;china&quot;, &quot;province&quot;: &quot;jiangsu&quot;, &quot;city&quot;: &quot;nanjing&quot; &#125;, &quot;name&quot;: &quot;tom&quot;, &quot;age&quot;: 30, &quot;join_date&quot;: &quot;2016-01-01&quot; &#125;&#125;,&#123; &quot;_index&quot;: &quot;company&quot;, &quot;_type&quot;: &quot;emp&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;jack&quot;, &quot;age&quot;: 27, &quot;join_date&quot;: &quot;2017-01-01&quot;, &quot;address&quot;: &#123; &quot;country&quot;: &quot;china&quot;, &quot;province&quot;: &quot;zhejiang&quot;, &quot;city&quot;: &quot;hangzhou&quot; &#125; &#125;&#125;,&#123; &quot;_index&quot;: &quot;company&quot;, &quot;_type&quot;: &quot;emp&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;address&quot;: &#123; &quot;country&quot;: &quot;china&quot;, &quot;province&quot;: &quot;shanxi&quot;, &quot;city&quot;: &quot;xian&quot; &#125;, &quot;name&quot;: &quot;marry&quot;, &quot;age&quot;: 35, &quot;join_date&quot;: &quot;2015-01-01&quot; &#125;&#125; 现在有一个搜索请求, 搜索年龄必须大于等于30,同时join_date必须是2016-01-01 我们来构造一个包含query和filter的搜索请求123456789101112131415161718192021GET /company/emp/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; // 组合搜索 &quot;must&quot;: [ // 必须满足的条件 &#123; &quot;match&quot;: &#123; &quot;join_date&quot;: &quot;2016-01-01&quot; &#125; &#125; ], &quot;filter&quot;: &#123; // 过滤器 &quot;range&quot;: &#123; &quot;age&quot;: &#123; &quot;gte&quot;: 30 &#125; &#125; &#125; &#125; &#125;&#125; 返回值: 12345678910111213141516171819202122232425262728293031&#123; &quot;took&quot;: 16, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;company&quot;, &quot;_type&quot;: &quot;emp&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;address&quot;: &#123; &quot;country&quot;: &quot;china&quot;, &quot;province&quot;: &quot;jiangsu&quot;, &quot;city&quot;: &quot;nanjing&quot; &#125;, &quot;name&quot;: &quot;tom&quot;, &quot;age&quot;: 30, &quot;join_date&quot;: &quot;2016-01-01&quot; &#125; &#125; ] &#125;&#125; 可以看到搜到了一条满足条件的数据. query 与 filter 对比 filter:仅仅只是按照搜索条件过滤出需要的数据而已,不计算任何相关度分数,对相关度没有任何影响. query: 会去计算每个document相对于搜索条件的相关度,并按照相关度进行排序. 一般来说,我们在搜索的时候需要将最匹配的数据先返回的时候,就用query,如果只是需要根据条件筛选出一些数据,不关注其相关度,就用filter 除非你的这些搜索条件,你希望越符合这些搜索条件的document越排在前面,那么这些搜索条件要放到query中去.如果你不希望一些搜索条件来影响你的document排序的话,那么就放在filter中即可 query 与 filter 性能filter不需要计算相关度分数进行排序,同时还有内置的cache,自动缓存最常使用的filter数据query相反,要计算相关度分数,按照分数进行排序,而且无法cache结果]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-23-_mapping复杂数据类型和object类型底层数据存储]]></title>
    <url>%2F2018%2F11%2F22%2FElasticsearch-23-mapping%E5%A4%8D%E6%9D%82%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%92%8Cobject%E7%B1%BB%E5%9E%8B%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[几种复杂的数据类型multivalue field比如数据是1&#123;&quot;tags&quot;:[&quot;tag1&quot;,&quot;tag2&quot;]&#125; 这种数据建立索引时,与string类型是一样的, 数组中的数据是不能混的,要放字符串都放字符串. empty field比如 null, [], [null]这样的数据 object field我们先来添加一个document1234567891011PUT /company/emp/1&#123; &quot;name&quot;:&quot;jack&quot;, &quot;age&quot;:27, &quot;join_date&quot;:&quot;2017-01-01&quot;, &quot;address&quot;:&#123; &quot;country&quot;:&quot;china&quot;, &quot;province&quot;:&quot;zhejiang&quot;, &quot;city&quot;:&quot;hangzhou&quot; &#125;&#125; 像上面这个address就是object类型的, 我们来看一下这个type的_mapping1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&#123; &quot;company&quot;: &#123; &quot;mappings&quot;: &#123; &quot;emp&quot;: &#123; &quot;properties&quot;: &#123; &quot;address&quot;: &#123; &quot;properties&quot;: &#123; &quot;city&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;country&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;province&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125;, &quot;age&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;join_date&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 可以看到 address中的每个field都有对应的type等 其实像我们刚才添加的这个document,它的数据在es底层是像这样存储的12345678&#123; &quot;name&quot;:[jack], &quot;age&quot;:[27], &quot;join_date&quot;:[2017-01-01], &quot;address.country&quot;:[china], &quot;address.province&quot;:[zhejiang], &quot;address.city&quot;:[hangzhou]&#125; 再比如有更复杂的数据,比如下面我们再加一个,数据是1234567&#123; &quot;authors&quot;: [ &#123; &quot;age&quot;: 26, &quot;name&quot;: &quot;Jack White&quot;&#125;, &#123; &quot;age&quot;: 55, &quot;name&quot;: &quot;Tom Jones&quot;&#125;, &#123; &quot;age&quot;: 39, &quot;name&quot;: &quot;Kitty Smith&quot;&#125; ]&#125; 像这种包含json数组的数据,底层会从横向转为列式存储,就像这样1234&#123; &quot;authors.age&quot;: [26, 55, 39], &quot;authors.name&quot;: [jack, white, tom, jones, kitty, smith]&#125; 就是一列存在一起]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-22-mapping详解]]></title>
    <url>%2F2018%2F11%2F22%2FElasticsearch-22-mapping%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[什么是mapping往es里面直接插入数据,es会自动建立索引,同时建立type以及对应的mapping. mapping中就定义了每个field的数据类型 不同的数据类型,可能有的是精确搜索(exact value),有的是全文检索(full text). exact value在建立倒排索引的时候,分词是将整个值一起作为一个关键词建立到倒排索引中的;而full text是会经过各种处理的,分词 normalization 才会建立到倒排索引中. 一个搜索过来的时候对exact value field或者是full text field进行搜索的行为也是不一样的,会跟建立倒排索引的行为保持一致;比如说exact value搜索的时候,就是直接按照整个值进行匹配;full text query string,也会进行分词和normalization再去倒排索引中去搜索 可以用es的dynamic mapping 让其自动建立mapping,包括自动设置数据类型,也可以提前手动创建index的type的mapping,自己对各自的field进行设置,包括数据类型,索引行为,分词器等等. 总结: mapping,就是index的type的元数据,每个type都有一个自己的mapping,决定了数据类型,建立倒排索引的行为,还有进行搜索的行为 核心数据类型mapping 下的核心数据类型有: 字符串: String整型: byte, short, integer, long浮点型: float, double布尔型: boolean日期类型: date dynamic mapping 数据类型映射 数据 映射后的数据类型 true/fasle boolean 123 long 123.45 double 2017-01-01 date “hello world” string/text 查询mapping语法:1GET /index/_mapping/type 手动建立mapping只能在创建index的时候手动建立mapping,或者新增field mapping, 不能修改 filed mapping 之间我们建立过一个website的index,我们先删掉.1DELETE /website 现在来手动建立这个索引,并手动创建mapping1234567891011121314151617181920212223242526PUT /website&#123; &quot;mappings&quot;: &#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;author_id&quot;:&#123; // field &quot;type&quot;: &quot;long&quot; // 类型 &#125;, &quot;title&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;english&quot; // 指定分词器 &#125;, &quot;content&quot;:&#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;post_date&quot;:&#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;publisher_id&quot;:&#123; &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; // 不分词,就是 exact value ,上面的类型一定要写string,否则不生效 &#125; &#125; &#125; &#125;&#125; analyzed:分词not_analyzed:不分词no:直接不建立到倒排索引里,也就是说 搜索不到 好了,创建完成,然后我们来尝试修改一下这个mapping.123456789101112PUT /website&#123; &quot;mappings&quot;: &#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;author_id&quot;:&#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;index_already_exists_exception&quot;, &quot;reason&quot;: &quot;index [website/-6NKQPj3TPWDrrlxhalkmw] already exists&quot;, &quot;index_uuid&quot;: &quot;-6NKQPj3TPWDrrlxhalkmw&quot;, &quot;index&quot;: &quot;website&quot; &#125; ], &quot;type&quot;: &quot;index_already_exists_exception&quot;, &quot;reason&quot;: &quot;index [website/-6NKQPj3TPWDrrlxhalkmw] already exists&quot;, &quot;index_uuid&quot;: &quot;-6NKQPj3TPWDrrlxhalkmw&quot;, &quot;index&quot;: &quot;website&quot; &#125;, &quot;status&quot;: 400&#125; 运行后发现,报错了,原因就是建立好的field mapping是不能去修改的,但是我们可以新增一个field,并指定type等123456789PUT /website/_mapping/article&#123; &quot;properties&quot;: &#123; &quot;new_field&quot;:&#123; &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125; &#125;&#125; 返回值:123&#123; &quot;acknowledged&quot;: true&#125; 可以看到已经新增成功了. 测试mapping完成后我们测试一下分词的效果, content这个field是普通的text类型,我们来测试一下12345GET /website/_analyze&#123; &quot;field&quot;: &quot;content&quot;, &quot;text&quot;: &quot;my-dogs&quot;&#125; 返回值:123456789101112131415161718&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;my&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;dogs&quot;, &quot;start_offset&quot;: 3, &quot;end_offset&quot;: 7, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 &#125; ]&#125; 可以看到my-dogs 被拆分为 my dogs,去掉了 - ,没有进行单复数转换等,因为默认的分词器就是standard analyzer(标准分词器) 再来试一下new_field,我们在设置的时候这个filed是不能分词的12345GET website/_analyze&#123; &quot;field&quot;: &quot;new_field&quot;, &quot;text&quot;: &quot;my dogs&quot;&#125; 返回值:12345678910111213&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;remote_transport_exception&quot;, &quot;reason&quot;: &quot;[f57uV91][127.0.0.1:9300][indices:admin/analyze[s]]&quot; &#125; ], &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Can&apos;t process field [new_field], Analysis requests are only supported on tokenized fields&quot; &#125;, &quot;status&quot;: 400&#125; 报错了,因为这个field是不能分词的.]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-21-query string分词和mapping案例遗留问题揭秘]]></title>
    <url>%2F2018%2F11%2F22%2FElasticsearch-21-query-string%E5%88%86%E8%AF%8D%E5%92%8Cmapping%E6%A1%88%E4%BE%8B%E9%81%97%E7%95%99%E9%97%AE%E9%A2%98%E6%8F%AD%E7%A7%98%2F</url>
    <content type="text"><![CDATA[query string 分词query string必须以和index建立时相同的analyzer进行分词. 比如,我们有一个document,其中有一个field,它的值是:hello you and me,建立倒排索引.我们要搜索这个document对应的index,搜索文本是hello me ,搜索请求就是:1GET /index/type/_search?q=field:hello me “hello me”就是query string,默认情况下,es会使用它对应的field建立倒排索引时相同的分词器进行分词和normalization,只有这样,才能实现正确的搜索. 举个例子,document在建立倒排索引的时候,会把dogs转为dog,然后我们在搜索的时候传一个dogs过去,就找不到了,所以搜索传过去的dogs也必须变为dog才能实现正确的搜索. mapping引入案例遗留问题揭秘这里有一个知识点: 不同类型的field,可能有的就是full text(全文检索),有的就是exact value(精确搜索) 在初始mapping中,我们引入了一个小案例,当时的查询结果是:1234GET /website/article/_search?q=2017 3条结果 GET /website/article/_search?q=2017-01-01 3条结果GET /website/article/_search?q=post_date:2017-01-01 1条结果GET /website/article/_search?q=post_date:2017 1条结果 首先看第一个查询,我们没有指定用哪一个field进行查询,那默认的就是 _all 查询,之前有说过 _all的话会把document中的所有field的值当做字符串拼接, _all搜索的时候是full text,要分词进行normalization后查询 我们来看一下第一个document中的数据:123456&#123; &quot;post_date&quot;: &quot;2017-01-01&quot;, &quot;title&quot;: &quot;my first article&quot;, &quot;content&quot;: &quot;this is my first article in this website&quot;, &quot;author_id&quot;: 11400&#125; 它的_all就是 “2017-01-01 my first article this is my first article in this website 11400” 三个document的 _all中分别有 2017-01-01 2017-01-02 2017-01-03 这个建立倒排索引就是 word document1 document2 document3 2017 √ √ √ 01 √ 02 √ 03 √ 这时候第一个搜索 _all 查询2017 肯定能查到3条第二个搜索请求的query string 会被分为 2017,01,01, 所以也能查到3条数据 然后是第三个请求,是指定post_date这个filed去查询, post_date 是个date类型的,而不是字符串类型, date类型的数据会按照exact value去建立索引 word document1 document2 document3 2017-01-01 √ 2017-01-02 √ 2017-01-03 √ 所以搜索第三个请求时可以搜索到1条结果. 按照上面的说法的话,第4个请求应该是搜索不到结果的,但是实际上有一条结果,这个在这里不讲解,因为是es 5.2以后做的一个优化 分词器测试12345GET /_analyze&#123; &quot;analyzer&quot;: &quot;standard&quot;, // 指定分词器 &quot;text&quot;: &quot;Text to analyze&quot; // 要拆分的文本&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-20-分词器详解]]></title>
    <url>%2F2018%2F11%2F22%2FElasticsearch-20-%E5%88%86%E8%AF%8D%E5%99%A8%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[什么是分词器作用: 拆分词语,进行normalization操作(提升recall召回率) 比如说,有一个句子,然后将这个句子拆分成一个一个的单词,同时对每个单词进行normalization(时态转换,单复数转换等等). recall召回率:简单来说就是搜索的时候,增加能够搜索到的结果的数量. 分词器一共做了三件事:character filter:在一段文本进行分词之前,先进行预处理,比如说过滤html标签(&lt;span&gt;123&lt;/span&gt; 转换为123), &amp; 转换为 andtokenizer:分词,比如 hello me 分为 hello 和 metoken filter:进行大小写转换,停用词去掉,近义词转换等normalization操作,比如 dogs –&gt; dog, liked –&gt; like, Tom –&gt; tom, a/an/the去掉,等等 分词器很重要,讲一段文本进行各种处理,最后处理好的结果才会拿去建立倒排索引. 内置分词器的介绍比如某个document中的某一个field的值是: Set the shape to semi-transparent by calling set_trans(5) standard analyzer(标准分词器) :将句子拆分为set, the, shape, to, semi, transparent, by, calling, set_trans, 5,做了大写转小写,-去除,()去除等操作 simple analyzer(简单分词器):拆分为set,the,shape,to,semi,transparent,by,calling,set,trans,可以看到做了大写转小写，-去除，(5)去除，_去除 等操作 whitespace analyzer(空格分词器)：Set,the,shape,semi-transparent,by,calling,set_trans(5), 简单的按照空格进行分词 language analyzer(语言分词器，比如说英语分词器)：set,shape,semi,transpar,call,set_tran,5,可以看到大写转小写,the没有任何含义,被干掉了,以及一些拆分的,transparent转换成了transpar,calling转化时态call,等等 默认的分词器是standard analyzer(标准分词器)]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-19-倒排索引核心原理]]></title>
    <url>%2F2018%2F11%2F21%2FElasticsearch-19-%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[场景假设我们现在有两个document. document1: I really liked my small dogs, and I think my mom also liked them. document2: He never liked any dogs, so I hope that my mom will not expect me to liked him. 第一步 分词,初步建立倒排索引两个document中的数据将会被分词,比如分成这样 word document1 document2 I √ √ really √ liked √ √ my √ √ small √ dogs √ and √ think √ mom √ √ also √ them √ He √ never √ any √ so √ hope √ that √ will √ not √ expect √ me √ to √ him √ 这个时候我们如果搜索 mother like little dog 的时候,不会有任何结果的先回对搜索条件拆词,拆分为motherlikelittledog 这个时候去上面的倒排索引去匹配,发现没有一个词是可以匹配的到的. 这显然不是我门想要的搜索结果 其实建立倒排索引的时候,还会做一件事,就是进行normalization标准化,包括时态转换，复数，同义词，大小写等,对拆出的各个单词进行相应的处理,以便后面搜索的时候能够搜索到相关联document的概率 进行normalization后的倒排索引: word document1 document2 normalization I √ √ really √ like √ √ liked – &gt;like my √ √ little √ small –&gt; little dog √ √ dogs –&gt; dog and √ think √ mom √ √ also √ them √ He √ never √ any √ so √ hope √ that √ will √ not √ expect √ me √ to √ him √ 这时候再按上面的搜索条件 mother like little dog 搜索,将搜索条件分词,进行normalization后mother –&gt; momlike –&gt; likelittle –&gt; littledog –&gt; dog 这时候拿关键词去匹配上面的倒排索引,就能把document1和document2都搜索出来]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-18-精确匹配与全文检索]]></title>
    <url>%2F2018%2F11%2F21%2FElasticsearch-18-%E7%B2%BE%E7%A1%AE%E5%8C%B9%E9%85%8D%E4%B8%8E%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[精确匹配(exact value)比如我们在之前的例子,通过精确匹配搜索 2017-01-01的时候, field中必须包含2017-01-01 才能搜索出来,如果只搜索个01,这样是搜不出来的 全文检索 (full text)不单纯的只是匹配一个完整的值,而是可以对值进行分词后进行匹配,还可以通过缩写 时态 大小写 同义词等进行匹配 缩写搜索比如查询 cn 可以将 China 搜索出来 格式转换比如查询 likes 可以将 like liked 搜索出来 大小写转换比如查询 tom 可以将 Tom 搜索出来 同义词搜索比如查询 love 它的同义词 like 也可以搜索出来]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-17-初识mapping]]></title>
    <url>%2F2018%2F11%2F21%2FElasticsearch-17-%E5%88%9D%E8%AF%86mapping%2F</url>
    <content type="text"><![CDATA[首先,我们先插入几条数据,让ES为我们自动建立一个索引1234567891011121314151617181920212223PUT /website/article/1&#123; &quot;post_date&quot;: &quot;2017-01-01&quot;, &quot;title&quot;: &quot;my first article&quot;, &quot;content&quot;: &quot;this is my first article in this website&quot;, &quot;author_id&quot;: 11400&#125;PUT /website/article/2&#123; &quot;post_date&quot;: &quot;2017-01-02&quot;, &quot;title&quot;: &quot;my second article&quot;, &quot;content&quot;: &quot;this is my second article in this website&quot;, &quot;author_id&quot;: 11400&#125;PUT /website/article/3&#123; &quot;post_date&quot;: &quot;2017-01-03&quot;, &quot;title&quot;: &quot;my third article&quot;, &quot;content&quot;: &quot;this is my third article in this website&quot;, &quot;author_id&quot;: 11400&#125; 对这些数据进行几次搜索_all 搜索 20171GET /website/article/_search?q=2017 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 0.28004453, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.28004453, &quot;_source&quot;: &#123; &quot;post_date&quot;: &quot;2017-01-02&quot;, &quot;title&quot;: &quot;my second article&quot;, &quot;content&quot;: &quot;this is my second article in this website&quot;, &quot;author_id&quot;: 11400 &#125; &#125;, &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.28004453, &quot;_source&quot;: &#123; &quot;post_date&quot;: &quot;2017-01-01&quot;, &quot;title&quot;: &quot;my first article&quot;, &quot;content&quot;: &quot;this is my first article in this website&quot;, &quot;author_id&quot;: 11400 &#125; &#125;, &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.28004453, &quot;_source&quot;: &#123; &quot;post_date&quot;: &quot;2017-01-03&quot;, &quot;title&quot;: &quot;my third article&quot;, &quot;content&quot;: &quot;this is my third article in this website&quot;, &quot;author_id&quot;: 11400 &#125; &#125; ] &#125;&#125; 一共查询出来3条结果 指定 post_date 搜索20171GET /website/article/_search?q=post_date:2017 返回值:123456789101112131415161718192021222324252627&#123; &quot;took&quot;: 1, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;post_date&quot;: &quot;2017-01-01&quot;, &quot;title&quot;: &quot;my first article&quot;, &quot;content&quot;: &quot;this is my first article in this website&quot;, &quot;author_id&quot;: 11400 &#125; &#125; ] &#125;&#125; 一共查询出来1条结果 _all 搜索 2017-01-011GET /website/article/_search?q=2017-01-01 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&#123; &quot;took&quot;: 9, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 1.0566096, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1.0566096, &quot;_source&quot;: &#123; &quot;post_date&quot;: &quot;2017-01-01&quot;, &quot;title&quot;: &quot;my first article&quot;, &quot;content&quot;: &quot;this is my first article in this website&quot;, &quot;author_id&quot;: 11400 &#125; &#125;, &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.84013355, &quot;_source&quot;: &#123; &quot;post_date&quot;: &quot;2017-01-02&quot;, &quot;title&quot;: &quot;my second article&quot;, &quot;content&quot;: &quot;this is my second article in this website&quot;, &quot;author_id&quot;: 11400 &#125; &#125;, &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.84013355, &quot;_source&quot;: &#123; &quot;post_date&quot;: &quot;2017-01-03&quot;, &quot;title&quot;: &quot;my third article&quot;, &quot;content&quot;: &quot;this is my third article in this website&quot;, &quot;author_id&quot;: 11400 &#125; &#125; ] &#125;&#125; 一共查询出来3条结果 指定 post_date 搜索2017-01-011GET /website/article/_search?q=post_date:2017-01-01 返回值:123456789101112131415161718192021222324252627&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;post_date&quot;: &quot;2017-01-01&quot;, &quot;title&quot;: &quot;my first article&quot;, &quot;content&quot;: &quot;this is my first article in this website&quot;, &quot;author_id&quot;: 11400 &#125; &#125; ] &#125;&#125; 一共查询出来1条结果 总体结果:1234GET /website/article/_search?q=2017 3条结果 GET /website/article/_search?q=2017-01-01 3条结果GET /website/article/_search?q=post_date:2017-01-01 1条结果GET /website/article/_search?q=post_date:2017 1条结果 mapping 概念自动或手动为index中的type建立的一种数据结构相关的配置,简称为mappingdynamic mapping自动为我们建立index,创建type,以及type对应的mapping,mapping中包含了每个field对应的数据类型,以及如何分词等设置 查询mapping查询语法:1GET /index/_mapping/type 比如查询我们上面例子中的mapping1GET /website/_mapping/article 返回值:12345678910111213141516171819202122232425262728293031323334&#123; &quot;website&quot;: &#123; &quot;mappings&quot;: &#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;author_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;post_date&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 可以看到 里面包含了我们每个field的数据类型等信息. 搜索结果为什么不一致?因为es自动建立mapping的时候,设置了各个filed的数据类型,不同的数据类型的分词 搜索等行为是不一样的,所以出现了_all 搜索和指定field搜索时的数据不一致的情况]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-16-_search搜索详解]]></title>
    <url>%2F2018%2F11%2F21%2FElasticsearch-16-_search%E6%90%9C%E7%B4%A2%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[multi-index和multi-type搜索模式就是一次性搜索多个index和type下的数据 示例 搜索所有index,所有type下的所有数据 1GET /_search 指定一个index,搜索其下所有的type的数据 1GET /index/_search 查询某个index下指定的type的数据 1GET /index/type/_search 同时搜索多个index下的所有数据 1GET /index1,index2,index3,.../_search 按照通配符去匹配多个index 12GET /*1,*2/_search# 查询以 1 和 2 结尾的index 搜索一个index下指定的多个type的数据 1GET /index/type1,type2/_search 搜索多个index下的多个type的数据 1GET /index1,index2/type1,type2/_search 搜索所有index下的指定type的数据 1GET /_all/type1,type2/_search 搜索原理客户端发送一个请求,会把请求打到所有的primary shard上去执行,因为每个shard都包含部分数据,所以每个shard上都可能包含搜索请求的结果但是如果primary shard有replica shard,那么请求也可以打到replica shard上面 分页搜索查询时传入参数 size 和 from 即可 示例比如我们要查询movies/movie下的数据一共是6条,分三页查询12345678# 查询第一页GET /movies/movie/_search?size=2&amp;from=0# 查询第二页GET /movies/movie/_search?size=2&amp;from=2# 查询第三页GET /movies/movie/_search?size=2&amp;from=4 from 是从0开始的 deep paging问题什么是deep paging问题? 为什么会产生这个问题? 他的底层原理是什么? 场景,比如我们现在有4个shard 一共有60000条数据,在其中3个shard中,每个有20000条数据,这个时候要进行搜索第1000页的数据,每页显示10条,实际上这里拿到的是第10001~10010条数据 假设这个请求先打到一个不包含这些数据的节点上去,那么这个节点就是一个协调节点(coordinate node),然后这个协调节点会将请求转发到包含数据的节点上去,如图: 查询60000条数据中的第1000页,实际上每个shard都要将内部的20000条数据中的1000页,也就是10001~10010条的数据拿出来, 这时候实际上不是只返回这10条数据 是返回第一条到10010条数据, 3个shard都返回10010条数据给coordinate node,coordinate node 总共会受到30030条数据,然后进行排序,在这30030条数据中取到第10页,也就是这些数据中的第10001~10010条数据返回. 总的来说就是先要把所有shard上的数据集中起来排序后再去分页. 搜索过深的时候,就要在coordinate node上保存大量的数据,还要进行排序,排序之后,再取出对应的那一页,所以这个过程,既耗费网络带宽,耗费内存,还耗费CPU,影响性能,我们应该尽量避免出现这种deep paging的操作 query string其实就是在http请求中,把一些搜索的参数做为query string附加到url上面. 示例查询 /movies/movie 下title包含kill这个词的数据1GET /movies/movie/_search?q=title:kill 查询 /movies/movie 下title必须包含kill这个词的数据1GET /movies/movie/_search?q=+title:kill 查询/movies/movie 下title不包含kill这个词的数据1GET /movies/movie/_search?q=-title:kill 其实第一个和第二个的作用是差不多的,主要是 “+” 和 “-“ 的区别 一个是必须包含,一个是不包含 _all metadata 原理和作用查询所有filed下包含kill的数据1GET /movies/movie/_search?q=kill 上面这个查询中并没有指定具体是哪个field包含kill这个词,是直接搜索的所有field的.当我们添加一个document的时候,它里面包含了多个field,此时es会自动将多个field的值,用字符串的方式串联起来,变成一个长字符串,作为_all 的值,同时对 _all进行分词建立索引.当我们搜索没有指定具体哪一个field的时候,就默认对_all 进行搜索,其实它里面就包含了所有field的值 举例说明我们新添加一个document,内容是123456&#123; &quot;name&quot;:&quot;jack&quot;, &quot;age&quot;:26, &quot;email&quot; : &quot;jack@sina.com&quot;, &quot;address&quot;:&quot;hangzhou&quot;&#125; “jack 26 jack@sina.com hangzhou”,就作为这一条document的_all元数据的值,同时进行分词后建立对应的倒排索引]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-15-写一致性原理及quorum机制]]></title>
    <url>%2F2018%2F11%2F20%2FElasticsearch-15-%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7%E5%8E%9F%E7%90%86%E5%8F%8Aquorum%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[概念我们在发送任何一个增删改操作的时候,都可以带上一个consistency参数,指明我们想要的写一致性是什么,比如:1PUT /index/type/id?consistency= consistency的值可以有三个: one:要求这个写操作,只要有一个primary shard是active活跃可用的状态就可以执行. all:要求这个写操作,必须所有的primary shard和replica shard都是active活跃的,才可以执行 quorum:默认的值,要求所有的shard中,必须大部分的shard都是active活跃的,才可以执行. 那么怎么算大部分shard都是活跃的呢,es有一个计算的公式 quorum机制前置条件当replica shard 的数量大于1的时候才会生效 计算公式12quorum = int((number_of_priamry_shard + number_of_replica_shard) / 2) + 1# 这里的number_of_replica_shard是相对于 primary shard的数量 举例说明如果现在有3个primary shard, number_of_replica_shard = 3, 也就是说一共有3 + 3 * 3 = 12个shard,根据公式, int((3 + 3) / 2) + 1 = 4 意思就是说,在所有的12个shard中必须是有4个shard是active状态的才可以执行写操作 如果节点的数量少于quorum数量,可能导致quorum不齐全,进而导致无法执行任何写操作 特殊场景处理如果说我们就一个primary shard,replica = 1,此时总共就两个shard,按照公式int((1 + 1) / 2) + 1 = 2,此时要求两个shard都是活跃的才行,但是我们可能就有一个节点,只能有一个primary shard,那么这个情况下是不是就无法写入了呢? es提供了一种特殊的处理场景,就是说当number_of_replicas &gt; 1的时候才生效,如果没有这种特殊处理的话,单节点集群就无法正常工作. 超时等待当quorum不齐全时, 会默认等待一分钟,等待期间,期望活跃的shard数量可以增加,时间到了还不能添加的话就超时.我们也可以在写操作的时候,加一个timeout的参数,比如1PUT /index/type/id?timeout=30ms 就是说我们自己去设定当quorum不齐全的时候es的timeout时长.]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-14-document增删改查原理]]></title>
    <url>%2F2018%2F11%2F20%2FElasticsearch-14-document%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[熟悉了es路由规则以后,再来看一下document的增删改查的原理. 场景现在有3个es节点在一个集群内, 有3个primary shard 对应的replica数量是1 就是说有3个 primary shard 和3个replica,总共6个shard, 现在有个客户端要创建一个document到es中 增删改操作客户端随便选择一个node,然后将请求发送到node上去, 因为任意一个node都知道document应该存在哪个shard上,所以请求发给哪一个都是一样的. 比如图中,请求节点到达node1了,那node就成为了coordinate node(协调节点),协调节点通过我们之前说到的路由公式,来计算这个document应该在哪个shard上面,然后将请求发送到这个节点上去, 比如应该放到shard2 中, shard2在自己本地创建document,创建倒排索引,创建完毕后会将数据同步到他对应的replica上去 写入完毕 同步数据到replica完成后,通知协调节点,然后协调节点返回响应给客户端 查询操作客户端先发送一个查询请求到任意的一个node上去 如图,请求发到node1 上去后node1就成为了coordinate node(协调节点) ,协调节点对document进行路由,路由之后就知道document在哪个primary shard上了,对于读请求,不一定就将请求转发到primary shard上去,因为replica shard也可以承担读请求的 es采用round-robin随机轮询算法在primary shard以及其所有replica中随机选择一个，让读请求负载均衡. 然后对应的shard去查询,将查询结果返回给协调节点,协调节点最后响应给客户端 特殊情况如果查询的document还在建立索引的过程中,这个时候只有primary shard上有,任何一个replica shard上都没有,此时如果请求路由到replica shard上可能会导致无法读取到document,但是document完成索引建立 将数据同步到replica shard上之后,就都有了]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-13-document路由原理]]></title>
    <url>%2F2018%2F11%2F20%2FElasticsearch-13-document%E8%B7%AF%E7%94%B1%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[概念我们知道,一个index的数据会被分为多片,每片都放在一个shard中.所以,一个document只能存在于一个shard中.当客户端创建document的时候,es此时就要决定这个document是要放在哪一个shard中. 这个过程就被是document routing 数据路由 路由算法12shard = hash(routing) % number_of_primary_shards# routing的哈希值 除以 primary shard的数量,然后取余. 每次增删改查一个document的时候,都会带过来一个routing number,默认的就是这个document的id(可以是手动指定,也可以是自动生成). 举个例子,现在有一个index,有3个shard P0 P1 P2,假设routing = _id, _id = 1, es会将这个routing值传入一个hash函数中,返回一个routing值的hash值, 假如hash(routing) = 21, 然后将hash函数产出的值对这个index的primary shard的数量求余数, 21 % 3 = 0,就决定了这个document应该放在P0上面. 决定一个document在哪个shard上,最重要的一个值就是routing值,默认是id也可以手动指定,相同的routing值,每次过来,从hash函数中产出的hash值一定是相同的 无论hash值是多少,无论是什么数字,对number_of_primary_shard求余数,结果一定是在0~number_of_primary_shard-1 这个范围之内的. 再来想想我们之前说过,primary shard的数量在创建完index之后是不能去修改的,还是上面那个例子,数据被放到了P0中,如果现在加了一个primary shard的数量会怎么样呢.我们去查询这个document的时候,_id = 1, hash值还是21, 21 % 4 = 1,计算结果是数据在P1上面,但是实际在P0上,就会间接导致数据的丢失. routing值的作用和如何手动指定默认的routing就是_id,也可以在发送请求的时候,手动指定一个routing ,如下:1PUT /index/type/id?routing=user_id 手动指定routing是很有用的,可以保证说,某一类的document一定被路由到一个shard上去,那么在后续进行应用级别的负载均衡,以及提升批量读取的性能的时候,是很有帮助的]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-12-批量增删改查操作]]></title>
    <url>%2F2018%2F11%2F19%2FElasticsearch-12-%E6%89%B9%E9%87%8F%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[批量查询批量查询的优点: 如果用 GET /index_name/type_name/id 这样的查询去查询100条数据的话,就要发送100次网络请求,开销比较大,如果是批量查询的话,查询100条数据只需要用1次网络请求,网络请求的性能开销会很少. _mget批量查询比如我们要查询test_index/test_type下面 id是1和2的两条数据执行代码:123456789101112131415GET /_mget&#123; &quot;docs&quot;:[ &#123; &quot;_index&quot;:&quot;test_index&quot;, // 索引名 &quot;_type&quot;:&quot;test_type&quot;, // 类型名 &quot;_id&quot;:1 // id &#125;, &#123; &quot;_index&quot;:&quot;test_index&quot;, &quot;_type&quot;:&quot;test_type&quot;, &quot;_id&quot;:2 &#125; ] &#125; 返回值:12345678910111213141516171819202122232425&#123; &quot;docs&quot;: [ &#123; &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;test_content&quot;: &quot;test content&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_version&quot;: 2, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;num&quot;: 1, &quot;tags&quot;: [] &#125; &#125; ]&#125; GET请求后面直接跟_mget端点, 将参数封装在一个docs数组里面,数组中的一个元素就是一个请求条件, 执行后返回值也封装在了一个docs数组中,一个元素对应一个返回结果. 如果说查询的是同一个index下的不同type的数据,直接将请求跟在url上就可以了,请求体中值需要指明type 和 id即可12345678910111213GET /test_index/_mget&#123; &quot;docs&quot;:[ &#123; &quot;_type&quot;:&quot;test_type&quot;, // 类型名称 &quot;_id&quot;:1 // id &#125;, &#123; &quot;_type&quot;:&quot;test_type&quot;, &quot;_id&quot;:2 &#125; ] &#125; 如果查询的是同一个index下的同一个type下的内容,那么index和type都写在url上,请求体中只需要一个ids的数组即可1234GET /test_index/test_type/_mget&#123; &quot;ids&quot;:[1,2]&#125; 可以看出来ids不需要包在docs中, 但是返回值还是会包在docs数组中. 总结:一般来说,在进行查询的时候,如果一次要查询多条数据的话,一定要用batch批量操作的api,尽可能减少网络请求的开销,可以大大提升性能. _bulk批量增删改_bulk 可以操作批量增删改,语法如下:12&#123;&quot;action&quot;:&#123;&quot;metadata&quot;&#125;&#125;&#123;&quot;data&quot;&#125; bulk api对json的语法有严格的要求,每个json不能换行,只能放一行,同时一个json和另一个json之间必须换行,每个操作需要两个json字符串,删除操作只需要一个. 举例,比如现在要创建一个document,放到bulk里面是这样的12&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;test_index&quot;,&quot;_type&quot;:&quot;test_type&quot;,&quot;_id&quot;:1&#125;&#125;&#123;&quot;test_field&quot;:&quot;test1&quot;,&quot;test_field2&quot;:&quot;test2&quot;&#125; 那么有哪些类型的操作可以执行呢? delete:,删除一个document create:相当于 PUT /index/type/id/_create,强制创建 index:普通的PUT操作,可以是创建,也可以是全量替换 update:相当于 partial update操作 示例:12345678POST /_bulk&#123;&quot;delete&quot;:&#123;&quot;_index&quot;:&quot;test_index&quot;,&quot;_type&quot;:&quot;test_type&quot;,&quot;_id&quot;:3&#125;&#125;&#123;&quot;create&quot;:&#123;&quot;_index&quot;:&quot;test_index&quot;,&quot;_type&quot;:&quot;test_type&quot;,&quot;_id&quot;:2&#125;&#125;&#123;&quot;test_field&quot;:&quot;already exists&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;test_index&quot;,&quot;_type&quot;:&quot;test_type&quot;,&quot;_id&quot;:1&#125;&#125;&#123;&quot;test_field&quot;:&quot;replace all&quot;&#125;&#123;&quot;update&quot;:&#123;&quot;_index&quot;:&quot;test_index&quot;,&quot;_type&quot;:&quot;test_type&quot;,&quot;_id&quot;:4,&quot;_retry_on_conflict&quot;:3&#125;&#125;&#123;&quot;doc&quot;:&#123;&quot;test_content&quot;:&quot;update id is4&quot;&#125;&#125; 上面这个请求中: 删除了id为4document 强制创建一个document,id为2 (id为2的已经存在了) 将id为1的document全量替换 partial update更新id为4的document,更新失败的时候重试3次 执行返回结果:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&#123; &quot;took&quot;: 79, &quot;errors&quot;: true, &quot;items&quot;: [ &#123; &quot;delete&quot;: &#123; &quot;found&quot;: true, &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_version&quot;: 2, &quot;result&quot;: &quot;deleted&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;status&quot;: 200 &#125; &#125;, &#123; &quot;create&quot;: &#123; &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;status&quot;: 409, &quot;error&quot;: &#123; &quot;type&quot;: &quot;version_conflict_engine_exception&quot;, &quot;reason&quot;: &quot;[test_type][2]: version conflict, document already exists (current version [2])&quot;, &quot;index_uuid&quot;: &quot;qFba3qxtTO6YxlO3m7qtug&quot;, &quot;shard&quot;: &quot;2&quot;, &quot;index&quot;: &quot;test_index&quot; &#125; &#125; &#125;, &#123; &quot;index&quot;: &#123; &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 2, &quot;result&quot;: &quot;updated&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: false, &quot;status&quot;: 200 &#125; &#125;, &#123; &quot;update&quot;: &#123; &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_version&quot;: 2, &quot;result&quot;: &quot;updated&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;status&quot;: 200 &#125; &#125; ]&#125; 可以看到,第2个操作失败了,原因是已经存在了,但是其他操作仍然执行成功了,所以bulk操作中，任意一个操作失败，是不会影响其他的操作的，但是在返回结果里，会告诉你异常日志 bulk size最佳大小bulk request会加载到内存里面,如果太大的话,性能反而会下降,因此需要反复尝试一个最佳的bulk size. 一般从1000~5000 条数据开始,或者数据大小在5~15MB之间,尝试逐渐增加 奇特的json格式详解上面有说过bulk的请求体 每个json不能换行,只能放一行,同时一个json和另一个jso之间必须换行,那为什么要这样设计呢? 第一点,bulk中的操作都可能要转发到不同的node的shard去执行.第二,如果允许任意换行,es拿到json后就得进行如下处理: 将json数组解析为JSONArray对象,这个时候,整个数据就会在内存中出现两份,一份是json文本,一份是JSONArray对象 解析json数组里的每个json,对每个请求中的document进行路由 为路由到一个shard上的多个请求,创建一个请求数组 将这个请求数组进行序列化 将序列化后的请求数组发送到对应的节点上去 所以按照这种方式的话,会耗费更多的内存,更多的jvm gc开销上文中有说到过bulk size的最佳大小,如果说现在有100个bulk请求,每个请求10MB,那就是 1000MB的数据,每个请求的json都copy一份为JSONArray对象,此时内存中的占用就会翻倍,会占用到2000MB+的内存,占用更多的内存就可能积压其他请求的内存使用量,就可能导致其他请求的性能急速下降. 另外,占用更多的内存,gc回收次数更多,每次要回收的垃圾对象更多,耗费的时间更多,会导致es的java虚拟机停止工作线程的时间更多 使用这种不换行的格式的话 不需要转为json对象,不会在内存中copy数据,直接按照换行符切割即可 对每两个一组的json,读取metadata,进行document路由. 直接将对应的json发送到node上去 最大的优势在于,不需要将json数组解析为JSONArray对象,不需要浪费内存空间,尽可能保证性能]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-11-基于groovy脚本进行partial update]]></title>
    <url>%2F2018%2F11%2F19%2FElasticsearch-11-%E5%9F%BA%E4%BA%8Egroovy%E8%84%9A%E6%9C%AC%E8%BF%9B%E8%A1%8Cpartial-update%2F</url>
    <content type="text"><![CDATA[基于groovy脚本进行partial update首先添加一条数据到es中12345PUT test_index/test_type/2&#123; &quot;num&quot;:0, &quot;tags&quot;:[]&#125; 内置脚本1234POST test_index/test_type/2/_update&#123; &quot;script&quot;: &quot;ctx._source.num+=1&quot;&#125; 通过ctx._source.filed 去操作filed的值 执行完毕后去查询一下1GET test_index/test_type/2 返回值:1234567891011&#123; &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_version&quot;: 2, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;num&quot;: 1, &quot;tags&quot;: [] &#125;&#125; 可以看到num已经变成了1 外部脚本用外部脚本进行partial update首先要在es的config/scripts目录下新建groovy脚本,比如我们要在tags下加一个值,新建脚本test-add-tags.groovy,文件内容是1ctx._source.tags+=new_tag 然后用外部脚本进行更新12345678910POST test_index/test_type/2/_update&#123; &quot;script&quot;: &#123; &quot;lang&quot;: &quot;groovy&quot;, // 指定脚本语言 &quot;file&quot;: &quot;test-add-tags&quot;, // 文件名称(不包含文件后缀名) &quot;params&quot;: &#123; // 参数的集合 &quot;new_tag&quot;:&quot;tag1&quot; // 参数名和值 &#125; &#125;&#125; 用外部脚本删除document同样需要在config/scripts目录下新建groovy脚本, 脚本名称:test-delete-document,脚本内容是:1ctx.op=ctx._source.num == count ? &apos;delete&apos; : &apos;none&apos; 就是num的值跟count相等的时候才删除,如果不相等,什么都不做.然后执行partial update12345678910POST test_index/test_type/2/_update&#123; &quot;script&quot;: &#123; &quot;lang&quot;: &quot;groovy&quot;, &quot;file&quot;: &quot;test-delete-document&quot;, &quot;params&quot;: &#123; &quot;count&quot;:1 &#125; &#125;&#125; 传入的参数count是1,这个时候document中的num的值也是1,所以这document会被删除掉 upsert操作此时这个id是2的document已经被删除,如果这个时候直接去进行partial update,会报错,因为这个document不存在这时候可以使用upsert去进行初始化操作执行:12345678POST /test_index/test_type/2/_update&#123; &quot;script&quot;: &quot;ctx._source.num+=1&quot;, &quot;upsert&quot;: &#123; &quot;num&quot;:0, &quot;tags&quot;:[] &#125;&#125; 执行完成后,查询发现num是0 tags是[],并没有去执行script中的脚本.再次运行上面的代码, 查询后发现num已经更新为1 upsert操作:如果指定的document不存在,就执行upsert中的初始化操作,如果指定的document存在,就执行doc或者script中指定的partial update操作.]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-10-并发控制方案]]></title>
    <url>%2F2018%2F11%2F17%2FElasticsearch-10-%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[并发场景假设现在是矮子电商场景下,用户需要去购买商品,程序的工作流程是: 读取商品信息,比如商品名称价格等. 用户下单去购买商品 更新商品库存es的工作流程: 先查询document数据,商品信息等,显示到页面,同时在内存中缓存该document的数据 当网页发生购买以后,直接基于内存中的数据进行计算和操作 将计算后的结果返回到es中 现在有两个线程同时去执行这个流程,比如购买一台电脑库存是100,线程A 线程B并发执行,两个线程查询到的库存都是100,这个时候用户去下单购买,线程A 将商品库存减一是99件,线程B也将库存减一也是99件,然后A先更新es的中的商品库存,更新为99,此时线程B也去更新商品库存为99,这个时候一共是发生了两次购买操作,正常结果是库存应该减少两件,但其实只减少了一件,这就导致了并发冲突,使得数据结果不正确. 有些场景下,在不管数据正不正确的情况下这样操作是无所谓,比如我们只管将数据写入es,不管数据是什么样的,或者是说算错了也没关系的这些情况,但是一般情况下我们是需要做并发控制的防止数据错乱 并发控制方案悲观锁并发控制方案还是上面的这个场景,使用悲观锁控制并发,就是线程A去读取商品信息,读取的时候给这个商品信息加锁,然后进行一系列的操作,比如减少库存等,将库存减一后,再更新到商品信息中去,然后释放锁,在这个过程中,线程B如果去请求这个商品数据是请求不到的,在线程A释放锁之前,B是获取不到的,当A释放锁之后,这个时候商品库存已经变化了,B拿到的数据就是A已经操作完成后的数据,同时线程B也会对数据加锁,操作完成之后再释放. 悲观锁的并发控制方案,就是在各种情况下都上锁,上锁之后,就只有一个线程可以操作这条数据了,当然,不同场景下上的锁也不同,比如行级锁 表级锁 读锁 写锁等. 乐观锁并发控制方案es就是使用的乐观锁,使用乐观锁控制并发的时候,并不会对数据进行加锁,而是通过版本号控制(version),这点和zookeeper相似,还是上面的场景,线程A B 同时去请求商品信息,然后拿到的都是一样的,比如库存是100, 这时候比如数据的version=1, 假设线程A先把库存-1然后更新到es中,这时候es会拿版本号和线程A中的数据的版本号进行对比,这时候version都是1写入成功,库存更新为99 es中数据的版本号变更为2,这时候线程B也去进行更新,发现es中数据的版本号是2,但是线程B的版本号还是1,版本号不同的情况下,线程B重新请求es的数据 然后再将库存-1,版本号也变为2,然后再去更新es中的数据,这个时候库存变为98 版本号变为3. es内部基于乐观锁控制并发的原理假设现在有两个节点一个shard一个replica,里面有一条document 数据是test1 version也是1 现在有两个请求同时去修改document数据,我们期待的结果是1先修改为test2,然后2再修改为test3 但是这个两个请求时并发的所以第2个请求可能先到达es先进性修改, 如果没有乐观锁并发控制的话,第2个请求先到达将内容修改为test3,第1个请求后到,再去修改为test2,这个时候数据就变成了test,跟我们预期的结果就不一样了,因为按照顺序来说是先修改为test2 再修改为test3 同理shard去将数据同步到replica的时候也会多线程异步执行的,如果没有乐观锁的控制,数据也可能发生错误但是在有乐观锁的情况下,基于version去控制,如果说第二个请求先到先修改了数据 version已经加1了比如变为2,replica先同步了第二个请求的数据,第一个请求再去同步的时候去比对一下版本号,version还是1,那么es就会直接把这条数据扔掉,这个时候数据就会保持一个正确的状态 基于external version进行乐观锁并发控制es提供了一个feature,就是说你可以不用他提供的内部版本号来进行并发控制,可以基于你自己维护的一个版本号来进行并发控制.举个例子,假如你的数据在mysql里面也有一份,然后你的应用本身就维护了一个版本号,无论是自己生成的或者是程序控制的.这个时候进行乐观锁并发控制的时候可能并不是想要用内部的_version来进行控制,而是用你自己维护的那个版本号来控制. 基于内部的_version进行控制时,只需要在请求后面加 _version=版本号即可,基于自己的version控制的话需要在请求后面加?_version=版本号&amp;version_type=external这两种版本控制的唯一区别就在于 基于内部_version控制时,只有当你提供的version和es中的version一毛一样的时候才能修改,只要不一样就报错,而基于自己的版本号控制时,只有当你提供的version比es中的_version大的时候才能完成修改 两种锁的优缺点 悲观锁 优点:方便,直接加锁,对应用程序来说是透明的,不需要做额外的操作 缺点:并发能力很低,同一时间只能有一个线程访问操作数据 乐观锁 优点:并发能力高,不给数据加锁,可以有大量线程并发操作 缺点:麻烦,每次更新的时候都要先去比对版本号,然后版本号不一致时,可能需要重新加载数据,再次修改然后再写,而且这个过程可能要重复好几次]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-9-document核心元数据解析]]></title>
    <url>%2F2018%2F11%2F16%2FElasticsearch-9-document%E6%A0%B8%E5%BF%83%E5%85%83%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[本文主要写document中的_index,_type,_id,_source这几个元数据先随便添加一个document1234PUT /test_index/test_type/1&#123; &quot;test_content&quot;:&quot;test content&quot;&#125; 查询1GET /test_index/test_type/1 返回值12345678910&#123; &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;test_content&quot;: &quot;test content&quot; &#125;&#125; 返回值详解_index元数据 代表document存放在哪一个index中 类似的数据放在一个索引中,非类似的数据应该放到不同的索引中,什么意思呢,比如现在有一个商品类的数据(product)和一个商品销售的数据(sale),那么应该把product的数据放到一个index里面,sale的数据放到另一个index里面, 如果把所有的数据都放到一个index里面,这样做是不合适的. index中包含了很多类似的document,类似就是指这些document的fields很大一部分是相同的, 比如说现在有3个document但是每个document里面的fields是完全不一样的,这就是不类似的,就不太适合放到同一个index里面了 索引的命名规则,必须是小写的 不能用大写,第二 不能用下划线开头,第三 不能包含逗号 详细来说一下上面的第2点,为什么说应该把类似的数据放到同一个index里面, 比如我们现在把product的数据和sale的数据都放到了同一个index下, 这些数据将会被分配到不同的shard上面,每个shard上面既有product数据也有sale数据, 现在网站后台需要做sale的聚合分析,而且sale的数据量很大,这个时候所有的shard都会去执行这次聚合分析,耗费大量的资源,与此同时在网站前端用户需要去搜索product中的数据,因为这些shard都在做后台的聚合分析占用了大量的资源,导致前端用户搜索product数据时会变慢,影响用户体验,所以相同类型的数据放到一个index上,在自己独立的shard中与其他数据不在一个shard中就不会互相影响 _type元数据 代表document属于index中的哪个类别 一个索引通常会划分为多个type,逻辑上对index中有些许不同的数据进行分类,因为相同类型的数据,会有很多相同的fields,但是也可能有些许的差别,比如商品数据放到了一个index中,但是商品下面还划分为电子商品 生鲜商品 等等. type命名 名称是可以大小写的,但是不能用下划线开头也不能包含逗号 _id元数据 代表document的唯一标识,与index和type一起,可以定位一个document 我们可以手动指定document的id,也可以不指定 es会为我们自动创建一个id Q:什么时候适合使用手动生成id,什么时候使用自动生成的id?A:一般来说是从某些其他的系统中导入数据到es时,会采取这种方式,就是系统中已经有数据的唯一标识了,比如我们现在有一个商品的数据库,要把里面的数据导入到es里面做搜索,数据库里面肯定会有一个数据的主键primary key,这时将数据导入到es中的时候就适合用数据库中已有的primary key做id自动生成的id,我们先来看一下,是什么样的先添加一个数据,不指定id1234POST /test_index/test_type&#123; &quot;test_content&quot;:&quot;test1&quot;&#125; 返回值:12345678910111213&#123; &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;AWcctb8Zlcpuqacodv55&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true&#125; 可以看到id是一个字符串,长度是固定为20位的,URL安全,base64编码,使用的是GUID生成策略,分布式系统并行生成时不可能发生冲突 _source元数据就是我们在创建document的时候,在request中写入的json数据,当我们get的时候,会把这个json数据原封不动的返回到_source中来.如果我们发送的有多个field 在返回时只想要指定的field 只需要在get请求后面加?_source=field1,field2…即可]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-8-容错机制]]></title>
    <url>%2F2018%2F11%2F16%2FElasticsearch-8-%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[容错机制假设场景,现在一共有9个shard,其中3个shard 6个replica,一共有三个es节点,node1是master节点,具体如下图: 如果下载master节点挂掉,shard1,replica2-1,replica3-1 节点会丢失,在master节点挂掉的一瞬间 shard1就没了,此时shard1就不是active状态了,集群中不是所有的primary shard都是active状态了,所以集群的状态会在这一瞬间变为red 容错第一步集群会自动选举另外一个node成为新的master,比如node2,承担起master的责任来 容错第二步新的master会将丢失掉的primary shard的某个replica shard提升为primary shard,此时集群的状态是yellow了,因为所有的primary shard都变成了活跃状态, 但是因为少了一个replica shard 所以不是所有的replica shard都是active状态了 容错第三步新的master重启之前宕机的节点,将丢失的副本都拷贝到该node上去,而且该node会使用宕机之前已有的shard的数据,只是同步一下宕机后发生的修改.此时集群的状态变为green,因为所有的primary shard 和 replica shard都是active状态了]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-7-扩容机制]]></title>
    <url>%2F2018%2F11%2F16%2FElasticsearch-7-%E6%89%A9%E5%AE%B9%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[横向扩容上文中有提到有两个es节点的环境下shard和replica的分配 两个node环境下replica和shard的分配目前集群中有两个es节点,创建一个index,设置有3个shard每个shard对应一个replica,如下图: 新添加一个节点到集群中前面有说到过es集群会自动做负载均衡,如果我们现在加一个es节点到集群中来的话,es会按照一定的规则(一个shard和它对应的replica不会被分配到同一个节点上去)将部分shard分配到新的节点上去,如图: 横向扩容的好处横向扩容后,每台节点上的shard会减少,就意味着每个shard可以占用节点上的更多资源比如IO/CPU/Memory,整个系统的性能会更好 扩容的极限上面我们一共有6个shard(3个shard 3个replica),最多扩容到6个节点,就是说每个节点上都有一个shard,这个时候每个shard可以占用他所在服务器的所有资源,性能是最好的 突破扩容瓶颈如何超出系统的扩容瓶颈呢, 比如现在我们有6个shard但是要扩容到9个节点,同时想让系统的性能更好,这个时候我们可以增加replica的数量,因为primary shard的数量是不能变的,我们只能改变replica的数量,比如有3个primary shard,设置replica的数量为2, 这个时候总共有 3 + 3 x 2 = 9个shard 就可以分布到9个节点上去,因为 replica 也可以处理读请求 所以整个集群的性能会更好. 集群容错性比如集群中有3个节点,一共6个shard,这个时候 如果某一个节点宕机,比如node3宕机了,如下图: 这个时候shard3 和 replica1已经丢失了,剩下其他两个节点上的shard1 replica2 shard2 replica3 ,replica3 因为replica3 是shard3的副本,所以node3 宕机是是不会造成数据的丢失的, 所以3个shard 3个replica分布在3个节点上任意一个节点宕机都不会造成数据的丢失]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-6-shard&replica机制]]></title>
    <url>%2F2018%2F11%2F16%2FElasticsearch-6-shard-replica%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[shard&amp;replica机制梳理 一个index可以包含多个shard, index中的数据会均匀的分配到每个shard中,就是es分片的机制. 每个shard都是一个最小的工作单元,承载部分数据,es是基于Lucene去开发的,其实每个shard就是Lucene的实例,有完整的建立索引和处理请求的能力 增减节点的时候shard会自动在nodes中负载均衡,比如一共有6个es节点,但是有7个shard 这个时候其中的一个es节点会有两个shard,如果这时候集群中再加进来一个es节点,那么承载两个shard的节点会分配一个shard到新加的节点上去 每个document肯定只会存在于某一个primary shard中以及其对应的replica shard中,不可能同时存在于两个 primary shard中 replica shard是primary shard的副本,负责容错,以及承担读请求负载. primary shard的数量在创建索引的时候就固定了,replica shard的数量可以随时修改 一个index中primary shard的默认数量是5,replica shard默认是1(就是每个primary shard都有1个 replica),默认有10个shard 5个primary shard和5个replica shard primary shard不能和自己的replica shard放在同一个节点上,如果放在同一个节点上 当这个节点宕机的时候primary shard和replica shard都丢失了,就起不到容错的作用, 同一个节点上一个放其他节点的replica shard 单个es节点环境下创建index比如我们现在创建一个index 设置有3个primary shard 每个 shard对应一个replica创建代码如下:1234567PUT /test_index&#123; &quot;settings&quot; : &#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 1 &#125;&#125; 这时候es集群的状态是yellow,而且只会将3个primary shard分配到这个es节点上去,对应的3个replica是不会被创建的,就是上面提到的 shard自己的replica是不能跟自己放在一起的,此时集群可以正常工作,但是这个节点一旦宕机的话,数据就会全部丢失,而且集群不可用,无法承接任何请求 两个es节点环境下shard的分配在上面单节点的环境下如果在添加一个es节点到集群中, 此时新添加进来的节点会承载之前节点的primary shard的replica shard, primary shard 和 replica shard中的数据是一样的, replica 和 primary shard 同样可以处理读请求.]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-5-基础分布式架构]]></title>
    <url>%2F2018%2F11%2F16%2FElasticsearch-5-%E5%9F%BA%E7%A1%80%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[上文中写了Elasticsearch的聚合分析,下钻分析,嵌套聚合等, 本文主要是写Elasticsearch的分布式机制, 扩容策略等 Elasticsearch对分布式机制的透明隐藏特性Elasticsearch是一套分布式的系统,分布式是为了应对大数据量 分片机制: 往ES插入数据的时候ES集群会自动分配到某一shard上,比如 之前用rest API插入数据时,我们并没有关心数据是怎么分配的,是分配到哪个shard上的 cluster discovery:集群发现机制,就是新启动一个es节点时,集群会自动发现节点并加入集群. shard负载均衡:比如我们现在有3个es节点,总共要有25个shard要分配到3个节点上去,es会自动进行均匀分配,以保持每个节点均衡的读写负载请求 shard副本 请求路由:将请求自动路由到对应处理的shard上 集群扩容 shard重分配 我们在实际使用中并不需要关心这些,Elasticsearch会自动处理 Elasticsearch的垂直扩容与水平扩容假设现在有6台服务器,每台服务器容量是1T,马上数据量要增长到8T,这个时候有两种方案: 垂直扩容:重新购置两台服务器,每台服务器的容量是2T,替换两台老的服务器,那么现在服务器总容量就是 4x1T + 2x2T =8T 水平扩容:新购置两台服务器,每台容量1T,直接加到集群中去,那么现在总容量就是8 x 1T = 8T 业界经常采用的方案，采购越来越多的普通服务器，性能比较一般，但是很多普通服务器组织在一起，就能构成强大的计算和存储能力 master节点 管理es集群的元数据,比如说索引的创建和删除,维护索引元数据,节点的增加和移除,维护集群的元数据 默认情况下,会自动选择出一台节点,作为master节点 节点对等的分布式架构每个es节点都能接收请求,接收请求后自动路由到可以处理该请求的shard,并收集返回数据.]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-4-聚合分析]]></title>
    <url>%2F2018%2F11%2F16%2FElasticsearch-4-%E8%81%9A%E5%90%88%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[上文中,添加了6个电影的document,接下来做这些document的聚合分析,统计等.上文添加的6个电影数据中都包含有genres 的一个数组 统计每个genres下的电影数量1234567891011GET /movies/movie/_search&#123; &quot;size&quot;: 0, // size不设置的话 hits中会把对进行聚合的所有数据返回. &quot;aggs&quot;: &#123; &quot;group_by_genres&quot;: &#123; // group_by_genres 是自定义的名字 &quot;terms&quot;: &#123; &quot;field&quot;: &quot;genres&quot; // 要做聚合的field &#125; &#125; &#125;&#125; 运行一下,会发现报错 如下:123456789101112131415161718192021222324252627282930&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [genres] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.&quot; &#125; ], &quot;type&quot;: &quot;search_phase_execution_exception&quot;, &quot;reason&quot;: &quot;all shards failed&quot;, &quot;phase&quot;: &quot;query&quot;, &quot;grouped&quot;: true, &quot;failed_shards&quot;: [ &#123; &quot;shard&quot;: 0, &quot;index&quot;: &quot;movies&quot;, &quot;node&quot;: &quot;f57uV91xS_GRTQS2Ho81rg&quot;, &quot;reason&quot;: &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [genres] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.&quot; &#125; &#125; ], &quot;caused_by&quot;: &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [genres] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.&quot; &#125; &#125;, &quot;status&quot;: 400&#125; 错误原因是:默认情况下，在文本字段上禁用Fielddata。在[genres]上设置fielddata=true，以便通过反转索引来加载内存中的fielddata。请注意，这可能会占用大量内存 这里我们需要将文本field的fielddata属性设置为true,具体原因之后再说.123456789PUT /movies/_mapping/movie&#123; &quot;properties&quot;: &#123; &quot;genres&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;fielddata&quot;: true &#125; &#125;&#125; 然后再执行上面的查询, 返回结果为:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&#123; &quot;took&quot;: 5, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 6, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_genres&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;drama&quot;, &quot;doc_count&quot;: 4 &#125;, &#123; &quot;key&quot;: &quot;crime&quot;, &quot;doc_count&quot;: 3 &#125;, &#123; &quot;key&quot;: &quot;biography&quot;, &quot;doc_count&quot;: 2 &#125;, &#123; &quot;key&quot;: &quot;action&quot;, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: &quot;adventure&quot;, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: &quot;cirme&quot;, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: &quot;drame&quot;, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: &quot;mystery&quot;, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: &quot;thriller&quot;, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: &quot;war&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125; &#125;&#125; 具体的聚合结果返回到了 aggregations 下的 buckets 下, key为每个genres下的元素, doc_count 是包含该key的电影数量. 对名称中包含kill的电影，计算每个genres下的电影数量12345678910111213141516GET /movies/movie/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;kill&quot; &#125; &#125;, &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_genres&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;genres&quot; &#125; &#125; &#125;&#125; 其实就是在上个查询的基础上加了一个query条件,先查询query,将返回的结果再进行聚合分析 先按genres分组,然后计算每个genres下的电影的年份的平均值123456789101112131415161718GET movies/movie/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_genres&quot;: &#123; // 自定义分组名称 &quot;terms&quot;: &#123; &quot;field&quot;: &quot;genres&quot; // 聚合genres &#125;, &quot;aggs&quot;: &#123; &quot;avg_year&quot;: &#123; // 在上面分组的基础上 在进行聚合分析 &quot;avg&quot;: &#123; // 计算平均值 &quot;field&quot;: &quot;year&quot; &#125; &#125; &#125; &#125; &#125;&#125; 平均值计算是按照每组里面的数据进行平均 计算每个genres下的电影的平均年份，并且按照平均年份降序排序123456789101112131415161718192021GET movies/movie/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_genres&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;genres&quot;, &quot;order&quot;: &#123; &quot;avg_year&quot;: &quot;desc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;avg_year&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;year&quot; &#125; &#125; &#125; &#125; &#125;&#125; 在上一个分组计算平均值的基础上 在上层的terms里面加一个order 要排序的字段就是下面一层聚合计算平均值的名称”avg_year” 按照指定的年份范围区间进行分组，然后在每组内再按照genres进行分组，最后再计算每组的平均年份上文中添加的数据 电影年份有 1962 1972 1979 2007 2003, 用range来进行分组 分为1960-1970 1970-1980 2000-2010 三组12345678910111213141516171819202122232425GET /movies/movie/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_year&quot;: &#123; &quot;range&quot;: &#123; &quot;field&quot;: &quot;year&quot;, &quot;ranges&quot;: [ &#123; &quot;from&quot;: 1960, &quot;to&quot;: 1970 &#125;, &#123; &quot;from&quot;: 1970, &quot;to&quot;: 1980 &#125;, &#123; &quot;from&quot;: 2000, &quot;to&quot;: 2010 &#125; ] &#125; &#125; &#125;&#125; 三组年份的电影分好以后,再往下一层按genres分一层,分好之后再往下聚合,用来计算平均值123456789101112131415161718192021222324252627282930313233343536373839GET /movies/movie/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_year&quot;: &#123; &quot;range&quot;: &#123; &quot;field&quot;: &quot;year&quot;, &quot;ranges&quot;: [ &#123; &quot;from&quot;: 1960, &quot;to&quot;: 1970 &#125;, &#123; &quot;from&quot;: 1970, &quot;to&quot;: 1980 &#125;, &#123; &quot;from&quot;: 2000, &quot;to&quot;: 2010 &#125; ] &#125;, &quot;aggs&quot;: &#123; &quot;group_by_genres&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;genres&quot; &#125;, &quot;aggs&quot;: &#123; &quot;avg_year&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;year&quot; &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-3-花式查询]]></title>
    <url>%2F2018%2F11%2F16%2FElasticsearch-3-%E8%8A%B1%E5%BC%8F%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[新增语法:12345678910111213PUT indexName/typeName/id&#123; json数据&#125;或POST indexName/typeName&#123; json数据&#125;# 如果不指定id的话 es会自动分配一个id 示例下面添加了6个电影信息,索引名称是movies,类型名称是movie1234567891011121314151617181920212223242526272829303132333435363738394041424344454647PUT movies/movie/1&#123; &quot;title&quot;:&quot;The Godfather&quot;, &quot;director&quot;:&quot;Francis Ford Coppola&quot;, &quot;year&quot;:1972, &quot;genres&quot;: [&quot;Cirme&quot;,&quot;Drame&quot;]&#125;PUT movies/movie/2&#123; &quot;title&quot;: &quot;Lawrence of Arabia&quot;, &quot;director&quot;: &quot;David Lean&quot;, &quot;year&quot;: 1962, &quot;genres&quot;: [&quot;Adventure&quot;, &quot;Biography&quot;, &quot;Drama&quot;]&#125;PUT movies/movie/3&#123; &quot;title&quot;: &quot;To Kill a Mockingbird&quot;, &quot;director&quot;: &quot;Robert Mulligan&quot;, &quot;year&quot;: 1962, &quot;genres&quot;: [&quot;Crime&quot;, &quot;Drama&quot;, &quot;Mystery&quot;]&#125;PUT movies/movie/4&#123; &quot;title&quot;: &quot;Apocalypse Now&quot;, &quot;director&quot;: &quot;Francis Ford Coppola&quot;, &quot;year&quot;: 1979, &quot;genres&quot;: [&quot;Drama&quot;, &quot;War&quot;]&#125;PUT movies/movie/5&#123; &quot;title&quot;: &quot;Kill Bill: Vol. 1&quot;, &quot;director&quot;: &quot;Quentin Tarantino&quot;, &quot;year&quot;: 2003, &quot;genres&quot;: [&quot;Action&quot;, &quot;Crime&quot;, &quot;Thriller&quot;]&#125;PUT movies/movie/6&#123; &quot;title&quot;: &quot;The Assassination of Jesse James by the Coward Robert Ford&quot;, &quot;director&quot;: &quot;Andrew Dominik&quot;, &quot;year&quot;: 2007, &quot;genres&quot;: [&quot;Biography&quot;, &quot;Crime&quot;, &quot;Drama&quot;]&#125; 添加完毕之后,通过搜索查询一下这几部电影 (使用_search端点)语法:12GET [index_name]/[type_name]/_search # index_name和type_name都是可选的 示例123GET _search //搜索所有索引和所有类型。GET movies/_search //在电影索引中搜索所有类型GET movies/movie/_search //在电影索引中显式搜索电影类型的文档 query DSLDSL: Domain Specified Language ,特定领域的语言http request body: 请求体,可以用json格式来构建查询语法,比较方便,可以构建各种复杂语法. 查询所有的电影1234GET movies/movie/_search&#123; &quot;query&quot;: &#123;&quot;match_all&quot;: &#123;&#125;&#125;&#125; 查询名称包含kill的电影,同时年份按照降序排序12345678910111213GET movies/movie/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;kill&quot; &#125; &#125;, &quot;sort&quot;: [ &#123; &quot;year&quot;: &quot;desc&quot; &#125; ]&#125; 分页查询,假设每页显示一条数据,现在查询第二页123456GET movies/movie/_search&#123; &quot;query&quot;: &#123;&quot;match_all&quot;: &#123;&#125;&#125;, &quot;from&quot;: 1, &quot;size&quot;: 1&#125; 指定查询出来的电影只要名称和年份12345GET movies/movie/_search&#123; &quot;query&quot;: &#123;&quot;match_all&quot;: &#123;&#125;&#125;, &quot;_source&quot;: [&quot;title&quot;,&quot;year&quot;]&#125; query filter 过滤器查询电影名称包含kill,而且年份大于2000年的电影12345678910111213141516171819GET /movies/movie/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;:&#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;kill&quot; &#125; &#125;, &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;year&quot;: &#123; &quot;gt&quot;: 2000 &#125; &#125; &#125; &#125; &#125;&#125; full-text search (全文检索)123456GET /movies/movie/_search&#123; &quot;query&quot;: &#123;&quot;match&quot;: &#123; &quot;title&quot;: &quot;Kill Bill Lawrence&quot; &#125;&#125;&#125; 首先 将 /movies/movie/下面的所有数据的title的值拆分,建立倒排索引,然后搜索关键字会被拆分为Kill和Bill和Lawrence 然后在倒排索引中检索对应的数据 全文检索会将输入的关键字拆解开来，去倒排索引里面去一一匹配，只要能匹配上任意一个拆解后的单词，就可以作为结果返回 phrase search 短语搜索12345678GET /movies/movie/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;title&quot;: &quot;kill&quot; &#125; &#125;&#125; 跟全文检索相反,要求输入关键字 必须在指定的字段文本中,完全包含一模一样的,才可以算匹配,才能做为返回结果.比如 查询kill 会返回 “Kill Bill: Vol. 1” 和 “To Kill a Mockingbird” 查询kill bill 只会返回 “Kill Bill: Vol. 1” highlight search 高亮搜索结果GET /movies/movie/_search{ “query”: { “match”: { “title”: “kill” } }, “highlight”: { “fields”: { “title”:{} } }} 返回会在 highlight &gt; title 中 将关键字加 &lt;em&gt;&lt;/em&gt; 标签 搜索结果说明 took：耗费了几毫秒 timed_out：是否超时 _shards：一个搜索请求会达到一个index的所有primary shard上,当然 每个primary shard都可能有一个或多个replica shard,所以请求也可以到primary shard的其中一个replica shard上去,shards fail的条件(primary和replica全部挂掉)，不影响其他shard. hits.total：查询结果的数量 hits.max_score：本次搜索的所有结果中，最大的相关度分数是多少，每一条document对于search的相关度，越相关，_score分数越大，排位越靠前 hits.hits：默认查询前10条数据,完整数据,_score 降序排序 time_out机制详解查询默认是没有timeout的,可以手动指定1GET /_search?timeout=10ms 意思就是在timeout的时间范围内,将搜索到的所有结果直接返回给客户端,不需要等数据全部查到后返回,确保一次搜索请求可以在指定的时间内完成,为一些时间敏感的搜索应用提供良好的支持 代码地址 代码地址]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-2-集群状态检查和CRUD操作]]></title>
    <url>%2F2018%2F11%2F16%2FElasticsearch-2-%E9%9B%86%E7%BE%A4%E7%8A%B6%E6%80%81%E6%A3%80%E6%9F%A5%E5%92%8CCRUD%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[检查集群的健康状态Kibana中1GET _cat/health?v 返回值12epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1537168122 15:08:42 elasticsearch yellow 1 1 1 1 0 0 1 0 - 50.0% status:集群的健康状况green:每个索引的primary shard和replica shard都是active状态的yellow:每个缩影的primary shard都是active状态的,但是部分replica shard不是active状态,处于不可用的状态.red:不是所有索引的primary shard都是active状态,部分索引有数据丢失了 当只启动一个es的时候 集群状态是yellow的,，就启动了一个es进程，相当于就只有一个node。现在es中有一个index，就是kibana自己内置建立的index。由于默认的配置是给每个index分配5个primary shard和5个replica shard，而且primary shard和replica shard不能在同一台机器上（为了容错）。现在kibana自己建立的index是1个primary shard和1个replica shard。当前就一个node，所以只有1个primary shard被分配了和启动了，但是一个replica shard没有第二台机器去启动。此时只要启动第二个es进程，就会在es集群中有2个node，然后那1个replica shard就会自动分配过去，然后cluster status就会变成green状态。 索引管理快速查看集群中有哪些索引:1GET /_cat/indices?v 返回值12health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizeyellow open .kibana rUm9n9wMRQCCrRDEhqneBg 1 1 1 0 3.1kb 3.1kb 创建索引1PUT /test_index?pretty 删除索引1DELETE /test_index?pretty document的CRUD操作新增document 建立索引12345678910111213141516171819PUT /index_name/type_name/id&#123; json数据&#125;// 返回值&#123; &quot;_index&quot;: &quot;index_name&quot;, &quot;_type&quot;: &quot;type_name&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true&#125; es会自动建立index和type，不需要提前创建，而且es默认会对document每个field都建立倒排索引，让其可以被搜索 检索/查询文档1234567891011121314151617181920GET index_name/type_name/id// 返回值&#123; &quot;_index&quot;: &quot;index_name&quot;, &quot;_type&quot;: &quot;type_name&quot;, &quot;_id&quot;: &quot;id&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;gaolujie yagao&quot;, &quot;desc&quot;: &quot;gaoxiao meibai&quot;, &quot;price&quot;: 30, &quot;producer&quot;: &quot;gaolujie producer&quot;, &quot;tags&quot;: [ &quot;meibai&quot;, &quot;fangzhu&quot; ] &#125;&#125; 更新document全量替换123456PUT /index_name/type_name/id&#123; json数据&#125;// 不做更新的filed也需要传过去 上面这个操作是全量替换,其实是es先把旧的数据标记为deleted,不做物理删除,然后再创建一个新的document出来,id也和原来的一样,当es数据量增大时,es才会去删除被标记为deleted的数据,来释放空间 那么如何强制创建呢,就是上面这个操作不想去更新,就是想去再创建一个新的document,那么这时候可以在请求后面加一个参数 ?_create,但是执行时候发现这样会报错,因为这个id已经存在了引起冲突,所以会报错 partial update12345678POST /index_name/type_name/id/_update&#123; &quot;doc&quot;:&#123; &quot;需要更新的列&quot;:&quot;值&quot; &#125;&#125;// 上面这个更新看起来就比较方便了,每次只需要传递少数的发生修改的filed过去即可,不需要将全量的document数据发送过去. 两者对比我们使用全量替换的时候,当我们只需要修改document中其中某一个字段的时候必须知道其他字段的值,这时候我们需要进行一次查询,然后将不需要修改的字段原封不动的写入到语句中去,可以很容易的看出进行一次数据修改,需要用于先去查询,然后更新再写回到es中去,耗费的时间相对较长,而且在写入数据的时候,可能别人已经对数据进行修改了,容易产生并发冲突 然后我们来看一下partial update,其实partial update的执行和全量替换是一样的,也是先获取document,然后将传过来的filed更新到document中,将老的document标记为deleted,用新的数据创建新的document, 但是相较于全量替换,partial update的所有查询修改都是发生在es内部的,操作基本都是毫秒级别的,耗费的时间相对较小,并发冲突的可能性也就相对较小. 总结partial update与全量替换的执行步骤其实是一致的,但是partial update所有的查询和修改操作都是在es内部的shard中进行的,避免了网络数据传输的开销,提升 了性能. 第二减少了查询和修改中的时间间隔,有效减少了并发冲突. 删除document1DELETE /index_name/type_name/id 这里的删除也是做的逻辑删除,es把要删除的document标记为deleted状态,也是在存储空间不够的时候es才会去做物理删除来释放空间]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-1-Windows下安装Es]]></title>
    <url>%2F2018%2F11%2F16%2FElasticsearch-1-Windows%E4%B8%8B%E5%AE%89%E8%A3%85Es%2F</url>
    <content type="text"><![CDATA[下载elasticsearchelasticsearch-5.2.0.zip 解压 cmd进入bin目录,启动1elasticsearch.bat 检查是否启动成功1http://localhost:9200/?pretty 返回值12345678910111213&#123; &quot;name&quot; : &quot;f57uV91&quot;, // 节点名称 &quot;cluster_name&quot; : &quot;elasticsearch&quot;, // 所在集群名称 &quot;cluster_uuid&quot; : &quot;uyiZiUqnSSaV-eazKkv_sg&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;5.2.0&quot;, // 版本号 &quot;build_hash&quot; : &quot;24e05b9&quot;, &quot;build_date&quot; : &quot;2017-01-24T19:52:35.800Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;6.4.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 下载Kibanakibana-5.2.0-windows-x86.zip 解压 进入bin目录,启动kibana.bat localhost:5601/ 进入Dev Tools]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-0-核心概念]]></title>
    <url>%2F2018%2F11%2F16%2FElasticsearch-0-%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[全文检索把要搜索的内容 进行分词储存为倒排索引, 搜索的时候,去扫描的是内容中的所有关键词而不是去扫描内容本身内容中能拆分出来多少词 倒排索引中就会有多少行 先遍历倒排索引中的关键词,然后去数据源中找到数据返回 Lucene封装了建立倒排索引,以及搜索的代码,包括各种算法, 提供了java使用的api,用Lucene可以将已有的数据建立索引,Lucene会在本地磁盘上组织索引的数据结构,另外可以用Lucene提供的提供的功能和api对磁盘上的索引数据进行搜索 ElasticsearchLucene索引是建立在磁盘上的,当数据量大的时候,建立的索引数据占用的空间也会越来越大, 可能会需要多台机器来存放索引数据,这时前端搜索关键字的时候,就需要去判断去哪台机器上的索引搜索,而且服务器宕机的情况下索引数据可能会丢失等.这个时候就需要Elasticsearch,ES底层封装了Lucene,可以配置多个ES节点优点: 自动维护数据的分布到多个节点的索引建立,还有搜索请求分布到多个节点的执行 自动维护数据的冗余副本,保证某些机器宕机之后不会丢失数据 封装了更多的高级功能,便于快速开发应用,提供更复杂的搜索功能,聚合分析的功能,基于地理位置的搜索等 功能: 分布式的搜索引擎和数据分析引擎 数据分析:比如查询一个电商网站中,某一类商品销量前10的店铺;某类商品中一个月内访问量最高的商品等 全文检索,结构化检索,数据分析 结构化检索:比如搜索某一类的商品有哪些 对海量的数据进行实时的处理 ES可以自动将海量的数据分散到多台服务器上存储和检索 进实时是指秒级别的数据搜索和分析 特点: 可以作为一个大型分布式集群技术,处理PB级别的数据, 也可以运行在单机上 将全文检索和数据分析合并在了一起 对用户而言 开箱即用 对传统数据库的补充,提供了很多数据库不能提供的功能,如全文检索,同义词处理,相关度排名,复杂数据分析,海量数据的近实时处理等 核心概念Near RealTime(NRT): 近实时, 从数据写入到可以被搜索到大概有1s的延迟; 基于ES执行搜索和分析可以达到秒级 Cluster: 集群,包含多个节点,节点可以通过配置集群名称来标识是属于哪个集群的 Node:节点 集群中的一个节点, 节点也有名称(默认是随机分配的), 默认节点会被加入到名为”elasticsearch”的集群 Document&amp;field: 文档,ES中的最小数据单元,一个document可以是任意一条数据, 数据结构通常是JSON 一个document中会有多个field,每个field就是json中的一个字段 Index:索引,包含一堆相似结构的文档数据, 比如有一个订单索引,商品分类索引,索引会有一个名称,一个index代表了一类类似的或者相同的document,index可以包含多个document,比如创建一个product index(商品索引),里面就可能存放了所有的商品数据 Type:类型,每个索引里可以有一个或多个type, type是index中的一个逻辑分类,一个type下的document都有相同的filed shard:单机无法存储大量数据,ES可以将一个索引中的数据拆分成多个shard,分布在多台服务器上存储.有了shard就可以横向扩展,当数据量增加的时候,直接再增加一个es节点就可以了,搜索和分析的操作分布到多台服务器上执行可以提高吞吐量和性能. replica: 服务器可能随时故障或宕机,此时shard节点会丢失,因此可以为每个shard创建多个replica副本, replica可以在shard宕机的情况下提供备用服务 同时可以提升搜索操作的吞吐量和性能.primary shard(建立索引时一次设置,不能修改,默认5个)replica shard(随时修改数量,默认1个&lt;对应每个primary shard的1个&gt;), 默认每个索引10个shard,5个 primary shard,5个replica shard 最小的高可用配置是两台服务器 ES对应到数据库document —&gt; 行type —&gt; 表index —&gt; 库]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
</search>
