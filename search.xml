<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spring Boot-集成gRPC提供开放接口]]></title>
    <url>%2F2020%2F10%2F26%2FSpring-Boot-%E9%9B%86%E6%88%90gRPC%E6%8F%90%E4%BE%9B%E5%BC%80%E6%94%BE%E6%8E%A5%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[述gRPC 是谷歌的一个开源 RPC 框架, 因为最近要做一个跨语言平台的交互,所以选用了 gRPC 这个框架,下面记录一下从零开始跟 Spring Boot 集成的过程. proto文件编写使用 gRPC 第一步,要先编写一个 .proto 文件,这个文件的作用是定义接口的一些入参返回值等等,比如下面这个 HelloWorld.proto 文件 123456789101112131415161718syntax = &quot;proto3&quot;;option java_multiple_files = true;message HelloRequest&#123; string txt = 1;&#125;message HelloResponse&#123; int32 status = 1; string msg = 2; string data = 3;&#125;service HelloService&#123; // 方法名(参数) 返回值(返回类型) rpc sayHello(HelloRequest) returns (HelloResponse);&#125; syntax第一行 syntax = &quot;proto3&quot;; 这个是定义这个 proto 文件的版本, 如果学过 docker 的话就不难理解,我们编写 dockerfile 的时候也要定义一下 version 版本号, 这里要注意的是,这个版本要跟你引入的jar包的版本匹配,不然编译之后会有很多包找不到 option选项, 这个相当于一个配置, 比如上面文件中的 option java_multiple_files = true; 这个配置表示:指定在 proto 文件中定义的所有消息,枚举和服务在生成java类的时候都会生成对应的java类文件,而不是以内部类的形式出现. 到这儿可能有点懵,先来了解一下这个 proto 文件使用的整体流程: 编写 proto 文件,定义方法名,入参出参等基本信息 通过工具编译 proto 文件,这步操作会生成一堆 java 类, 对应的就有我们上面定义的入参的model,出参的model, 以及构建入参出参的一些工具类等等 把生成的类复制到我们的项目中, 然后编写实现类, 实现类中就是我们具体的业务逻辑 启动服务端,提供给客户端访问 到这儿应该能理解 option java_multiple_files = true; 这个配置的作用了吧, 就是如果配置的是 false, 他会把所有需要的类都放到一个 java 类中, 以内部类的形式存在, 如果设置的是 true 的话, 那就会各生成各的. option 这个设置还有别的项, Google 一下都有 messagemessage 这个是消息定义,可以理解为 Java 中的实体类, 声明实体类,然后定义类中的属性. 字段类型跟java中的基本类型差不多,稍微有点不一样,这个可以 Google 一下. 这里要注意一点,上面我们定义的字段 string txt = 1; 这个 1 并不是这个字段的初始值,而是给这个字段分配的标量, 每一个被定义在消息中的字段都会被分配给一个唯一的标量,这些标量用于标识你定义在二进制消息格式中的属性,标量一旦被定义就不允许在使用过程中再次被改变,标量的值在1～15的这个范围里占一个字节编码. service这个就相当于定义一个 Java 中的接口, 然后下面的 rpc 是定义具体的接口名以及入参出参, 这部分应该比较好理解 引入依赖现在 Java 项目基本都是用 spring boot 了吧, 所以找一个开源的 starter 可以帮我们快速集成 grpc 需要用到的一些配置, 引入下面这个包 12345&lt;dependency&gt; &lt;groupId&gt;io.github.lognet&lt;/groupId&gt; &lt;artifactId&gt;grpc-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.4.1&lt;/version&gt;&lt;/dependency&gt; 这个项目的GitHub在这里 包导入之后,还需要加一个插件用来把 proto 文件编译成我们需要的 java 文件,插件导入如下:123456789101112131415161718192021222324252627282930&lt;build&gt; &lt;finalName&gt;grpc-java&lt;/finalName&gt; &lt;extensions&gt; &lt;extension&gt; &lt;groupId&gt;kr.motd.maven&lt;/groupId&gt; &lt;artifactId&gt;os-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.6.2&lt;/version&gt; &lt;/extension&gt; &lt;/extensions&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.xolstice.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;protobuf-maven-plugin&lt;/artifactId&gt; &lt;version&gt;0.6.1&lt;/version&gt; &lt;configuration&gt; &lt;protocArtifact&gt;com.google.protobuf:protoc:3.11.0:exe:$&#123;os.detected.classifier&#125;&lt;/protocArtifact&gt; &lt;pluginId&gt;grpc-java&lt;/pluginId&gt; &lt;pluginArtifact&gt;io.grpc:protoc-gen-grpc-java:1.27.2:exe:$&#123;os.detected.classifier&#125;&lt;/pluginArtifact&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;compile-custom&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 编译代码上面配置完成之后,使用 mvn clean generate-sources 命令编译 proto 文件, 然后在如下文件夹中可以找到编译结果 这些类需要复制到我们的项目对应的包中 编写实现类上面生成的只是一些接口,构建请求返回的工具类等等, 具体的业务逻辑我们还没有写, 新建一个类,继承 HelloServiceGrpc.HelloServiceImplBase , 继承的这个类是我们上面自动生成的, 找后缀是 Grpc 的, 然后内部类以 ImplBase 后缀结尾的这个 之后重写我们的业务方法, 具体代码如下: 1234567891011@GRpcServicepublic class HelloServiceGrpcImpl extends HelloServiceGrpc.HelloServiceImplBase &#123; @Override public void sayHello(HelloRequest request, StreamObserver&lt;HelloResponse&gt; responseObserver) &#123; System.out.println(request.getTxt()); responseObserver.onNext(HelloResponse.newBuilder().setStatus(200).setMsg(&quot;返回消息&quot;).build()); responseObserver.onCompleted(); &#125; &#125; 这里入参出参就都可以拿到了,这个方法里可以写具体的业务逻辑,比如连接数据库,增删改查等等 服务启动在上面的实现类中,加入 @GRpcService 这个注解,之后就会被容器扫描到,然后对外提供服务, 默认的接口是 5656 , 可以在配置文件中通过 grpc.server.port 这个值修改 到这儿服务端方面的东西就OK了, 一般我们为了不被打, 写完接口还得自测一下嘛, 所以还需要看看客户端的一些配置 客户端配置客户端需要一个配置类,如下: 123456789101112131415@Configurationpublic class GrpcConfig &#123; @Bean ManagedChannel channel()&#123; return ManagedChannelBuilder.forAddress(&quot;127.0.0.1&quot;, 6565) .usePlaintext() .build(); &#125; @Bean HelloServiceGrpc.HelloServiceBlockingStub helloServiceBlockingStub()&#123; return HelloServiceGrpc.newBlockingStub(channel()); &#125; &#125; 这个类就是设置一个 channel ,配置地址,当然这个地址可以写在配置文件里, 然后还要 @Bean 注入一个类, 是 Grpc 结尾然后 BlockingStub 结尾的内部类, 注入到容器中, 之后就可以在单元测试或者别的地方使用了 12345678910111213@Autowiredprivate HelloServiceGrpc.HelloServiceBlockingStub helloServiceBlockingStub;@Testpublic void contextLoads() &#123; HelloResponse helloResponse = helloServiceBlockingStub.sayHello(HelloRequest.newBuilder().setTxt(&quot;test&quot;).build()); System.out.println(helloResponse.getStatus()); System.out.println(helloResponse.getMsg()); System.out.println(helloResponse.getData()); &#125; 导入公共类定义 proto 文件的时候,像上面这种 HelloResponse 的 message 其实是通用的,就可以单独用一个文件定义一下,然后用 import 关键字导入进来, 就不用在每个文件中都写一次了,如下: 1import &quot;common/CommonResponse.proto&quot;; 这里有一点主意的是,只能用绝对路劲,不能使用 ./ 或者 ../ 这种,否则编译会报错 以上就是 grpc 的一个简单的示例,更高级的用法可以看一下 grpc-spring-boot-starter 的 GitHub,这个里面提供了很多的示例 就先记到这儿,下课.]]></content>
      <categories>
        <category>杂记</category>
      </categories>
      <tags>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sentinel-配置持久化]]></title>
    <url>%2F2020%2F08%2F17%2FSentinel-%E9%85%8D%E7%BD%AE%E6%8C%81%E4%B9%85%E5%8C%96%2F</url>
    <content type="text"><![CDATA[述上文通过硬编码的方式配置了服务的限流和降级, 在 sentinel 的可视化界面中,也可以直接去设置这些东西,比如下面这样: 配置资源名, QPS 阈值,或者线程数等等,这样操作比在微服务应用中写代码要方便一点,但是默认情况下,在可视化界面创建的这些规则都是存在内存中的,服务重启之后就失效了,还得重新再设置一下,这种情况显然是不能接受的,所以要把这些配置给持久化, Sentinel 提供了三种持久化的方式 zookeeper, apollo ,nacos 本文将使用最常见的 zookeeper 来实现 sentinel 的持久化,下面记录一下配置过程,以及配置过程中遇到的一些坑 源码下载去 Github 中把 Sentinel 的源码拉下来 这里有一堆项目, 拉下来之后切换到对应版本的分支上去 sentinel-dashboard 修改后端代码其他代码不用管,只打开 sentinel-dashboard 这个项目 第一步,打开pom文件, 找到 zookeeper 相关配置,如下: 这里把这个 &lt;scope&gt;test&lt;/scope&gt; 注释掉即可 第二步,项目的test目录下,找到zookeeper的相关配置,如下: 之后复制到 com.alibaba.csp.sentinel.dashboard.rule 这个目录下面,如图: 这里可以看到 test 目录下已经提供了 zookeeper, apollo ,nacos 这三种持久化方式的代码,需要哪个复制哪个就好了 第三步, 配置 zookeeper 地址, 在 ZookeeperConfig 这个类中,设置自己的 zookeeper 地址 第四步, 修改 com.alibaba.csp.sentinel.dashboard.controller.v2.FlowControllerV2 这个类,如下: 这里只修改 ruleProvider 和 rulePublisher 即可,指定是 zookeeper 相关的类 到这儿后端相关的代码就改造完毕了, 前端方面还有几个要注意的地方 前端代码修改前端页面上要修改两个html页面, 第一个是 sidebar.html ,如下: 这里有两个流控规则的 &lt;li&gt; 标签, 上面一个注释掉, 下面一个 ui-sref 值改成 dashboard.flow({app: entry.app}) 然后还有一个界面是 flow_v2.html 修改如图: 这里踩的一个坑是,他页面默认调用的是 FlowControllerV1 这个 Controller ,这个类中操作都是在内存中的, 并没有走持久化的操作,我们上面修改的是 FlowControllerV2 , 所以只要让前端调用 FlowControllerV2 这个类中的接口就可以了, 至于前端页面中修改的几个地方的意思就涉及到我的知识盲区了, 总之最重要的一点就是前端调用的接口是要有持久化操作的接口 应用代码修改上面 sentinel 源码修改完成之后,就可以直接启动了, 之后页面操作的配置数据都会放到 zookeeper 中去, 所以我们在具体应用的代码中,还得从 zookeeper 中读取数据, 修改步骤如下: 引入依赖123456&lt;!-- 哨兵持久化配置数据源 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-datasource-zookeeper&lt;/artifactId&gt; &lt;version&gt;1.5.2&lt;/version&gt;&lt;/dependency&gt; 修改配置文件1234sentinel: zookeeper: address: 127.0.0.1:2181 path: /sentinel_rule_config 这里这个 path 就是 sentinel 存数据的节点, 默认就是 /sentinel_rule_config 加载配置新建类 SentinelConfig 代码如下: 12345678910111213141516171819202122232425262728@Componentclass SentinelConfig( @Value(&quot;\$&#123;sentinel.zookeeper.address&#125;&quot;) val zkServer: String, @Value(&quot;\$&#123;sentinel.zookeeper.path&#125;&quot;) val zkPath: String, @Value(&quot;\$&#123;spring.application.name&#125;&quot;) val appName: String) &#123; private val log:Logger = LoggerFactory.getLogger(this.javaClass) override fun toString(): String &#123; return &quot;SentinelConfig(log=$log, zkServer=&apos;$zkServer&apos;, zkPath=&apos;$zkPath&apos;, appName=&apos;$appName&apos;)&quot; &#125; init &#123; log.info(&quot;SentinelConfig:[&#123;&#125;]&quot;, this.toString()) // 声明一个zookeeper数据源 // 从zookeeper中根据 $zkPath/$appName 读出一段配置来,然后转成 FlowRule 这个对象 val zookeeperDataSource = ZookeeperDataSource(zkServer, &quot;$zkPath/$appName&quot;) &#123; source: String? -&gt; JSON.parseArray(source, FlowRule::class.java) &#125; // 添加到规则中 FlowRuleManager.register2Property(zookeeperDataSource.property) &#125;&#125; 这里也很简单,先从 application 中加载几个属性, 然后在类初始化的时候,声明一个 zookeeper 数据源,然后通过 FlowRuleManager 加载即可 然后之前通过硬编码方式设置的那些规则就都可以删掉了,然后通过可视化界面配置即可 zookeeper版本这里再记录一个坑, 我用的 zookeeper 版本是 3.4.13, sentinel 项目中使用的 curator 版本是 4.0.1, 这俩是不兼容的,会导致新建数据失败, 解决办法就是排除 curator 中的 zookeeper 依赖,然后再引入我们自己的版本,pom文件修改如下: 123456789101112131415161718192021222324252627282930&lt;!--for Zookeeper rule publisher sample--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;$&#123;curator.version&#125;&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;!-- &lt;scope&gt;test&lt;/scope&gt;--&gt;&lt;/dependency&gt;&lt;!--zookeeper--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.13&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 到这儿,持久化的配置就OK了, sentinel 端操作的数据都是放到 zookeeper 端, 然后应用通过 zookeeper 获取数据 总结sentinel修改流程: 修改pom文件,支持持久化 从test中复制持久化配置到代码中 修改 provider 和 publisher 为对应的持久化类 修改前端代码调用持久化的接口 应用代码修改流程: 引入持久化相关的包 配置zk地址及节点 读取配置数据源并读取]]></content>
      <categories>
        <category>Sentinel</category>
      </categories>
      <tags>
        <tag>Sentinel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sentinel-服务流控降级配置]]></title>
    <url>%2F2020%2F08%2F17%2FSentinel-%E6%9C%8D%E5%8A%A1%E6%B5%81%E6%8E%A7%E9%99%8D%E7%BA%A7%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[述记录一次配置 Sentinel 的过程,以及遇到的一些问题 SentinelSentinel 是一个面向云原生微服务的高可用流控防护组件,作用跟 Hystrix 类似. 具体介绍可以看 Github 可视化界面可视化界面的安装,先去 github 的 release 里面,选一个版本, 下载一个 jar 如下; 之后直接启动这个jar就可以了 1java -Dserver.port=8080 -Dscp.sentinel.dashboard.server=localhost:8080 -Dproject.name=sentinel-dashboard -jar sentinel-dashboard-1.7.2.jar 访问 localhost:8080 ,默认的账号密码都是 sentinel ,登录进去 默认是一个空的,没有应用的,到这儿可视化界面就安装成功了 Spring Boot 配置使用依赖引入Sentinel 要具体配置在每一个微服务上, 首先需要引入 Sentinel 的 starter 依赖123456&lt;!-- 阿里哨兵 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt; &lt;version&gt;0.9.0.RELEASE&lt;/version&gt;&lt;/dependency&gt; 配置修改在 application 配置文件中,要做一些 sentinel 相关的配置,如下:12345678910spring: application: name: orderApi cloud: sentinel: transport: # sentinel 会再启动一个端口,去和dashboard去通讯 port: 8719 # sentinel 控制台地址 dashboard: localhost:8080 流控使用资源声明以流控为例, 首先要声明资源, Spring Boot 中,使用注解的方式就可以,如下: 12345@PostMapping@SentinelResource(value = &quot;createOrder&quot;)public OrderInfo createOrder(@RequestBody OrderInfo info, @AuthenticationPrincipal String username) &#123; return null&#125; 使用 @SentinelResource 这个注解声明一个资源,里面的 value 值就是资源名称 配置使用声明好资源之后,还要对这个资源设置一些规则,比如每秒最多多少QPS,代码如下: 123456789// 设置流控规则val flowRule = FlowRule()flowRule.resource = &quot;createOrder&quot;flowRule.grade = RuleConstant.FLOW_GRADE_QPSflowRule.count = 10.0val rules = listOf(flowRule)// 加载规则FlowRuleManager.loadRules(rules) 上面代码是 kotlin 的, 步骤如下: 创建一个 FlowRule 对象 然后设置一些规则,比如 resource 就是我们定义的资源名, 然后 grade 是一个限流的策略, count 就是请求的数量 声明一个集合把 FlowRule 放到集合中去 最后调用 FlowRuleManager.loadRules() 这个方法,让规则加载到系统中 这段代码要在 Spring Boot 项目启动后执行,至于使用启动器还是别的方式可以自己选,总之配置的代码要在启动后执行一次 执行完成后,规则就生效了,按照上面的配置QPS到10的时候服务就限流了 服务降级方法声明服务降级配置跟流控类似的,第一步还是在注解上面配置,注解如下: 1@SentinelResource(value = &quot;createOrder&quot;,blockHandler = &quot;createOrderOnBlock&quot;) 多配置了一个属性 blockHandler 值是 createOrderOnBlock, createOrderOnBlock 是降级处理的方法名,默认是在当前类的,如下: 12345public OrderInfo createOrderOnBlock(@RequestBody OrderInfo info, @AuthenticationPrincipal String username, BlockException ex) &#123; log.info(&quot;createOrder 服务已降级:&#123;&#125;&quot;, ex.getClass().getSimpleName()); info.setProductId(99L); return info;&#125; 参数返回值和资源声明的方法都是一样的, 多了一个 BlockException ,服务熔断之后就会走这个降级的方法 配置使用跟流控一样的,降级也是要在代码中做一些配置的,如下: 123456789101112131415// 设置降级规则val degradeRule = DegradeRule()degradeRule.resource = &quot;createOrder&quot;// 定义规则 , 每秒请求数 &gt;= 5 的时候才会生效// DEGRADE_GRADE_RT: 服务响应时间 (秒)// DEGRADE_GRADE_EXCEPTION_RATIO: 在一定时间内异常的比例 值是 0-1 (秒)// DEGRADE_GRADE_EXCEPTION_COUNT: 精确的值,在一定时间(时间窗口是1分钟)内错了几个之后熔断degradeRule.grade = RuleConstant.DEGRADE_GRADE_RT// 时间是毫秒,表示服务响应时间&gt; 10ms 后会熔断degradeRule.count = 10.0// 熔断的持续时间,单位是秒degradeRule.timeWindow = 10val degradeRules = listOf(degradeRule)DegradeRuleManager.loadRules(degradeRules) 声明一个 DegradeRule 对象 设置资源名,还有降级规则,熔断的持续时间等等(代码注释中都标明了) 之后放到一个 list 中 调用 DegradeRuleManager.loadRules() 方法加载到系统中 服务降级默认是在 QPS 大于等于5的时候才会生效, 同样的这段代码也是要在系统启动之后执行. 关于限流和降级的配置先记录到这里]]></content>
      <categories>
        <category>Sentinel</category>
      </categories>
      <tags>
        <tag>Sentinel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kotlin-Spring Boot 混合开发 @Value 注解的使用]]></title>
    <url>%2F2020%2F08%2F17%2FKotlin-Spring-Boot-%E6%B7%B7%E5%90%88%E5%BC%80%E5%8F%91-Value-%E6%B3%A8%E8%A7%A3%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[述在 kotlin 和 Java 的混合开发中,偶尔会用到 Spring Boot 的 @Value() 这个注解, 下面记录一下 kotlin 中,使用 Spring Boot 注入属性的几种方式. 环境假设,在 Spring Boot 的 application.yml 中有这样的配置123spring: application: name: testApp 这样的配置要怎么注入到实体类中的属性呢? @Value 注解第一种方式,使用@Value 这个注解,在Java中的写法如下: 12345@Componentclass Test&#123; @Value(&quot;$&#123;spring.application.name&#125;&quot;) private String appName;&#125; 对应的 kotlin 中, 代码如下: 12345678@Componentclass SentinelConfig( @Value(&quot;\$&#123;spring.application.name&#125;&quot;) val appName: String) &#123; // ....&#125; kotlin 的写法有几点和Java不一样的 要注入的参数必须声明在 kotlin 的构造中,声明在类中是不起作用的. kotlin 中的 $ 是保留字,需要转义才能使用. @ConfigurationProperties 注解@ConfigurationProperties 这个注解, Java 的用法就不再介绍了, kotlin 中,代码如下: 123456@Component@ConfigurationProperties(prefix = &quot;xxx.xxx.xx&quot;)data class Properties( val xxx: String val yyy: String) 或者 123456@Component@ConfigurationProperties(prefix = &quot;xxx.xxx.xx&quot;)class Properties()&#123; lateinit var xxx: String lateinit var yyy: String&#125; 下面这种 lateinit 的方式我没有试过, 但是使用 @Value 注解和 lateinit 这种方式注入是行不通的. 常用的属性注入就这些,先记到这儿]]></content>
      <categories>
        <category>Kotlin</category>
      </categories>
      <tags>
        <tag>Kotlin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUC-线程协作-3-CyclicBarrier]]></title>
    <url>%2F2020%2F04%2F13%2FJUC-%E7%BA%BF%E7%A8%8B%E5%8D%8F%E4%BD%9C-3-CyclicBarrier%2F</url>
    <content type="text"><![CDATA[述CyclicBarrier 循环栅栏同样是一个线程协作工具,和 CountDownLatch 很像,都能阻塞一组线程 作用当有大量线程相互配合,分别计算不同任务,并且需要最后统一汇总的时候,我们可以使用 CyclicBarrier,CyclicBarrier 可以构造一个集结点,当某一个线程执行完毕,它就会到集结点等待,直到所有线程都到了集结点,那么该栅栏就被撤销,所有线程再统一出发,继续执行剩下的任务. 常用方法 构造函数,有两个参数,第一个是等待线程的数量,第二个是一个 Runnable 的实现 await(): 调用的线程进入阻塞状态等待 代码示例123456789101112131415161718192021222324252627282930313233@Slf4jpublic class CyclicBarrierDemo &#123; public static void main(String[] args) &#123; CyclicBarrier cyclicBarrier = new CyclicBarrier(5, () -&gt; log.info(&quot;所有线程都到了...&quot;)); for (int i = 0; i &lt; 5; i++) &#123; new Thread(new Task(i + 1, cyclicBarrier)).start(); &#125; &#125; @AllArgsConstructor @Slf4j static class Task implements Runnable&#123; private int id; private CyclicBarrier cyclicBarrier; @Override public void run() &#123; log.info(&quot;当前线程正在执行,id:[&#123;&#125;]&quot;, id); try &#123; Thread.sleep((long) (Math.random() * 10000)); cyclicBarrier.await(); log.info(&quot;所有线程都到了,继续执行,当前线程id:&#123;&#125;&quot;,id); &#125; catch (InterruptedException | BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 控制台输出如下:123456789101110:40:34.645 [Thread-1] INFO com.learning.java.cooperation.CyclicBarrierDemo$Task - 当前线程正在执行,id:[2]10:40:34.645 [Thread-0] INFO com.learning.java.cooperation.CyclicBarrierDemo$Task - 当前线程正在执行,id:[1]10:40:34.645 [Thread-2] INFO com.learning.java.cooperation.CyclicBarrierDemo$Task - 当前线程正在执行,id:[3]10:40:34.645 [Thread-3] INFO com.learning.java.cooperation.CyclicBarrierDemo$Task - 当前线程正在执行,id:[4]10:40:34.645 [Thread-4] INFO com.learning.java.cooperation.CyclicBarrierDemo$Task - 当前线程正在执行,id:[5]10:40:43.843 [Thread-4] INFO com.learning.java.cooperation.CyclicBarrierDemo - 所有线程都到了...10:40:43.843 [Thread-4] INFO com.learning.java.cooperation.CyclicBarrierDemo$Task - 所有线程都到了,继续执行,当前线程id:510:40:43.843 [Thread-2] INFO com.learning.java.cooperation.CyclicBarrierDemo$Task - 所有线程都到了,继续执行,当前线程id:310:40:43.843 [Thread-0] INFO com.learning.java.cooperation.CyclicBarrierDemo$Task - 所有线程都到了,继续执行,当前线程id:110:40:43.843 [Thread-1] INFO com.learning.java.cooperation.CyclicBarrierDemo$Task - 所有线程都到了,继续执行,当前线程id:210:40:43.843 [Thread-3] INFO com.learning.java.cooperation.CyclicBarrierDemo$Task - 所有线程都到了,继续执行,当前线程id:4 首先声明一个数量是 5 的 CyclicBarrier, 然后新建5个线程,之后都调用 await() 方法,等到5个线程都调用 await() 方法之后,就会执行构造中传入的 Runnable 的实现, 然后所有线程唤醒,继续执行 对比 CountDownLatch 作用不同: CyclicBarrier 要等到固定数量的线程都到达了栅栏位置才能继续执行, CountDownLatch 只需要在计数器为0的时候就可以继续执行,也就是说 CountDownLatch 用于事件,而 CyclicBarrier 是用于线程的 可重用性不同: CountDownLatch 再倒数触发门闩打开之后就不能再重用了,而 CyclicBarrier 是可以重复使用的]]></content>
      <categories>
        <category>JUC</category>
      </categories>
      <tags>
        <tag>JUC</tag>
        <tag>线程协作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUC-线程协作-2-Condition]]></title>
    <url>%2F2020%2F04%2F13%2FJUC-%E7%BA%BF%E7%A8%8B%E5%8D%8F%E4%BD%9C-2-Condition%2F</url>
    <content type="text"><![CDATA[述在线程运行过程中,需要等待某个条件的时候,就可以用 Condition ,调用它的 await() 方法,进入阻塞状态等待,当另一个线程去执行对应的条件,直到这个条件达成的时候就去执行它的 signal() 方法,这时候JVM会从被阻塞的线程中找等待该条件的线程,然后唤醒,继续执行 常用方法 await(): 调用后进入阻塞状态 signalAll(): 唤起全部的正在等待的线程 signal(): 公平的,唤起一个等待时间最长的线程 使用案例代码如下: 1234567891011121314151617181920212223242526272829303132333435@Slf4jpublic class ConditionTest &#123; private ReentrantLock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); void doSomeThing1() &#123; lock.lock(); try &#123; log.info(&quot;条件不满足,等待...&quot;); condition.await(); log.info(&quot;条件满足,继续执行...&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; void doSomeThing2()&#123; lock.lock(); try &#123; log.info(&quot;准备工作完成,唤醒其他的线程...&quot;); condition.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125; public static void main(String[] args) &#123; ConditionTest conditionTest = new ConditionTest(); new Thread(conditionTest::doSomeThing1).start(); new Thread(conditionTest::doSomeThing2).start(); &#125;&#125; 控制台运行输出如下: 12311:34:34.581 [Thread-0] INFO com.learning.java.cooperation.ConditionTest - 条件不满足,等待...11:34:34.613 [Thread-1] INFO com.learning.java.cooperation.ConditionTest - 准备工作完成,唤醒其他的线程...11:34:34.613 [Thread-0] INFO com.learning.java.cooperation.ConditionTest - 条件满足,继续执行... 注意点 实际上,如果说 Lock 用来代替 synchronized, 那么 Condition 就是用来代替相对应的 Object.wait/notify 的,所以在用法和性质上,几乎都一样 await() 方法会自动释放持有的Lock锁,和 Object.wait 一样,不需要自己手动先释放锁 调用 await() 的时候,必须先持有锁,否则会抛异常]]></content>
      <categories>
        <category>JUC</category>
      </categories>
      <tags>
        <tag>JUC</tag>
        <tag>线程协作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUC-线程协作-1-Semaphore]]></title>
    <url>%2F2020%2F04%2F13%2FJUC-%E7%BA%BF%E7%A8%8B%E5%8D%8F%E4%BD%9C-1-Semaphore%2F</url>
    <content type="text"><![CDATA[述Semaphore 信号量,也是一个线程协作的工具,可以用来限制或管理数量有限的资源的使用 用法信号量的作用是维护一个 “许可证” 的计数,线程可以获取许可证,然后信号量剩余许可证数量减一,当信号量所拥有的许可证为0的时候,下一个想要获取许可证的线程就需要等待,直到有另外的线程释放了许可证 常用的几个方法如下 构造函数: 和上文的 CountDownLatch 一样, Semaphore 在初始化的时候也需要指定许可证的数量,还有一个构造参数是配置公平策略,如果是 true ,当有了新的许可证的时候,会把它给等待时间最长的线程 acquire()/acquireUninterruptibly(): 这两个方法是用来获取许可证的,从名字上来看就知道,后面的是可以在获取的过程种被打断的 tryAcquire(timeout)/tryAcquire(): 这两个方法是尝试获取许可证,在规定时间内立即返回,不会阻塞 release(): 释放许可证 使用案例用个具体的代码演示以下信号量的使用 12345678910111213141516171819202122232425262728293031@Slf4jpublic class SemaphoreTest &#123; static Semaphore semaphore = new Semaphore(3); public static void main(String[] args) &#123; for (int i = 0; i &lt; 100; i++) &#123; new Thread(new Task()).start(); &#125; &#125; @Slf4j static class Task implements Runnable&#123; @Override public void run() &#123; try &#123; semaphore.acquire(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; log.info(&quot;获取到了许可证,执行具体逻辑...&quot;); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; log.info(&quot;完成任务,释放许可证...&quot;); semaphore.release(); &#125; &#125;&#125; 新建一个3个许可证的信号量,然后新建 100 个线程都去获取证书,执行后再释放掉,看一下控制台输出 12345678910111213141516171811:03:17.556 [Thread-1] INFO com.learning.java.cooperation.SemaphoreTest$Task - 获取到了许可证,执行具体逻辑...11:03:17.556 [Thread-0] INFO com.learning.java.cooperation.SemaphoreTest$Task - 获取到了许可证,执行具体逻辑...11:03:17.556 [Thread-2] INFO com.learning.java.cooperation.SemaphoreTest$Task - 获取到了许可证,执行具体逻辑...11:03:18.579 [Thread-0] INFO com.learning.java.cooperation.SemaphoreTest$Task - 完成任务,释放许可证...11:03:18.579 [Thread-2] INFO com.learning.java.cooperation.SemaphoreTest$Task - 完成任务,释放许可证...11:03:18.579 [Thread-1] INFO com.learning.java.cooperation.SemaphoreTest$Task - 完成任务,释放许可证...11:03:18.579 [Thread-4] INFO com.learning.java.cooperation.SemaphoreTest$Task - 获取到了许可证,执行具体逻辑...11:03:18.579 [Thread-5] INFO com.learning.java.cooperation.SemaphoreTest$Task - 获取到了许可证,执行具体逻辑...11:03:18.579 [Thread-3] INFO com.learning.java.cooperation.SemaphoreTest$Task - 获取到了许可证,执行具体逻辑...11:03:19.579 [Thread-3] INFO com.learning.java.cooperation.SemaphoreTest$Task - 完成任务,释放许可证...11:03:19.579 [Thread-5] INFO com.learning.java.cooperation.SemaphoreTest$Task - 完成任务,释放许可证...11:03:19.579 [Thread-4] INFO com.learning.java.cooperation.SemaphoreTest$Task - 完成任务,释放许可证...11:03:19.579 [Thread-6] INFO com.learning.java.cooperation.SemaphoreTest$Task - 获取到了许可证,执行具体逻辑...11:03:19.579 [Thread-7] INFO com.learning.java.cooperation.SemaphoreTest$Task - 获取到了许可证,执行具体逻辑...11:03:19.579 [Thread-29] INFO com.learning.java.cooperation.SemaphoreTest$Task - 获取到了许可证,执行具体逻辑...11:03:20.579 [Thread-6] INFO com.learning.java.cooperation.SemaphoreTest$Task - 完成任务,释放许可证...11:03:20.579 [Thread-7] INFO com.learning.java.cooperation.SemaphoreTest$Task - 完成任务,释放许可证...11:03:20.579 [Thread-29] INFO com.learning.java.cooperation.SemaphoreTest$Task - 完成任务,释放许可证... 可以看到每次执行的都是3个线程,释放之后才会有下一批线程获取到,这里上面构造函数中没有设置为公平的,所以这里获取到许可证的线程并不是按顺序的 注意点 acquire() 和 release() 方法都有一个重载的方法,可以传入一个 int 类型的参数,表示获取/释放几个许可证,但是并不强制获取几个就得释放几个,比如获取两个释放一个也是可以的,但最后许可证会越来越少,会导致程序卡死 初始化的时候一般设置为公平的,更加合理 并不是必须由获取许可证的线程释放许可证, 比如A线程获取了许可证,可以由B线程释放]]></content>
      <categories>
        <category>JUC</category>
      </categories>
      <tags>
        <tag>JUC</tag>
        <tag>线程协作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUC-线程协作-0-CountDownLatch]]></title>
    <url>%2F2020%2F04%2F13%2FJUC-%E7%BA%BF%E7%A8%8B%E5%8D%8F%E4%BD%9C-0-CountDownLatch%2F</url>
    <content type="text"><![CDATA[述线程的运行和运行时机是不受我们自己控制的,一些情况下我们可能需要多个线程之间配合来完成一些任务,比如线程A等B线程执行完成之后运行等 CountDownLatchCountDownLatch 是一个线程协作的工具类,是一个计数器,主要用到的有以下几个方法: 构造方法,参数是 count ,是需要倒数的值 await(): 调用 await() 方法的线程会被挂起,等到 count 为 0 的时候才会继续执行 countDown(): 每次调用会将 count 值减1,直到为0的时候等待的线程就会被唤醒 常见用法用法一CountDownLatch 的第一种用法就是一个线程等待多个线程都执行完毕,再继续自己的工作,代码示例如下 123456789101112131415161718192021222324252627@Slf4jpublic class CountDownLatchTest &#123; public static void main(String[] args) throws InterruptedException &#123; ExecutorService threadPool = Executors.newFixedThreadPool(5); CountDownLatch countDownLatch = new CountDownLatch(5); for (int i = 0; i &lt; 5; i++) &#123; threadPool.submit(()-&gt;&#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; log.info(&quot;已完成...&quot;); countDownLatch.countDown(); &#125; &#125;); &#125; log.info(&quot;正在等待其他线程....&quot;); countDownLatch.await(); log.info(&quot;所有线程执行完毕...&quot;); &#125;&#125; 运行输出如下: 123456710:25:42.359 [main] INFO com.learning.java.cooperation.CountDownLatchTest - 正在等待其他线程....10:25:43.352 [pool-1-thread-2] INFO com.learning.java.cooperation.CountDownLatchTest - 已完成...10:25:43.352 [pool-1-thread-3] INFO com.learning.java.cooperation.CountDownLatchTest - 已完成...10:25:43.352 [pool-1-thread-1] INFO com.learning.java.cooperation.CountDownLatchTest - 已完成...10:25:43.353 [pool-1-thread-5] INFO com.learning.java.cooperation.CountDownLatchTest - 已完成...10:25:43.353 [pool-1-thread-4] INFO com.learning.java.cooperation.CountDownLatchTest - 已完成...10:25:43.353 [main] INFO com.learning.java.cooperation.CountDownLatchTest - 所有线程执行完毕... 首先新建一个 count 为 5 的 CountDownLatch 计数器,然后新建5个线程,之后主线程调用 await() 等待,等待5个线程都调用了 countDown() 之后, count 为 0, 主线程继续运行 用法二第二种用法是多个线程等待某一个线程的信号,然后同时开始执行,代码示例如下: 1234567891011121314151617181920212223242526@Slf4jpublic class CountDownLatchTest &#123; public static void main(String[] args) throws InterruptedException &#123; ExecutorService threadPool = Executors.newFixedThreadPool(5); CountDownLatch countDownLatch = new CountDownLatch(1); for (int i = 0; i &lt; 5; i++) &#123; threadPool.submit(()-&gt;&#123; try &#123; log.info(&quot;就绪,等待主线程信号&quot;); // 挂起线程,等待主线程信号 countDownLatch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; log.info(&quot;已完成...&quot;); &#125; &#125;); &#125; Thread.sleep(2000); log.info(&quot;主线程准备完毕..&quot;); countDownLatch.countDown(); &#125;&#125; 控制台输出如下 123456789101110:41:52.079 [pool-1-thread-1] INFO com.learning.java.cooperation.CountDownLatchTest - 就绪,等待主线程信号10:41:52.079 [pool-1-thread-5] INFO com.learning.java.cooperation.CountDownLatchTest - 就绪,等待主线程信号10:41:52.080 [pool-1-thread-2] INFO com.learning.java.cooperation.CountDownLatchTest - 就绪,等待主线程信号10:41:52.088 [pool-1-thread-3] INFO com.learning.java.cooperation.CountDownLatchTest - 就绪,等待主线程信号10:41:52.092 [pool-1-thread-4] INFO com.learning.java.cooperation.CountDownLatchTest - 就绪,等待主线程信号10:41:54.072 [main] INFO com.learning.java.cooperation.CountDownLatchTest - 主线程准备完毕..10:41:54.072 [pool-1-thread-1] INFO com.learning.java.cooperation.CountDownLatchTest - 已完成...10:41:54.072 [pool-1-thread-5] INFO com.learning.java.cooperation.CountDownLatchTest - 已完成...10:41:54.072 [pool-1-thread-2] INFO com.learning.java.cooperation.CountDownLatchTest - 已完成...10:41:54.072 [pool-1-thread-3] INFO com.learning.java.cooperation.CountDownLatchTest - 已完成...10:41:54.072 [pool-1-thread-4] INFO com.learning.java.cooperation.CountDownLatchTest - 已完成... 这种场景一般用于压测, 多个线程准备好之后,一起发送请求给服务器 总结 了解 CountDownLatch 两种常见的使用场景: 一等多和多等一 CountDownLatch 实例化的时候需要传入倒数的次数,等到倒数为0的时候,所有调用过 await() 方法的线程就会被唤醒 CountDownLatch 不能回滚重置,也就是不能重复使用]]></content>
      <categories>
        <category>JUC</category>
      </categories>
      <tags>
        <tag>JUC</tag>
        <tag>线程协作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUC-并发容器-3-队列]]></title>
    <url>%2F2020%2F04%2F13%2FJUC-%E5%B9%B6%E5%8F%91%E5%AE%B9%E5%99%A8-3-%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[述除了集合,还有一种数据结构就是队列,前面在学习线程池的时候已经接触过了,用队列可以在线程之间传递数据,最常见的就是生产者和消费者模式,队列又分为阻塞队列和非阻塞队列 阻塞队列和非阻塞队列 队列是一个逻辑结构,对应的物理结构可能是由数组或链表实现的 那么阻塞队列和非阻塞队列有什么区别 举个例子,现在有一个队列,长度是10,现在已经满了,当第11个数据放进去的时候,阻塞队列可以设置一个超时时间,如果在这个时间内,队列中有数据出队了,那他就能放进去了,如果超时就放弃,非阻塞队列直接就丢失了,不会等 对于出队来说,也就是从队列中获取数据,如果是非阻塞队列,队列中没数据的时候取出来的是null,而阻塞队列会等,等到有数据了取出来 阻塞队列阻塞队列的实现有 ArrayBlockingQueue,LinkedBlockingQueue,PriorityBlockingQueue,SynchronousQueue, 阻塞队列是线程安全的,阻塞队列又分为有界无界,无界队列的容量是 Integer.MAX_VALUE 常用方法 take(): 获取并移除队列头节点,如果队列中没有数据,就阻塞,直到队列中有数据了 put(): 向队列中插入元素,如果满了,就阻塞 add(): 向队列中添加元素,如果满了就抛异常 remove(): 移除元素,如果是空也抛异常 element(): 获取头结点,如果是空也会抛异常 offer(): 向队列添加元素,如果满了就返回false poll(): 取出元素,如果空就返回null,取出的同时会删除这个元素 peek(): 取出元素,如果空就返回null,取出的同时不会删除这个元素 ArrayBlockingQueueArrayBlockingQueue 是一个有界阻塞队列,可以指定容量,同时可以指定公平与非公平,如果是保证公平的话,那么等待了最长时间的线程会被优先处理,不过这会同时带来一定的性能损耗. 案例新建一个长度为3的队列,然后生产者线程往里放,消费者线程往出取 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162@Slf4jpublic class ArrayBlockingQueueTest &#123; public static void main(String[] args) throws InterruptedException &#123; ArrayBlockingQueue&lt;String&gt; blockingQueue = new ArrayBlockingQueue&lt;&gt;(3); new Thread(new Provider(blockingQueue)).start(); new Thread(new Consumer(blockingQueue)).start(); &#125;&#125;@AllArgsConstructor@Slf4jclass Provider implements Runnable &#123; BlockingQueue&lt;String&gt; queue; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; queue.put(&quot;Candidate:&quot; + (i + 1)); log.info(&quot;放入了第&quot; + (i + 1) + &quot;个数据&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; // 放一个停止的标记 try &#123; queue.put(&quot;over&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125;@AllArgsConstructor@Slf4jclass Consumer implements Runnable &#123; BlockingQueue&lt;String&gt; queue; @Override public void run() &#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // 从队列中取东西 String msg = null; try &#123; while(!&quot;over&quot;.equals((msg = queue.take())))&#123; log.info(&quot;从队列中取出内容:&#123;&#125;&quot;,msg); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; log.info(&quot;没有数据了...&quot;); &#125;&#125; LinkedBlockingQueueLinkedBlockingQueue 是一个无界的阻塞队列,容量是 Integer.MAX_VALUE ,数据是一个链表结构的,看一下他的源码 首先是一个内部类 Node ,用来存放数据 这里就可以看出来他是一个链表结构的 然后是两个重要的属性 这里可以看到, put 和 take 是两把锁,所以放入和取出互不干扰,最后再看看 put 方法. 具体用法跟上面的 ArrayBlockingQueue 类似 PriorityBlockingQueuePriorityBlockingQueue 是一个支持优先级的阻塞队列,可以自定义顺序,也是一个无界队列, PriorityQueue 的线程安全版本 SynchronousQueueSynchronousQueue 这个队列的容量为0,它不会取持有元素,做的事情就是直接投递数据,这个队列没有 peek() 等函数,因为它没有头节点,我们之前用的可缓存线程池就是用的这个队列 延迟队列队列对应的还有延迟队列, Dueue ,根据延迟时间排序.元素需要实现 Delayed 接口 非阻塞并发队列非阻塞并发队列只有一个 ConcurrentLinkedQueue ,链表数据结构,使用CAS非阻塞算法实现线程安全,适合在性能要求较高的并发场景,用的相对较少]]></content>
      <categories>
        <category>JUC</category>
      </categories>
      <tags>
        <tag>JUC</tag>
        <tag>并发容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUC-并发容器-2-CopyOnWriteArrayList]]></title>
    <url>%2F2020%2F04%2F13%2FJUC-%E5%B9%B6%E5%8F%91%E5%AE%B9%E5%99%A8-2-CopyOnWriteArrayList%2F</url>
    <content type="text"><![CDATA[述了解了并发安全的 HashMap 之后,再来看看并发安全的 ArrayList ,就是 CopyOnWriteArrayList, 早期的版本中有 Vector 和 SynchronizedList 但是这两个锁的粒度太大,所以并发效率并不高, Copy-On-Write 并发容器还包括 CopyOnWriteArraySet 用来替代同步 Set 适用场景CopyOnWriteArrayList 适用于读操作尽可能快,写操作可以慢一点的地方,或者读取比写入多的地方 读写规则CopyOnWriteArrayList 读取完全不需要加锁,写入也不会阻塞读取操作,只有写入和写入之间需要同步等待 设计思想CopyOnWrite 通俗的来说,就是往容器中添加一个元素的时候,不是直接往当前容器添加,而是先将当前的容器复制一份作为一个新的容器,然后在新的容器中添加,添加完成之后,再将原容器的引用指向新的容器,所以 CopyOnWrite 容器进行并发的读的时候,不需要加锁,因为当前容器不会添加任何元素,但是这样做将不能保证数据的实时性 用一个案例来看一下,代码如下 12345678910111213public class ListTest &#123; public static void main(String[] args) &#123; CopyOnWriteArrayList&lt;Integer&gt; list = new CopyOnWriteArrayList&lt;&gt;(new Integer[]&#123;1, 2, 3, 4, 5&#125;); Iterator&lt;Integer&gt; iterator1 = list.iterator(); list.add(6); Iterator&lt;Integer&gt; iterator2 = list.iterator(); iterator1.forEachRemaining(System.out::println); iterator2.forEachRemaining(System.out::println); &#125;&#125; 这段代码的运行结果如下123456789101112345123456 可以看到 iterator1 这个迭代器并没有6这元素 实现原理看一下这个类的源码 这个类里面维护的就是一个数组 然后看看 add() 和 get() 方法 首先上锁,保证只有一个线程修改,先复制,改完之后把新的再放到上面的数组中,数组引用是 volatile 修饰的,因此将旧的数组引用指向新的数组,根据 volatile 的 happens-before 规则,写线程对数组引用的修改对读线程是可见的 get() 方法就很简单了,直接返回 缺点 不能保证数据一致性 内存占用,写操作的时候,内存中会有两个对象 总结了解 CopyOnWriteArrayList 的原理,以及可能会带来的一些问题]]></content>
      <categories>
        <category>JUC</category>
      </categories>
      <tags>
        <tag>JUC</tag>
        <tag>并发容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUC-并发容器-1-ConcurrentHashMap]]></title>
    <url>%2F2020%2F04%2F13%2FJUC-%E5%B9%B6%E5%8F%91%E5%AE%B9%E5%99%A8-1-ConcurrentHashMap%2F</url>
    <content type="text"><![CDATA[述上文中分析了JDK1.7和1.8中的 HashMap 的数据结构以及一些重要的方法,本文同样从这两个版本,分析一下 ConcurrentHashMap JDK1.7 中的 ConcurrentHashMap数据结构如图 Java 7 中的 ConcurrentHashMap 最外层是多个 segment, 每个 segment 的底层数据结构和 HashMap 类似的,仍然是数组和链表结构, segment 默认有16个,初始化完成之后就无法扩容,存放数据的时候,首先要先定位到具体的 segment 中 每个 Segment 都有独立的 ReentrantLock 锁,每个 Segment 之间互不干扰,挺高并发效率, 比如初始化的16个 Segment ,就可以有16个线程同时在这个 ConcurrentHashMap 进行读写 源码部分这里就不再贴了,感兴趣可以自己看看 JDK1.8 中的 ConcurrentHashMap数据结构1.8 中数据结构如图 跟 HashMap 是一样的,1.7 的版本虽然是解决了并发的问题,但是 HashMap 在 1.7 版本中的问题还是存在,就是遍历链表效率太低了,1.8版本中的 ConcurrentHashMap 抛弃了 Segment ,采用 CAS + synchronized 的方式保证并发安全 存放数据也是用1.8中的 Node ,代码如下 这里可以看到,value和next都是用了 volatile 修饰,保证可见性 重点方法介绍首先看一下 put() 方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778final V putVal(K key, V value, boolean onlyIfAbsent) &#123; // key或value为空直接抛异常 if (key == null || value == null) throw new NullPointerException(); // 计算hash int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; // 判断是否需要初始化 if (tab == null || (n = tab.length) == 0) // 空的话先去初始化 tab = initTable(); // 计算当前key定位出来的node即代码中的 f ,为空,表示可以写入数据 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; // 利用CAS写入 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; // 如果当前位置是 MOVED == -1 ,表示这个槽点正在进行扩容 else if ((fh = f.hash) == MOVED) // 调用帮助扩容的方法 tab = helpTransfer(tab, f); else &#123; // 这个槽里有值了,就会进入下面的代码 V oldVal = null; // 把当前的node锁住 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; if (fh &gt;= 0) &#123; // 链表操作 binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; // 如果链表中有了这个key了,就覆盖然后break if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; // 如果没有的话,放个新的,加到链表最后 if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; else if (f instanceof TreeBin) &#123; // 如果不是链表.这里进行红黑树的操作 Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; // 添加完成之后,判断是不是需要将链表转成红黑树 if (binCount != 0) &#123; if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; addCount(1L, binCount); return null;&#125; 以上就是 put() 方法的一个大致流程,下面再看看 get() 方法,代码如下 1234567891011121314151617181920212223public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; // 计算hash值 int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; // 如果在桶上,就直接返回 if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; // 如果是红黑树,就按树的方式获取值 else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; // 最后是按链表的方式遍历 while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; 总结最后总结一下 ConcurrentHashMap 中的 put() get() 流程 首先 put() 流程 判断key-value都不为空 计算hash值 根据对应位置的节点的类型赋值,或者是 helpTransfer ,或者增长链表,或者增加红黑树节点 检查是否满足转红黑树阈值,如果满足就转成红黑树 返回 oldValue get() 方法流程 计算hash值 找到对应位置,不同情况做不同处理, 直接取值/红黑树里找/遍历链表取值 返回找到的结果]]></content>
      <categories>
        <category>JUC</category>
      </categories>
      <tags>
        <tag>JUC</tag>
        <tag>并发容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUC-并发容器-0-HashMap]]></title>
    <url>%2F2020%2F04%2F13%2FJUC-%E5%B9%B6%E5%8F%91%E5%AE%B9%E5%99%A8-0-HashMap%2F</url>
    <content type="text"><![CDATA[述并发编程中,并发容器也是非常重要的一个工具,我们平时用的 ArrayList, HashMap 等等这些集合类都不是线程安全的,在并发环境下就可能会出现问题,JDK给我们提供了一些线程安全的容器,供我们使用 过时的并发容器在早期的版本中, Hashtable 和 Vector 就对应的是线程安全的 HashMap 和 ArrayList 这两个线程安全类,本质上就是给方法加上 synchronized 锁,把方法都变成同步方法,这种方式的性能是很差的 后来又可以使用 Collections.synchronizedList(new ArrayList&lt;&gt;());, Collections.synchronizedMap(new HashMap&lt;K, V&gt;()); 这种方法把一个普通集合转成并发容器,这种方式,这种方式的原理其实也是用 synchronized 锁,区别是这种方式是锁的一部分代码块,上面的是锁的整个方法, 性能上有些许提升,但也还是很差 所以后来就出现了 ConcurrentHashMap 和 CopyOnWriteArrayList 这样的性能较为优秀的并发容器,这两个也是本文的重点 为什么HashMap不是线程安全的 同时put碰撞导致数据丢失,HashMap 使用的是链表的数据结构, 如果多个线程同时put操作,两个key的hash值相同的话,他们两个会落到同一个地方,最终结果肯定是只有一个线程put成功了,另一个的数据就丢失了 扩容导致数据丢失,多个线程往里面put数据,当容量不够用的时候,多个线程同时去扩容,最终也是只有一个线程扩容成功,那么另一个线程的数据就会丢失 可能会死循环造成CPU100%, 具体原因可以参考这里 HashMap数据结构在了解 ConcurrentHashMap 之前,首先需要了解一下 HashMap HashMap 底层是通过数组+链表组成的,在 jdk1.7和jdk1.8中的实现有些区别 JDK1.7中的 HashMapjdk1.7 中 HashMap 的数据结构是这样的 这里就是一个数组+链表的结构,进入1.7中 HashMap 的源码看一下 这里是 HashMap 的几个重点的属性,这里重点看一下负载因子, HashMap 中默认的容量是16,负载因子是0.75,Map在使用的时候不断往里放数据,当容量达到了 16 * 0.75 = 12,就需要将当前的16的容量进行扩容,而扩容这个过程涉及到了 rehash,复制数据等操作,非常耗费性能,所以阿里巴巴开发规范中,推荐尽可能在Map初始化的时候就给定容量 然后这里具体存放数据的就是 Entry 看一下他的源码 这里的 next 就是用于实现链表的数据结构 知道了基本的数据结构后,再来看一下里面的几个重要方法 首先是 put 方法,代码如下123456789101112131415161718192021222324public V put(K key, V value) &#123; // 如果 key 为空,则 put 一个空值进去 if (key == null) return putForNullKey(value); // 根据 key 计算出 hashcode int hash = hash(key.hashCode()); // 根据计算出的 hashcode 定位出所在桶 int i = indexFor(hash, table.length); // 如果桶是一个链表则需要遍历判断里面的 hashcode、key 是否和传入 key 相等,如果相等则进行覆盖,并返回原来的值 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; // 如果桶是空的,说明当前位置没有数据存入,新增一个 Entry 对象写入当前位置 modCount++; addEntry(hash, key, value, i); return null;&#125; 这里再看一下 addEntry 这个方法 12345678910111213141516171819void addEntry(int hash, K key, V value, int bucketIndex) &#123; // 判断是否需要扩容 if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) &#123; // 如果需要扩容,就 *2 resize(2 * table.length); // 将当前的key重新计算hash hash = (null != key) ? hash(key) : 0; // 定位 bucketIndex = indexFor(hash, table.length); &#125; createEntry(hash, key, value, bucketIndex);&#125;// 将当前位置的桶传入到新建的桶中，如果当前桶有值就会在位置形成链表void createEntry(int hash, K key, V value, int bucketIndex) &#123; Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++;&#125; 再来看看 get() 方法 12345678910111213141516171819202122232425public V get(Object key) &#123; // 为空就取空key的返回 if (key == null) return getForNullKey(); Entry&lt;K,V&gt; entry = getEntry(key); return null == entry ? null : entry.getValue();&#125;final Entry&lt;K,V&gt; getEntry(Object key) &#123; if (size == 0) &#123; return null; &#125; // 根据key计算hash值 int hash = (key == null) ? 0 : hash(key); // 判断是否为链表.不是链表就根据 key、key 的 hashcode 是否相等来返回值,链表则需要遍历直到 key 及 hashcode 相等时候就返回值 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; return null;&#125; 到这里 1.7 的 HashMap 就介绍完了,这个数据结构有一个需要优化的地方,就是当 hash 冲突比较严重的时候,链表就会越来越长,这样查询的效率也会越来越低的 所以再来看看 1.8 中的 HashMap 是如何处理的 JDK1.8中的 HashMap首先看一下数据结构,如图 相对于1.7的改变就是加了红黑树,当链表太长,达到设置的阈值的时候,就会将整个链表转成红黑树 下面来看看1.8中的几个重点方法,首先是 put() 方法,代码如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 判断当前桶是否为空, if ((tab = table) == null || (n = tab.length) == 0) // 空的话 resize() 初始化 n = (tab = resize()).length; // 根据当前 key 的 hashcode 定位到具体的桶中并判断是否为空 if ((p = tab[i = (n - 1) &amp; hash]) == null) // 为空表明没有 Hash 冲突就直接在当前位置创建一个新桶即可 tab[i] = newNode(hash, key, value, null); else &#123; // 以下就是hash冲突的情况 Node&lt;K,V&gt; e; K k; // 比较当前桶中的 key、key 的 hashcode 与写入的 key 是否相等，相等就赋值给 e,在后面统一返回 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 如果当前桶是红黑树 else if (p instanceof TreeNode) // 按红黑树的方式写入数据 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; // 如果是个链表,就需要将当前的 key、value 封装成一个新节点写入到当前桶的后面(形成链表) for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); // 这里判断链表的大小是否大于转红黑树的阈值 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st // 达到转红黑树的阈值的时候,就把整个链表都转成红黑树 treeifyBin(tab, hash); break; &#125; // 如果在遍历过程中找到 key 相同时直接退出遍历 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // 如果 e != null 就相当于存在相同的 key,那就需要将值覆盖 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 最后判断是否需要进行扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 各种赋值比较可能有点绕,多看几次, 下面再看看 get() 方法 123456789101112131415161718192021222324252627282930313233343536public V get(Object key) &#123; Node&lt;K,V&gt; e; // 根据hash值定位桶,如果桶为空直接返回 return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;/** * Implements Map.get and related methods * * @param hash hash for key * @param key the key * @return the node, or null if none */final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 首先判断桶的第一个位置(有可能是链表、红黑树)的key,是否为查询的key,如果是的话直接返回 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; // 不是的话 if ((e = first.next) != null) &#123; // 判断是不是红黑树,如果是,就按红黑树的方法查询 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 不是红黑树就是链表,按链表的方式循环查找 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 1.8 中转为红黑树的数据结构,提高了查询效率,但是在并发环境下的问题还是存在的,因此就诞生了 ConcurrentHashMap 专用于并发环境下的 HashMap 总结本文主要了解 HashMap 的数据结构,以及一个进化的过程,为了解 ConcurrentHashMap 打基础]]></content>
      <categories>
        <category>JUC</category>
      </categories>
      <tags>
        <tag>JUC</tag>
        <tag>并发容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUC-CAS-0-核心概念]]></title>
    <url>%2F2020%2F04%2F13%2FJUC-CAS-0-%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[述上文中介绍了原子类,在原子类中有一个常用的方法就是 compareAndSet() 这个方法,这个方法其实就是一个 CAS 方法, CAS(Compare-and-Swap) 字面意思比较并替换,是一种实现并发算法时常用的技术 CASCAS有三个操作数: 内存值V,预期值A,要修改的值B,当切仅当预期值A和内存值相同的时候,才将内存值修改为B 通俗点来说,就是我认为V的值应该是A,如果真的是的话,那我就把A改成B,如果不是的话,说明这个值已经被别人修改过了,就不会去改了,避免多个人修改出错 他其实就是乐观锁的实现原理 伪代码理解CAS根据上面的描述,写一个伪代码来理解CAS操作,代码如下:123456789101112public class CasTest &#123; private volatile int value; public synchronized int compareAndSwap(int expectedValue, int newValue) &#123; int oldValue = value; if (oldValue == expectedValue) &#123; value = newValue; &#125; return oldValue; &#125;&#125; CAS其实就是这样的,先拿内存值跟预期值对比,如果相等,设置成新的值,但是真正的CAS并不是上面代码这样操作的,真实的操作应该是去调用CPU的特殊指令的 缺陷CAS虽然很高效的解决了原子操作的问题但同时他也有一些缺陷 循环时间长开销大,他就是个自旋锁,会一直占着CPU 只能保证一个共享变量的原子操作 ABA问题 什么是ABA问题呢? 如果内存地址V初次读取的值是A，并且在准备赋值的时候检查到它的值仍然为A，那我们就能说它的值没有被其他线程改变过了吗？如果在这段期间它的值曾经被改成了B，后来又被改回为A，那CAS操作就会误认为它从来没有被改变过。这个漏洞称为CAS操作的“ABA”问题。Java并发包为了解决这个问题，提供了一个带有标记的原子引用类“AtomicStampedReference”，它可以通过控制变量值的版本来保证CAS的正确性。因此，在使用CAS前要考虑清楚“ABA”问题是否会影响程序并发的正确性，如果需要解决ABA问题，改用传统的互斥同步可能会比原子类更高效。 总结了解CAS的概念和原理,以及他的优缺点]]></content>
      <categories>
        <category>JUC</category>
      </categories>
      <tags>
        <tag>JUC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUC-原子类-0-介绍及基本使用]]></title>
    <url>%2F2020%2F04%2F13%2FJUC-%E5%8E%9F%E5%AD%90%E7%B1%BB-0-%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[述在并发编程中,原子类也是经常使用的一个工具,利用原子类,可以把一些操作变成一个原子操作,在多线程的情况下不需要加锁也可以保证线程安全 原子类的作用及优势原子类的作用跟锁是类似的,都是为了保证在并发环境下的线程安全,原子类相比于锁,有一定的优势 锁的粒度更细: 原子类可以把竞争范围缩小到变量级别,通常我们手动加锁的粒度都会大于原子变量的粒度 效率更高: 一般情况下,原子类的效率会比使用锁的效率高,除了高度竞争的情况 常用原子类常用的原子类有以下几种 基本原子类型 Atmoic*: AtmoicInteger,AtmoicLong,AtmoicBoolean 作用: 对基本类型的操作变为原子操作 原子数组类型 Atmoic*Array: AtmoicIntegerArray, AtmoicLongArray, AtmoicReferenceArray 作用: 对数组的操作为原子操作 引用类型原子类 Atmoic*Reference: AtomicReference, AtomicStampedReference, AtomicMarkableReference 作用: 将一个对象转为原子操作 升级类型原子类 Atomic*FieldUpdater: AtomicIntegerfieldupdater, AtomicLongFieldUpdater, AtomicReferenceFieldUpdater作用: 将一个非原子变量转换为原子变量 累加器 Adder: LongAdder, DoubleAdder Accumulator: LongAccumulator ,DoubleAccumulator 作用: 高效累加操作 原子基本类型使用以 AtmoicInteger 为例,了解一下基本的用法,以及常用的方法 123456789101112131415161718192021222324252627public class AtomicIntegerTest &#123; public static void main(String[] args) &#123; AtomicInteger atomicInteger = new AtomicInteger(0); // 获取当前值 int i = atomicInteger.get(); // 获取当前值并设置新的值 int j = atomicInteger.getAndSet(1); // 获取当前值并自增 int k = atomicInteger.getAndIncrement(); // 获取当前值并自减 int l = atomicInteger.getAndDecrement(); // 获取当前值,并加上指定的值 int m = atomicInteger.getAndAdd(5); // 对比并设置值,如果当前的值为预期的值,则以原子的方式将该值设置为输入值 boolean b = atomicInteger.compareAndSet(5, 6); &#125;&#125; 原子数组这里就不再介绍了 引用原子类型介绍AtomicReference 这个类的作用,和 AtmoicInteger 本质上没有却别, AtmoicInteger 是让一个整形变量保证原子性, AtomicReference 可以让一个对象保证原子性 在之前锁的章节,有一个自旋锁的案例,已经用到这个类型了, 代码再贴过来回顾一下 12345678910111213141516public class SpinLock &#123; private AtomicReference&lt;Thread&gt; sign = new AtomicReference&lt;&gt;(); private void lock()&#123; Thread current = Thread.currentThread(); while (!sign.compareAndSet(null, current)) &#123; &#125; &#125; private void unLock()&#123; Thread current = Thread.currentThread(); sign.compareAndSet(current, null); &#125; &#125; 这里就用到了一个原子类的 compareAndSet 这个CAS操作,对比并赋值 升级类型原子类的使用升级类型原子类,功能是把一个普通的变量升级为原子类,主要的使用场景是当一个普通变量偶尔需要原子操作的时候使用 以 AtomicIntegerfieldupdater 为例,就是将一个整形升级为原子整形 1234567891011121314151617public class AtomicIntegerFieldUpdaterTest implements Runnable&#123; static Candidate testCandidate; public static AtomicIntegerFieldUpdater&lt;Candidate&gt; scoreUpdater = AtomicIntegerFieldUpdater.newUpdater(Candidate.class, &quot;score&quot;); @Override public void run() &#123; for (int i = 0; i &lt; 1000; i++) &#123; scoreUpdater.getAndIncrement(testCandidate); &#125; &#125; public static class Candidate&#123; volatile int score; &#125;&#125; 这里就会把 testCandidate 这个对象的 score 变为原子操作 Adder累加器在高并发下 LongAdder 的效率比 AtomicLong 要高 使用方式就是 new LongAdder(); 然后调用 add() 方法就可以了 那么为什么 AtomicLong 的性能不如 LongAdder 呢? AtomicLong 每次执行修改操作,都要做一次 flush 和 refresh 操作,在高并发的情况下效率就很低了,而 LongAdder 每个线程都会有一个自己的计数器,用来在自己的线程内计算,这样就不会被其他线程干扰,等最后要获取值的时候,把所有线程的和在加起来就是总和 对比 AtomicLong在低争用下, AtomicLong 和 LongAdder 这两个类具有相似的特征,但是在竞争激烈的情况下,LongAdder 的预期吞吐量要高得多,但要消耗更多的空间 LongAdder 适合的场景是统计求和计数的场景,而且 LongAdder 基本只提供了 add 方法,而 AtomicLong 还具有cas方法 Accumulator 累加器Accumulator 是一个更加灵活的累加器,使用方式上跟上面的 LongAdder 是有些不同的,如下1234567LongAccumulator accumulator = new LongAccumulator(Long::sum, 0);LongAccumulator accumulator1 = new LongAccumulator((x, y) -&gt; x + y, 0);accumulator.accumulate(1);// 获取后清空accumulator.getThenReset(); 代码中的 accumulator 和 accumulator1 作用都是一样的, 就是lambda的两种写法,下面这个看着更直观一点 LongAccumulator 需要两个参数,第一个是 LongBinaryOperator 的实现他是一个函数式接口, 然后传参的时候传具体的实现就行了,第二个参数 0 就是 x 的初始值 上面代码中,刚初始化的时候 x 就是 0 ,然后第一次累加的时候会把x的值给y,然后再把传进来的x跟y做累加,结果赋值给 y当然这个表达式不一定就是累加,也可以做自己的实现,所以这个比上面的 LongAdder 要更加灵活 总结了解常用的原子类,以及一些常用的方法]]></content>
      <categories>
        <category>JUC</category>
      </categories>
      <tags>
        <tag>JUC</tag>
        <tag>原子类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUC-锁-1-锁的分类]]></title>
    <url>%2F2020%2F04%2F13%2FJUC-%E9%94%81-1-%E9%94%81%E7%9A%84%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[述Java 锁有很多种,从不通的角度来看,锁大概有以下几种分类 下面分别看一下这几种锁 悲观锁和乐观锁简单来说,悲观锁就是面对同步资源的时候,首先认为会有别的线程会来修改数据,所以先上锁,锁住之后再去修改资源,上面提到的 synchronized 和 Lock 就都是悲观锁 乐观锁就是先认为没有线程去修改这个资源,所以不上锁先去修改,等修改完成提交的时候再做检查,如果这个时间段有别的线程修改了,那就做其他的处理,没有的话就提交,比如我们的git仓库,也是提交的时候才去判断有没有别人修改,有的话解决冲突或者其他操作,没有的话直接提交成功 开销对比悲观锁的开销要高于乐观锁,但是特点是一劳永逸,临界区持锁的时间就算越来越差,也不会对互斥锁的开销造成影响 乐观锁最开始的消耗是要比悲观锁小的,因为不用先去上锁,但是如果自旋时间很长,或者不停的重试,那么消耗的资源也会越来越多 使用场景两种锁各有个的使用场景 悲观锁: 适用于并发写入多,临界区持锁时间较长的情况,悲观锁可以避免大量的无用自旋操作,比如临界区有IO操作,临界区代码复杂或循环量大,或者线程竞争激烈的情况 乐观锁: 适用于并发写入少,大部分是读取的场景,不加锁能让读取的性能大大提高 可重入锁与非可重入锁 可重入锁，也叫做递归锁，指的是同一个线程T在进入外层函数A获得锁L之后，T继续进入内层递归函数B时，仍然有获取该锁L的代码，在不释放锁L的情况下，可以重复获取该锁L。非可重入锁，也叫做自旋锁，对比上面，指的是同一个线程T在进入外层函数A获得锁L之后，T继续进入内层递归函数B时，仍然有获取该锁L的代码，必须要先释放进入函数A的锁L，才可以获取进入函数B的锁L。 简单来说就是同一个线程能不能重复获取自己已经获取到的锁. 案例用一个简单的demo来理解可重入锁与非可重入锁 不可重入锁123456789101112131415public class Lock&#123; private boolean isLocked = false; public synchronized void lock() throws InterruptedException&#123; while(isLocked)&#123; wait(); &#125; isLocked = true; &#125; public synchronized void unlock()&#123; isLocked = false; notify(); &#125;&#125; 可重入锁123456789101112131415161718192021222324public class Lock&#123; boolean isLocked = false; Thread lockedBy = null; int lockedCount = 0; public synchronized void lock() throws InterruptedException&#123; Thread thread = Thread.currentThread(); while(isLocked &amp;&amp; lockedBy != thread)&#123; wait(); &#125; isLocked = true; lockedCount++; lockedBy = thread; &#125; public synchronized void unlock()&#123; if(Thread.currentThread() == this.lockedBy)&#123; lockedCount--; if(lockedCount == 0)&#123; isLocked = false; notify(); &#125; &#125; &#125;&#125; 可以看到不可重入锁,就是一个有没有上锁的标记,如果之前的锁没有被释放那就不能再次获取, 可重入锁呢,维护了一个当前获取到锁的线程,还有一个锁的个数,表示同一个线程可以反复获取这个锁,每获取一次 lockCount 递增 ReentrantLock 和 synchronized 就是可重入锁,可重入锁的作用其实就是为了防止死锁 常用方法 isHeldByCurrentThread(): 查看锁是否被当前线程持有 getQueueLength(): 返回当前正在等待这把锁的队列有多长 这两个方法一般在开发调试的时候用 公平锁和非公平锁从名字上看,公平锁就是保障了各个线程获取锁都是按照顺序来的,先到的线程先获取锁,而非公平锁则不一定按照顺序,是可以插队的 在公平锁中,比如线程 1234 依次去获取锁, 线程1首先获取到了锁,然后它处理完成之后会释放,释放之后会唤醒下一个线程,依次获取锁 而非公平锁中,线程1释放掉锁之后,唤醒线程2的这个过程中,如果有别的线程比如线程5去请求锁,那么线程5是可以先获取到的,就是插队,因为线程2的唤醒需要CPU的上下文切换,这个需要一定的时间,线程1释放锁和线程2被唤醒的这段时间,锁是空闲的,所以在非公平锁中,就可以先让别的线程获取到,这样做的目的主要是利用锁的空档期,提高效率 优缺点 类型 优势 劣势 公平锁 各个线程公平平等,在等待一段时间之后总有执行的机会 更慢,吞吐量小 非公平锁 更快,吞吐量大 可能会产生线程饥饿的问题(某些线程长时间等待但是得不到执行) ReentrantLock 默认就是一个非公平锁,如果要设置为公平锁的话可以在构造种传入true new ReentrantLock(true) 共享锁和排他锁典型的就是读写锁(ReentrantReadWriteLock),比如读操作可以有很多线程一起读,但是写操作只能有一个线程去写,而且在写入的时候,别的线程也不能进行读的操作 如果没有读写锁,只用 ReentrantLock 那么虽然可以保证线程安全,但是也会浪费一部分资源,因为多个读操作并没有线程安全问题,所以在读的地方使用读锁,在写的地方用写锁,可以提高程序执行效率 读写锁的规则 多个线程申请读锁,都可以申请到 如果有一个线程已经占用了读锁,则此时其他线程如果申请写锁,则申请写锁的线程会一直等读锁被释放 如果有一个线程获取到了写锁,则其他线程不管申请读锁还是写锁,都得等当前的写锁被释放 总结一下,就是要么多个线程读,要么多个线程写 使用方式12345private static ReentrantReadWriteLock reentrantLock = new ReentrantReadWriteLock();// 读锁private static ReentrantReadWriteLock.ReadLock readLock = reentrantLock.readLock();// 写锁private static ReentrantReadWriteLock.WriteLock writeLock = reentrantLock.writeLock(); 读锁插队策略在非公平锁的情况下,假设有这样一个场景,线程2和线程4正在读,线程3想要写入,但是拿不到锁,于是在等待队列里面等待,线程5不在等待队列里,但是它想要读,那么线程5能插队直接获取到读锁吗 这里无非有两种情况,第一种可以插队,这样效率高,但是读请求多的情况下写线程就会造成饥饿,一直获取不到锁, 第二种情况就是不能插队,等写线程写完再获取 ReentrantReadWriteLock 实现的就是第二种情况 ReentrantReadWriteLock 的插队策略就是 公平锁: 不允许插队 非公平锁: 写锁可以插队,读锁仅在等待队列头节点不是想获取写锁的线程的时候可以插队 升降级策略 写锁可以降级为读锁,读锁不能升级为写锁,主要是为了防止死锁 适用场景读写锁适用于读多写少的情况,合理使用可以提高并发效率 自旋锁和阻塞锁自旋锁是采用让当前线程不停的在循环体内执行实现,当循环的条件被其它线程改变时才能进入临界区,用代码举个例子 123456789101112131415public class SpinLock &#123; private AtomicReference&lt;Thread&gt; sign = new AtomicReference&lt;&gt;(); private void lock()&#123; Thread current = Thread.currentThread(); while (!sign.compareAndSet(null, current)) &#123; &#125; &#125; private void unLock()&#123; Thread current = Thread.currentThread(); sign.compareAndSet(current, null); &#125;&#125; 这里就是一个自旋锁,上锁的时候,一直循环,直到上锁成功 如果物理机器有多个处理器,能够让两个或以上的线程同时并行执行,我们就可以让后面那个请求锁的线程不放弃CPU的执行时间,看看持有锁的线程是否很快就会释放锁 而为了让当前线程“稍等一下”,我们需让当前线程进行自旋,如果在自旋完成后前面锁定同步资源的线程已经释放了锁,那么当前线程就可以不必阻塞而是直接获取同步资源,从而避免切换线程的开销,这就是自旋锁, 而阻塞锁,就是改变线程的状态,让线程进入等待的状态,等待唤醒,之后再去竞争锁 优缺点分析 自旋锁: 由于自旋锁只是将当前线程不停地执行循环体,不进行线程状态的改变,所以响应速度更快.但当线程数不停增加时,性能下降明显,因为每个线程都需要执行,占用CPU时间.如果线程竞争不激烈,并且保持锁的时间段.适合使用自旋锁 阻塞锁: 阻塞锁的优势在于,阻塞的线程不会占用cpu时间,不会导致 CPU 占用率过高,但进入时间以及恢复时间都要比自旋锁略慢,在竞争激烈的情况下,阻塞锁的性能要明显高于自旋锁 可中断锁和不可中断锁Java中 synchronized 就是一个不可中断锁,而 ReentrantLock 就是一个可中断锁,之前介绍的 tryLock() 和 lockInterruptibly() 方法都可以中断锁的获取 简单来说在获取锁的过程中能放弃获取锁,中断获取锁的过程,那就是可中断锁,否则就是不可中断锁 总结关于锁就先介绍到这里,掌握这几种锁的特点,在开发过程中选择最合适的使用]]></content>
      <categories>
        <category>JUC</category>
      </categories>
      <tags>
        <tag>JUC</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUC-锁-0-Lock的基本使用]]></title>
    <url>%2F2020%2F04%2F13%2FJUC-%E9%94%81-0-Lock%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[述在并发开发中,锁也是经常使用的一个工具,主要是用于对并发资源的访问,常用的锁就是 Lock 和 synchronized ,这两个锁都可以达到线程安全的目的,但是在使用上有较大差别. Lock 和 synchronizedLock 一般情况用于 synchronized 不满足要求或者不合适的时候, Lock 是一个接口,最常用的实现类就是 ReentrantLock ,一般情况下 Lock 只允许一个线程访问共享资源,但是也有特殊的锁可以实现并发访问,比如读写锁(ReadWriteLock)中的读锁(ReadLock) synchronized 的劣势既然有了 synchronized 为什么还需要 Lock ? 相比于 Lock , synchronized 有以下几个不足之处: 效率低,锁的释放情况少,试图获得锁时不能设定超时时间,而且不能中断一个正在获得锁的线程 不够灵活,加锁和释放的时机单一,每个锁仅有单一的条件(某个对象)在一些情况下时不够的 无法知道是否成功获取锁 Lock 的基本使用常用方法Lock 中有以下4个获取锁的方法 lock(): 阻塞去获取锁,直到获取成功(所以一旦出现死锁的情况线程将永久阻塞),跟 synchronized 相比, lock 不会再异常中释放锁,所以一定要在 finally 中释放锁 tryLock(): 尝试获取锁,不会阻塞,立即返回有没有成功获取到,获取到的话返回 true ,否则返回 false tryLock(long time, TimeUnit unit): 加一个等待时间,在等待时间内尝试获取锁,超时就放弃,获取到的话返回 true ,否则返回 false lockInterruptibly(): 这个方法相当于把 tryLock(long time, TimeUnit unit) 中的时间设置为无限,但是这个方法获取锁的过程中,线程可以中断 释放锁的方法为 unlock() synchronized 和 Lock 都可以保证可见性,就是说,当前线程获取到锁之后,可以看到上一个线程做的所有操作 总结对锁做一个简单的介绍,以及 Lock 的基本使用]]></content>
      <categories>
        <category>JUC</category>
      </categories>
      <tags>
        <tag>JUC</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUC-ThreadLocal-1-原理解析]]></title>
    <url>%2F2020%2F04%2F13%2FJUC-ThreadLocal-1-%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[述上文中了解了 ThreadLocal 的常用场景和最基本的用法之后再来看一下它的原理以及一些要注意的事项 Thread,ThreadLocal,ThreadLocalMap首先需要了解一下 Thread ,ThreadLocal ,ThreadLocalMap 这三个类之间的关系,如图 就是一个线程 (Thread) 对应有一个 ThreadLocalMap , ThreadLocalMap 中存放这一堆的 ThreadLocal 我们可以看一下具体对应的代码 Thread类: 然后再点进去 ThreadLocalMap 类里 可以看到 ThreadLocalMap 这里存储的是一个键值对 Entry 类型的 重要方法解析第一个 ThreadLocal.setInitialValue() ,设置初始化值的方法 首先看一下源码 总体来说这个方法就是给 ThreadLocal 去设置值的, 这里首先会调用 initialValue 这个方法,这个是不是很熟悉, 就是我们上文中的案例中初始化 ThreadLocal 时,重写的方法,默认是返回空的 setInitialValue() 这个方法会在 get() 方法中调用,再来看一下 get() 方法的源码 先去Map中找,没有的话再去初始化,从这里也可以看出,我们设置的初始化值是懒加载的 还有两个常用的方法就是 set() 和 remove() 这两个这里就不贴代码了 注意事项内存泄露使用 ThreadLocal 应该注意内存泄露的风险 内存泄露是指某个对象已经没用了, 但是占用的内存不能被会GC回收 为什么会出现内存泄露我们来看一下 这里看一下存放键值对的这个 Entry ,它是继承了一个 WeakReference 弱引用类型的,用来存放key 弱引用的特点是如果这个对象只被弱引用关联,那么这个对象就会被回收,所以这里的key是可以被回收掉的,但是 value 是个强引用,它不会被回收掉 所以 ThreadLocalMap 的每个 Entry 都包含了一个key的弱引用,和一个value的强引用, 正常情况下,当线程终止保存在 ThreadLocal 中的 value 就会被回收, 但是如果一个线程要持续很久,那它的value就不会被回收掉,即使这个value已经没用了 所以因为 value 和 Thread 之间存在强引用,就可能导致内存泄露 解决方案那么针对这种情况,我们应该如何去解决呢? 其实jdk已经帮我们做了一些处理, 在调用 set() remove() rehash() 这些方法的时候,系统会扫描所有的key为null的 Entry,把这些对应的 value 都置空,这样value对象就可以被回收,具体是调用的 resize() 这个方法,代码如下 虽然jdk帮我们处理掉一些情况,但是如果 ThreadLocal 用不到了,那基本也不会去调用这几个方法, resize() 也就不会被触发,那么value还是不能被回收的 这种情况下,我们只好在使用完毕之后手动去调用 remove() 方法,避免内存泄露,这也是阿里巴巴开发规范中所规定的 共享对象第二个要注意的就是 ThreadLocal 中不能存放共享变量,如果放进去的是一个共享变量,比如用 static 修饰的对象,那么 ThreadLocal.get() 获取到的还是这个对象本身,就还是会有线程安全的问题 总结 了解 Thread,ThreadLocal,ThreadLocalMap 之间的关系 了解常用的几个方法 setInitialValue() get() set() remove() 最后还有使用 ThreadLocal 造成的内存泄漏的原因和解决方法]]></content>
      <categories>
        <category>JUC</category>
      </categories>
      <tags>
        <tag>JUC</tag>
        <tag>ThreadLocal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUC-ThreadLocal-0-基本使用]]></title>
    <url>%2F2020%2F04%2F13%2FJUC-ThreadLocal-0-%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[述了解了线程池之后,再来看一个常用的类,就是 ThreadLocal 这个类在面试中也是很常见的,下面来看一下这个类常见的使用场景 常见使用场景ThreadLocal 比较常见的有两个地方 每个线程都需要一个独享的对象,通常是工具类,比如经常用的 SimpleDateFormat 和 Random ,这两个类都不是线程安全的类,使用 ThreadLocal 就可以保证线程安全 每个线程内需要保存一个全局变量,让不通的方法使用,这种场景可能需要把这个全局变量一级一级通过参数传递, 使用 ThreadLocal 可以避免这种参数传递的麻烦 下面分别看以下这两个场景 第一种场景以常见的时间转换工具类 SimpleDateFormat 为例,写一个测试的示例,代码如下: 1234567891011121314@Slf4jpublic class ThreadLocalTest1 &#123; public String date(long mill)&#123; Date date = new Date(mill); SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd hh:mm:ss&quot;); return simpleDateFormat.format(date); &#125; public static void main(String[] args) &#123; Thread thread = new Thread(() -&gt; new ThreadLocalTest1().date(System.currentTimeMillis())); thread.start(); &#125;&#125; 代码很简单,就是创建一个 SimpleDateFormat 做时间转换,上面这段只有一个线程,而且 SimpleDateFormat 也是局部变量,所以是没有线程安全的问题的 当任务数量很大的时候,每个线程都会去执行创建 SimpleDateFormat 的实例,这就可能造成资源的浪费,我们可以把这个工具类转成共享变量去处理,如下: 12345678910111213141516171819@Slf4jpublic class ThreadLocalTest1 &#123; private SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd hh:mm:ss&quot;); public String date(long mill) &#123; Date date = new Date(mill); return simpleDateFormat.format(date); &#125; public static void main(String[] args) &#123; Thread thread = new Thread(() -&gt; new ThreadLocalTest1().date(System.currentTimeMillis())); thread.start(); Thread thread1 = new Thread(() -&gt; new ThreadLocalTest1().date(System.currentTimeMillis())); thread1.start(); Thread thread2 = new Thread(() -&gt; new ThreadLocalTest1().date(System.currentTimeMillis())); thread2.start(); &#125;&#125; 这样就只会创建一个对象,但是这里是会有线程安全的问题的, 这里可以去通过加锁同步的方式去实现,但是同步的方式效率太低了,所以这种情况下就可以去使用 ThreadLocal ,使每个线程都有自己的实例副本,不共享,然后我们对上面这个类做一个改造,代码如下 1234567891011121314151617181920212223242526@Slf4jpublic class ThreadLocalTest1 &#123; public String date(long mill) &#123; Date date = new Date(mill); return ThreadSafeSimpleDateFormat.simpleDateFormatThreadLocal.get().format(date); &#125; public static void main(String[] args) &#123; Thread thread = new Thread(() -&gt; new ThreadLocalTest1().date(System.currentTimeMillis())); thread.start(); &#125;&#125;class ThreadSafeSimpleDateFormat&#123;// public static ThreadLocal&lt;SimpleDateFormat&gt; simpleDateFormatThreadLocal = new ThreadLocal&lt;SimpleDateFormat&gt;()&#123;// @Override// protected SimpleDateFormat initialValue() &#123;// return new SimpleDateFormat(&quot;yyyy-MM-dd hh:mm:ss&quot;);// &#125;// &#125;; public static ThreadLocal&lt;SimpleDateFormat&gt; simpleDateFormatThreadLocal = ThreadLocal.withInitial(() -&gt; new SimpleDateFormat(&quot;yyyy-MM-dd hh:mm:ss&quot;));&#125; 这里首先创建一个类,ThreadSafeSimpleDateFormat 里面就存放了 ThreadLocal&lt;SimpleDateFormat&gt; 需要的时候通过这个类去拿 ThreadLocal 初始化有两种方法,一种是直接重写 initialValue() 方法,另一种是 ThreadLocal.withInitial() 用 lambda 实现,第二种更加简洁,但是效果一样的 以上就是 ThreadLocal 的第一种用法,然后看第二种场景 第二种方式举个例子,假如有一个web请求,这个请求,会调用依次调用 service1() service2() service3() 这三个方法,这三个方法里面都需要一个user参数,通常情况就是在最上面一层获取到user对象,然后一级一级通过参数传递下去,这样就有点繁琐,这种情况下也可以通过 ThreadLocal 实现 代码如下:123456789101112131415161718192021222324252627282930313233343536373839404142@Slf4jpublic class ThreadLocalTest2 &#123; public static void main(String[] args) &#123; new Service1().process(); &#125;&#125;class Service1&#123; public void process()&#123; User user = new User(&quot;张三&quot;); UserHolder.userThreadLocal.set(user); new Service2().process(); &#125;&#125;@Slf4jclass Service2&#123; public void process()&#123; User user = UserHolder.userThreadLocal.get(); log.info(&quot;获取到用户信息:&#123;&#125;&quot;, user.getName()); new Service3().process(); &#125;&#125;@Slf4jclass Service3&#123; public void process()&#123; User user = UserHolder.userThreadLocal.get(); log.info(&quot;获取到用户信息:&#123;&#125;&quot;, user.getName()); &#125;&#125;class UserHolder&#123; public static ThreadLocal&lt;User&gt; userThreadLocal = new ThreadLocal&lt;User&gt;();&#125;@AllArgsConstructor@NoArgsConstructor@Dataclass User&#123; private String name;&#125; 这里主要看一下 UserHolder ,这里直接使用 new 来创建对象,并没有做初始化,之后在 Service1 中通过 set() 方法把对象传过去 按上面这种实现方式就可以实现不同的请求对应不通的线程,然后各个线程存储自己对应的 User 对象,这里主要强调的是同一个线程内的不同方法之间的共享 总结通过两个案例对 ThreadLocal 的用法有一个基本的了解 总结一下 ThreadLocal 的两个作用 让某个需要用到的对象在线程间隔离,每个线程都有自己独立的对象 在同一个线程的任何方法中都可以轻松获取到该对象 然后是 ThreadLocal 设置对象的两种方式: 通过 initialValue 设置对象,然后这个是会懒加载的,在调用 get() 方法的时候才回去初始化 第二种就是通过 set() 方法设置 使用 ThreadLocal 的优点: 线程安全 不需要加锁,提高执行效率 高效利用内存,节省开销 免去繁琐的传参]]></content>
      <categories>
        <category>JUC</category>
      </categories>
      <tags>
        <tag>JUC</tag>
        <tag>ThreadLocal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUC-线程池-2-钩子方法的使用]]></title>
    <url>%2F2020%2F04%2F13%2FJUC-%E7%BA%BF%E7%A8%8B%E6%B1%A0-2-%E9%92%A9%E5%AD%90%E6%96%B9%E6%B3%95%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[述上文提到了线程池的钩子方法,其实就是线程池在执行每个任务的前后执行一些操作,还有线程池的暂停与继续,等等一些辅助功能,下面看一下如何使用 钩子方法的使用ThreadPoolExecutor 提供了3个钩子方法,需要子类去根据自己的需要重写,三个方法如下: 123protected void beforeExecute(Thread t, Runnable r) &#123; &#125; // 任务执行前protected void afterExecute(Runnable r, Throwable t) &#123; &#125; // 任务执行后protected void terminated() &#123; &#125; // 线程池执行结束后 所以我们要新建一个类,然后继承 ThreadPoolExecutor ,重写这几个方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108@Slf4jpublic class PauseableThreadPool extends ThreadPoolExecutor &#123; public PauseableThreadPool(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) &#123; super(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue); &#125; public PauseableThreadPool(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory) &#123; super(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory); &#125; public PauseableThreadPool(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, RejectedExecutionHandler handler) &#123; super(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, handler); &#125; public PauseableThreadPool(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; super(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory, handler); &#125; private final ReentrantLock lock = new ReentrantLock(); private Condition unPaused = lock.newCondition(); private boolean isPaused; @Override protected void beforeExecute(Thread t, Runnable r) &#123; super.beforeExecute(t, r); lock.lock(); try &#123; while (isPaused) &#123; // 如果是暂停状态 阻塞挂起 unPaused.await(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; @Override protected void afterExecute(Runnable r, Throwable t) &#123; super.afterExecute(r, t); // 每个任务执行完成之后,输出 log.info(&quot;执行完了一个任务...&quot;); &#125; @Override protected void terminated() &#123; super.terminated(); log.info(&quot;线程池已经关闭....&quot;); &#125; /** * 暂停线程池 */ public void pause()&#123; lock.lock(); try &#123; // 设置状态为暂停 isPaused = true; &#125; finally &#123; lock.unlock(); &#125; &#125; /** * 继续执行 */ public void resume()&#123; lock.lock(); try &#123; isPaused = false; // 唤醒 unPaused.signalAll(); &#125; finally &#123; lock.unlock(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; PauseableThreadPool threadPool = new PauseableThreadPool(5, 5, 0L, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); Runnable task = ()-&gt;&#123; log.info(&quot;执行任务....&quot;); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;; for (int i = 0; i &lt; 1000; i++) &#123; threadPool.execute(task); &#125; Thread.sleep(1000); threadPool.pause(); log.info(&quot;线程池已暂停&quot;); Thread.sleep(10000); threadPool.resume(); log.info(&quot;线程池恢复执行&quot;); &#125;&#125; 代码看着比较多,但是很简单, 上面是一堆构造不用管,然后就是个暂停和继续方法, 其实就是操作一个共享变量,然后再每个任务运行之前会调用 beforeExecute() 方法,这个方法会判断暂停状态是否为true, 是的话就阻塞,之后调用了继续的方法后再唤醒继续执行 然后写个main方法去测试一下效果1234567891011121314151617181920212223242526public static void main(String[] args) throws InterruptedException &#123; PauseableThreadPool threadPool = new PauseableThreadPool(5, 5, 0L, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); Runnable task = ()-&gt;&#123; log.info(&quot;执行任务....&quot;); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;; for (int i = 0; i &lt; 1000; i++) &#123; threadPool.execute(task); &#125; Thread.sleep(1000); threadPool.pause(); log.info(&quot;线程池已暂停&quot;); Thread.sleep(10000); threadPool.resume(); log.info(&quot;线程池恢复执行&quot;); threadPool.shutdown();&#125; 总结掌握钩子方法的作用,以及如何实现自定义的钩子方法]]></content>
      <categories>
        <category>JUC</category>
      </categories>
      <tags>
        <tag>JUC</tag>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUC-线程池-1-创建及使用]]></title>
    <url>%2F2020%2F04%2F13%2FJUC-%E7%BA%BF%E7%A8%8B%E6%B1%A0-1-%E5%88%9B%E5%BB%BA%E5%8F%8A%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[述上文中对线程池做了一个基本的了解,说了这么多,如何去创建一个线程池呢? JUC 包里面给我们提供了一个工具类就是 java.util.concurrent.Executors ,通过这个类我们就可以创建几种不同类型的线程池,来看一下这个类中常用的几个方法 固定数量线程池通过 Executors.newFixedThreadPool() 这个方法创建,代码如下: 123456789101112131415161718192021@Slf4jpublic class ThreadPoolTest &#123; public static void main(String[] args) &#123; ExecutorService newFixedThreadPool = Executors.newFixedThreadPool(5); for (int i = 0; i &lt; 1000 ; i++) &#123; newFixedThreadPool.submit(new TestTask()); &#125; log.info(&quot;done...&quot;); &#125; static class TestTask implements Runnable&#123; @Override public void run() &#123; log.info(&quot;test&quot;); &#125; &#125;&#125; 这里就是创建了固定数量为 5 的线程池,然后执行这段代码看一下输出,日志如下: 1234567891011121314151622:31:17.806 [pool-1-thread-5] INFO com.learning.java.threadpool.ThreadPoolTest - test22:31:17.806 [pool-1-thread-3] INFO com.learning.java.threadpool.ThreadPoolTest - test22:31:17.806 [pool-1-thread-2] INFO com.learning.java.threadpool.ThreadPoolTest - test22:31:17.806 [pool-1-thread-4] INFO com.learning.java.threadpool.ThreadPoolTest - test22:31:17.806 [pool-1-thread-1] INFO com.learning.java.threadpool.ThreadPoolTest - test22:31:17.806 [main] INFO com.learning.java.threadpool.ThreadPoolTest - done...22:31:17.817 [pool-1-thread-5] INFO com.learning.java.threadpool.ThreadPoolTest - test22:31:17.817 [pool-1-thread-3] INFO com.learning.java.threadpool.ThreadPoolTest - test22:31:17.817 [pool-1-thread-2] INFO com.learning.java.threadpool.ThreadPoolTest - test22:31:17.817 [pool-1-thread-4] INFO com.learning.java.threadpool.ThreadPoolTest - test22:31:17.817 [pool-1-thread-1] INFO com.learning.java.threadpool.ThreadPoolTest - test22:31:17.818 [pool-1-thread-5] INFO com.learning.java.threadpool.ThreadPoolTest - test22:31:17.818 [pool-1-thread-3] INFO com.learning.java.threadpool.ThreadPoolTest - test22:31:17.818 [pool-1-thread-2] INFO com.learning.java.threadpool.ThreadPoolTest - test22:31:17.818 [pool-1-thread-4] INFO com.learning.java.threadpool.ThreadPoolTest - test22:31:17.818 [pool-1-thread-1] INFO com.learning.java.threadpool.ThreadPoolTest - test 这里可以看到,执行1000个任务,执行的线程都是 pool-1-thread-1 ~ pool-1-thread-5, 就这5个线程,点进去看一下 newFixedThreadPool() 这个方法的源码 可以看到,本质上还是调用 ThreadPoolExecutor 的构造,然后设置 corePoolSize 和 maximumPoolSize 都是传进来的参数,然后 keepAliveTime 回收时间是 0 ,因为都没有额外线程,所以这个参数设置多少都没有意义, 最后任务队列是一个无界队列,所以这个线程池可以一直往里面放任务,当任务数量太多的时候,就可能会导致 OOM 单线程池单线程池,字面意思,就是只有一个线程,通过 Executors.newSingleThreadExecutor() 创建,代码如下: 123456789101112131415161718192021@Slf4jpublic class ThreadPoolTest &#123; public static void main(String[] args) &#123; ExecutorService newSingleThreadExecutor = Executors.newSingleThreadExecutor(); for (int i = 0; i &lt; 1000 ; i++) &#123; newSingleThreadExecutor.submit(new TestTask()); &#125; log.info(&quot;done...&quot;); &#125; static class TestTask implements Runnable&#123; @Override public void run() &#123; log.info(&quot;test&quot;); &#125; &#125;&#125; 同样执行 1000 个任务,看一下返回值 1234567891022:47:45.710 [pool-1-thread-1] INFO com.learning.java.threadpool.ThreadPoolTest - test22:47:45.710 [main] INFO com.learning.java.threadpool.ThreadPoolTest - done...22:47:45.720 [pool-1-thread-1] INFO com.learning.java.threadpool.ThreadPoolTest - test22:47:45.720 [pool-1-thread-1] INFO com.learning.java.threadpool.ThreadPoolTest - test22:47:45.720 [pool-1-thread-1] INFO com.learning.java.threadpool.ThreadPoolTest - test22:47:45.720 [pool-1-thread-1] INFO com.learning.java.threadpool.ThreadPoolTest - test22:47:45.720 [pool-1-thread-1] INFO com.learning.java.threadpool.ThreadPoolTest - test22:47:45.720 [pool-1-thread-1] INFO com.learning.java.threadpool.ThreadPoolTest - test22:47:45.721 [pool-1-thread-1] INFO com.learning.java.threadpool.ThreadPoolTest - test22:47:45.721 [pool-1-thread-1] INFO com.learning.java.threadpool.ThreadPoolTest - test 这里可以看到,不管多少任务,都是一个线程去执行的. 点进去 newSingleThreadExecutor() 方法看一下源码 通过源码可以看到,这里 corePoolSize 和 maximumPoolSize 都是 1 ,表示这个线程池最大能创建一个线程,然后任务队列是一个无界队列, 也是能一直接受任务,但是有可能造成OOM 可缓存线程池通过 Executors.newCachedThreadPool() 方法创建 测试代码如下: 123456789101112131415161718192021@Slf4jpublic class ThreadPoolTest &#123; public static void main(String[] args) &#123; ExecutorService newCachedThreadPool = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 1000 ; i++) &#123; newCachedThreadPool.submit(new TestTask()); &#125; log.info(&quot;done...&quot;); &#125; static class TestTask implements Runnable&#123; @Override public void run() &#123; log.info(&quot;test&quot;); &#125; &#125;&#125; 控制台部分打印如下 12345678910111213141516171819202122:54:35.874 [pool-1-thread-95] INFO com.learning.java.threadpool.ThreadPoolTest - test22:54:35.874 [main] INFO com.learning.java.threadpool.ThreadPoolTest - done...22:54:35.891 [pool-1-thread-315] INFO com.learning.java.threadpool.ThreadPoolTest - test22:54:35.921 [pool-1-thread-105] INFO com.learning.java.threadpool.ThreadPoolTest - test22:54:35.924 [pool-1-thread-109] INFO com.learning.java.threadpool.ThreadPoolTest - test22:54:35.924 [pool-1-thread-113] INFO com.learning.java.threadpool.ThreadPoolTest - test22:54:35.925 [pool-1-thread-112] INFO com.learning.java.threadpool.ThreadPoolTest - test22:54:35.927 [pool-1-thread-116] INFO com.learning.java.threadpool.ThreadPoolTest - test22:54:35.935 [pool-1-thread-117] INFO com.learning.java.threadpool.ThreadPoolTest - test22:54:35.942 [pool-1-thread-120] INFO com.learning.java.threadpool.ThreadPoolTest - test22:54:35.942 [pool-1-thread-121] INFO com.learning.java.threadpool.ThreadPoolTest - test22:54:35.944 [pool-1-thread-124] INFO com.learning.java.threadpool.ThreadPoolTest - test22:54:35.945 [pool-1-thread-125] INFO com.learning.java.threadpool.ThreadPoolTest - test22:54:35.946 [pool-1-thread-128] INFO com.learning.java.threadpool.ThreadPoolTest - test22:54:35.948 [pool-1-thread-129] INFO com.learning.java.threadpool.ThreadPoolTest - test22:54:35.949 [pool-1-thread-132] INFO com.learning.java.threadpool.ThreadPoolTest - test22:54:35.950 [pool-1-thread-136] INFO com.learning.java.threadpool.ThreadPoolTest - test22:54:35.950 [pool-1-thread-133] INFO com.learning.java.threadpool.ThreadPoolTest - test22:54:35.951 [pool-1-thread-137] INFO com.learning.java.threadpool.ThreadPoolTest - test22:54:35.951 [pool-1-thread-140] INFO com.learning.java.threadpool.ThreadPoolTest - test22:54:35.952 [pool-1-thread-141] INFO com.learning.java.threadpool.ThreadPoolTest - test 这个线程池的特点是会一直创建新的线程去处理任务,但是会自动回收多余的线程,同样看一下 newCachedThreadPool() 方法的源码 这里的 maximumPoolSize 被设置为 Integer.MAX_VALUE 也就是说这个线程池可能会创建数量非常多的线程,然后队列是一个 SynchronousQueue 直接交接的,因为能一直创建线程,所以这个放其他队列也没有意义,所以说,这个线程池也可能导致 OOM 定期及周期性任务线程池通过 Executors.newScheduledThreadPool() 方法创建, 这个线程池是支持延迟执行,和周期性执行任务的,代码如下 12345678910111213141516@Slf4jpublic class ThreadPoolTest &#123; public static void main(String[] args) &#123; ScheduledExecutorService scheduledExecutorService = Executors.newScheduledThreadPool(5); // 延迟5秒执行 scheduledExecutorService.schedule(new TestTask(), 5, TimeUnit.MINUTES); // 周期执行 先延迟 1 秒,然后每隔3秒执行一次 scheduledExecutorService.scheduleAtFixedRate(new TestTask(), 1, 3, TimeUnit.SECONDS); log.info(&quot;done...&quot;); &#125;&#125; 以上就是一些常用的线程池 线程池的正确创建姿势在阿里巴巴开发规范中,不能使用 Executors 去创建线程池, 在实际开发环境中应该结合自己的业务设置线程池的参数,比如实际机器的内存多大,能否接受任务被拒绝等,还有线程的名字等等 那么问题来了,我怎么知道我应该设置多少线程数? CPU密集型(加密、计算hash等)最佳线程数为CPU核心数的1-2倍左右 耗时IO型(读写数据库、文件、网络读写等)最佳线程数一般会大于cpu核心数很多倍,以JVM线程监控显示繁忙情况为依据,保证线程空闲可以衔接上,参考Brain Goetz推荐的计算方法:线程数=CPU核心数* ( 1+平均等待时间/平均工作时间) 线程池的关闭介绍完了线程池的创建之后,还要了一下关闭 关闭线程池可以使用线程池的 shutDown() 方法,这个方法会优雅的关闭线程池, 就是说调用了 shutDown() 方法之后,线程池不会立即关闭,而是先把当前线程池里面有的任务都执行完,然后再关,当然这个方法调用之后就不会去接受新的任务了 查看是否调用了 shutDown() 方法可以调用 isShutDown() 方法,调用 shutDown() 方法之后再调用 isShutDown() 会返回 true 如果说想要看看线程实际有没有关闭,也就是有没有执行完所有的任务后关闭,可以调用 isTerminated() 方法查看 还有一个方法是 awaitTermination(timeout, timeUnit) ,调用了这个方法之后会阻塞掉,然后在传入的时间内如果线程池执行完毕后关闭了会返回 true, 如果没有执行完成会返回 false ,这个方法只是用于检测,并不会关闭 最后还有一个方法就是 shutDownNow() 字面意思,就是立即关闭,不会等线程执行完成,但是这个方法会把所有没有执行完的任务返回回来, 返回值是个list 拒绝策略最开始介绍线程池的时候就提到了线程池的拒绝策略,拒绝策略触发的时机有两种 当线程池是关闭状态的时候,提交任务会被拒绝 当线程池的最大线程和任务队列饱和的时候会拒绝 对于线程池,有以下 4 种拒绝策略: AbortPolicy 直接抛出异常 DiscardPolicy 直接丢弃 DiscardOldestPolicy 丢弃队列种最老的任务 CallerRunsPolicy 谁提交的任务谁去执行,比如主线程往线程池里提交任务,被拒绝之后,任务就会由主线程自己执行 线程池的状态线程有状态同样的线程池也有状态,一共有以下几种: RUNNING: 接受任务并处理排队任务 SHUTDOWN: 不接受新任务,但处理排队任务,就是调用了 shutDown() 方法之后 STOP: 不接受新任务,也不处理排队任务,并且中断正在进行的任务,就是调用 shutDownNow() 方法之后 TIDYING: 所有任务都终止, workerCount 为零,这个时候线程池是 TIDYING 状态,会运行 terminate() 钩子方法,钩子方法具体后面再说 TERMINATED: terminate() 方法执行完毕之后 总结本文介绍了几种常用的线程池,以及创建线程池的方式 固定数量线程池 newFixedThreadPool() 单线程池 newSingleThreadExecutor() 可缓存线程池 newCachedThreadPool() 延迟执行/周期性执行线程池 newScheduledThreadPool() 以及他们的特点,最后还有创建线程池的最佳方式,总的来说,就是要注意以下几个点 避免任务堆积 避免线程数过度增加 排查线程泄露 还有线程池的关闭方法 shutDown() 优雅关闭线程池 shutDownNow() 立即关闭线程池 isShutDown() 是否是关闭状态 isTerminated() 查看是否执行完毕及关闭 awaitTermination() 阻塞一定时间,检测任务是否执行完成关闭 最后是线程池的几种状态]]></content>
      <categories>
        <category>JUC</category>
      </categories>
      <tags>
        <tag>JUC</tag>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUC-线程池-0-介绍]]></title>
    <url>%2F2020%2F04%2F13%2FJUC-%E7%BA%BF%E7%A8%8B%E6%B1%A0-0-%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[述在日常开发中,当我们需要使用多线程来处理一些任务的时候,可以创建一个线程,然后去调用 start() 启动,这种方式比较简单,但是同时也带来一些问题,比如在任务数量多的情况下,频繁的创建销毁线程,就会给服务器带来较大的压力,处理效率也不是特别高, 所以在这种情况下,我们就可以使用线程池来处理这些任务,创建固定数量的线程去处理任务,避免一直创建和销毁线程带来的开销. 下面就对线程池做一些详细的了解. 线程池的作用线程池的作用就是创建一定数量的线程,这些线程都保持工作的状态,有任务进来就交给这些线程,这些线程都可以去反复执行任务,从而避免反复创建线程的损耗. 使用线程池可以加快响应速度,合理利用CPU和内存,且可以统一管理 线程池的适用场景当服务器收到大量的请求时,使用线程池技术就比较合适,可以减少线程的创建和销毁次数,提高服务器的工作效率,在日常开发中,如果需要创建5个以上的线程,就可以使用线程池来管理 创建和停止线程池上面说了这么多线程池的好处,下面来看一下如何创建线程池, 首先要看一下线程池的构造函数 线程池的构造函数进入 ThreadPoolExecutor 这个类看一下它的构造函数, 如图 这里可以看到一共有以下几个参数,分别看一下具体的意思 参数名 作用 corePoolSize 核心线程数 maximumPoolSize 最大线程数 keepAliveTime 保持存活时间 unit 时间单位 workQueue 任务存储队列 threadFactory 创建线程工厂 handler 线程池无法接受新的任务的时候的拒绝策略 那么这些参数具体的作用是什么? 首先是 corePoolSize 和 maximumPoolSize corePoolSize: 核心线程数,线程池在初始化完成之后,默认情况下线程池中是没有线程的,线程池会等有任务到来的时候去创建新线程执行 maximumPoolSize: 在线程池运行过程中,可能会在核心线程数的基础上再创建一些线程,但是总的线程数有一个上线,就是 maximumPoolSize keepAliveTime: 就是在核心线程以外的线程的回收时间,当核心线程数意外的线程空闲超过 keepAliveTime 这个时间的时候就会被回收 workQueue: 存放任务的队列,当核心线程都有任务在执行的时候,新来的任务就会被放到任务队列中等待执行 threadFactory: 创建线程的工厂,如果不设置的话,默认是 DefaultThreadFactory ,通常情况下也不会去自定义 handler: 线程池处理任务达到极限的时候就不能再接受新的任务了,这个参数就是设置拒绝的策略 创建线程的规则了解了构造函数中参数的作用之后,再来了解一下线程池创建线程的规则 当有新的任务进来的时候,会有以下判断: 判断线程池当前线程数是否小于 corePoolSize, 如果当前线程数量小于核心线程数量,就会新建一个线程来运行 如果线程数大于等于 corePoolSize 但小于 maximumPoolSize ,就会将任务放到队列中去 如果队列满了,并且线程数小于 maximumPoolSize 就会创建新的线程来执行 如果队列满了,并且线程数大于等于 maximumPoolSize 就拒绝该任务 画个图来看一下这个流程 所以判断是否新创建线程的顺序是 corePoolSize -&gt; workQueue -&gt; maximumPoolSize 举个例子,现在有一个 corePoolSize=5, maximumPoolSize=10, workQueue=100 的线程池,有一批任务提交给这个线程池, 然后前五个任务都会创建新的线程去执行,然后后面的任务都会先存到队列里面,直到队列里面放了100个任务,队列满了之后,再进来的任务将会创建新的线程执行,最多创建10个,之后再有任务进来,就会拒绝,所以这个线程池最多能同时处理 115 个任务 增减线程的特点通过上面这个创建线程的规则,可以看出以下几个线程池的特点 创建线程池的时候 corePoolSize 和 maximumPoolSize 设置为相同的时候,就是一个固定线程数量的线程池 线程池希望保持较小的线程数,只有在任务特别多,负载高的情况下再创建额外的线程去处理 maximumPoolSize 可以设置为很高的值,比如 Integer.MAX_VALUE, 就表示线程池可以容纳任意数量的任务 只有当 workQueue 满了的时候才会创建额外的线程去处理,所以如果使用的是无界队列,线程池的线程数量就不会超过 corePoolSize 常见的队列类型有以下三种: 直接交接: SynchronousQueue 无界队列: LinkedBlockingQueue 有界队列: ArrayBlockingQueue 这里的直接交接就是这个队列不存放任务,直接交给线程池去执行 总结 了解线程池的作用,以及线程池解决了哪些问题 创建线程池的几个关键参数 线程池创建线程的规则]]></content>
      <categories>
        <category>JUC</category>
      </categories>
      <tags>
        <tag>JUC</tag>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-23-自定义starter]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-23-%E8%87%AA%E5%AE%9A%E4%B9%89starter%2F</url>
    <content type="text"><![CDATA[述本文来了解一下 Spring Boot 中的 starter, starter 是 Spring Boot 中的一个可拔插的插件,需要使用的时候只要引入对应的包, 比如我们要使用web环境,就会引入 spring-boot-starter-web starter 和 jar包导入的区别就是 starter 可以实现自动注入, 而jar包只会把对应的类导入,并不会加载到容器中, 所以 starter 可以大幅提升开发效率 常用的starter 名称 描述 spring-boot-starter-thymeleaf 使MVC Web Application 支持 Thymeleaf spring-boot-starter-mail 提供 Java Mail 和 Spring email 的发送支持 spring-boot-starter-data-redis 通过 Spring Data Redis/Jedis client 使用 Redis spring-boot-starter-web 提供web支持 … … 自定义 starter 搭建下面我们来搭建一个自定义的 starter 首先新建一个项目 spring-boot-starter-weather , 然后引入 spring-boot-autoconfigure 这个包 新建类 WeatherSource 代码如下:123456789@Data@ConfigurationProperties(prefix = &quot;weather&quot;)public class WeatherSource &#123; private String type; private String rate;&#125; 再建一个 WeatherService 如下:1234567891011121314151617public class WeatherService &#123; private WeatherSource weatherSource; public WeatherService(WeatherSource weatherSource) &#123; this.weatherSource = weatherSource; &#125; public String getType()&#123; return weatherSource.getType(); &#125; public String getRate()&#123; return weatherSource.getRate(); &#125;&#125; 然后还需要一个自动装配的类 123456789101112131415@Configuration@EnableConfigurationProperties(WeatherSource.class)@ConditionalOnProperty(name = &quot;weather.enable&quot;, havingValue = &quot;enable&quot;)public class WeatherAutoConfiguration &#123; @Autowired private WeatherSource weatherSource; @Bean @ConditionalOnMissingBean(WeatherService.class) public WeatherService weatherService()&#123; return new WeatherService(weatherSource); &#125;&#125; 这个类意思是属性中必须存在 weather.enable 且值是 enable 的情况下才生效,且在容器中不存在 WeatherService 这个类的时候才注入我们默认的 WeatherService 然后最重要的一步,在 Resource 目录下新建 META-INF 目录,然后新建 spring.factories 文件内容如下 1org.springframework.boot.autoconfigure.EnableAutoConfiguration=com.zhou.springboot.starter.weather.WeatherAutoConfiguration 就是把我们上面的配置类写进去 然后 maven 打包把jar装到本地,就可以在别的项目中引入这个包使用了 注意记得配置 weather.enable=enable 才会生效 原理解析上面我们实现了自定义的 stater ,那么 starter 的实现原理是怎么样的,下面来看一下 在 web 容器启动的时候,我们对 @EnableAutoConfiguration 这个注解做了分析, 在处理 @Import 注解的时候, 会进入下面这个代码段 这段代码我们在之前详细的分析过了 这里就会找到所有的 EnableAutoConfiguration 的实现,然后加载到容器中 所以我们上面才会新建一个 spring.factories 文件,把实现类写进去 总结了解了 starter 的作用后,我们自定义实现了一个 starter 自定义 starter 的步骤: 新建一个 Spring Boot 项目 引入 spring-boot-autoconfigure 包 编写属性源以及自动配置类 在 spring.factories 里添加自动配置类实现 打成jar包供外部使用 自动配置流程: 启动类的 @SpringBootApplication 注解 处理 @EnableAutoConfiguration 注解 Import 的 AutoConfigurationImportSelector ConfigurationClassParser 处理配置类 获取 spring.factories 里面的所有 EnableAutoConfiguration 的实现类然后处理]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-22-@Conditional注解使用]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-22-Conditional%E6%B3%A8%E8%A7%A3%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[述在我们 Spring Boot 项目中,很多地方都用到了 @Conditional 注解用来指定一些条件, 这个注解的作用是根据是否满足某一个特定的条件来决定是否创建某个 Bean ,这个注解的存在也是 Spring Boot 实现自动配置的关键基础能力,比如我们在 web 工厂类加载的部分就遇到过这个注解 常见的 @Conditional 注解 注解 作用 @ConditionalOnBean 当某个Bean存在的情况下 @ConditionalOnMissingBean 当某个Bean不存在的情况下 @ConditionalOnClass 当某个类存在的情况下 @ConditionalOnMissingClass 当某个类不存在的情况下 @ConditionalOnWebApplication 处于web环境下 @ConditionalOnProperty 当某个 property 存在的时候 @ConditionalOnNotWebApplication 不处于web环境的情况下 @ConditionalOnJava 在特定的Java版本下 动手实践了解了 @Condition 注解的作用之后,我们来写一个类实际使用测试一下 首先新建一个类, 如下1234@Component@ConditionalOnProperty(&quot;com.test&quot;)public class ConditionTest &#123;&#125; 这里首先是 @Component 注解, 表示要注入到容器中,然后用到了 @ConditionalOnProperty 注解,表示 com.test 这个属性要在环境变量中存在 然后写个测试类看一下 1234567891011121314151617@SpringBootTestpublic class ConditionalTests implements ApplicationContextAware &#123; private ApplicationContext applicationContext; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; this.applicationContext = applicationContext; &#125; @Test public void test1()&#123; System.out.println(applicationContext.getBean(ConditionTest.class)); &#125;&#125; 这里直接运行, 从容器中找这个类, 看一下输出结果 这里可以看到,我们在容器中是找不到这个类的,因为在环境变量中并没有 com.test 这个属性 然后我们在 application 配置文件中配置一下这个属性12com: test: test 再次运行,看一下输出结果 这次就成功注入到容器中了 原理分析来看一下上面用的 @ConditionOnProperty 注解 这里其实用了一个 @Condition 注解, 点进去 这个注解有一个属性,是一个类,这个类必须是 Condition 的子类,进入这个类看一下 可以看到这个接口只有一个方法 matches() 这个方法返回是 true 的话表示符合条件,就会被注入到容器中,如果是 false 则不会被注入到容器中 我们这里可以看一下他这个注解是怎么判断的 @ConditionOnProperty 这个注解用的是 @Conditional(OnPropertyCondition.class) 进入 OnPropertyCondition 这个类, 这个类是继承自 SpringBootCondition 这个抽象类, 所以我们先看一下 SpringBootCondition.matches 这个方法 这里可以看到重点的判断方法还是 getMatchOutcome() 这个方法,这个方法是具体子类实现的, 然后回到 OnPropertyCondition 看一下这个方法 这里的判断是在 determineOutcome() 这个方法当中,点进去 这里首先做了一个包装,然后调用它的 collectProperties() 方法做判断 这里就是调用环境变量,然后做具体的验证 经过上面的分析,我们是不是可以仿着这个,自己实现一个 @Condition 注解 自定义实现 @Condition 注解首先我们需要一个类,实现 Condition 然后重写他的 matches() 方法,代码如下 12345678910111213141516171819public class MyCondition implements Condition &#123; @Override public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; // 获取对应注解的属性的值 String[] values = (String[]) metadata.getAnnotationAttributes(&quot;com.zhou.springboot.example.conditional.MyConditionAnnotation&quot;) .get(&quot;value&quot;); for (String value : values) &#123; // 从环境变量中判断,有没有这个属性, 没有的话 return false if (StringUtils.isEmpty(context.getEnvironment().getProperty(value))) &#123; return false; &#125; &#125; return true; &#125;&#125; 然后就需要创建一个注解 123456789@Retention(RetentionPolicy.RUNTIME)@Target(&#123; ElementType.TYPE, ElementType.METHOD &#125;)@Documented@Conditional(MyCondition.class)public @interface MyConditionAnnotation &#123; String[] value() default &#123;&#125;;&#125; 这里加上 @Conditional 注解,指定我们刚刚创建的类,然后就可以在之前的 ConditionTest 类中使用测试了 12345@Component@MyConditionAnnotation(&#123;&quot;com.test1&quot;, &quot;com.test2&quot;&#125;)public class ConditionTest &#123;&#125; 这里整体作用就是 com.test1 和 com.test2 这两个属性都有值的情况下,这个类才会被注入到容器中 总结关于 @Condition 注解的使用就介绍到这里,下面做一个总结 作用: 通过 @Condition 注解可以实现根据是否满足某一个特定的条件来决定是否创建某个 Bean 自定义 @Condition 注解的流程 实现一个自定义注解并且引入 @Condition 注解 实现 Condition 接口,并且重写 matches() 方法,符合条件就返回 true 自定义注解引入上面创建的 Condition 接口的实现]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-21-web个性化配置原理解析]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-21-web%E4%B8%AA%E6%80%A7%E5%8C%96%E9%85%8D%E7%BD%AE%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[述在项目中,我们可以自定义一些 web 容器的配置,比如最常见的端口号,我们可以通过 server.port 配置,那么这些自定义的配置,是如何被应用到web容器中的, 下面来看一下 web 容器的个性化配置原理解析 源码解析首先进入 createWebServer() 方法中 然后进入获取 WebServerFactory 的 getWebServerFactory() 方法 这里获取 WebServerFactory 的流程我们在上文分析过了,这里注意最后的创建 Bean 的方法, 我们之前在 Bean 实例化流程的部分了解到, Bean 实例化完成之后会循环所有的 BeanPostProcessor 的实现,调用其 postProcessBeforeInitialization() 方法, 如下 在这个地方,我们重点关注 WebServerFactoryCustomizerBeanPostProcessor 这个类 点进去他的 postProcessBeforeInitialization() 方法,看一下做了哪些事情 继续往下 这里就是先调用一个 getCustomizers() 方法,获取返回的集合, 然后循环集合,调用每个元素的 customize() 方法 getCustomizers()一个一个看, 首先是 getCustomizers() 这个方法, 点进去 这里会调用 getWebServerFactoryCustomizerBeans() 这个方法,构建一个集合,然后排序返回,看一下这个方法里面是怎么处理的 这里会获取所有的 WebServerFactoryCustomizer 的实现,然后返回, 这里打一个断点,然后执行,看一下返回结果 这里一共有 5 个 WebServerFactoryCustomizer 的实现 先看一下第一个 TomcatWebSocketServletWebServerCustomizer, 这个类是怎么被注入的,如下 这个类看着不太熟悉,没关系看下一个 ServletWebServerFactoryCustomizer ,看一下这个是什么时间被注入到容器中的 这个类看上去是不是就熟悉了, 就是上文中说的通过 @EnableAutoConfiguration 这个注解去注入的类 这里的 5 个 WebServerFactoryCustomizer 的实现都是通过这种方式注入的,其他就不一个个看了, 重点关注 ServletWebServerFactoryCustomizer 这个类 在上图中可以看到,这个类也是通过 @Bean 注解注入的,这里用到了一个参数就是 ServerProperties ,点进去看一下 这个是不是也很熟悉,就是一个配置类,我们在配置文件中配置的 server.port 就会被绑定到这个类里面 customize()回过头来看 customize() 这个方法, 上面获取到 WebServerFactoryCustomizer 的实现之后,就会一次调用他们的 customize() 方法,还是先看 ServletWebServerFactoryCustomizer 这个类的 customize() 这里就是把 ServerProperties 里面的属性都赋值给 webFactory 再来看一个 TomcatWebServerFactoryCustomizer 和上面一样,也是把 ServerProperties 里面的 tomcat 相关配置赋值给 webFactory 其他几个做的事情大致都是这样, 到这儿就知道了整个web个性化配置生效的原理了 总结属性注入流程 个性化配置生效流程 举例 ServletWebServerFactoryCustomizer 的处理流程]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-20-web工厂类加载解析]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-20-web%E5%B7%A5%E5%8E%82%E7%B1%BB%E5%8A%A0%E8%BD%BD%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[述上文中了解了web容器的一个整体的启动流程, 在 createWebServer() 这个方法中,调用了 getWebServerFactory() 这个方法, 通过这个方法获取 webServerFactory 工厂, 就是去容器中找 ServletWebServerFactory 的实现类, 那么 ServletWebServerFactory 的实现类是什么时候被注入到容器中的, 下面来看一下 ServletWebServerFactory 工厂类加载的流程 源码跟踪首先我们进入启动类的 @SpringBootApplication 注解看一下 进入这个注解 可以看到这里通过 @Import 注解导入了一个 AutoConfigurationImportSelector , 我们进入这个类看一下 这里可以看到,这个类是实现的 DeferredImportSelector 这个类, 对于 @Import 注解的处理,我们之前在配置类解析的时候有说过了, 进入 @Import 注解处理的方法中,看一下具体的处理流程 @Import 处理 可以看到,这里会调用 DeferredImportSelectorHandler 的 handle 方法, 点进去 这里进来做了个判断, deferredImportSelectors 是肯定不为 null 的, 所以这里做的事情就是把传进来的 DeferredImportSelector 和当前的配置类,包装到一个 DeferredImportSelectorHolder 对象当中去,然后放到上面的 deferredImportSelectors 集合中去, 我们可以看一下 DeferredImportSelectorHolder 的构造方法 通过上面的处理流程我们可以看到这里只是把 DeferredImportSelector 的实现都包装起来添加到一个集合中去的, 并没有做实际的处理,那么这个集合是什么时候做处理的, 我们来看一下配置类解析最开始的 parse() 方法, 如下 这里在方法的最后会调用 DeferredImportSelectorHandler.process() 这个方法,这个方法就是去处理集合中的数据的 DeferredImportSelectorHandler$process来具体看一下这个方法做的事情 这里主要是调用了两个方法,先循环调用 DeferredImportSelectorGroupingHandler.register() 这个方法, 然后调用 DeferredImportSelectorGroupingHandler.processGroupImports() 这个方法处理, 我们分别来看一下这两个方法都做了哪些事情 DeferredImportSelectorGroupingHandler$register 首先是第一个 getImportGroup() 方法, 这个方法在 AutoConfigurationImportSelector 这个实现中,返回的是 AutoConfigurationGroup.class 如图 接下来是往 groupings 里面put进去了一个key-value, key就是上面返回来的 group , 然后 value 是通过key计算出来的, 首先调用了 createGroup() 这个方法 这里主要是返回一个 Group 对象,然后赋值了一些其他属性, 最后又 new 了一个 DeferredImportSelectorGrouping 对象,看一下这个对象的构造 这里只是做了一个包装 总体而言,这个 DeferredImportSelectorGroupingHandler$register 方法就是获取一个 group , 然后下面的 DeferredImportSelectorGroupingHandler.processGroupImports() 这个方法会做具体的处理 DeferredImportSelectorGroupingHandler$processGroupImports 这里首先是循环 groupings 的 values ,就是上一部添加进去的 DeferredImportSelectorGrouping 对象, 然后调用他的 getImports() 方法获取一个集合,然后循环, 先看一下 getImports() 这个方法是怎么处理的 这里先会循环 deferredImports 这个集合,然后调用 this.group.process() 这个方法, 点进去,这里还是看 AutoConfigurationImportSelector 这个类的具体实现 这里的重点是 getAutoConfigurationEntry() 这个方法, 点进去 调用的方法比较多,首先这里的 getAttributes() 方法就是获取注解中的一些属性, 这里也就是 @EnableAutoConfiguration 这个注解的属性, 如下 我们这里并没有做这两个属性的配置,所以获取到的是空的, 然后进入下一个方法 getCandidateConfigurations() 这一步会获取 spring.factories 中的所有 EnableAutoConfiguration 这个类的实现, 然后下一个方法 removeDuplicates() 对上面获取到的集合做一个去重的操作 下面的几步都是针对需要过滤的配置类做一些处理,由于我们这里注解并没有配置,所以不会有需要忽略的类,这几个方法就不详细看了,然后是 filter() 这个方法 这个方法的作用主要是对上面获取到的类做一些过滤,通过 getAutoConfigurationImportFilters() 这个方法获取过滤器, 这个方法如下 上面这两部我们可以打个断点看一下 这里可以看到获取到的 attributes 都是空的, 然后 configurations 有124个, 接着往下执行 经过过滤之后只剩下 23 个了,回到上层看一下 grouping,getImports() 这个方法的具体返回 这里就是上一步获取到的 23 个配置类, 我们这里重点关注 ServletWebServerFactoryAutoConfiguration ,打开这个类看一下 这个类会通过 @Import 注解注入一系列的类, 这里我们进入 EmbeddedTomcat 这个类,如下 可以看到这里有几个 @Conditional 指定的条件, 第一个就是 @ConditionalOnClass({ Servlet.class, Tomcat.class, UpgradeProtocol.class }) ,然后是 @ConditionalOnMissingBean(value = ServletWebServerFactory.class, search = SearchStrategy.CURRENT) 就是 @ConditionalOnClass 注解指定的类存在且 @ConditionalOnMissingBean 指定的类不存在的情况下, 这个配置类才会生效, 如果生效的话,就会执行下面的 @Bean 方法, 这个方法就是向容器注入 TomcatServletWebServerFactory 这个工厂类的 到这里整个工厂类的加载流程就完成了 总结这里可能有点乱,可以打个断点把总体流程执行一遍,这里还是画个流程图来整理一下]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-19-web容器启动解析]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-19-web%E5%AE%B9%E5%99%A8%E5%90%AF%E5%8A%A8%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[述接下来了解一下 Spring Boot 中, web 环境启动的一些流程,比如 Spring Boot 是如何判断当前是否是 web 环境的,又是怎么启动 tomcat 容器的, 如何将 tomcat 替换成别的 web 容器等等 WebApplicationType在 Spring Boot 中,一共有三种容器类型,分别是 NONE, SERVLET, REACTIVE, 我们通常用的 web 服务就是 SERVLET 环境 首先来关注一个问题, 容器是如何识别当前是那种环境的? 容器启动流程Spring Boot 容器启动分两步, 先是 new 一个 SpringApplication 对象, 然后调用 run 方法, 在创建 SpringApplication 对象的这一步,就会决定我们的容器是哪种类型的, 我们先看一下 SpringApplication 的构造 SpringApplication 构造 如图这里调用了 WebApplicationType.deduceFromClasspath() 这个方法来判断容器是哪种类型的,点进去看一下 可以看到,这里就是通过判断几个类的存在或者不存在来确定是哪种容器类型的,默认就是 SERVLET 环境, 然后接下来再看一下 run 方法中跟容器相关的事情 run 进入这里的 createApplicationContext() 方法 这里这个方法主要作用就是通过容器类型,创建对应的 ApplicationContext 然后再来看一下我们熟悉的 refresh() 方法, 点进去 看一下这里的 onRefresh() 方法, 我们之前大致介绍过一下,这里详细来看一下 servlet 环境中做的事情,进入 ServletWebServerApplicationContext 这里进入下面的 createWebServer() 方法看一下 这个方法是我们重点关注的, 首先来看第一个 getWebServerFactory() getWebServerFactory() 这里可以看到,我们的容器中只能有一个 ServletWebServerFactory 的实现类,多了少了都会报错. 以默认的 Tomcat 为例,接着往下看, 获取到 factory 之后,会调用 getWebServer() 这个方法, 点进去这个方法看一下,做了哪些事情 TomcatServletWebServerFactory$getWebServer进入 TomcatServletWebServerFactory 的 getWebServer() 这个方法 这里首先 new 了一个 Tomcat 的实例,然后做了一些属性的配置,然后调用 getTomcatWebServer() 这个方法, 点进去继续往下看 这里也是做一些属性的配置,总之最后就是返回了一个 Tomcat 的实例 initPropertySources()返回上面的 onRefresh() 方法中, 继续往下是调用了一个 initPropertySources() 方法, 点进去看一下这个方法做了些什么 这里就是去判断了两个环境变量,然后如果存在的话,做了个替换的操作 finishRefresh()回到 refresh() 方法当中,再来看一下 finishRefresh() 这个方法 进入对应的实现类 这里做了两件事,先启动容器,然后推送事件 到这里整个容器启动的大致流程就完成了 总结大致流程了解了之后,画画图来总结一下 首先是容器类型判断流程 然后是创建容器前的准备流程 容器创建流程图 容器启动流程图]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-18-配置类解析核心方法]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-18-%E9%85%8D%E7%BD%AE%E7%B1%BB%E8%A7%A3%E6%9E%90%E6%A0%B8%E5%BF%83%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[述接着上文,继续分析配置类的解析的核心方法, 上文中我们留了一个 ConfigurationClassParser.doProcessConfigurationClass() 方法,没有详细分析,下面来看一下这个类具体是怎么处理的 源码跟踪 从上往下依次看 内部类处理先看一下代码片段 首先判断有没有 @Component 注解, 有的话进入下面的 processMemberClasses() 方法 这里就是对内部类的一个递归处理,我们写个例子看一下 案例以启动类为例,这里添加两个内部类进去,如下 然后在上面的 processMemberClasses() 方法中打个断点 这里就获取到两个内部类,然后通过 ConfigurationClassUtils.isConfigurationCandidate() 这个方法做个判断,看是否需要处理,需要的话后面再调用 processConfigurationClass() 方法去处理,这两个方法我们上文中详细分析过了,这里就不再赘述了 上面还有一个 importStack 集合,主要是为了防止相互依赖的问题, 比如 A配置类中依赖B,B配置类中又依赖A @PropertySource 处理接着往下看,是 @PropertySource 注解的处理, @PropertySource 可以指定一个配置文件,具体加载就是在这里处理的 继续往下 首先获取到文件,然后调用了一个 addPropertySource() 方法,点进去 这里主要是对环境变量的一些处理 @ComponentScan 处理再往下看是 @ComponentScan 注解的处理,在启动类中的 @ComponentScan 注解如下 这里就配置了个过滤器,用来判断哪些类不需要加载 回到方法中 这里的重点是 this.componentScanParser.parse() 这个方法,他会把 @ComponentScan 注解配置的要扫描的包下的所有的 @Component 修饰的类都扫描出来,进入这个方法 这里主要是获取一些注解中的属性,然后填充到对象中,比较重要的是这个 basePackages 的获取,这里我们可以在注解中手动指定要扫描的包路径,如果没有手动指定的话,会获取当前类所在的包路径,这里就解释了为什么 Spring Boot 项目的启动类为什么要放在最上层的包中 然后继续往下,是调用了一个 doScan() 方法,点进去 这里的重点是 findCandidateComponents() 这个方法, 点进去看一下 然后这里判断是否要处理的方法是 isCandidateComponent() ,如下 这里会调用我们注解中配置的过滤器,判断判断一下是否匹配,然后觉得是否需要处理 案例这里我们以启动类为例, 看一下他的扫描结果 首先是 scanCandidateComponents() 方法中的返回 然后看一下外层的 findCandidateComponents() 的返回 @Import 处理@ComponentScan 处理完成之后会处理 @Import 注解, 会调用一个 processImports() 去处理,主要代码如下 这个方法的主要作用是处理 ImportSelector , DeferredImportSelector 还有 ImportBeanDefinitionRegistrar 这几个类的实现, 我们可以通过实现这几个类,往容器中注入 Bean 案例关于这几个接口的使用,我们写以下几个类来测试一下 首先,新建 MyImportSelector 实现 ImportSelector, 代码如下12345678910@Slf4jpublic class MyImportSelector implements ImportSelector &#123; @Override public String[] selectImports(AnnotationMetadata importingClassMetadata) &#123; log.info(this.getClass().getName()); return new String[]&#123;&quot;com.zhou.springboot.example.model.Test&quot;&#125;; &#125;&#125; 这个类作用就是将返回的数组中对应的类注册到容器中 然后新建 MyDeferredImportSelector 实现 DeferredImportSelector ,代码如下12345678910@Slf4jpublic class MyDeferredImportSelector implements DeferredImportSelector &#123; @Override public String[] selectImports(AnnotationMetadata importingClassMetadata) &#123; log.info(this.getClass().getName()); return new String[0]; &#125;&#125; 这里和上面的 MyImportSelector 的作用其实是一样的 最后一个新建 MyImportBeanDefinitionRegistrar 实现 ImportBeanDefinitionRegistrar ,代码如下12345678910@Slf4jpublic class MyImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar &#123; @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry, BeanNameGenerator importBeanNameGenerator) &#123; log.info(this.getClass().getName()); RootBeanDefinition rootBeanDefinition = new RootBeanDefinition(); rootBeanDefinition.setBeanClass(Test.class); registry.registerBeanDefinition(&quot;test2&quot;, rootBeanDefinition); &#125;&#125; 这里也是往容器中注入一个 BeanDefinition ,最后需要使用 @Import 注解把这几个类手动引入才生效, 以启动类为例,修改如下1234567@SpringBootApplication@Import(&#123;MyImportSelector.class, MyDeferredImportSelector.class, MyImportBeanDefinitionRegistrar.class&#125;)public class App &#123; public static void main(String[] args) &#123; // ... 省略其他代码 &#125;&#125; 启动项目看一下日志输出 这里三个类的作用其实是一样的, 只是处理的顺序不一样 ImportSelector &gt; DeferredImportSelector &gt; ImportBeanDefinitionRegistrar这种注入方式也不常用,就不详细介绍了 @ImportResource 处理再往下看,就是对 @ImportResource 这个注解的处理, Spring Boot 中注入 Bean 可以通过注解,也可以通过 xml 的方式, 通过 xml 方式注入的话, 就需要使用 @ImportResource 注解引入具体的 xml ,处理代码如下: 这里,最终只会把文件找出来,然后调用 configClass.addImportedResource() 这个方法添加到一个集合中去, 在这一步并不会做具体的处理 这种方式也基本不常用,也不做过多介绍 @Bean 处理再往下就是对 @Bean 注解的处理,代码如下 这里会通过 retrieveBeanMethodMetadata() 这个方法找到这个类中,所有的 @Bean 修饰的方法,然后通过 configClass.addBeanMethod() 添加到一个集合中去, 跟 xml 的方式一样,这里只是保存起来不做具体处理 接口默认方法处理最后是对接口的默认方法处理,如下 点进去如下 这些都处理完成之后,会判断有没有父类,有的话返回去,上层方法会继续循环处理这些东西 最后来看一下 xml 和 @Bean 注入的 bean 是什么时候被处理的 回到最外层的 ConfigurationClassPostProcessor$processConfigBeanDefinitions() 这个方法中, 这个方法就是配置类处理的最外层的方法,我们之前最开始分析的, 进到之前说过的那个 do-while 循环中, 如下 上面的 parse 方法把这些类都处理完毕后,就会调用 loadBeanDefinitions() 这个方法, 在这个方法中,就会处理 xml 和 所有 @Bean 注入的 bean ,如下 总结本文对上文中的 doProcessConfigurationClass() 方法做了详细的分析, 就是下面这个 8 个模块的处理 可以 debug 跟一下效果会更好]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-17-配置类解析逻辑]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-17-%E9%85%8D%E7%BD%AE%E7%B1%BB%E8%A7%A3%E6%9E%90%E9%80%BB%E8%BE%91%2F</url>
    <content type="text"><![CDATA[述上文中,了解了配置类解析的一个入口和整体的流程,本文来详细的了解一下配置类解析的逻辑,也就是上文说的 ConfigurationClassPostProcessor.processConfigBeanDefinitions() 这个方法中 do-while 循环的 do 部分,看一下他具体是如何处理的 源码跟踪重点是这里的 parse() 方法,进入看一下具体处理逻辑 ConfigurationClassParser$parse 这里做个类型判断,通常情况都是 AnnotatedBeanDefinition 类型的, 进入下面的 parse() 方法看一下 这里 3 个重载方法都一样,都是 new 了一个 ConfigurationClass 对象,然后调用了 processConfigurationClass() 这个方法, 然后以 AnnotatedBeanDefinition 类型为例,看看 ConfigurationClass 对象中都有哪些属性 ConfigurationClassParser$processConfigurationClass然后看一下 processConfigurationClass 这个方法 这里有个比较重要的集合是 configurationClasses ,是一个 Map 集合, processConfigurationClass() 方法处理完毕后,会把当前类放进去这个方法里面先调用了一个 asSourceClass() 这个方法,然后是一个 do-while 循环,里面又调用了 doProcessConfigurationClass() 方法 ConfigurationClassParser$asSourceClass先来看一下这个 asSourceClass() 方法,如下 最终是返回了一个 SourceClass 对象, 构造方法如下 ConfigurationClassParser$doProcessConfigurationClass返回来再来看一下 doProcessConfigurationClass() 这个方法,如下 这里会一次判断是否有这些注解,然后依次处理,具体处理逻辑我们之后再说, 最终会进入一个if判断,如果有父类就返回父类,没有的话,返回 null ,具体的判断逻辑如下: 如果有父类,并且符合其他条件,就返回出去,供外面的 do-while 继续循环 最终结束后,parse方法就处理完成了,之后会进入下面的 parser.validate() 方法 ConfigurationClassParser$validate点进去看一下,具体是如何做的验证 继续往下 验证完成之后,回到 do 方法体内, 会声明一个 configClasses 集合,然后值是 parse 方法解析出来的所有配置类,然后移除掉处理过的,再获得一个 ConfigurationClassBeanDefinitionReader 对象,调用其 loadBeanDefinitions() 方法 ConfigurationClassBeanDefinitionReader$loadBeanDefinitions 这个方法的主要作用就是,加载所有的 BeanMethod , 然后将返回值注册到容器中, 这就是为什么我们用 @Bean 注解修饰的方法返回的对象会注册到 Spring 容器中 处理完成之后添加到一个处理过 alreadyParsed 的集合中,然后把 candidates 结合清空 之后是一个判断详细的看一下这个判断的具体作用 这个判断的作用就是处理 @Bean 注入的新的 BeanDefinition ,之后继续 do-while 循环,直到没有新的 BeanDefinition 以上就是这个 do-while 循环中做的全部事情, 其实重点的处理逻辑都在 ConfigurationClassParser$doProcessConfigurationClass 这个方法中, 下文中会详细分析 总结最后把这个 do-while 中的逻辑做一个总结 配置类的解析入口是在 ConfigurationClassPostProcessor$processConfigBeanDefinitions() 这个方法的 do-while 循环体当中, 循环终止的条件是配置类没有向 BeanFactory 中注册新的 BeanDefinition 然后整个 do-while 循环的流程图如下: 然后 parse() 方法的逻辑: 最后 doProcessConfigurationClass() 方法中的处理逻辑]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-16-配置类解析流程]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-16-%E9%85%8D%E7%BD%AE%E7%B1%BB%E8%A7%A3%E6%9E%90%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[述配置类,是我们在 Spring Boot 项目中经常使用的, 就是用 @Configuration 注解修饰的类, 然后在配置类中,我们可以使用 @Bean 注解修饰一个方法, 这个方法返回的 class 对象,就会被注入到 Spring 容器中,下面我们来大致了解一下 Spring Boot 框架中配置类的解析流程 源码回顾配置类其实是在我们之前说过的 refresh() 方法中处理的,具体的在 invokeBeanFactoryPostProcessors() 这个方法中,我们之前有详细介绍过,这里我们先来回顾一下这个方法 配置类的解析,就在其中一个 BeanDefinitionRegistryPostProcessor 中就是 ConfigurationClassPostProcessor, 我们来看一下他的 postProcessBeanDefinitionRegistry() 方法 配置类解析进入 ConfigurationClassPostProcessor.postProcessBeanDefinitionRegistry() ,如下: 然后这里先做了判断, 重点逻辑在 processConfigBeanDefinitions() 这个方法中,点进去 ConfigurationClassPostProcessor$processConfigBeanDefinitions() 这里首先是两个判断,重点关心一下 ConfigurationClassUtils.checkConfigurationClassCandidate() 这个判断的逻辑 ConfigurationClassUtils$checkConfigurationClassCandidate()点进去这个方法,也是比较长,分开来看 这一步,主要做一些判断,然后拿到类的元数据,一般来说我们都是用的注解,所以会进入第一个 if 判断中,最终获取到类的元数据 metadata ,然后往下 根据获取到的元数据,做了两个判断,首先第一个判断是判断是否是 @Configuration 类, 如果不是的话,会进入第二个 if 条件,这里调用了一个 isConfigurationCandidate() 方法来做判断, 我们进去看一下具体的判断逻辑 这里用到的 candidateIndicators 这个集合的属性如下 总体来说,就是判断是不是这4个类型的注解,然后如果不是的话最后判断这个类里有没有方法用了 @Bean 的注解 回到前面的 ConfigurationClassUtils$checkConfigurationClassCandidate() 方法中,这两个判断,如果是 @Configuration 注解的类,则会设置一个 Attribute,key是常量 CONFIGURATION_CLASS_ATTRIBUTE 值是 full, 另一种情况的值是 lite 最后获取一下排序的值,设置一下返回 回到 ConfigurationClassPostProcessor$processConfigBeanDefinitions() 方法中,继续往下看 上面的判断返回 true 的话,会new一个 BeanDefinitionHolder 对象,然后添加到 configCandidates 集合中去,然后做个排序,继续往下看 这里的do-while循环,就是具体的解析逻辑,我们下文中详细分析 总结具体的流程就是上面这样, 最后画画图做个总结 配置类解析入口: postProcessBeanDefinitionRegistry() 的处理逻辑: processConfigBeanDefinitions() 的处理逻辑:]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-15-异常处理流程]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-15-%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[述上文中了解了 Spring Boot 的异常报告器的工作原理,但是并没有看到他的调用时机, 其实就是在 run 方法的 catch 部分的 handleRunFailure() 中做处理,下面我们来看一下这个方法的具体处理流程 源码跟踪首先还是进入 run 方法,进到 catch 块中,找到 handleRunFailure() 这个方法,如下 之后进到这个方法看一下 这个方法一共做了五件事: 处理退出状态码 发送失败事件 错误报告 关闭 context 把异常重新抛出去 一个一个看一下这几件事情的具体处理流程 状态码处理首先进入 handleExitCode() 这个方法看一下 这个里面调用了 getExitCodeFromException() 去获取状态码,点进去继续往下看 具体的计算方法如下 总体来说,就是找到 ExitCodeExceptionMapper 的实现类,然后根据具体的错误,找到对应的状态码, 0是正常退出, 否则为非正常退出 发送失败事件状态码处理完毕后,判断了 listeners 是否为空,然后发送了一个失败事件, 判空原因主要是防止监听器还没加载完毕,程序就出错了,具体发送事件的原理这里就不分析了,可以回头到监听器的那里去看 错误报告接着往下看一下 reportFailure() 错误报告的这个方法 这里先去 exceptionReporters 集合,其实就我们上文中介绍的 FailureAnalyzers 这一个类, 当然我们也可以手动写一个 SpringBootExceptionReporter 的实现,但是这里的 reportException() 返回 true 的话,就不会进入下一个了,具体的报告流程我们在上文中详细分析过了 关闭 context接着往下看,调用了 context.close() 这方法,点进去,看看具体做了哪些事情 这里先调用了 doClose() 方法, 然后把钩子方法移除掉, 钩子方法就是在 jvm 退出的时候做的一些事情, 然后我们来关心一下 doClose() 这个方法 重新抛出异常最后一步会调用 ReflectionUtils.rethrowRuntimeException(exception) 这个方法把异常重新抛出去,这里点进去看一下 这里先把异常做个包装,然后再抛出去,以上就是全部的异常处理流程 总结handleExitCode() 作用: 计算退出状态码,为0是正常退出,非0则是异常退出 发送 ExitCodeEvent 事件 记录 exitCode 的值 listeners.failed() 作用: 发送 ApplicationFailedEvent 事件 reportFailure() 作用: 调用异常报告器, 成功的话记录已处理的异常 context.close() 作用: 更改应用上下文状态 销毁单例Bean 将 beanFactory 置空 关闭web容器(web环境中) 移除 shutdownHook 钩子方法 ReflectionUtils.rethrowRuntimeException(exception) 作用: 包装异常,重新抛出 钩子方法 (shutdownHook) 介绍: jvm 退出的时候执行的业务逻辑 通过 Runtime.getRuntime().addShutdownHook() 方法添加 通过 Runtime.getRuntime().removeShutdownHook() 方法移除]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-14-异常报告器]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-14-%E5%BC%82%E5%B8%B8%E6%8A%A5%E5%91%8A%E5%99%A8%2F</url>
    <content type="text"><![CDATA[述本文我们来了解一下 Spring Boot 的异常报告器,它是 Spring Boot 帮我们定位程序错误的一个工具,就是在程序运行错误的时候,在控制台打印出一个错误报告,方便我们排查问题,下面来看一下异常报告器是如何工作的 异常报告器解析首先还是进入启动类的run方法看一下 SpringBootExceptionReporter这里是定义了一个 SpringBootExceptionReporter 的集合,我们来看一下这个类 然后会通过 SpringFactoriesLoader 去加载他的实现,来看一下 spring.factories 中这个类有哪些实现 FailureAnalyzers可以看到,这里只有一个实现类就是 FailureAnalyzers ,在 run 方法中的 getSpringFactoriesInstances() 方法,会创建对应实现类的实例,也就是说会进入 FailureAnalyzers 的构造,我们来看一下 在上面的构造中,调用了 loadFailureAnalyzers() 和 prepareFailureAnalyzers() 这两个方法,分别看一下 loadFailureAnalyzers() prepareFailureAnalyzers() 其实就是做了一件事,找到所有的 FailureAnalyzer 的实现类,然后把实现类需要的属性赋值上去,我们再来看一下 FailureAnalyzer 有哪些实现类 FailureAnalyzer 先是一个抽象的实现 AbstractFailureAnalyzer 然后下面是具体的子类实现,这个类做的事情也简单,只有一个 analyze() 方法,传入一个错误之后,返回 FailureAnalysis 对象,然后我们进入 FailureAnalysis 看看他有哪些属性 他其实就是对错误的一些描述 SpringBootExceptionReporter实现类的处理返回来再来看一下 SpringBootExceptionReporter 的实现类 FailureAnalyzers 实现的 reportException() 这个方法 看一下这两个方法具体做的事情 这里是先循环遍历所有的 FailureAnalyzer ,然后得到一个 FailureAnalysis ,再调用 report() 方法处理 analyze()所以我们这里先关注一下 analyze() 这个方法,看一下 FailureAnalysis 这个对象是怎么返回来的,先进入他的抽象实现 AbstractFailureAnalyzer.analyze() 这里调用了 findCause() 和 analyze() , findCause() 中又调用了 getCauseType(),看一下这几个方法做的事情 findCause() 这个方法主要是找到子类关心的错误,做一下转换, findCause() 方法中,参数是一个 Throwable 对象,那他里面为什么要循环一下, 这里主要是防止类似 throw new AException(new BException(new CException(&quot;cause&quot;))) 这种错误 analyze()至于 analyze() 方法 这里只是一个抽象的实现,具体会交给子类去处理 返回来看 report() 方法中, 用到了 FailureAnalysisReporter 这个类的实现,这个类的实现在 spring 中也只有一个,就是 LoggingFailureAnalysisReporter 如下 看到上面这个文本格式,应该都不陌生了,都见过 总结以上,就是 Spring Boot 的异常报告器的工作原理,回过头来画画图 框架内实现流程: reportException() 处理流程: analyze() 处理流程图:]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-13-Spring Profile解析]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-13-Spring-Profile%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[述在日常开发中,有这样一种情况,就是开发环境,测试环境,生产环境,所需要的配置不同,我们通常会使用不同的配置文件来解决这个问题,比如开发环境我们有一个 application-dev.yml 的配置,然后生产环境有一个 application-prod.yml 的配置, 然后再不同的环境中,我们可以通过配置 spring.profiles.active 这个属性,来指定哪个配置文件生效 相关配置这里先介绍几个常用的 profile 相关的配置 12# 指定生效的配置 spring.profiles.active=xx 12# 默认生效的配置(这里要注意,如果配置了 spring.profiles.active 则 spring.profiles.default 不生效,配置在application配置文件中也不生效)spring.profiles.default=xx 12# 配置多个配置文件同时生效spring.profiles.include=xx,xx 12# 指定profile前缀(默认就是application,这个配置要写在命令行中才会生效) spring.config.name=xx 源码解析在上一篇文章中我们了解了属性加载的整体流程, 其中 ConfigFileApplicationListener 这个类实现了 EnvironmentPostProcessor ,然后他的 addPropertySources() 方法中,首先加了一个随机数的属性集,然后就是调用了 Loader.load() 这个方法,具体如图: 这里我们重点关注两个方法: initializeProfiles() load() 一个一个看,先进入 initializeProfiles() 这个方法中 initializeProfiles() getDefaultProfiles()这里进入 this.environment.getDefaultProfiles() 这个方法,看一下他是怎么获取默认的 profile 的 这里先是用 this.defaultProfiles 和 getReservedDefaultProfiles() 这方法的返回值做了一个对比, 我们看一下这两个东西的值到底是什么 可以看到,这里的 defaultProfiles 其实也是调用的 getReservedDefaultProfiles() 这个方法,如下: 这里就是一个单元素集合,值是 default ,这里判断对比一致之后,找我们配置中有没有 spring.profiles.default ,有的话进入 setDefaultProfiles() 这个方法,进去看一下 所以 getDefaultProfiles() 这个方法做的事情就是,从配置中找我们有没有配置 spring.profiles.default ,有的话用我们自定义配置的,没有的话,返回默认的 default 回到上面的 initializeProfiles() 看一下,只有在 profiles 集合size是1的情况下,才会去找默认的profiles, 也就解释了配置了 spring.profiles.active 之后, spring.profiles.default 不会生效的原因 综上所述,如果我们项目中,什么也没有配置的话, initializeProfiles() 这个方法执行完毕后, profiles 这个集合中应该会有两个属性, 一个最开始添加的 null ,一个是默认的 defaultProfile 对象 load()回到 Loader.load 方法中 initializeProfiles() 走完之后, profiles 集合不为空,然后进入循环, poll() 方法弹出集合中的第一个元素,第一次循环应该是最先添加的 null , 然后做一个判断 isDefaultProfile() 是否是默认的 profile, null 显然不是,就会进入下面的 load() 方法,我们看一下这个方法的源码 getSearchLocations()这里首先调用的是 getSearchLocations() 获取一个路径的集合,点进去看一下 这里的这个默认路径 DEFAULT_SEARCH_LOCATIONS 如下 一共 4 个路径, classpath 的根目录和 classpath 下的 /config 目录,还有文件的根目录和文件目录下的 /config 目录 getSearchNames()再来看一下 getSearchNames() 的源码 获取到名称之后,再次循环,然后调用同名 load 方法,点进去 这里打了一个断点, 这里有两个 propertySourceLoaders ,然后第二层循环中,会调用他们的 getFileExtensions() 方法,我们分别看一下这两个方法的返回 首先是 PropertiesPropertySourceLoader 然后是 YamlPropertySourceLoader 所以这里 PropertiesPropertySourceLoader 是处理 properties 和 xml 两种格式的文件, YamlPropertySourceLoader 是处理 yml 和 yaml 两种格式的文件 然后最终会调用 loadForFileExtension() 这个方法,点进去看一下 这里就是拼接一下配置文件名,然后最终又是调用了一个 load() 方法, 进去看一下 这里的重点是断点处的这个 loadDocument() 方法,这方法会把我们的配置文件解析出来, 然后转成 Document 对象, 点进去看一下 这里在这个 loader.load 方法打个断点运行看一下 这里就是我们项目中的 application.yml 解析出来的一个 key-value 集合,然后下面调用的了 asDocuments() 方法,点进去 这里做了两件事, 首先是通过 Binder 对象,将配置绑定到对应的类中, 然后创建了 Document 对象, 我们看一下他调用的这个构造是赋了哪些值 这里的 activeProfiles 和 includeProfiles 就是上面在配置文件中解析出来的 spring.profiles.active 和 spring.profiles.include 的值, 最终这个 Document 集合返回之后,会循环然后把他们的 activeProfiles 和 includeProfiles 都添加到 this.profiles 集合中去, 这里就解释了为什么我们在配置文件中设置的 spring.profiles.active 和 spring.profiles.include 也会生效 然后回到最开始的load方法中 这个循环会处理完所有的profile,包括在配置文件中读取出来的 profile ,然后最后又去处理了一次profile是null的情况,主要是防止上面的循环没进, 读取一次 application.yml 或者 application.properties 之后会调用一个 addLoadedPropertySources() 方法, 进去看一下 这里就把所有读取到的属性都加到属性集中去 到这里,配置文件的解析就完成了 总结回头来画画图总结一下配置文件加载的流程 加载入口 initializeProfiles() 方法流程图 profiles集合处理流程 load() 方法流程图 配置文件读取流程图 addLoadedPropertySources() 方法流程图]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-12-属性加载解析]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-12-%E5%B1%9E%E6%80%A7%E5%8A%A0%E8%BD%BD%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[述上文中了解了如何通过 Spring Aware 来获取属性,最终其实是使用 Environment 对象来获取系统中的属性的,那么 Environment 对象是怎么来的, Spring Boot 是如何将我们的属性配置,转成一个 Environment 对象的, 下面来看一下获取 Environment 对象的源码 源码解析还是先进入项目的run方法,找到关于 Environment 的部分,如下: 进入这个方法看一下, 先大致了解一下这个方法做的事情 然后我们就围绕着这个方法的流程,看一下他这里的每一步都怎么处理的 getOrCreateEnvironment()进入第一个 getOrCreateEnvironment() 方法,如下: 然后我们看一下获取到的这个 StandardServletEnvironment 对象的类图 可以看到这里先继承了 StandardEnvironment 然后 StandardEnvironment 又继承了 AbstractEnvironment,所以这里这个构造首先是会去调用父类的构造,也就是 AbstractEnvironment, 我们进去这个类看一下他的构造方法 这里调用了 customizePropertySources 方法,传入了 propertySources,继续点进去看一下 可以看到这里是一个空的实现, 具体的实现由子类完成,我们创建的子类是 StandardServletEnvironment ,所以这里会调用 StandardServletEnvironment 的 customizePropertySources() 方法,进入 这里最后调用的父类就是 StandardEnvironment.customizePropertySources() 这个方法 综上所述,这里 new StandardServletEnvironment() 就是添加了几个属性源 configureEnvironment()回到 prepareEnvironment() 方法,再来看下面的 configureEnvironment() 这个方法 这里又调用了两个方法 configurePropertySources() 和 configureProfiles(), 分别看一下 configurePropertySources() 这里来看一下这个 SimpleCommandLinePropertySource 的构造 进入 parse() 方法 所以这个方法就是用来解析命令行的配置,然后添加到属性源中 configureProfiles() 这里这个集合的用处我们之后再说 ConfigurationPropertySources.attach继续往下看 prepareEnvironment 中的 ConfigurationPropertySources.attach, 点进去,如下: environmentPrepared上面的流程都执行完成之后,会发送一个 ApplicationEnvironmentPreparedEvent 事件,这里就会调用 Spring Boot 的广播器发送,具体逻辑我们在监听器的那里已经看过了, 最后会进入 SimpleApplicationEventMulticaster.multicastEvent() 这个方法发送 我们这里打个断点进去,如下 通过 getApplicationListeners() 这个方法,拿到所有对这个事件感兴趣的监听器然后依次调用他们的 onApplicationEvent() 方法 然后如图中,这里一共有 7 个监听器对这个事件感兴趣,我们这里重点关注第一个, 也就是 ConfigFileApplicationListener ,然后进去看一下他的 onApplicationEvent() 这个方法 ConfigFileApplicationListener 这里主要是通过 SpringFactoriesLoader 找到所有的 EnvironmentPostProcessor, 然后会把当前类 ConfigFileApplicationListener 也放到 EnvironmentPostProcessor 的集合中,因为 ConfigFileApplicationListener 也实现了 EnvironmentPostProcessor 这个接口 拿到之后做个排序,我们来看一下他一共有哪些 EnvironmentPostProcessor 这里一共找到了 5 个 EnvironmentPostProcessor 的实现,我们重点关注前面4个 SystemEnvironmentPropertySourceEnvironmentPostProcessor断点继续往下走,先看第一个 SystemEnvironmentPropertySourceEnvironmentPostProcessor 这个类就是把之前的命令行的数据源转换成了一个新的对象,然后把之前的替换掉,实际属性并没有新增或者删除 SpringApplicationJsonEnvironmentPostProcessor下一个是 SpringApplicationJsonEnvironmentPostProcessor 这里进入上面 JsonPropertyValue.get 这个方法看一下 这个类做的事情其实就是解析通过 SPRING_APPLICATION_JSON 这种方式指定的属性,最后添加到属性源中去 CloudFoundryVcapEnvironmentPostProcessor进入这个类看一下源码 这里是 Spring Cloud 环境中设置的一些属性,当前环境并不是 cloud 环境,所以不做详细分析 ConfigFileApplicationListener最后就是当前类 ConfigFileApplicationListener 这里首先添加了个随机数的属性源,然后创建了一个 Loader 对象,调用 load() 方法,我们点进去看一下 这部分是解析我们的配置文件,之后再详细分析 所以这个类做的事情就是添加随机数属性源,然后解析配置文件 bindToSpringApplication()回到 prepareEnvironment() 方法中,继续往下看一下 bindToSpringApplication() 这个方法 这个方法作用就是将环境变量中的 spring.main 绑定到 springApplication 这个类中 到这儿, prepareEnvironment() 这个方法做的事情我们就分析完了 ConfigurationClassParser之后我们再来看一个类 ConfigurationClassParser ,这个类的作用主要是用来解析 @PropertySource 配置的属性集,源码如下 这里就先大致的看一下,后面再去详细介绍 总结本文,我们了解了 Spring Boot 的属性加载, 分析了 prepareEnvironment() 这个方法,然后我们来做一个总结 getOrCreateEnvironment() 方法主要做的事情是: 添加 servletConfigInitParams 属性集 添加 servletContextInitParams 属性集 添加Jndi属性集 添加 systemProperties 属性集 添加 systemEnvironment 属性集 configureEnvironment() 方法主要做的事情是: 添加 defaultProperties 属性集 添加 commandLineArgs 属性集 listeners.environmentPrepared() 方法主要做的事情: 添加 SPRING_APPLICATION_JSON 属性集 添加 vcap 属性集,前提是在 Spring Cloud 的环境下 添加 random 属性集 添加 application-profile 属性集 ConfigurationPropertySources.attach() 方法做的事情: 添加 configurationProperties 属性集 ConfigurationClassParser 做的事情: 添加 @PropertySource 属性集]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-11-Spring Aware解析]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-11-Spring-Aware%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[述上文中,介绍了几种 Spring Boot 中的属性配置方式,那么配置完成之后,要在系统里面怎么获取,下面来介绍一种通过 Spring Aware 获取属性的方式 获取属性这里利用之前学过的启动加载器去打印要获取的属性, 这里还是创建一个类 EnvironmentApplicationRunner 然后去实现 ApplicationRunner ,然后再去实现一个 EnvironmentAware 用来获取 Environment 对象,代码如下:12345678910111213141516@Component@Slf4jpublic class EnvironmentApplicationRunner implements ApplicationRunner, EnvironmentAware &#123; private Environment environment; @Override public void run(ApplicationArguments args) throws Exception &#123; log.info(environment.getProperty(&quot;environment.test&quot;)); &#125; @Override public void setEnvironment(Environment environment) &#123; this.environment = environment; &#125;&#125; 之后运行,日志打印12020-02-14 15:15:44.427 INFO 32056 --- [ main] c.z.s.e.s.EnvironmentApplicationRunner : 30 这里已经成功获取到系统配置的属性了 Spring Aware在 Spring 框架中,Bean 是感知不到容器的存在的,当我们需要使用 Spring 容器的一些功能或者资源的时候,就需要用到 Spring Aware 来实现, 比如我们上面用的 EnvironmentAware 就可以用来获取 Environment 对象 常用的 Spring AwareSpirng 提供了一些常用的 Aware, 如下: 类名 作用 BeanNameAware 获取容器中的Bean名称 BeanClassLoaderAware 获得类加载器 BeanFactoryAware 获得BeanFactory EnvironmentAware 获取环境变量 EmbeddedValueResolverAware 获取容器加载的 properties 文件属性值 ResourceLoaderAware 获取资源加载器 ApplicationEventPublisherAware 获取系统广播器 MessageSourceAware 获得文本信息 ApplicationContextAware 获得当前应用上下文 原理解析为什么被Spring管理的类,只要实现了Aware接口,Spring就会自动把对应的对象注入进来,带着这个问题,我们来看一下源码. 首先,从我们熟悉的 refresh() 方法入手, 这里面的 prepareBeanFactory() 方法,这个方法具体的作用我们之前有写,这里就只看关于 Aware 的一些代码,如下: 对于 BeanPostProcessor 我们已经很熟悉了,然后我们进入这个类,看一下他里面的方法 然后再看一下这里调用的 invokeAwareInterface() 这个方法 这个类中,会处理一部分的 Aware ,那么这个方法是什么时机被调用的,在 doCreateBean() 这个方法后面会调用 initializeBean() 这个方法,进入看一下: 这里有先是一个 Aware 相关的方法 invokeAwareMethods() ,进去看一下 这个方法中,会处理一部分的 Aware , 然后就是 applyBeanPostProcessorsBeforeInitialization() 这个方法,进入看一下 这里就会执行 prepareBeanFactory() 方法中添加的那个 ApplicationContextAwareProcessor ,处理其他的 Aware Spring Aware 的加载流程大概就是这样 自定义Spring Aware了解了 Spring Aware 的实现原理,之后,我们可以自定义一个 Spring Aware, 首先仿照上面用到的 EnvironmentAware ,新建一个 MySpringAware继承 Aware 接口, 代码如下:12345public interface MySpringAware extends Aware &#123; void setAwareModel(AwareModel awareModel);&#125; 然后这里用到的 AwareModel 代码如下:1234567@Component@Datapublic class AwareModel &#123; private String name = &quot;defaultName&quot;;&#125; 然后我们需要一个 BeanPostProcessor 对应源码中的 ApplicationContextAwareProcessor,代码如下:123456789101112131415161718@Componentpublic class MyAwareProcessor implements BeanPostProcessor &#123; private final ConfigurableApplicationContext applicationContext; public MyAwareProcessor(ConfigurableApplicationContext applicationContext) &#123; this.applicationContext = applicationContext; &#125; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; if (bean instanceof MySpringAware) &#123; ((MySpringAware) bean).setAwareModel((AwareModel) applicationContext.getBean(&quot;awareModel&quot;)); &#125; return bean; &#125;&#125; 最后在我们开始的那个启动加载器里,实现我们自定义的 Aware 来测试一下是否生效123456789101112131415161718192021222324@Component@Slf4jpublic class EnvironmentApplicationRunner implements ApplicationRunner, EnvironmentAware, MySpringAware &#123; private Environment environment; private AwareModel awareModel; @Override public void run(ApplicationArguments args) throws Exception &#123; log.info(environment.getProperty(&quot;environment.test&quot;)); log.info(awareModel.getName()); &#125; @Override public void setEnvironment(Environment environment) &#123; this.environment = environment; &#125; @Override public void setAwareModel(AwareModel awareModel) &#123; this.awareModel = awareModel; &#125;&#125; 启动,日志输出如下:122020-02-14 17:17:38.894 INFO 22756 --- [ main] c.z.s.e.s.EnvironmentApplicationRunner : 342020-02-14 17:17:38.894 INFO 22756 --- [ main] c.z.s.e.s.EnvironmentApplicationRunner : defaultName 总结了解Spring Aware的用法,还有一些常用的 Spring Aware, 以及他们的工作原理,源码部分也比较简单 Aware加载流程图如下: 自定义Aware流程图如下:]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-10-属性配置]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-10-%E5%B1%9E%E6%80%A7%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[述在项目开发过程中,有很多工作是针对配置文件的,比如 Spring Boot 提供的一些属性,还有我们自定义的一些配置,下面就来了解一下 Spring Boot 中的属性配置的几种方式 命令行配置方式我们在服务器中手动启动一个jar包的时候,用的命令,比如是 java -jar xxx.jar ,在这个命令后面,我们可以通过 -- 的做一些属性配置, 比如说要修改端口号 java -jar xxx.jar --server.port=8888 ,这种方式就相当于是在 application 配置文件中指定端口号为 8888 ,在命令行方式启动 Spring Boot 应用时,连续的两个减号 -- 就是对 application 配置文件中的属性值进行赋值的标识. SPRING_APPLICATION_JSON类似命令行的配置方式, 在运行命令后添加 --SPRING_APPLICATION_JSON={\&quot;xxxx\&quot;:\&quot;xxx\&quot;} ,以json的格式配置,这种方式不是很常用 java:comp/env中的JNDI属性基本上不用,不做介绍 Java系统属性就是可以通过 System.getProperties() 获得的内容 操作系统的环境变量举个例子,我们系统中有 JAVA_HOME 这个环境变量, application 配置文件中做出如下配置:1java.home.path: #&#123;JAVA_HOME&#125; 然后系统启动之后,获取 java.home.path 的值就是,系统环境变量中的 JAVA_HOME 的值 通过random.*配置的随机属性application 配置文件如下:12environment: test: $&#123;random.int[30,40]&#125; 系统启动之后获取 environment.test 的值,就是一个 30-40 内的随机数 jar包外的{profile}配置文件位于当前应用jar包之外,针对不同 {profile} 环境的配置文件内容,例如: application-dev.properties 或是 application-dev.yml jar包内的{profile}配置文件位于当前应用jar包之内,针对不同 {profile} 环境的配置文件内容,例如: application-dev.properties 或是 application-dev.yml jar包外的application配置文件位于当前应用jar包之外的 application.properties 和 application.yml jar包内的application配置文件位于当前应用jar包之内的 application.properties 和 application.yml @PropertySource注解在 @Configuration 注解修改的类中,通过 @PropertySource 注解定义的属性 硬编码方式配置的属性通过 SpringApplication.setDefaultProperties 定义的内容 总结Spring Boot 中属性配置就以上几种方式,当然也可以通过我们之前的系统初始化器去配置. 以上几种方式的优先级如下: 命令行中传入的参数 SPRING_APPLICATION_JSON 中的属性 java:comp/env中的JNDI属性 Java的系统属性,可以通过 System.getProperties() 获得的内容. 操作系统的环境变量 通过 random.* 配置的随机属性 位于当前应用jar包之外，针对不同{profile}环境的配置文件内容 位于当前应用jar包之内，针对不同{profile}环境的配置文件内容 位于当前应用jar包之外的 application.properties 和YAML配置内容 位于当前应用jar包之内的 application.properties 和YAML配置内容 在 @Configuration 注解修改的类中, 通过 @PropertySource 注解定义的属性 应用默认属性,使用 SpringApplication.setDefaultProperties 定义的内容 以上几种方式数字越小优先级越高, 另外 properties 文件和 yml 文件同时存在的话,会使用 properties 文件中的配置]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-9-启动加载器]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-9-%E5%90%AF%E5%8A%A8%E5%8A%A0%E8%BD%BD%E5%99%A8%2F</url>
    <content type="text"><![CDATA[述在我们的日常开发中,可能需要在容器启动之后,立马做一些事情,这时候就可以借助 Spring Boot 的启动加载器去实现. 启动加载器启动加载器有两种方式实现,分别是实现 CommandLineRunner 和 ApplicationRunner ,然后重写里面的 run 方法 实现CommandLineRunner方式新建类 FirstCommandLineRunner 然后实现 CommandLineRunner ,代码如下:1234567891011@Component@Order(1)@Slf4jpublic class FirstCommandLineRunner implements CommandLineRunner &#123; @Override public void run(String... args) throws Exception &#123; log.info(&quot;First CommandLineRunner ...&quot;); &#125;&#125; 再搞一个 SecondCommandLineRunner 1234567891011@Component@Order(2)@Slf4jpublic class SecondCommandLineRunner implements CommandLineRunner &#123; @Override public void run(String... args) throws Exception &#123; log.info(&quot;Second CommandLineRunner ...&quot;); &#125;&#125; 然后启动项目, 控制台输出如下:1232020-02-13 18:15:55.765 INFO 32312 --- [ main] com.zhou.springboot.example.App : Started App in 3.444 seconds (JVM running for 8.238)2020-02-13 18:15:55.768 INFO 32312 --- [ main] c.z.s.e.startup.FirstCommandLineRunner : First CommandLineRunner ...2020-02-13 18:15:55.768 INFO 32312 --- [ main] c.z.s.e.startup.SecondCommandLineRunner : Second CommandLineRunner ... 可以看到这里的启动加载器已经正常执行,而且执行顺序只需要通过 @Order 注解来指定执行顺序即可 再来看下第二种方式 实现ApplicationRunner方式新建类 FirstApplicationRunner 实现 ApplicationRunner ,如下:1234567891011@Component@Slf4j@Order(1)public class FirstApplicationRunner implements ApplicationRunner &#123; @Override public void run(ApplicationArguments args) throws Exception &#123; log.info(&quot;first application runner...&quot;); &#125;&#125; 同样的再搞一个1234567891011@Component@Slf4j@Order(2)public class SecondApplicationRunner implements ApplicationRunner &#123; @Override public void run(ApplicationArguments args) throws Exception &#123; log.info(&quot;second application runner...&quot;); &#125;&#125; 同样的这种方式的执行顺序也是通过 @Order 指定的 执行顺序上面四个启动加载器 FirstApplicationRunner 和 FirstCommandLineRunner 的 Order 值都是1, 然后 SecondApplicationRunner 和 SecondCommandLineRunner 的 Order 值都是2, 启动项目看以下这四个的执行顺序 12342020-02-13 18:15:55.767 INFO 32312 --- [ main] c.z.s.e.startup.FirstApplicationRunner : first application runner...2020-02-13 18:15:55.768 INFO 32312 --- [ main] c.z.s.e.startup.FirstCommandLineRunner : First CommandLineRunner ...2020-02-13 18:15:55.768 INFO 32312 --- [ main] c.z.s.e.startup.SecondApplicationRunner : second application runner...2020-02-13 18:15:55.768 INFO 32312 --- [ main] c.z.s.e.startup.SecondCommandLineRunner : Second CommandLineRunner ... 这里可以看到,在 Order 值相同的情况下, ApplicationRunner 要优先于 CommandLineRunner 执行 原因是什么,下面来看一下启动加载器的源码 源码跟踪还是先进入启动类的run方法,如下: 然后再进入 callRunners() 方法 具体调用源码就是这样,比较简单 总结实现启动加载器的两种方式: 实现 ApplicationRunner 接口,重写 run() 方法 实现 CommandLineRunner 接口,重写 run() 方法 执行优先级 在 Order 值相同的情况下, ApplicationRunner 要优先于 CommandLineRunner 执行]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-8-Banner解析]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-8-Banner%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[述上文中,分析了Spring容器的 refresh 流程,还有 Bean 的实例化流程,本文来了解一下 Spring Boot 中的 Banner 的解析,还有他的原理 自定义Banner Spring Boot 默认的 Banner 图如下: 1234567 . ____ _ __ _ _ /\\ / ___&apos;_ __ _ _(_)_ __ __ _ \ \ \ \( ( )\___ | &apos;_ | &apos;_| | &apos;_ \/ _` | \ \ \ \ \\/ ___)| |_)| | | | | || (_| | ) ) ) ) &apos; |____| .__|_| |_|_| |_\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.2.4.RELEASE) 这里就是一个 Spring 的字符画,加上Spring Boot的版本号, 我们可以把这个图换成我们自定义的一些文字或者图片 文字Banner12345678910111213141516171819// .::::.// .::::::::.// :::::::::::// ..:::::::::::&apos;// &apos;::::::::::::&apos;// .::::::::::// &apos;::::::::::::::..// ..::::::::::::.// ``::::::::::::::::// ::::``:::::::::&apos; .:::.// ::::&apos; &apos;:::::&apos; .::::::::.// .::::&apos; :::: .:::::::&apos;::::.// .:::&apos; ::::: .:::::::::&apos; &apos;:::::.// .::&apos; :::::.:::::::::&apos; &apos;:::::.// .::&apos; ::::::::::::::&apos; ``::::.// ...::: ::::::::::::&apos; ``::.// ````&apos;:. &apos;:::::::::&apos; ::::..// &apos;.:::::&apos; &apos;:&apos;````..// 比如我们要在项目中,把Banner换成上面这个,只需要在项目的 Resource 目录下新建一个 Banner.txt , 然后复制进去就可以了 这里名称默认的就是 Banner.txt, 如果名称不是这个的话, 比如是 MyBanner.txt ,就需要在 application 配置文件中做以下设置123spring: banner: location: MyBanner.txt 图片bannerSpring Boot 支持 gif,jpg,png 这三种格式的图片banner, 打印的时候会将图片转换成一些字符画来输出, 设置方法和文字的banner的是一样的,默认的图片名称也是 Banner ,然后如果名称不是默认名称的话,也是通过修改 application 配置文件指定,配置如下:1234spring: banner: image: location: xxx.jpg 对于图片banner,还有一些其他的设置,比如图片的长宽等,都是通过 spring.banner.image.* 来配置的 兜底banner兜底banner,就是上面两种方式都找不到的情况下,会输出的,这个需要通过硬编码的方式设置, 修改项目启动类,如下:12345678@SpringBootApplicationpublic class App &#123; public static void main(String[] args) &#123; SpringApplication springApplication = new SpringApplication(App.class); springApplication.setBanner(new ResourceBanner(new ClassPathResource(&quot;,MyBanner.txt&quot;))); springApplication.run(args); &#125;&#125; 这里通过 springApplication.setBanner() 这个方法去设置 关闭banner如果不想输出任何的banner,也可以通过配置关闭, application 配置文件中配置如下:123spring: main: banner-mode: off 这里可以配置的值,一共是三种: off: 关闭banner console: 控制台打印 log: 打印到日志文件 banner解析原理了解了如何替换banner之后,来了解以下他的原理,banner是如何被 Spring Boot 识别并且打印的 首先进入run方法,找到打印banner的方法,如下: 点进去 这里不管是log方式还是console的方式,调用的方法其实都是一样的,只是传入的参数不一样,继续点进去往下看 这里分为两步,先获取,然后打印 获取Banner进入 getBanner() 方法,如下: 然后图片和文字banner的获取如下: 打印Banner上面一步,获取到banner之后,就调用他的打印方法 这里可能获取到的是我们配置的banner,也可能是 Spring Boot 的默认的banner,也就是 SpringBootBanner 这个类,先来看一下默认的,点进去 然后进入默认的 SpringBootBanner 然后再来看一下文字类型的 banner 的打印, 进入 ResourceBanner 如下: 最后是图片类型的 banner ,进入 ImageBanner 总结到这儿,就了解了banner的自定义配置,以及他们是如何被解析出来并打印的,这部分比较简单,用处其实也不大,大致了解了就可以了]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-7-Bean实例化流程]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-7-Bean%E5%AE%9E%E4%BE%8B%E5%8C%96%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[述上文中大致了解了Spring容器的refresh的过程,其中 finishBeanFactoryInitialization() 方法没有详细说明, 这个方法就是 Spring 容器实例化 Bean 的方法, 下面来详细看一下这个方法 Spring容器Bean实例化流程首先,进入 finishBeanFactoryInitialization() 方法 继续往下, 进入 preInstantiateSingletons() 方法 初始化Beanbean初始化调用的是 AbstractBeanFactory.getBean(),最终调用的是 AbstractBeanFactory.doGetBean() 进入看下源码: 前面都是一系列的检查,然后最后判断一下是否是单例,然后调用 getSingleton() 方法,进入这个方法看一下 返回上一个方法,进入 AbstractAutowireCapableBeanFactory.createBean() 方法 我们先进入 resolveBeforeInstantiation() 这个方法,看一下他具体是怎么处理的 这个方法是实例化之前的处理, 给 InstantiationAwareBeanPostProcessor 一个机会返回代理对象来替代真正的 bean 实例,从而跳过 Spring 默认的实例化过程, 达到”短路”效果. 这里会执行 InstantiationAwareBeanPostProcessor 的 postProcessBeforeInstantiation 方法,该方法可以返回 bean 实例的代理,从而跳过 Spring 默认的实例化过程,具体代码在 applyBeanPostProcessorsBeforeInstantiation()中, 如下: 然后这个方法如果返回了bean,就会进入 applyBeanPostProcessorsAfterInitialization() 方法做一些后置处理,如下: 在这个过程中,如果没有返回Bean的实例,那么就会走 Spring 的默认的实例化流程,也就是上面 AbstractAutowireCapableBeanFactory.doCreateBean() 方法 主要部分就是这些,后面是处理一些 @Autowired 注解注入的值,还有一些后置处理器等等 总结Spring 默认创建Bean实例的流程图: 短路操作流程图:]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-6-refresh方法解析]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-6-refresh%E6%96%B9%E6%B3%95%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[述前面介绍了一些Spring容器在初始化时的系统初始化器以及监听器的加载等,Spring容器创建之后,会调用它的refresh方法,refresh的时候会做很多事情:比如完成配置类的解析.各种 BeanFactoryPostProcessor 和 BeanPostProcessor 的注册、国际化配置的初始化、web内置容器的构造等等. 本文来看一下refresh方法的具体源码,看下他具体做了哪些事情 源码跟踪首先,run方法点进去: 然后找到 refreshContext() 方法,继续往下 这里就是 refresh 方法做的所有的事情,这里面一共调用了 12 个方法,而且是一个同步的代码块,也就是只能有一个线程调用,下面一个一个的看一下他们的具体作用 prepareRefresh()代码如下: 这个方法做的事情主要就是: 配置启动开始时间 设置容器状态是active 初始化属性源信息(Property) 校验环境变量中的必填字段 obtainFreshBeanFactory()源码如下: 这个方法比较简单,先看一下 refreshBeanFactory() 方法,如下: 然后是 getBeanFactory() 这里默认返回的BeanFactory就是 DefaultListableBeanFactory prepareBeanFactory() 这里主要是对上面获取到的 BeanFactory 做一些设置,主要如下: 设置classloader(用于加载bean),设置表达式解析器(解析bean定义中的一些表达式),添加属性编辑注册器(注册属性编辑器) 添加 ApplicationContextAwareProcessor 这个后置处理器 取消 EnvironmentAware,EmbeddedValueResolverAware,ResourceLoaderAware,ApplicationEventPublisherAware,MessageSourceAware,ApplicationContextAware 这几个接口的自动注入, 这几个接口会交给 ApplicationContextAwareProcessor 去处理 设置特殊类型对应的bean, BeanFactory 对应上一步获取到的 beanFactory, 然后 ResourceLoader,ApplicationEventPublisher,ApplicationContext 这三个接口对应的bean都设置为当前的Spirng容器 最后会注入一些其他信息的bean,比如 environment , systemProperties 等 postProcessBeanFactory()这个方法的作用主要是 BeanFactory 设置之后再进行后续的一些 BeanFactory 操作, 这个方法在通常情况下是一个空的实现,会交给子类来实现,比如我们项目中添加了web环境的依赖,那就会交给 web 环境的实现类去实现,如下: 再来看一下父类的 postProcessBeanFactory 方法 这里就是对web环境的一些设置,注册一些web环境特有的作用域等 所以这个方法的主要作用就是: 交给子类重写,以在 BeanFactory 完成创建后做进一步的设置 invokeBeanFactoryPostProcessors() 这里获取所有的 BeanFactoryPostProcessors 的实现,然后交给 PostProcessorRegistrationDelegate 执行他的 invokeBeanFactoryPostProcessors() 方法 这里需要知道以下两个接口: BeanFactoryPostProcessor: 这个接口的作用是对Spring容器中已经存在的Bean做修改, 使用上面获取的 BeanFactory 也就是 DefaultListableBeanFactory 对 Bean 进行处理 BeanDefinitionRegistryPostProcessor: 这个接口继承自 BeanFactoryPostProcessor ,作用跟 BeanFactoryPostProcessor 是一样的,他是使用的 BeanDefinitionRegistry 对 Bean 进行处理 然后进入 PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors() 方法看一下,这个方法比较长, 分开每个部分看: 第一部分 第二部分 第三部分 这里这个循环,主要是防止 BeanDefinitionRegistryPostProcessor 的实现类中,又注入了一个 BeanDefinitionRegistryPostProcessor 的实现,所以就一直循环,然后直到所有的 BeanDefinitionRegistryPostProcessor 都处理完成 流程图前面这几部分的流程图如下: 第一部分: 第二部分: 第三部分: 继续往下看,这个方法的源码 第四部分 这里和上面的逻辑基本是一样的,最后会清空一个这个方法产生的缓存,然后这个方法就结束了 总结总结一下这个方法的作用: 调用 BeanDefinitionRegistryPostProcessor 的实现,向容器内添加 Bean 的定义 调用 BeanFactoryPostProcessor 的实现,向容器内 Bean 的定义添加属性 registerBeanPostProcessors()代码如下: 这里调用的是 PostProcessorRegistrationDelegate 的 registerBeanPostProcessors() 方法, 然后看一下具体的实现: 大致流程和上个方法是一样的 先找出实现了 PriorityOrdered 接口的 BeanPostProcessor 并排序后加到 BeanFactory 的 BeanPostProcessor 集合中 找出实现了 Ordered 接口的 BeanPostProcessor 并排序后加到 BeanFactory 的 BeanPostProcessor 集合中 没有实现 PriorityOrdered 和 Ordered 接口的 BeanPostProcessor 加到 BeanFactory 的 BeanPostProcessor 集合中 这个方法主要作用就是: 找到所有的 BeanPostProcessor 的实现 排序后注册到容器中 initMessageSource()代码如下: 这个方法的作用就是: 在Spring容器中初始化一些国际化相关的属性 initApplicationEventMulticaster()代码如下: 这方法的作用是: 初始化事件广播器,注册到容器中 onRefresh()代码如下: 这里是一个空的模板方法,具体的会交给子类去实现,比如我们的web环境中,就会交给子类 ServletWebServerApplicationContext 去实现,如下: web环境中的 onRefresh() 方法会创建一个 web 容器,比如tomcat registerListeners()代码如下: 这里的 earlyApplicationEvents 这个集合,就是在我们容器的事件派发机制还没有完善的时候发出去的事件,都会存放在这个集合里面,然后在这一步都推送出去 所以这个类的作用就是: 把Spring容器内的事件监听器和 BeanFactory 中的事件监听器都添加到事件广播器中 派发早期事件 finishBeanFactoryInitialization()这个方法的作用是: 实例化 BeanFactory 中已经被注册但是未实例化的所有实例(懒加载的不需要实例化),比如 invokeBeanFactoryPostProcessors 方法中根据各种注解解析出来的类,在这个时候都会被初始化. 实例化的过程各种BeanPostProcessor开始起作用 这个方法的源码放在下一章详细说明 finishRefresh()代码如下: 这个方法的主要作用如下: 初始化生命周期处理器 调用生命周期处理器的 onRefresh() 方法 发布 ContextRefreshedEvent 事件 JMX相关处理 总结Spring 容器的 refresh 过程就是上述11个方法,内容比较多,每个方法都是大致写了一下流程,可以建个项目debug跟着走一下,会更清晰]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-5-自定义监听器]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-5-%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9B%91%E5%90%AC%E5%99%A8%2F</url>
    <content type="text"><![CDATA[述了解了监听器的工作原理之后,我们来看一下自定义的监听器如何实现 监听器的实现跟之前的系统初始化器的实现基本是一致的 第一种方式创建监听器,实现 ApplicationListener ,泛型是些对应的感兴趣的事件,如下:12345678910@Order(1)@Slf4jpublic class FirstListener implements ApplicationListener&lt;ApplicationStartedEvent&gt; &#123; @Override public void onApplicationEvent(ApplicationStartedEvent event) &#123; log.info(&quot;监听到:&#123;&#125;&quot;, event.getClass().getName()); &#125;&#125; 和系统初始化器一样,也需要在 spring.factories 中配置,如下:1org.springframework.context.ApplicationListener=com.zhou.springboot.example.listener.FirstListener 第二种方式123456789@Order(2)@Slf4jpublic class SecondListener implements ApplicationListener&lt;ApplicationStartedEvent&gt; &#123; @Override public void onApplicationEvent(ApplicationStartedEvent event) &#123; log.info(&quot;监听到:&#123;&#125;&quot;, event.getClass().getName()); &#125;&#125; 修改启动类,如下:12345678910@SpringBootApplicationpublic class App &#123; public static void main(String[] args) &#123; SpringApplication springApplication = new SpringApplication(App.class); springApplication.addInitializers(new SecondInitializer()); springApplication.addListeners(new SecondListener()); springApplication.run(args); &#125;&#125; 第三种方式12345678910@Order(3)@Slf4jpublic class ThirdListener implements ApplicationListener&lt;ApplicationStartedEvent&gt; &#123; @Override public void onApplicationEvent(ApplicationStartedEvent event) &#123; log.info(&quot;监听到:&#123;&#125;&quot;, event.getClass().getName()); &#125;&#125; 在 application 配置文件中配置123context: listener: classes: com.zhou.springboot.example.listener.ThirdListener 第四种方式前面三种方式都是和系统初始化器的配置方法一样的,监听器还有一种实现方式如下: 123456789101112131415@Order(4)@Slf4jpublic class FourthListener implements SmartApplicationListener &#123; @Override public boolean supportsEventType(Class&lt;? extends ApplicationEvent&gt; eventType) &#123; return ApplicationStartedEvent.class.isAssignableFrom(eventType) || ApplicationPreparedEvent.class.isAssignableFrom(eventType); &#125; @Override public void onApplicationEvent(ApplicationEvent event) &#123; log.info(&quot;监听到:&#123;&#125;&quot;, event.getClass().getName()); &#125;&#125; 这里是实现 SmartApplicationListener 接口, 在 supportsEventType() 方法中,配置自己感兴趣的事件,然后下面的 onApplicationEvent() 是具体的处理方法 同样的,这个监听器也是需要加到配置中去的,上面三种方式随便哪一种都行, 比如加到 application 配置文件中去:123context: listener: classes: com.zhou.springboot.example.listener.ThirdListener,com.zhou.springboot.example.listener.FourthListener 运行测试启动项目,日志如下:1234567891011121314151617181920212223242526 . ____ _ __ _ _ /\\ / ___&apos;_ __ _ _(_)_ __ __ _ \ \ \ \( ( )\___ | &apos;_ | &apos;_| | &apos;_ \/ _` | \ \ \ \ \\/ ___)| |_)| | | | | || (_| | ) ) ) ) &apos; |____| .__|_| |_|_| |_\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.2.4.RELEASE)2020-02-03 17:27:35.225 INFO 8832 --- [ main] c.z.s.e.initializers.ThirdInitializer : ThirdInitializer done...2020-02-03 17:27:35.232 INFO 8832 --- [ main] c.z.s.e.initializers.FirstInitializer : FirstInitializer done...2020-02-03 17:27:35.232 INFO 8832 --- [ main] c.z.s.e.initializers.SecondInitializer : SecondInitializer done...2020-02-03 17:27:35.258 INFO 8832 --- [ main] com.zhou.springboot.example.App : Starting App on DESKTOP-B1V47DV with PID 8832 (D:\Demo\SpringBoot\spring-boot-example\build\classes\java\main started by Administrator in D:\Demo\SpringBoot\spring-boot-example)2020-02-03 17:27:35.260 INFO 8832 --- [ main] com.zhou.springboot.example.App : No active profile set, falling back to default profiles: default2020-02-03 17:27:35.368 INFO 8832 --- [ main] c.z.s.example.listener.FourthListener : 监听到:org.springframework.boot.context.event.ApplicationPreparedEvent2020-02-03 17:27:37.582 INFO 8832 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 8080 (http)2020-02-03 17:27:37.609 INFO 8832 --- [ main] o.apache.catalina.core.StandardService : Starting service [Tomcat]2020-02-03 17:27:37.610 INFO 8832 --- [ main] org.apache.catalina.core.StandardEngine : Starting Servlet engine: [Apache Tomcat/9.0.30]2020-02-03 17:27:37.930 INFO 8832 --- [ main] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext2020-02-03 17:27:37.930 INFO 8832 --- [ main] o.s.web.context.ContextLoader : Root WebApplicationContext: initialization completed in 2559 ms2020-02-03 17:27:38.360 INFO 8832 --- [ main] o.s.s.concurrent.ThreadPoolTaskExecutor : Initializing ExecutorService &apos;applicationTaskExecutor&apos;2020-02-03 17:27:38.771 INFO 8832 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 8080 (http) with context path &apos;&apos;2020-02-03 17:27:38.779 INFO 8832 --- [ main] com.zhou.springboot.example.App : Started App in 4.564 seconds (JVM running for 9.864)2020-02-03 17:27:38.780 INFO 8832 --- [ main] c.z.s.example.listener.ThirdListener : 监听到:org.springframework.boot.context.event.ApplicationStartedEvent2020-02-03 17:27:38.780 INFO 8832 --- [ main] c.z.s.example.listener.FourthListener : 监听到:org.springframework.boot.context.event.ApplicationStartedEvent2020-02-03 17:27:38.781 INFO 8832 --- [ main] c.z.s.example.listener.FirstListener : 监听到:org.springframework.boot.context.event.ApplicationStartedEvent2020-02-03 17:27:38.781 INFO 8832 --- [ main] c.z.s.example.listener.SecondListener : 监听到:org.springframework.boot.context.event.ApplicationStartedEvent 这里可以看到 FourthListener 执行了两次,因为我们上面配置了两种事件, 然后同样的 application 配置文件指定的要优先于其他方式,原理和系统初始化器一样 总结自定义监听器的三种实现方式: 定义在 spring.factories 中,被 SpringFactoriesLoader 发现注册. 在 SpringApplication 初始化完毕之后手动添加 定义成环境变量,被 DelegatingApplicationListener 发现注册]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-4-监听器实现]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-4-%E7%9B%91%E5%90%AC%E5%99%A8%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[述上文中用一个简单的案例,来了解了一下监听器的实现,下面再来看一下监听器在Spring Boot框架中的实现,围绕以下四个方面来看以下: 事件 监听器 广播器 触发机制 监听器ApplicationListener 这个类,就是系统监听器的实现,代码如下: 这里就是一个函数式接口,然后这里有一个继承自 ApplicationEvent 的泛型,可以在声明的时候,表示对哪种事件感兴趣,然后里面的一个方法就是,监听到感兴趣的事件时,要做的事情. 这个接口跟我们之前demo中的 WeatherListener 是类似的 广播器ApplicationEventMulticaster 是系统广播器的接口, 看下源码: 这里的几个方法,其实就是添加监听器,移除监听器,广播事件这三种,和我们demo中的 EventMulticaster 也是对应的 事件Spring Boot 的系统事件,类图如下: 这里一共有 7 个事件,作用分别如下: ApplicationStartingEvent: ApplicationStartingEvent 是 SpringBoot 启动开始的时候执行的事件,在该事件中可以获取到 SpringApplication 对象,可做一些执行前的设置. ApplicationEnvironmentPreparedEvent: 表示系统环境变量已经创建完成,但是上下文 context 还没有创建 ApplicationContextInitializedEvent: 表示 ApplicationContext 上下文已经准备好 ApplicationPreparedEvent: 表示 SpringBoot上下文 context 创建完成,但此时 Spring 中的 bean 还没有完全加载完成.这里可以将上下文传递出去做一些额外的操作,但是在该监听器中是无法获取自定义 bean 并进行操作的. ApplicationStartedEvent: 表示所有的bean已经准备完成,但是还没有调用 ApplicationRunner 和 CommandLineRunner ApplicationReadyEvent: ApplicationRunner 和 CommandLineRunner 已经完成调用了,也意味着 SpringBoot 加载已经完成. ApplicationFailedEvent: 容器在启动的时候发生异常的时候发送 事件发发送顺序ApplicationStartingEvent -&gt; ApplicationEnvironmentPreparedEvent -&gt; ApplicationContextInitializedEvent -&gt; ApplicationPreparedEvent -&gt; ApplicationStartedEvent -&gt; ApplicationReadyEvent 监听器注册Spring Boot中提供了很多监听器,下面来看一下这些监听器是如何被注册到容器中的 类似之前的系统初始化器,我们来看一下源码, 如下: 这里其实和之前的系统初始化器的加载方式是一样的,都是通过 SpringFactoriesLoader 来加载的,点进去看一下源码: 这里和之前的系统初始化器是一样的,就不再详细介绍了 触发机制上面了解了事件,监听器,广播器,然后四个要素中还有触发机制,这些事件是怎么被发送的,下面来详细看一下 以 starting 事件为例,这个是在框架启动,也就是调用run方法的时候,进入run方法看一下源码,如下: 这里会把所有的监听器都封装到一个 SpringApplicationRunListeners 对象中,这个其实就是 SpringApplicationRunListener 对象的一个集合进入这个 starting 方法看一下,如下: 可以看到,这里是去调用的 SpringApplicationRunListener 的 starting() 方法,继续往下一级去看,进入 SpringApplicationRunListener 如下: 这里其实就是对所有系统事件的一个封装,在我们之前的demo中,我们是手动创建广播器,然后创建事件发送, 那这里其实就是对这一步进行了一个封装, Spring 容器在运行过程中是不需要构建具体的事件,然后再调用广播器广播, 都是通过这个方法完成的, 以 starting() 方法为例, 我们进入子类看一下: 这里首先找到了广播器,然后发送了一个 ApplicationStartingEvent 事件 案例改造以之前的天气案例为基础,我们也将代码改造成 Spring Boot 这种方式来试一下 首先,创建一个 WeatherRunListener 对应上面的 SpringApplicationRunListener, 代码如下:1234567891011121314151617181920@Componentpublic class WeatherRunListener &#123; @Autowired private WeatherEventMulticaster weatherEventMulticaster; /** * 发送下雪事件 */ public void snow()&#123; weatherEventMulticaster.multicastEvent(new SnowEvent()); &#125; /** * 发送下雨事件 */ public void rain()&#123; weatherEventMulticaster.multicastEvent(new RainEvent()); &#125;&#125; 这里需要给 WeatherEventMulticaster 加上 @Component 注解, 同样的 RainListener 和 SnowListener 也需要加上 @Component 注解然后修改 AbstractEventMulticaster , 如下:12345/** * 监听器的集合 */@Autowiredprivate List&lt;WeatherListener&gt; listeners; 这里只修改存放 WeatherListener 的集合即可, RainListener 和 SnowListener 已经交给了Spring容器去管理,所以这里直接让Spring容器去注入就好了 然后写个单元测试:12345@Testpublic void test1()&#123; weatherRunListener.rain(); weatherRunListener.snow();&#125; 执行,控制台输出如下:1234562020-02-02 18:28:21.830 INFO 16736 --- [ Test worker] c.z.s.e.m.WeatherEventMulticaster : begin broadcast weather event...2020-02-02 18:28:21.831 INFO 16736 --- [ Test worker] c.z.s.example.listener.RainListener : Its Rain2020-02-02 18:28:21.835 INFO 16736 --- [ Test worker] c.z.s.e.m.WeatherEventMulticaster : end broadcast weather event ...2020-02-02 18:28:21.835 INFO 16736 --- [ Test worker] c.z.s.e.m.WeatherEventMulticaster : begin broadcast weather event...2020-02-02 18:28:21.836 INFO 16736 --- [ Test worker] c.z.s.example.listener.SnowListener : Its snow2020-02-02 18:28:21.836 INFO 16736 --- [ Test worker] c.z.s.e.m.WeatherEventMulticaster : end broadcast weather event ... 这种方式就和 Spring Boot 发送事件的方式一样,做了一次封装 事件广播源码还是以 starting 方法为例,看一下广播器具体是怎么发送事件的 这里点击进入广播器的发送方法 这里的重点是 getApplicationListeners 这个方法,获取对当前事件感兴趣的监听器的集合,点进去往下看 这里主要是从缓存找,然后找不到的话再去计算,计算完成后放入缓存中,供下次使用,再进入具体计算的方法 进入 supportsEvent() 继续看 这里做了一次转换,如果不是 就做个适配 new GenericApplicationListenerAdapter(listener) ,把监听器都转成 GenericApplicationListener 然后做判断 总之这里是要转成一个 GenericApplicationListener 然后再做判断 这里打个断点走一下会更清晰 总结 了解监听器是如何注册到 Spring 容器中的 Spring Boot 中的事件发送机制 如何获取事件的所有感兴趣的监听器]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-3-监听器案例]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-3-%E7%9B%91%E5%90%AC%E5%99%A8%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[述前文中,了解了Spring Boot的系统初始化器,本文再来看一下监听器,监听器是一个专门用于对其他对象身上发生的事件或状态改变进行监听和相应处理的对象,当被监视的对象发生情况时,立即采取相应的行动. 下面用一个小案例来看一下监听器的具体使用 案例以一个天气事件为案例,我们一共需要三个元素: 事件 监听器 广播 广播把事件广播出去,然后对应的监听器处理自己感兴趣的事件即可 事件首先来看一下事件部分的编码,首先创建一个获取天气的抽象类,代码如下:123456789public abstract class WeatherEvent &#123; /** * 获取天气 * @return */ public abstract String getWeather();&#125; 然后写两个实现的子类, 如下:12345678public class RainEvent extends WeatherEvent &#123; @Override public String getWeather() &#123; return &quot;Rain&quot;; &#125;&#125; 12345678public class SnowEvent extends WeatherEvent &#123; @Override public String getWeather() &#123; return &quot;snow&quot;; &#125;&#125; 事件部分就是这样,然后看一下监听器的编码 监听器先写一个接口,然后不同的监听器做对应的实现12345public interface WeatherListener &#123; void onWeatherListener(WeatherEvent event);&#125; 然后两个实现类如下: 1234567891011@Slf4jpublic class RainListener implements WeatherListener &#123; @Override public void onWeatherListener(WeatherEvent event) &#123; if (event instanceof RainEvent) &#123; log.info(&quot;Its &#123;&#125;&quot;, event.getWeather()); &#125; &#125;&#125; 1234567891011@Slf4jpublic class SnowListener implements WeatherListener &#123; @Override public void onWeatherListener(WeatherEvent event) &#123; if (event instanceof SnowEvent) &#123; log.info(&quot;Its &#123;&#125;&quot;, event.getWeather()); &#125; &#125;&#125; 广播广播部分的实现如下,首先是一个接口:123456789101112131415161718192021public interface EventMulticaster &#123; /** * 广播事件 * @param event */ void multicastEvent(WeatherEvent event); /** * 添加监听器 * @param listener */ void addListener(WeatherListener listener); /** * 移除监听器 * @param listener */ void removeListener(WeatherListener listener);&#125; 然后写一个抽象的实现 12345678910111213141516171819202122232425262728293031323334public abstract class AbstractEventMulticaster implements EventMulticaster&#123; /** * 监听器的集合 */ private List&lt;WeatherListener&gt; listeners = new ArrayList&lt;&gt;(); @Override public void multicastEvent(WeatherEvent event) &#123; doStart(); listeners.forEach(x -&gt; x.onWeatherListener(event)); doEnd(); &#125; @Override public void addListener(WeatherListener listener) &#123; listeners.add(listener); &#125; @Override public void removeListener(WeatherListener listener) &#123; listeners.remove(listener); &#125; /** * 模板方法,广播开始方法 */ protected abstract void doStart(); /** * 模板方法,广播结束方法 */ protected abstract void doEnd();&#125; 最后是模板方法的实现:1234567891011121314@Slf4jpublic class WeatherEventMulticaster extends AbstractEventMulticaster &#123; @Override protected void doStart() &#123; log.info(&quot;begin broadcast weather event...&quot;); &#125; @Override protected void doEnd() &#123; log.info(&quot;end broadcast weather event ...&quot;); &#125;&#125; 测试写个测试类,如下:12345678910111213141516171819202122232425262728@SpringBootTestpublic class WeatherEventTests &#123; @Test public void test() &#123; // 创建广播 WeatherEventMulticaster multicaster = new WeatherEventMulticaster(); // 创建监听器 RainListener rainListener = new RainListener(); SnowListener snowListener = new SnowListener(); // 添加监听器 multicaster.addListener(rainListener); multicaster.addListener(snowListener); // 发送事件 multicaster.multicastEvent(new RainEvent()); multicaster.multicastEvent(new SnowEvent()); // 移除下雨监听器 multicaster.removeListener(rainListener); // 发送事件 multicaster.multicastEvent(new RainEvent()); multicaster.multicastEvent(new SnowEvent()); &#125;&#125; 运行,控制台输出如下:12345678910112020-02-01 18:11:17.945 INFO 17112 --- [ Test worker] c.z.s.e.m.WeatherEventMulticaster : begin broadcast weather event...2020-02-01 18:11:17.946 INFO 17112 --- [ Test worker] c.z.s.example.listener.RainListener : Its Rain2020-02-01 18:11:17.949 INFO 17112 --- [ Test worker] c.z.s.e.m.WeatherEventMulticaster : end broadcast weather event ...2020-02-01 18:11:17.950 INFO 17112 --- [ Test worker] c.z.s.e.m.WeatherEventMulticaster : begin broadcast weather event...2020-02-01 18:11:17.950 INFO 17112 --- [ Test worker] c.z.s.example.listener.SnowListener : Its snow2020-02-01 18:11:17.950 INFO 17112 --- [ Test worker] c.z.s.e.m.WeatherEventMulticaster : end broadcast weather event ...2020-02-01 18:11:17.950 INFO 17112 --- [ Test worker] c.z.s.e.m.WeatherEventMulticaster : begin broadcast weather event...2020-02-01 18:11:17.951 INFO 17112 --- [ Test worker] c.z.s.e.m.WeatherEventMulticaster : end broadcast weather event ...2020-02-01 18:11:17.951 INFO 17112 --- [ Test worker] c.z.s.e.m.WeatherEventMulticaster : begin broadcast weather event...2020-02-01 18:11:17.951 INFO 17112 --- [ Test worker] c.z.s.example.listener.SnowListener : Its snow2020-02-01 18:11:17.951 INFO 17112 --- [ Test worker] c.z.s.e.m.WeatherEventMulticaster : end broadcast weather event ... 总结以一个案例了解一下监听器, 重点就是以下几个要素 事件 监听器 广播器 触发机制]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-2-系统初始化器解析原理]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-2-%E7%B3%BB%E7%BB%9F%E5%88%9D%E5%A7%8B%E5%8C%96%E5%99%A8%E8%A7%A3%E6%9E%90%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[述上文中了解了通过 spring.factories 指定系统初始化器的加载原理,最后就是把所有的系统初始化器都创建了实例,然后通过 setInitializers() 方法放到了 SpringApplication 对象中,然后在 SpringApplication.run() 框架启动的时候,会执行集合中的这些系统初始化器,下面看一下具体是如何调用的 相关框架启动流程图我们的初始化器,都是重写了一个 initialize() 方法,那么这个方法是在哪里被调用的,如下图: 系统初始化器的解析,在刷新上下文之前执行,下面跟踪源码看一下具体实现 源码跟踪从启动类的run方法一步一步点进去看,如下: 这里先不管其他方法,只看刷新上下文之前的方法,点进去看一下: 点进去这个方法,源码如下: 这里首先通过 getInitializers() 方法,拿到所有的初始化器,然后循环判断,最后调用每个实例的 initialize() 方法 以上就是我们之前的第一种方式实现的系统初始化器的加载和解析原理,我们之前一共使用了三种系统初始化器,那么下面再来看下后面的两种的加载方式 硬编码方式加载原理第二种系统初始化器的实现,就是在启动类中,添加了一行代码 springApplication.addInitializers(new SecondInitializer()); ,这个比较简单,点击 addInitializers 方法,看下源码,如下: 很简单,就是拿到集合然后添加进去就完成了 application 配置文件加载原理再来看一下最后一种方式, 这里主要用到的一个类,是 DelegatingApplicationContextInitializer ,这个类其实也是一个系统初始化器,也是实现了 ApplicationContextInitializer&lt;ConfigurableApplicationContext&gt; 这个接口 然后这个类,其实是在 Spring Boot 一个jar包中的 spring.factories 中定义的,如下: 所以这个类,也是会被之前说的 SpringFactoriesLoader 去扫描到,然后来看一下这个类的源码 这里要注意的地方是,他实现了order接口,然后定义的order值是0,这也就解释了为什么通过 application.yml 配置的系统初始化器会被先执行然后上面的代码主要就是从配置的 context.initializer.classes 中,找到所有指定的类,然后创建实例,最后调用 applyInitializerClasses() 方法,代码如下: 这里拿到实例之后,最后循环调用实例的 initialize() 方法 总结系统初始化器实现的三种方式: 定义在 spring.factories 中,被 SpringFactoriesLoader 发现注册. 在 SpringApplication 初始化完毕之后手动添加 定义成环境变量,被 DelegatingApplicationContextInitializer 发现注册 作用: 在框架启动的过程中,向 Spring Boot 容器中配置一些属性]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-1-系统初始化器加载原理]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-1-%E7%B3%BB%E7%BB%9F%E5%88%9D%E5%A7%8B%E5%8C%96%E5%99%A8%E5%8A%A0%E8%BD%BD%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[述上文中,介绍了三种系统初始化器的使用,那么本文来看一下,系统初始化器是如何被Spring Boot容器加载的 框架初始化步骤图 Spring Boot 启动是分为两步,先是初始化框架,然后再启动,系统初始化器的加载如何,就再框架初始化的步骤中,下面来看一下代码 源码跟踪首先通过启动类,点击进入源码,然后找到创建 SpringApplication 对象的代码,如下: 这里重点看 setInitializers() 里面的 getSpringFactoriesInstances() 方法, 点进去,源码如下: 这里断点处的方法就是具体的实现,点进去,代码如下: 每段代码的作用都再图中标注了,这里主要就是从项目中的所有jar中,找到 META-INF/spring.factories 这个文件,然后读取里面的内容,然后返回成一个map 加载完毕之后,会为所有的初始化器创建实例,代码如下: 这里把所有的实例都创建出来,然后下面进行了一下排序,具体的排序源码这里就不贴了. 到这里所有的 spring.factories 里指定的系统初始化器就都被加载出来了 SpringFactoriesLoader本文的重点就是 SpringFactoriesLoader 这个类,总结如下: 框架内部使用的通用工厂加载机制 从classpath下面多个jar包的指定位置去读取文件,然后加载成对象 文件内容必须是key-value形式,即properties类型 key是全限定名(抽象类 || 接口), value是实现类,多个实现用逗号分隔 总结loadSpringFactories 的整体流程图:]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-0-系统初始化器]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-0-%E7%B3%BB%E7%BB%9F%E5%88%9D%E5%A7%8B%E5%8C%96%E5%99%A8%2F</url>
    <content type="text"><![CDATA[述在Spring Boot项目的启动过程中,我们可以手动的去通过系统初始化器,往容器中设置系统变量,或者是进行一些其他的操作,本文将看一下系统初始化器的几种定义方式. 准备工作首先新建一个空白的Spring Boot项目,启动测试没问题之后,开始下面的操作. 第一种方式首先新建一个类 FirstInitializer 然后实现 ApplicationContextInitializer&lt;ConfigurableApplicationContext&gt; 然后实现 initialize 方法,具体代码如下: 1234567891011121314151617181920@Order(1)@Slf4jpublic class FirstInitializer implements ApplicationContextInitializer&lt;ConfigurableApplicationContext&gt; &#123; @Override public void initialize(ConfigurableApplicationContext applicationContext) &#123; // 获取环境变量对象 ConfigurableEnvironment environment = applicationContext.getEnvironment(); Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(1); map.put(&quot;k1&quot;, &quot;v1&quot;); MapPropertySource mapPropertySource = new MapPropertySource(&quot;FirstInitializer&quot;, map); // 添加到环境变量中 environment.getPropertySources().addLast(mapPropertySource); log.info(&quot;FirstInitializer done...&quot;); &#125;&#125; 这里就是往环境变量中添加了一个属性k1,对应的值是v1,完成之后,还需要通过 spring.factories 文件来指定一下初始化器,否则这个类是不会被加载的 在 resource/META-INF/ 目录下创建 spring.factories 文件,内容如下:1org.springframework.context.ApplicationContextInitializer=com.zhou.springboot.example.initializers.FirstInitializer 测试验证到这里,这个系统初始化器就可以正常的被spring容器加载,然后我们写一个测试类去验证,创建 TestService ,代码如下:1234567891011121314@Servicepublic class TestService implements ApplicationContextAware &#123; private ApplicationContext applicationContext; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; this.applicationContext = applicationContext; &#125; public String test()&#123; return applicationContext.getEnvironment().getProperty(&quot;k1&quot;); &#125;&#125; 然后再写一个controller去调用就好了,启动项目,先看一下启动日志,如下:12345678910111213141516 . ____ _ __ _ _ /\\ / ___&apos;_ __ _ _(_)_ __ __ _ \ \ \ \( ( )\___ | &apos;_ | &apos;_| | &apos;_ \/ _` | \ \ \ \ \\/ ___)| |_)| | | | | || (_| | ) ) ) ) &apos; |____| .__|_| |_|_| |_\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.2.4.RELEASE)2020-01-31 11:43:42.107 INFO 9188 --- [ main] c.z.s.e.initializers.FirstInitializer : FirstInitializer done...2020-01-31 11:43:42.158 INFO 9188 --- [ main] com.zhou.springboot.example.App : Starting App on DESKTOP-B1V47DV with PID 9188 (D:\Demo\SpringBoot\spring-boot-example\build\classes\java\main started by Administrator in D:\Demo\SpringBoot\spring-boot-example)2020-01-31 11:43:42.160 INFO 9188 --- [ main] com.zhou.springboot.example.App : No active profile set, falling back to default profiles: default2020-01-31 11:43:44.138 INFO 9188 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 8080 (http)2020-01-31 11:43:44.153 INFO 9188 --- [ main] o.apache.catalina.core.StandardService : Starting service [Tomcat]....................... 这里可以看到,我们的初始化器是正常被执行了的,然后调用controller方法,返回如下: 返回正常,说明这个初始化器是OK的. 第二种方式下面来看一下第二种方式, 新建一个类 SecondInitializer 代码是和上面的 FirstInitializer 一样的,复制过来修改一下就好,如下:1234567891011121314151617181920@Order(2)@Slf4jpublic class SecondInitializer implements ApplicationContextInitializer&lt;ConfigurableApplicationContext&gt; &#123; @Override public void initialize(ConfigurableApplicationContext applicationContext) &#123; // 获取环境变量对象 ConfigurableEnvironment environment = applicationContext.getEnvironment(); Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(1); map.put(&quot;k2&quot;, &quot;v2&quot;); MapPropertySource mapPropertySource = new MapPropertySource(&quot;secondInitializer&quot;, map); // 添加到环境变量中 environment.getPropertySources().addLast(mapPropertySource); log.info(&quot;SecondInitializer done...&quot;); &#125;&#125; 然后这次要修改一下启动类,代码如下:12345678@SpringBootApplicationpublic class App &#123; public static void main(String[] args) &#123; SpringApplication springApplication = new SpringApplication(App.class); springApplication.addInitializers(new SecondInitializer()); springApplication.run(args); &#125;&#125; 这里通过 SpringApplication 对象来添加初始化器,然后启动项目看一下启动日志,如下:123456789101112131415 . ____ _ __ _ _ /\\ / ___&apos;_ __ _ _(_)_ __ __ _ \ \ \ \( ( )\___ | &apos;_ | &apos;_| | &apos;_ \/ _` | \ \ \ \ \\/ ___)| |_)| | | | | || (_| | ) ) ) ) &apos; |____| .__|_| |_|_| |_\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.2.4.RELEASE)2020-01-31 13:24:43.569 INFO 12272 --- [ main] c.z.s.e.initializers.FirstInitializer : FirstInitializer done...2020-01-31 13:24:43.576 INFO 12272 --- [ main] c.z.s.e.initializers.SecondInitializer : SecondInitializer done...2020-01-31 13:24:43.602 INFO 12272 --- [ main] com.zhou.springboot.example.App : Starting App on DESKTOP-B1V47DV with PID 12272 (D:\Demo\SpringBoot\spring-boot-example\build\classes\java\main started by Administrator in D:\Demo\SpringBoot\spring-boot-example)2020-01-31 13:24:43.604 INFO 12272 --- [ main] com.zhou.springboot.example.App : No active profile set, falling back to default profiles: default2020-01-31 13:24:45.813 INFO 12272 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 8080 (http)...................... 通过日志可以看到两个初始化器都是被正常加载的,然后他们加载的顺序也是我们通过 @Order 指定的顺序,然后也可以通过上面写的controller方法去验证一下 第三种方式下面来看第三种方式, 还是先创建一个类 ThirdInitializer ,代码和前面的一样,还是复制过来修改一哈,如下:1234567891011121314151617181920@Order(3)@Slf4jpublic class ThirdInitializer implements ApplicationContextInitializer&lt;ConfigurableApplicationContext&gt; &#123; @Override public void initialize(ConfigurableApplicationContext applicationContext) &#123; // 获取环境变量对象 ConfigurableEnvironment environment = applicationContext.getEnvironment(); Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(1); map.put(&quot;k3&quot;, &quot;v3&quot;); MapPropertySource mapPropertySource = new MapPropertySource(&quot;thirdInitializer&quot;, map); // 添加到环境变量中 environment.getPropertySources().addLast(mapPropertySource); log.info(&quot;ThirdInitializer done...&quot;); &#125;&#125; 然后这里要通过 application.yml 去添加这个初始化器,如下:123context: initializer: classes: com.zhou.springboot.example.initializers.ThirdInitializer 然后启动项目,看下输出日志,如下:1234567891011 . ____ _ __ _ _ /\\ / ___&apos;_ __ _ _(_)_ __ __ _ \ \ \ \( ( )\___ | &apos;_ | &apos;_| | &apos;_ \/ _` | \ \ \ \ \\/ ___)| |_)| | | | | || (_| | ) ) ) ) &apos; |____| .__|_| |_|_| |_\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.2.4.RELEASE)2020-01-31 13:38:15.059 INFO 15848 --- [ main] c.z.s.e.initializers.ThirdInitializer : ThirdInitializer done...2020-01-31 13:38:15.064 INFO 15848 --- [ main] c.z.s.e.initializers.FirstInitializer : FirstInitializer done...2020-01-31 13:38:15.064 INFO 15848 --- [ main] c.z.s.e.initializers.SecondInitializer : SecondInitializer done... 这里可以看到,这个初始化器是被正常加载了,但是这里有个问题,我们在 ThirdInitializer 指定的 @Order(3) 正常来说,他应该是最后一个执行的,但是这里变成了第一个执行的,这里是因为,在 application 配置中指定的系统初始化器是优先于前两种方法的 总结 三种方法的共同点都是要实现 ApplicationContextInitializer 接口 @Order 的值越小越先执行 application 配置文件指定的方式优先于其他方式]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-使用 AOP 切片拦截请求]]></title>
    <url>%2F2020%2F03%2F07%2FSpring-Boot-%E4%BD%BF%E7%94%A8-AOP-%E5%88%87%E7%89%87%E6%8B%A6%E6%88%AA%E8%AF%B7%E6%B1%82%2F</url>
    <content type="text"><![CDATA[AOPAOP即面向切面编程,在一个web服务中,比如说我们需要每次记录请求的处理时间,或者要记录日志等等功能的时候,如果要把这些写在代码中的话,就很麻烦,产生很多相同的代码,这时候就可以使用AOP, 通过对现有的程序定义一个切入点，然后在其前后切入不同的执行内容,下面来看一个简单的例子,就比如要记录每个请求的处理耗时. AOP示例首先需要引入AOP的依赖,如下:12345&lt;!-- AOP --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;&lt;/dependency&gt; 然后我们来创建一个类 TimerAspect,然后加入 @Aspect注解 表示这是一个切面类,还有一个是 @Component 交给Spring容器去管理 定义好类之后,还需要一个具体处理逻辑的方法,代码如下:123456789101112131415161718192021222324@Aspect@Component@Slf4jpublic class TimerAspect &#123; @Around(&quot;execution(* com.security.example.demo.controller.TestController.*(..))&quot;) public Object doAround(ProceedingJoinPoint pjp) throws Throwable &#123; // 拿到请求的所有参数 Object[] params = pjp.getArgs(); // 记录开始时间 long startTime = System.currentTimeMillis(); // 执行controller中具体的业务逻辑 Object object = pjp.proceed(); // 方法执行完成打印结束时间 log.info(&quot;方法执行结束,耗时:&#123;&#125;ms&quot;, System.currentTimeMillis() - startTime); // 返回的是具体controller的返回值 return object; &#125; &#125; 这里首先来看一下方法上面这个注解,一共有以下几种: @Before: 在切入点开始处执行 @After: 在切入点的结尾处执行 @AfterReturning: 在切入点方法return之后执行(可以对返回值做一些处理) @Around: 可以在切入点前后都执行内容,可以自己控制何时执行切入点自身的内容 @AfterThrowing: 用来处理当切入内容部分抛出异常之后的处理逻辑 然后再来看一下我们上面用的是@Around,这个也是最常用的,方法里面可以通过 ProceedingJoinPoint 对象,拿到方法的参数等,然后可以通过pjp.proceed();执行切入点自身的内容,并且拿到返回值,方法最后再把返回值返回去就ok了 这里还有一个是注解后面定义的切入点,在上面的例子中execution(* com.security.example.demo.controller.TestController.*(..)),就表示 com.security.example.demo.controller.TestController 这个类里面的所有方法 (..) 表示任意的参数,可以根据自己的实际业务调整 测试我们现在有这样一个接口:12345@GetMapping(&quot;&#123;id:\\d+&#125;&quot;)public User getInfo(@PathVariable Long id)&#123; log.info(&quot;查询id是[&#123;&#125;]的数据.....&quot;, id); return new User();&#125; 然后启动请求接口,看一下控制台的输出日志: AOP中的同步问题我们上面用的是@Around实现了打印请求耗时,当然也可以通过使用@doBefore 和 @doAfterReturning 这两个方法来实现, 这样的话就需要在类里面定义一个成员变量,这里就会有线程同步的问题, 可以通过使用 ThreadLocal 解决,但是会增加内存开销,所以还是直接用 @Around 方便 AOP切面的优先级假设我们有多个切面,比如一个切面先做参数处理, 另一个做返回处理, 这里就需要规定每个切面的执行顺序了, 可以通过 @Order(i) 这个注解来设置切面的优先级, i的值越小，优先级越高 举个例子, 一个是处理参数的切面 设置@Order(2), 一个是记录耗时的切面,设置@Order(1), 这里记录耗时的切面优先级就高,这时候的执行顺序是这样的: 在 @Before 中优先执行 @Order(1) 的内容, 再执行 @Order(2) 的内容 在 @After 和 @AfterReturning 中优先执行 @Order(2) 的内容,再执行 @Order(1) 的内容 总结一下就是: 在切入点前的操作,按order的值由小到大执行 在切入点后的操作,按order的值由大到小执行]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx-记录一次部署项目的经过]]></title>
    <url>%2F2019%2F10%2F25%2FNginx-%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1%E9%83%A8%E7%BD%B2%E9%A1%B9%E7%9B%AE%E7%9A%84%E7%BB%8F%E8%BF%87%2F</url>
    <content type="text"><![CDATA[述上文中,安装配置好了nginx,然后下面就可以去使用了 我这里打算是部署一个VUE的前端项目,然后还有一个 Spring Boot 的 jar 包, vue是需要用到nginx部署的, nginx的端口需要是80,或者https要443, 这里就以http的为例 nginx 用了80端口, 也就是说后端的jar就不可以用80了, 但是我的项目中又使用了微信的接口,微信要求端口必须是80, 那么问题来了, 我一台服务器中,用到了两个 80 端口,这怎么搞 其实也简单,再买一台服务器就好了,但是,我没有钱,你说有钱多好,我要有钱我也这么玩儿,主要是没钱. 没办法,只能想办法用一台服务器去部署了,通过网上查了一些相关资料, 了解到一个解决方案, 就是用两个二级域名, 比如说,一个 www.xxx.com 用来访问前端的项目, 还有一个 api.xxx.com 用来访问后端的接口, 然后nginx监听 80 端口,通过不同的域名转发到前后端. 大概就是这么个流程, 下面来看下具体怎么操作 VUE打包用nginx部署VUE项目在开发环境中,是使用 npm run dev 去运行的,但是在生产环境中,是要打包成一堆静态文件的, 命令是 npm run build 打包好了之后,会在项目的目录下生成一个 dist 的文件夹,里面有一个 index.html 和一个 static 文件夹,可能还有个icon,这个就是要传到服务器上去的东西 上文中,使用docker安装了nginx,并且映射出来几个目录, 其中有一个 /usr/local/docker/nginx/html 这个目录, 把打包好的文件都丢进去 上传完成之后需要去修改一下配置, /usr/local/docker/nginx/conf.d 这个目录下,有一个 default.conf ,用这个就可以,当然你也可以新建一个去写,这个文件夹下所有的.conf文件都会被 /usr/local/docker/nginx/conf 下面的 nginx.conf 收集起来 我这里就直接去修改 default.conf 了1vim default.conf 然后修改后的内容如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445server &#123; listen 80; server_name www.xxx.com; #charset koi8-r; #access_log /var/log/nginx/host.access.log main; location / &#123; root /usr/share/nginx/html; index index.html index.htm; try_files $uri $uri/ /index.html last; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125; # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \.php$ &#123; # proxy_pass http://127.0.0.1; #&#125; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \.php$ &#123; # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #&#125; # deny access to .htaccess files, if Apache&apos;s document root # concurs with nginx&apos;s one # #location ~ /\.ht &#123; # deny all; #&#125;&#125; 被注释掉的不用看,其实重点就是以下内容:12345678910server &#123; listen 80; server_name www.xxx.com; location / &#123; root /usr/share/nginx/html; index index.html index.htm; try_files $uri $uri/ /index.html last; &#125;&#125; 这段配置的意思就是,监听80 端口,然后域名是 www.xxx.com ,location里面的 root 就是放 vue 静态文件的路径, 这里要注意一下,这个目录是容器里面的目录, 因为这个配置文件也是对应的容器里面的, 然后 index 就是首页. 下面还有一行 try_files 这个我最开始是没有加的,然后发现打开网页之后刷新就会 404 ,然后网上找到了这种解决方案,具体什么原因可以参考这里 到这里,前端的项目就部署完成了 Spring Boot项目部署打包部署就不说了,比如我们现在后端项目是8080,然后按照上面说的,后端的接口是 api.xxx.com ,那么就需要nginx去拦截这个域名,然后做一个转发转发到8080 ,实现很简单,在上面的配置文件中,新加一个 server ,内容如下1234567server &#123; listen 80; server_name api.xxx.com; location / &#123; proxy_pass http://ip:8080; &#125;&#125; 加这样一个配置就可以了, 这里要注意,这个ip是要用公网的ip,我开始写成了 127.0.0.1 发现死活访问不到,后来发现原因是nginx用的是docker容器, 所以转发的也是容器里的 127.0.0.1 ,这就坑了,换成公网ip,或者是一个局域网的ip之后就可以了 最后注意前端vue项目中的请求接口地址要预先改成 api.xxx.com ,也就是后端接口的域名,然后再打包 我也是刚刚接触nginx,查了查资料,勉强能把自己的项目部署上去,之后有时间了再去深入了解]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx-通过docker安装使用]]></title>
    <url>%2F2019%2F10%2F25%2FNginx-%E9%80%9A%E8%BF%87docker%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[述最近打算部署一个个人的网站,由于之前做的都是后端的事情,部署个jar包就可以了,也没有完整的部署过一个项目,然后就去了解了一些nginx的用法,这里做一下记录 首先还是打算用docker安装, 比较方便,之后挂载一下文件以及配置的目录就可以了,不了解docker的同学,可以先去熟悉一下,下面看一下详细的操作 运行nginx容器命令:1docker run -d -p 80:80 nginx 这里直接先运行一个 nginx 容器,也没有去挂载外部的配置,原因是我们要把nginx里面的配置文件复制出来先 宿主机创建挂载文件夹要挂载的话,首先我们需要知道在容器里面,nginx的配置文件以及其他数据都放在哪里,如下: 容器中nginx.conf配置文件路径: /etc/nginx/nginx.conf default.conf配置文件的路径: /etc/nginx/conf.d/default.conf 默认首页文件夹html路径: /usr/share/nginx/html 日志文件路径: /var/log/nginx 这里一共需要四个文件夹,所以我们在宿主机中也需要创建这四个文件夹,命令如下:1mkdir -p /usr/local/docker/nginx/&#123;conf,conf.d,html,logs&#125; 创建好了之后,就是要把之前我们运行的那个容器里面的配置文件复制出来,命令如下:12docker cp 305a1a43b71f:/etc/nginx/nginx.conf /usr/local/docker/nginx/conf/nginx.confdocker cp 305a1a43b71f:/etc/nginx/conf.d/default.conf /usr/local/docker/nginx/conf.d/default.conf 305a1a43b71f是容器的id,可以通过 docker ps 查看,关于docker的用法不多做介绍了 重新运行nginx准备工作都ok了之后,把之前启动的那个容器给删掉1docker rm -f 305a1a43b71f 用以下命令再启动新的nginx:12345docker run --name nginx -d -p 80:80 \-v /usr/local/docker/nginx/html:/usr/share/nginx/html \-v /usr/local/docker/nginx/conf/nginx.conf:/etc/nginx/nginx.conf \-v /usr/local/docker/nginx/conf.d/default.conf:/etc/nginx/conf.d/default.conf \-v /usr/local/docker/nginx/logs:/var/log/nginx nginx 启动完成之后就ok了 测试最后在 /usr/local/docker/nginx/html 随便放一个html,然后就可以访问了]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-35-跨域问题及解决方案]]></title>
    <url>%2F2019%2F10%2F08%2FSpring-Security-35-%E8%B7%A8%E5%9F%9F%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[述最近在使用过程中,遇到了这样一个问题,就是跨域的处理,算是踩的一个坑,记录一下 Spring boot 中跨域的处理Spring boot中处理跨域的方式有很多种,这里不去一个一个细说,只写一个我用的解决方案,部分代码如下:123456789101112131415@Beanpublic WebMvcConfigurer corsConfigurer() &#123; return new WebMvcConfigurerAdapter() &#123; @Autowired private Constants constants; @Override public void addCorsMappings(CorsRegistry registry) &#123; registry.addMapping(&quot;/**&quot;).allowedOrigins(constants.getCorsDomain()).maxAge(3600).allowedMethods(&quot;*&quot;); &#125; &#125;;&#125; 这段代码,就随便写在一个配置类里面就可以了,然后这里有一个 constants.getCorsDomain() ,是一个数组,放的是允许跨域的域名 Spring Security遇到的问题上面在 Spring boot 中的配置配置完成之后,是不是以为跨域问题就OK了, 其实并没有,这个时候是可以通过浏览器发get请求,而且也没有跨域的问题,但是去请求一个post接口的话,就会有跨域的问题,如下: 原因问题出现的原因是浏览器在发送 POST 这种复杂请求的时候,会先发送一个 OPTIONS 的预检请求,来询问服务器是否支持跨域,支持的话,才会发送真实的 POST 登录请求, 但是 Spring Security ,会把所有的请求都拦截下来, 会认为这个 OPTIONS 请求也是需要登录才能访问的,所以这里会先返回一个401 解决方案针对上面的问题,解决方案就是把所有的 OPTIONS 请求都放行,配置如下:1.requestMatchers(CorsUtils::isPreFlightRequest).permitAll() 把这个加到权限的配置中去,然后再重启项目,再次访问,这时候 OPTIONS 请求可以正常发送返回了,而且真正的登录的 POST 请求也发送给服务器正常返回了,但是看不到返回值,原因还是跨域的问题,在Spring boot跨域的处理配置好的情况下,还需要给 Spring Security 做一些单独的配置 在资源服务器的配置中,如下:1234567891011@Override public void configure(HttpSecurity http) throws Exception &#123; // 省略部分代码... .and() .cors() // 省略部分代码... ; authorizeConfigManager.config(http.authorizeRequests()); &#125; 重点就是 .cors() 这个方法 配置完成之后,再次重启,然后访问,就会正常返回]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-34-自定义403返回]]></title>
    <url>%2F2019%2F10%2F08%2FSpring-Security-34-%E8%87%AA%E5%AE%9A%E4%B9%89403%E8%BF%94%E5%9B%9E%2F</url>
    <content type="text"><![CDATA[述之后的文章会记录一些在日常使用过程中遇到的一些问题 假设有如下一段配置:1.antMatchers(&quot;/test/*&quot;).hasRole(&quot;USER&quot;) 表示 /test/* 这种请求只有在是USER角色的情况下才能访问,这时候如果使用了一个没有USER角色的用户去访问这个接口,系统会认为没有权限,会默认跳转到登录页, 在一般情况下这种没有权限的处理应该是返回一个403的页面,或者是一个json格式的提示信息,并不会直接跳转登录页,下面来看一下如何去实现自定义的403返回 自定义403返回数据首先需要有一个类去实现 AccessDeniedHandler 接口, 代码如下:123456789101112131415161718@Component(&quot;myAccessDeniedHandler&quot;)public class MyAccessDeniedHandler implements AccessDeniedHandler &#123; @Override public void handle(HttpServletRequest request, HttpServletResponse response, AccessDeniedException accessDeniedException)throws IOException &#123; // 返回json形式的错误信息 response.setCharacterEncoding(&quot;UTF-8&quot;); response.setContentType(&quot;application/json&quot;); response.setStatus(HttpStatus.FORBIDDEN.value()); Map&lt;String, String&gt; result = new HashMap&lt;&gt;(1); result.put(&quot;message&quot;, &quot;没有操作权限&quot;); response.getWriter().println(new ObjectMapper().writeValueAsString(result)); response.getWriter().flush(); &#125;&#125; 这里我就很简单的返回了一个json提示信息,当然也可以去返回一个页面,或者再做一些日志记录等额外的操作,都是可以的 完成之后,还需要做一个配置才可以正常使用,就是在资源服务器的配置中,设置一下这个处理器,这里以app项目为例, MyResourcesServerConfig 中,需要配置的代码如下:1234567891011121314151617181920212223242526@Configuration@EnableResourceServerpublic class MyResourcesServerConfig extends ResourceServerConfigurerAdapter &#123; // 省略部分代码.... @Autowired private AccessDeniedHandler myAccessDeniedHandler; @Override public void configure(HttpSecurity http) throws Exception &#123; // 省略部分代码.... http // 省略部分代码.... .and() .exceptionHandling() .accessDeniedHandler(myAccessDeniedHandler) // 省略部分代码.... ; // 省略部分代码.... &#125;&#125; 好了,到这里配置完成之后,就可以正常使用了]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-33-权限配置分离]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-33-%E6%9D%83%E9%99%90%E9%85%8D%E7%BD%AE%E5%88%86%E7%A6%BB%2F</url>
    <content type="text"><![CDATA[述之前的权限配置中,遗留了一个问题,就是在我们公用模块的配置中,有如下的配置 这里这个 /user/register 的配置其实是调用方的url, 在我们的公用项目里面,并不知道这些路径,所以我们需要实现的功能就是,调用方去自己的项目里面配置他的权限,然后我们把这些配置集中起来加到 Spring Security 的配置中去,下面来看一下如何实现 基本思路 提供一个 AuthorizeConfigProvider 接口 权限模块的通用配置实现该接口,然后进行配置 其他应用也可以实现这个接口来自定义自己的配置 创建一个 AuthorizeConfigManager 来管理所有的 AuthorizeConfigProvider 实现类 最后统一加到 Spring Security 的配置中去 代码实现按照上面的思路,首先是 AuthorizeConfigProvider 接口,这个是公用的,所以放到 core 项目里面,代码如下:123456789public interface AuthorizeConfigProvider &#123; /** * 权限配置 * * @param config */ void config(ExpressionUrlAuthorizationConfigurer&lt;HttpSecurity&gt;.ExpressionInterceptUrlRegistry config);&#125; 这里的参数其实是 .authorizeRequests() 方法的返回值 然后提供一个我们默认的实现,就是web环境和 app 环境中的一些通用的配置,如下:12345678910111213141516171819202122@Componentpublic class GlobalAuthorizeConfigProvider implements AuthorizeConfigProvider &#123; @Autowired private SecurityProperties securityProperties; @Override public void config(ExpressionUrlAuthorizationConfigurer&lt;HttpSecurity&gt;.ExpressionInterceptUrlRegistry config) &#123; config.antMatchers( SecurityConstants.DEFAULT_UNAUTHENTICATION_URL, SecurityConstants.DEFAULT_LOGIN_PROCESSING_URL_MOBILE, SecurityConstants.DEFAULT_VALIDATE_CODE_URL_PREFIX + &quot;/*&quot;, securityProperties.getBrowser().getLoginPage(), SecurityConstants.DEFAULT_SIGN_UP_URL, securityProperties.getBrowser().getSignUpPage(), SecurityConstants.GET_SOCIAL_USER_URL, securityProperties.getBrowser().getSession().getSessionInvalidUrl() ) .permitAll(); &#125;&#125; 然后是 AuthorizeConfigManager, 代码如下:12345678public interface AuthorizeConfigManager &#123; /** * 权限配置 * @param config */ void config(ExpressionUrlAuthorizationConfigurer&lt;HttpSecurity&gt;.ExpressionInterceptUrlRegistry config);&#125; 这里也提供一个实现类, DefaultAuthorizeConfigManager, 代码如下:123456789101112131415161718@Componentpublic class DefaultAuthorizeConfigManager implements AuthorizeConfigManager &#123; /** * 收集系统中的所有的&#123;@link AuthorizeConfigProvider&#125;实现 */ @Autowired private Set&lt;AuthorizeConfigProvider&gt; authorizeConfigProviders; @Override public void config(ExpressionUrlAuthorizationConfigurer&lt;HttpSecurity&gt;.ExpressionInterceptUrlRegistry config) &#123; for (AuthorizeConfigProvider authorizeConfigProvider : authorizeConfigProviders) &#123; authorizeConfigProvider.config(config); &#125; // 除了上面配置的，其他的都需要登录后才能访问 config.anyRequest().authenticated(); &#125;&#125; 最后在我们的安全配置中注入进去就可以了,如下: 12345678910111213141516171819202122232425262728293031323334@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; // web网页登录的配置 applyPasswordAuthenticationConfig(http); http .apply(validateCodeSecurityConfig) .and() .apply(smsAuthenticationSecurityConfig) .and() .apply(securitySocialConfigurer) .and() .rememberMe() .tokenRepository(persistentTokenRepository()) .tokenValiditySeconds(securityProperties.getBrowser().getRememberMeSeconds()) .userDetailsService(userDetailsService) .and() .sessionManagement() .invalidSessionStrategy(invalidSessionStrategy) .maximumSessions(securityProperties.getBrowser().getSession().getMaximumSessions()) .maxSessionsPreventsLogin(securityProperties.getBrowser().getSession().isMaxSessionsPreventsLogin()) .expiredSessionStrategy(sessionInformationExpiredStrategy) .and() .and() .logout() .logoutUrl(securityProperties.getBrowser().getLogOutUrl()) .logoutSuccessHandler(logoutSuccessHandler) .and() .csrf().disable() ; authorizeConfigManager.config(http.authorizeRequests());&#125; 最后一行代码就是把所有的配置都配置进去, app的配置也一样的 这时候调用方如果想增加自己的权限,只需要实现 AuthorizeConfigProvider 接口就行,如下:1234567891011@Componentpublic class MyAuthorizeConfigProvider implements AuthorizeConfigProvider &#123; @Override public void config(ExpressionUrlAuthorizationConfigurer&lt;HttpSecurity&gt;.ExpressionInterceptUrlRegistry config) &#123; config.antMatchers(&quot;/user/register&quot;) .permitAll() .antMatchers(&quot;/users&quot;) .hasRole(&quot;ADMIN&quot;); &#125;&#125; ok,到这儿权限配置分离就ok了 本文代码传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-32-授权部分源码跟踪]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-32-%E6%8E%88%E6%9D%83%E9%83%A8%E5%88%86%E6%BA%90%E7%A0%81%E8%B7%9F%E8%B8%AA%2F</url>
    <content type="text"><![CDATA[述上文中介绍了授权的一些基本的使用,本文来看一下, Spring Security 的授权部分的源码 首先来看一下流程图,如下: 这里的重点是最后面的三个拦截器 AnonymousAuthenticationFilter: 如果用户没有认证,给一个匿名的认证信息 ExceptionTranslationFilter: 捕获 FilterSecurityInterceptor 抛出来的异常并进行处理 FilterSecurityInterceptor: 具体的权限验证的逻辑 AnonymousAuthenticationFilter首先来看一下 AnonymousAuthenticationFilter 这个类的源码,如下: 然后这个类的构造函数如下: 这个类的主要作用就是,如果用户没有认证的话,就生成一个默认的认证信息以及默认的权限, 对应的就是上文中认证信息是 anonymous 的表达式 然后重点是 FilterSecurityInterceptor FilterSecurityInterceptor先来看幅图,如下: FilterSecurityInterceptor 主要就是用了 AccessDecisionManager 这个东西 AccessDecisionManager 用来管理 AccessDecisionVoter 就是投票者,主要是两个,一个是 AbstractAccessDecisionManager 还有一个是 AccessDecisionVoter AbstractAccessDecisionManager 又分别有三个实现: AffirmativeBased: 只要有一票否决则不能访问 (默认使用) ConsensusBased: 通过/不通过 哪一个票数多就遵循哪一个 UnanimousBased: 只要有一票否决则不允许访问 AccessDecisionVoter 在 spring3以上的版本中,就只有一个实现,就是 WebExpressionVoter ,，它投过就过,不过就不过 图中上面还有几个类: SecurityConfig: 所有的配置信息 ConfigAttribute: 对应了每一个url的配置信息 SecurityContextHolder Authentication: 用户身份信息及用户拥有的权限信息 有了这些类之后,大致流程是这样的: 拿到配置信息,用户身份信息,请求信息,然后给投票者进行投票,最后根据策略进行决定是否放行 下面来详细看一下源码 假设现在有一个请求,/users 需要 ROLE_ADMIN 这个角色,然后现在没有登录,去访问 然后看下具体的权限验证的方法: 这里投票的默认实现是 AffirmativeBased ,源码如下: 走到这儿权限的判断就完成了,之后因为没有登录,所以会抛异常,最后在 ExceptionTranslationFilter 中去处理 ExceptionTranslationFilter源码如下: 大致整个授权的流程就是这样, 至于登录后再去访问,可以跟着这个流程去试试,这里就不再贴代码了]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-31-授权的使用]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-31-%E6%8E%88%E6%9D%83%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[述之前的部分都是说的是认证的流程,并没有详细的使用过权限,本文来看一下如何做权限控制. 其实在我们之前的使用中,已经用到了一部分的权限控制,如下:12345678910111213141516.authorizeRequests() // 匹配的是登录页的话放行 .antMatchers( SecurityConstants.DEFAULT_UNAUTHENTICATION_URL, SecurityConstants.DEFAULT_LOGIN_PROCESSING_URL_MOBILE, SecurityConstants.DEFAULT_VALIDATE_CODE_URL_PREFIX + &quot;/*&quot;, securityProperties.getBrowser().getLoginPage(), SecurityConstants.DEFAULT_SIGN_UP_URL, securityProperties.getBrowser().getSignUpPage(), SecurityConstants.GET_SOCIAL_USER_URL, securityProperties.getBrowser().getSession().getSessionInvalidUrl(), &quot;/user/register&quot; ) .permitAll() // 授权请求. anyRequest 就表示所有的请求都需要权限认证 .anyRequest().authenticated() 这里就是针对请求做权限的控制, .antMatchers() 方法用来匹配URL, 然后还有面的 .permitAll() 就是表示不需要任何权限 最后 anyRequest() 就是所有的请求,authenticated() 就表示需要登录之后才能访问,这些就是一些简单的权限控制,下面来详细的看一下,还有哪些使用方式 权限控制在实际的开发环境中,权限一般就是RBAC模型,就是用户去绑定角色,角色去绑定权限,这样的处理方式,下面来看一下如何去针对用户的角色去控制权限 举个例子,比如一个人员列表的接口 /users ,这个接口需要角色是管理员的用户才能访问, 那么代码中的配置如下:1.antMatchers(&quot;/users&quot;).hasRole(&quot;ADMIN&quot;) 这里首先是匹配 /users 这个接口, hasRole() 就表示访问这个接口需要的角色,然后参数是 ADMIN ,就是管理员角色,那这个角色是从哪里来呢? 之前我们在 UserDetailsService 里面有这样一段代码123456789101112131415161718@Overridepublic UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; log.info(&quot;进行用户认证...&quot;); User user = userService.findUserByPhoneNo(username); if (user == null) &#123; throw new UsernameNotFoundException(&quot;用户不存在&quot;); &#125; // 查询出来,返回去 return new org.springframework.security.core.userdetails.User( user.getId().toString(), user.getPassword(), user.getEnable(), true, true, !user.getLocked(), AuthorityUtils.commaSeparatedStringToAuthorityList(&quot;admin,ROLE_USER&quot;));&#125; 最后返回的 User 对象中,最后一个参数就是用户的权限和角色, 角色的前缀是 ROLE_ 这个是写死的, 我们上面的请求是要 ADMIN 的角色,所以这里需要返回的一个角色是 ROLE_ADMIN 代码如下:123456789return new org.springframework.security.core.userdetails.User( user.getId().toString(), user.getPassword(), user.getEnable(), true, true, !user.getLocked(), AuthorityUtils.commaSeparatedStringToAuthorityList(&quot;admin,ROLE_USER,ROLE_ADMIN&quot;));&#125; 匹配restful Api在restful的请求中,可能请求路径是一样的,但是请求的方式是不一样的,比如GET POST,那如何区分这种请求呢,如下:1.antMatchers(HttpMethod.GET, &quot;/users&quot;).hasRole(&quot;ADMIN&quot;) 这里就是表示是 GET /users 这个请求是需要 ADMIN 角色的 授权表达式Spring Security 提供了一下几种方法 表达式 说明 permitAll 永远返回true denyAll 永远返回false anonymous 当前用户是anonymous时返回true rememberMe 当前用户是rememberMe用户时返回true authenticated 当前用户不是anonymous时返回true fullAuthenticated 当前用户既不是anonymous也不是rememberMe用户时返回true hasRole(role) 用户拥有指定的角色权限时返回true hasAnyRole([role1,role2) 用户拥有任意一个指定的角色权限时返回true hasAuthority(authority) 用户拥有指定的权限时返回true hasAnyAuthority(authority1, authority2]) 用户拥有任意个指定的权限时返回true haslpAddress( ‘192.168.1.0/24’ ) 请求发送的lp匹配时返回true 这里有一个是 anonymous 这个之后再做说明 每一个表达式都对应了 .antMatchers() 后面的一个方法 ,比如:123.antMatchers(&quot;/url&quot;).permitAll().antMatchers(&quot;/url&quot;).denyAll().antMatchers(&quot;/url&quot;).hasRole(&quot;ADMIN&quot;) 组合使用举个例子,比如某个接口指定ip的管理员用户才能访问,这里就用到了两个表达式,那么如何去组合使用,如下:12.antMatchers(&quot;/url&quot;).access(&quot;haslpAddress( &apos;192.168.1.0/24&apos;) and hasRole(&apos;ADMIN&apos;)&quot;) 这里是使用 access() 方法去组合自己的表达式 关于授权的部分就先介绍到这里]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-30-单点登录案例]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-30-%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[述上文中,将默认的token替换成了 jwt ,本文将基于 jwt 实现一个单点登录的案例,流程图如下: 大概流程就是,应用A去请求登录,然后跳转到统一的认证服务器上去做认证,之后返回给应用A一个token, 这时候B应用去请求登录,认证服务器会发现应用A已经登录过了,那就不需要再认证了,直接返回令牌给应用B 关于单点登录的流程这里不做详细的描述了,下面看下如何实现 准备环境新建三个项目,不在之前的项目上做修改了 sso-server:认证服务器 sso-client-a:应用A sso-client-b:应用B 总的父工程依赖和之前的项目,一样,其他三个工程的依赖如下:12345678910111213141516171819202122232425262728&lt;!-- security --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- spring boot --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- spring-security-oauth2 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.security.oauth&lt;/groupId&gt; &lt;artifactId&gt;spring-security-oauth2&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- jwt --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-jwt&lt;/artifactId&gt;&lt;/dependency&gt; 可以直接从我的github拉下来用,传送门 认证服务器环境准备完成之后,实现一下认证服务器 server 项目中,新建配置类 SsoAuthorizationServerConfig ,代码如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Configuration@EnableAuthorizationServerpublic class SsoAuthorizationServerConfig extends AuthorizationServerConfigurerAdapter &#123; private static final String DEFAULT_SIGN_KEY = &quot;default-sign-key&quot;; @Override public void configure(AuthorizationServerEndpointsConfigurer endpoints) throws Exception &#123; endpoints.tokenStore(jwtTokenStore()) .accessTokenConverter(jwtAccessTokenConverter()); &#125; @Override public void configure(ClientDetailsServiceConfigurer clients) throws Exception &#123; clients.inMemory() .withClient(&quot;testAppId1&quot;) .secret(&quot;testSecert1&quot;) .authorizedGrantTypes(&quot;authorization_code&quot;, &quot;refresh_token&quot;, &quot;password&quot;) .redirectUris(&quot;http://127.0.0.1:8080/client-a/login&quot;) .scopes(&quot;all&quot;) .and() .withClient(&quot;testAppId2&quot;) .secret(&quot;testSecert2&quot;) .authorizedGrantTypes(&quot;authorization_code&quot;, &quot;refresh_token&quot;, &quot;password&quot;) .redirectUris(&quot;http://127.0.0.1:8081/client-b/login&quot;) .scopes(&quot;all&quot;); &#125; @Override public void configure(AuthorizationServerSecurityConfigurer security) throws Exception &#123; // 表是在访问认证服务器的 tokenKey(就是 DEFAULT_SIGN_KEY ) 的时候,需要经过身份认证 security.tokenKeyAccess(&quot;isAuthenticated()&quot;); &#125; @Bean public TokenStore jwtTokenStore()&#123; return new JwtTokenStore(jwtAccessTokenConverter()); &#125; @Bean public JwtAccessTokenConverter jwtAccessTokenConverter()&#123; // token生成中的一些处理 JwtAccessTokenConverter converter = new JwtAccessTokenConverter(); converter.setSigningKey(DEFAULT_SIGN_KEY); return converter; &#125;&#125; 这个跟我们之前的认证服务器的配置是一样的,这里的client的信息是基于内存配置的,之前配置的是基于数据库的, 这里重点是第三个configure() 方法, 之前我们只重写了两个 在认证服务器认证完成之后,会给应用返回一个 JWT 的token, 应用需要解析的话,就需要用到jwt的 SigningKey ,这里这个方法就表示在应用服务器从认证服务器获取 SigningKey 的时候,应用必须是已登录的状态 其他配置跟之前一样不多做解释了 application.yml 的配置如下:123456server: port: 9999 context-path: /serversecurity: user: password: 123456 认证服务器的配置就完成了 应用服务器应用服务器很简单,首先,启动类需要加一个注解,如下:123456789@EnableOAuth2Sso@SpringBootApplicationpublic class App &#123; public static void main(String[] args) &#123; SpringApplication.run(App.class, args); &#125;&#125; 然后给一个测试用的controller,如下:12345678910@RestController@RequestMapping(&quot;/user&quot;)public class UserController &#123; @GetMapping(&quot;/me&quot;) public Authentication me(Authentication authentication)&#123; return authentication; &#125;&#125; 然后重点在配置文件中, application.yml 如下:12345678910111213security: oauth2: client: client-id: testAppId1 client-secret: testSecert1 user-authorization-uri: http://127.0.0.1:9999/server/oauth/authorize access-token-uri: http://127.0.0.1:9999/server/oauth/token resource: jwt: key-uri: http://127.0.0.1:9999/server/oauth/token_keyserver: port: 8080 context-path: /client-a 主要就是一个认证的请求,获取token的请求,然后上面那个获取 jwt 的 SigningKey 的请求路径的配置 然后 client-b 的配置也是相同的 测试三个项目都启动好之后,随便访问一个客户端的 /user/me 接口,这时候会跳转到 127.0.0.1:9999 的认证界面上,由于没做任何配置,所以这里是一个默认的 basic 认证, 然后输入用户名密码,会弹出一个授权的界面,点击同意之后会跳转到用户信息的界面 之后再访问另一个客户端的 /user/me 接口,这次就不需要登录了,直接是授权的界面,点击同意,返回用户的信息 配置自定义登录页面上面由于没有做任何配置,所以默认的就是 basic 的认证方式,那如何配置登录页, 其实跟之前是一样的,新建一个配置的类 MyWebSecurityConfigurerAdapter 代码如下:1234567891011121314@Configurationpublic class MyWebSecurityConfigurerAdapter extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(HttpSecurity http) throws Exception &#123; http .formLogin() .and() // 所有的请求都必须授权后才能访问 .authorizeRequests() .anyRequest() .authenticated(); &#125;&#125; 然后这里是使用了一个默认的登录页,然后这个里面还可以配置各种,比如 userDetailService 等等,跟之前的是一样的 优化授权上面的示例中, 每次应用切换都需要做一次授权的操作,如下: 每次切换都要授权一次,很烦,所以想办法把这一步跳过,让他自己授权 跟踪源码, 有一个 WhitelabelApprovalEndpoint ,是一个返回授权页面的controller, 我们只要覆盖掉这个controller就可以了, 新建类 MyWhitelabelApprovalEndpoint 代码如下:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697@RestController@SessionAttributes(&quot;authorizationRequest&quot;)public class MyWhitelabelApprovalEndpoint &#123; @RequestMapping(&quot;/oauth/confirm_access&quot;) public ModelAndView getAccessConfirmation(Map&lt;String, Object&gt; model, HttpServletRequest request) throws Exception &#123; final String approvalContent = createTemplate(model, request); if (request.getAttribute(&quot;_csrf&quot;) != null) &#123; model.put(&quot;_csrf&quot;, request.getAttribute(&quot;_csrf&quot;)); &#125; View approvalView = new View() &#123; @Override public String getContentType() &#123; return &quot;text/html&quot;; &#125; @Override public void render(Map&lt;String, ?&gt; model, HttpServletRequest request, HttpServletResponse response) throws Exception &#123; response.setContentType(getContentType()); response.getWriter().append(approvalContent); &#125; &#125;; return new ModelAndView(approvalView, model); &#125; protected String createTemplate(Map&lt;String, Object&gt; model, HttpServletRequest request) &#123; AuthorizationRequest authorizationRequest = (AuthorizationRequest) model.get(&quot;authorizationRequest&quot;); String clientId = authorizationRequest.getClientId(); StringBuilder builder = new StringBuilder(); // 让body不显示 builder.append(&quot;&lt;html&gt;&lt;body style=&apos;display:none;&apos;&gt;&lt;h1&gt;OAuth Approval&lt;/h1&gt;&quot;); builder.append(&quot;&lt;p&gt;Do you authorize \&quot;&quot;).append(HtmlUtils.htmlEscape(clientId)); builder.append(&quot;\&quot; to access your protected resources?&lt;/p&gt;&quot;); builder.append(&quot;&lt;form id=\&quot;confirmationForm\&quot; name=\&quot;confirmationForm\&quot; action=\&quot;&quot;); String requestPath = ServletUriComponentsBuilder.fromContextPath(request).build().getPath(); if (requestPath == null) &#123; requestPath = &quot;&quot;; &#125; builder.append(requestPath).append(&quot;/oauth/authorize\&quot; method=\&quot;post\&quot;&gt;&quot;); builder.append(&quot;&lt;input name=\&quot;user_oauth_approval\&quot; value=\&quot;true\&quot; type=\&quot;hidden\&quot;/&gt;&quot;); String csrfTemplate = null; CsrfToken csrfToken = (CsrfToken) (model.containsKey(&quot;_csrf&quot;) ? model.get(&quot;_csrf&quot;) : request.getAttribute(&quot;_csrf&quot;)); if (csrfToken != null) &#123; csrfTemplate = &quot;&lt;input type=\&quot;hidden\&quot; name=\&quot;&quot; + HtmlUtils.htmlEscape(csrfToken.getParameterName()) + &quot;\&quot; value=\&quot;&quot; + HtmlUtils.htmlEscape(csrfToken.getToken()) + &quot;\&quot; /&gt;&quot;; &#125; if (csrfTemplate != null) &#123; builder.append(csrfTemplate); &#125; String authorizeInputTemplate = &quot;&lt;label&gt;&lt;input name=\&quot;authorize\&quot; value=\&quot;Authorize\&quot; type=\&quot;submit\&quot;/&gt;&lt;/label&gt;&lt;/form&gt;&quot;; if (model.containsKey(&quot;scopes&quot;) || request.getAttribute(&quot;scopes&quot;) != null) &#123; builder.append(createScopes(model, request)); builder.append(authorizeInputTemplate); &#125; else &#123; builder.append(authorizeInputTemplate); builder.append(&quot;&lt;form id=\&quot;denialForm\&quot; name=\&quot;denialForm\&quot; action=\&quot;&quot;); builder.append(requestPath).append(&quot;/oauth/authorize\&quot; method=\&quot;post\&quot;&gt;&quot;); builder.append(&quot;&lt;input name=\&quot;user_oauth_approval\&quot; value=\&quot;false\&quot; type=\&quot;hidden\&quot;/&gt;&quot;); if (csrfTemplate != null) &#123; builder.append(csrfTemplate); &#125; builder.append(&quot;&lt;label&gt;&lt;input name=\&quot;deny\&quot; value=\&quot;Deny\&quot; type=\&quot;submit\&quot;/&gt;&lt;/label&gt;&lt;/form&gt;&quot;); &#125; // 添加自动提交操作 builder.append(&quot;&lt;script&gt;document.getElementById(&apos;confirmationForm&apos;).submit()&lt;/script&gt;&quot;); builder.append(&quot;&lt;/body&gt;&lt;/html&gt;&quot;); return builder.toString(); &#125; private CharSequence createScopes(Map&lt;String, Object&gt; model, HttpServletRequest request) &#123; StringBuilder builder = new StringBuilder(&quot;&lt;ul&gt;&quot;); @SuppressWarnings(&quot;unchecked&quot;) Map&lt;String, String&gt; scopes = (Map&lt;String, String&gt;) (model.containsKey(&quot;scopes&quot;) ? model.get(&quot;scopes&quot;) : request.getAttribute(&quot;scopes&quot;)); for (String scope : scopes.keySet()) &#123; String approved = &quot;true&quot; .equals(scopes.get(scope)) ? &quot; checked&quot; : &quot;&quot;; String denied = !&quot;true&quot; .equals(scopes.get(scope)) ? &quot; checked&quot; : &quot;&quot;; scope = HtmlUtils.htmlEscape(scope); builder.append(&quot;&lt;li&gt;&lt;div class=\&quot;form-group\&quot;&gt;&quot;); builder.append(scope).append(&quot;: &lt;input type=\&quot;radio\&quot; name=\&quot;&quot;); builder.append(scope).append(&quot;\&quot; value=\&quot;true\&quot;&quot;).append(approved).append(&quot;&gt;Approve&lt;/input&gt; &quot;); builder.append(&quot;&lt;input type=\&quot;radio\&quot; name=\&quot;&quot;).append(scope).append(&quot;\&quot; value=\&quot;false\&quot;&quot;); builder.append(denied).append(&quot;&gt;Deny&lt;/input&gt;&lt;/div&gt;&lt;/li&gt;&quot;); &#125; builder.append(&quot;&lt;/ul&gt;&quot;); return builder.toString(); &#125;&#125; 就是直接把他的代码复制过来,然后加了一个自动提交的script脚本 本文代码传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-29-使用JWT替换默认的Token]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-29-%E4%BD%BF%E7%94%A8JWT%E6%9B%BF%E6%8D%A2%E9%BB%98%E8%AE%A4%E7%9A%84Token%2F</url>
    <content type="text"><![CDATA[述默认的token生成规则其实就是一个UUID,就是一个随机的字符串,然后存到redis中去,使用JWT的话,token中可以存放一些信息,我们服务端也不需要保存这个token, 服务器通过使用保存的密钥验证token的正确性,只要正确即通过验证 使用JWT,在分布式系统中,很好地解决了单点登录问题,很容易解决了session共享的问题.但是是无法作废已颁布的令牌/不易应对数据过期,因为 token 并没有保存到服务端, 下面来看一下如何去配置JWT 配置JWTTokenStoreConfig 这个类中要做一些修改,之前我们只在这个类里面配置了 redis 的存储,现在把 JWT 的配置也加上,如下:12345678910111213141516171819202122232425262728293031323334@Configurationpublic class TokenStoreConfig &#123; @Autowired private RedisConnectionFactory connectionFactory; @Bean @ConditionalOnProperty(prefix = &quot;core.security.oAuth2&quot;, name=&quot;tokenStore&quot;, havingValue=&quot;redis&quot;) public TokenStore redisTokenStore()&#123; return new RedisTokenStore(connectionFactory); &#125; @Configuration @ConditionalOnProperty(prefix = &quot;core.security.oAuth2&quot;, name=&quot;tokenStore&quot;, havingValue=&quot;jwt&quot;, matchIfMissing = true) public static class JwtTokenConfig&#123; @Autowired private SecurityProperties securityProperties; @Bean public TokenStore jwtTokenStore()&#123; return new JwtTokenStore(jwtAccessTokenConverter()); &#125; @Bean public JwtAccessTokenConverter jwtAccessTokenConverter()&#123; // token生成中的一些处理 JwtAccessTokenConverter converter = new JwtAccessTokenConverter(); converter.setSigningKey(securityProperties.getOAuth2().getJwtTokenSignKey()); return converter; &#125; &#125;&#125; 这里首先是一个内部的静态类 JwtTokenConfig 用来配置 JWT 的一些配置,第一个方法 jwtTokenStore() 就是配置token的存储,然后这里需要一个 JwtAccessTokenConverter 因为 TokenStore 只管 Token 的存储,生成规则还需要配置,所以 jwtAccessTokenConverter() 就是用来做一些 Token 的处理 这个类上有一个注解 @ConditionalOnProperty(prefix = &quot;core.security.oAuth2&quot;, name=&quot;tokenStore&quot;, havingValue=&quot;jwt&quot;, matchIfMissing = true) 意思就是,在配置中有 core.security.oAuth2.tokenStore 这个配置,而且值是 jwt 的话,就生效,最后有一个 matchIfMissing = true ,这个表示, 如果配置中没有这个配置的话,也生效 上面的 redisTokenStore 也加了这个注解,但是没有 matchIfMissing 默认是 false, 总的配置就是如果在配置中没有指定哪种 tokenStore 的话,就默认的用 jwt ,如果想要使用 redis 存储的话,必须明确的指定 core.security.oAuth2.tokenStore: redis 最后认证服务的配置中还需要做一些修改123456789101112131415161718192021@Configuration@EnableAuthorizationServerpublic class MyAuthorizationServerConfig extends AuthorizationServerConfigurerAdapter &#123; @Autowired(required = false) private JwtAccessTokenConverter jwtAccessTokenConverter; @Override public void configure(AuthorizationServerEndpointsConfigurer endpoints) throws Exception &#123; // ... 省略其他配置 // 只有配置使用JWT的时候才会生效 if (jwtAccessTokenConverter != null) &#123; endpoints.accessTokenConverter(jwtAccessTokenConverter); &#125; &#125; // ... 省略其他代码&#125; 这里,就是给 endpoints 指定一下 JwtAccessTokenConverter 就可以了 测试请求 Token 还是跟之前的方式一样的, 如下: 可以看到这里的token就是使用jwt了 在 jwt.io 中可以把刚刚生成的token解析一下,内容如下: 这个就是我们生成的 JWT 中包含的信息 TokenEnhancer 的使用TokenEnhancer 是一个增强器,JWT 中是可以放一些我们自定义的信息的,如果要加入一些我们自己的信息的话,就得使用 TokenEnhancer 还是修改 TokenStoreConfig ,在 JwtTokenConfig 这个内部类中,加一个配置12345@Bean@ConditionalOnBean(TokenEnhancer.class)public TokenEnhancer jwtTokenEnhancer()&#123; return new MyJwtTokenEnhancer();&#125; 然后 MyJwtTokenEnhancer 代码如下:1234567891011public class MyJwtTokenEnhancer implements TokenEnhancer &#123; @Override public OAuth2AccessToken enhance(OAuth2AccessToken accessToken, OAuth2Authentication authentication) &#123; Map&lt;String, Object&gt; info = new HashMap&lt;&gt;(1); info.put(&quot;custom&quot;, &quot;test&quot;); ((DefaultOAuth2AccessToken) accessToken).setAdditionalInformation(info); return accessToken; &#125;&#125; 这里加了 @ConditionalOnBean,是必须存在一个TokenEnhancer 的时候,才被创建, 之前的 JwtAccessTokenConverter 也是一个 TokenEnhancer 最后,认证服务器配置类 MyAuthorizationServerConfig 中,需要修改一下123456789101112131415161718192021@Autowired(required = false)private TokenEnhancer jwtTokenEnhancer;@Overridepublic void configure(AuthorizationServerEndpointsConfigurer endpoints) throws Exception &#123; endpoints.authenticationManager(authenticationManager); endpoints.userDetailsService(userDetailsService); endpoints.tokenStore(tokenStore); // 只有配置使用JWT的时候才会生效 if (jwtAccessTokenConverter != null &amp;&amp; jwtTokenEnhancer != null) &#123; TokenEnhancerChain enhancerChain = new TokenEnhancerChain(); List&lt;TokenEnhancer&gt; enhancers = new ArrayList&lt;&gt;(); enhancers.add(jwtTokenEnhancer); enhancers.add(jwtAccessTokenConverter); enhancerChain.setTokenEnhancers(enhancers); endpoints.tokenEnhancer(enhancerChain) .accessTokenConverter(jwtAccessTokenConverter); &#125;&#125; 测试获取token,然后解析结果如下: 自定义数据解析这里 Spring 在解析JWT的时候会解析成一个 Authentication 对象,并不会解析我们上面设置的自定义字段,这个还需要我们自己去解析 demo 项目中增加一个 jwt 解析的依赖 12345&lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt; &lt;artifactId&gt;jjwt&lt;/artifactId&gt; &lt;version&gt;0.7.0&lt;/version&gt;&lt;/dependency&gt; 然后再 /user/me 这个接口中做解析,代码如下:123456789101112@GetMapping(&quot;/me&quot;)public Object me(Authentication authentication, HttpServletRequest request) throws UnsupportedEncodingException &#123; // 从请求头中获取到token String jwtToken = StringUtils.substringAfter(request.getHeader(&quot;Authorization&quot;), AUTHORIZATION_PREFIX); log.info(&quot;请求头中的token:&#123;&#125;&quot;, jwtToken); // 获取配置中的 jwtTokenSignKey String jwtTokenSignKey = securityProperties.getOAuth2().getJwtTokenSignKey(); Claims claims = Jwts.parser().setSigningKey(jwtTokenSignKey.getBytes(&quot;UTF-8&quot;)).parseClaimsJws(jwtToken).getBody(); return claims;&#125; 效果如下: 本文代码传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-28-Token配置]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-28-Token%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[述前面的文章中,实现了我们三种自定义登录方式的 token 返回,做的也全是一些资源服务器的配置,本文再来看一些关于认证服务器的配置, 比如 Token 的过期时间,存储方式等等 认证服务器配置在之前的配置类 MyAuthorizationServerConfig 做修改,首先继承 AuthorizationServerConfigurerAdapter client自定义配置之前我们配置 client_id 和 client_serect 都是在应用的 application.yml 中去配置的,这种方式是基于内存的配置. 而且token的过期时间等等配置都是用的默认的配置, 在实际情况中, 每个应用的 client_id 和 client_serect 都应该放到数据库中去存储,就比如我们之前去QQ互联平台去创建应用一样, 下面看一下如何使用数据库存储client信息首先需要修改我们认证服务器的配置类 MyAuthorizationServerConfig ,然后继承 AuthorizationServerConfigurerAdapter, 代码如下:12345678910111213141516171819202122232425262728293031323334@Configuration@EnableAuthorizationServerpublic class MyAuthorizationServerConfig extends AuthorizationServerConfigurerAdapter &#123; @Autowired private AuthenticationManager authenticationManager; @Autowired private UserDetailsService userDetailsService; @Autowired private DataSource dataSource; @Autowired private TokenStore tokenStore; @Override public void configure(AuthorizationServerEndpointsConfigurer endpoints) throws Exception &#123; endpoints.authenticationManager(authenticationManager); endpoints.userDetailsService(userDetailsService); endpoints.tokenStore(tokenStore); &#125; @Override public void configure(ClientDetailsServiceConfigurer clients) throws Exception &#123; // 这个地方指的是从jdbc查出数据来存储 clients.withClientDetails(clientDetails()); &#125; private ClientDetailsService clientDetails() &#123; return new JdbcClientDetailsService(dataSource); &#125;&#125; 这里重写了两个方法,上面的这个是 endpoints 的一些配置, 只要重写了这个方法,就必须我们自己去指定 authenticationManager 和 userDetailsService 然后下面这个就是对 client 的一些配置,只要重写了这个方法,之前我们在 application.yml 中的配置就失效了 client的信息默认是在内存中的,这种方式对于单个服务器是完全正常的(即,在发生故障的情况下,低流量和热备份备份服务器).大多数项目可以从这里开始,也可以在开发模式下运行,以便轻松启动没有依赖关系的服务器. 我们这里只介绍用数据库存储的方式, 就是设置一下数据源就可以了 然后这里在数据库里面需要用到一张表,如下:1234567891011121314CREATE TABLE `oauth_client_details` ( `client_id` varchar(48) NOT NULL, `resource_ids` varchar(256) DEFAULT NULL, `client_secret` varchar(256) DEFAULT NULL, `scope` varchar(256) DEFAULT NULL, `authorized_grant_types` varchar(256) DEFAULT NULL, `web_server_redirect_uri` varchar(256) DEFAULT NULL, `authorities` varchar(256) DEFAULT NULL, `access_token_validity` int(11) DEFAULT NULL, `refresh_token_validity` int(11) DEFAULT NULL, `additional_information` varchar(4096) DEFAULT NULL, `autoapprove` varchar(256) DEFAULT NULL, PRIMARY KEY (`client_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 添一条测试数据123456789101112131415161718192021222324252627INSERT INTO oauth_client_details ( client_id, resource_ids, client_secret, scope, authorized_grant_types, web_server_redirect_uri, authorities, access_token_validity, refresh_token_validity, additional_information, autoapprove)VALUES( &apos;testAppid&apos;, NULL, &apos;testSecret&apos;, &apos;read,write,all&apos;, &apos;authorization_code,refresh_token,password&apos;, &apos;http://www.pinzhi365.com&apos;, &apos;ROLE_USER&apos;, 86400, 86400, NULL, FALSE); 具体的每列的作用,说明在这里 配置 Token 的存储方式在之前的配置中,Token都是存放在内存中的,只要重启应用,token就都失效了,通常情况下都是要放到redis里面的,下面看一下如何使用 redis 存储 Token 新建一个 TokenStoreConfig 类,如下:1234567891011@Configurationpublic class TokenStoreConfig &#123; @Autowired private RedisConnectionFactory connectionFactory; @Bean public TokenStore redisTokenStore()&#123; return new RedisTokenStore(connectionFactory); &#125;&#125; 然后在上面的配置类中的第一个 configure 方法中注入进去就ok了 踩坑这里遇到一个问题,我们项目目前的依赖 spring-boot-starter-data-redis 的默认实现是 Jedis, RedisTokenStore 中会使用 Pipeline 的功能,Jedis对于单节点可以支持 Pipeline,但是集群则没有支持 Pipeline, 因为我这里配置的是集群,所以这里就不能正常运行,错误如下: 1UnsupportedOperationException, Pipeline is currently not supported for JedisClusterConnection. 要解决这个问题,要么自己实现Jedis对Pipeline的支持,这比较复杂.另一种就是使用lettuce代替Jedis,luttuce对于集群使用Pipeline有很好的支持. 我们只需要引入luttuce的依赖:1234&lt;dependency&gt; &lt;groupId&gt;biz.paluch.redis&lt;/groupId&gt; &lt;artifactId&gt;lettuce&lt;/artifactId&gt;&lt;/dependency&gt; 然后在 redis 的配置类中,加入:12345678910111213@Primary@Beanpublic LettuceConnectionFactory redisConnectionFactory(RedisProperties redisProperties) &#123; RedisProperties.Cluster cluster = redisProperties.getCluster(); if (cluster != null) &#123; RedisClusterConfiguration configuration = new RedisClusterConfiguration(cluster.getNodes()); return new LettuceConnectionFactory(configuration); &#125; else &#123; LettuceConnectionFactory connectionFactory = new LettuceConnectionFactory(redisProperties.getHost(), redisProperties.getPort()); connectionFactory.setPassword(redisProperties.getPassword()); return connectionFactory; &#125;&#125; 这样就ok了 测试如下: 接口可以正常访问,就说明数据库配置的client信息已经生效了, 然后再看一下redis中 上面生成的token也正常的放到了redis中 本文代码传送门 (忘记创建分支了,跟前面一章写在了同一个分支上)]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-27-注册逻辑重构]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-27-%E6%B3%A8%E5%86%8C%E9%80%BB%E8%BE%91%E9%87%8D%E6%9E%84%2F</url>
    <content type="text"><![CDATA[述社交账号登录的逻辑完成之后,再来重构一下用户注册的逻辑, 回顾一下之前浏览器端写的注册逻辑 用户授权第三方登录之后,social 拿到用户的信息 查询 userconnection 表,看有没有对应的数据 没有找到的话,跳转到我们配置的注册页,默认是 /signUp 到注册页填写信息,发送请求,然后调用数据库,最终拿到我们业务系统中的唯一id 调用 social 提供的 providerSignInUtils 做绑定 大概流程就是这样, 在app环境中,这个流程会有一些问题,浏览器没有cookie,也就没有 JSESSIONID ,第一次第三方登录请求放到session中的数据,在注册的请求中是拿不到的,而且这个跳转是重定向到登录页了,在App中应该返回一个json的提示信息即可 解决思路session中拿不到的数据,我们可以放到redis里面, 重定向到登录页的时候,我们可以让他重定向到一个请求中去,然后返回json的数据, 用户调用注册请求的时候,从redis中获取之前的缓存,更新到数据库就可以了 注意要把之前配置的自动注册的类先关掉 修改注册跳转路径我们之前配置的注册地址是在 SocialConfig 中,通过 @Bean 注入 securitySocialConfigurer 的时候配置的,如下 这个配置在浏览器中是没有问题的,不需要修改,只需要修改在app环境中的配置, app项目中,新建类 SpringSocialConfigurerPostProcessor 代码如下:123456789101112131415161718192021222324252627282930313233public class SpringSocialConfigurerPostProcessor implements BeanPostProcessor&#123; /** * 任何bean初始化之前做的事情 * @param bean * @param beanName * @return * @throws BeansException */ @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; return bean; &#125; /** * 任何bean初始化之后做的事情 * 我们这里要做的就是在 SpringSocialConfigurer 加载完了之后,把他的注册跳转的url改掉 * @param bean * @param beanName * @return * @throws BeansException */ @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; if (StringUtils.equals(beanName,&quot;securitySocialConfigurer&quot;))&#123; MySpringSocialConfigurer mySpringSocialConfigurer = (MySpringSocialConfigurer) bean; mySpringSocialConfigurer.signupUrl(SecurityConstants.DEFAULT_APP_SIGNUP_URL); return mySpringSocialConfigurer; &#125; return null; &#125;&#125; 这个类的作用就是,在 SocialConfig 中,通过 @Bean 注入的 securitySocialConfigurer 这个类,加载完成之后,修改一下他的 signupUrl 改为redis存储app项目中,新建一个 AppSignUpUtil ,代码如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960@Componentpublic class AppSignUpUtil &#123; @Autowired private RedisTemplate redisTemplate; @Autowired private UsersConnectionRepository usersConnectionRepository; @Autowired private ConnectionFactoryLocator connectionFactoryLocator; /** * 将第三方用户信息放到redis缓存中去 * @param request * @param connectionData */ public void saveConnectionData(HttpServletRequest request, ConnectionData connectionData) &#123; redisTemplate.opsForValue().set(getRedisKey(request), connectionData, 10, TimeUnit.MINUTES); &#125; /** * 绑定业务系统中的用户 * @param request * @param userId */ public void doPostSignUp(HttpServletRequest request, String userId)&#123; // 判断缓存中有没有存第三方的用户信息 String redisKey = getRedisKey(request); if (!redisTemplate.hasKey(redisKey)) &#123; throw new AppSerectException(&quot;无法找到缓存中的用户社交账号信息&quot;); &#125; // 从缓存中取出来 ConnectionData connectionData = (ConnectionData) redisTemplate.opsForValue().get(redisKey); // ConnectionData对象转成 connection对象 Connection&lt;?&gt; connection = connectionFactoryLocator.getConnectionFactory(connectionData.getProviderId()).createConnection(connectionData); // 操作数据库 创建connection信息 usersConnectionRepository.createConnectionRepository(userId).addConnection(connection); &#125; /** * 获取缓存的key * @param request * @return */ private String getRedisKey(HttpServletRequest request) &#123; // 从请求中获取设备id String deviceId = request.getHeader(&quot;deviceId&quot;); if (StringUtils.isBlank(deviceId)) &#123; throw new AppSerectException(&quot;deviceId不能为空&quot;); &#125; return Constants.SOCIAL_USER_INFO_KEY_PREFIX + deviceId; &#125;&#125; 这里主要是两个方法,一个放缓存,一个是绑定业务系统与第三方用户的信息, 跟我们之前用的 ProviderSignInUtils 是一样的 controller处理上面处理了 APP 的跳转路径,对应的应该有一个 controller 去处理具体的逻辑, 新建 SocialController ,代码如下:123456789101112131415161718192021222324252627@RestControllerpublic class SocialController &#123; @Autowired private ProviderSignInUtils providerSignInUtils; @Autowired private AppSignUpUtil appSignUpUtil; @GetMapping(SecurityConstants.DEFAULT_APP_SIGNUP_URL) public SocialUserInfo getSocialUserInfo(HttpServletRequest request)&#123; SocialUserInfo userInfo = new SocialUserInfo(); Connection&lt;?&gt; connection = providerSignInUtils.getConnectionFromSession(new ServletWebRequest(request)); userInfo.setProviderId(connection.getKey().getProviderId()); userInfo.setProviderUserId(connection.getKey().getProviderUserId()); userInfo.setNickname(connection.getDisplayName()); userInfo.setHeadimg(connection.getImageUrl()); // 存到redis中去 appSignUpUtil.saveConnectionData(request, connection.createData()); return userInfo; &#125;&#125; 这里其实就是把第三方的用户信息存到缓存中去 url拦截记得把配置的注册路径配置到不需要权限的集合里面去 注册接口修改浏览器的注册接口使用的是1providerSignInUtils.doPostSignUp(String.valueOf(userId), new ServletWebRequest(request)); 在 App 中,需要改成我们刚才创建的 AppSignUpUtil1appSignUpUtil.doPostSignUp(request,String.valueOf(userId)); 测试的话,还是按照之前的流程,先用web环境拿到code,然后切换浏览器环境 本文代码传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-26-社交账号登录重构]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-26-%E7%A4%BE%E4%BA%A4%E8%B4%A6%E5%8F%B7%E7%99%BB%E5%BD%95%E9%87%8D%E6%9E%84%2F</url>
    <content type="text"><![CDATA[述上文中实现了用户名密码登录与短信验证码登录的重构,还剩最后一种登录方式,就是第三方社交账号的登录,下面再把第三方登录重构一下 在App里面使用第三方登录和之前的浏览器里面的第三方登录在流程上有一些区别, 在App的环境中,使用第三方用户访问的不是我们服务中的某个路径, 而是服务提供商提供的一个SDK, 不同的服务商的SDK都是不一样的,所以他们的 SDK 走的授权模式也不一样,一般是简化模式和授权码模式这两种 简化模式流程图如下: 简化模式中,第三方服务提供商直接会返回 openId 和 AccessToken ,到这儿的时候 App 就可以用 openId 去获取第三方社交账号的用户信息了,但是并不能访问我们后台提供的那些REST服务, 因为这里拿到的 token 是第三方社交账号的token,如果要访问我们自己的服务的话,还需要一个我们系统里生成的 token, 就跟前面的用户名密码登录之后的返回是一样的,最终返回一个我们系统的 Token ,简单来说,就是需要一个用第三方的 OpenId 换我们系统的 Token 的服务 这个和我们之前自己写的短信验证码登录的方式其实是一样的,只不过这里是要去验证 openId ,那传过来的 openId 从数据库里面查一下对应的用户信息, 如果存在的话,就生成一个 token 返回给用户 代码实现跟之前的短信验证码的登录是一样的, OpenIdAuthenticationToken OpenIdAuthenticationFilter OpenIdAuthenticationProvider 把之前短信的直接复制过来改改就能用, OpenIdAuthenticationToken 如下:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public class OpenIdAuthenticationToken extends AbstractAuthenticationToken &#123; private static final long serialVersionUID = SpringSecurityCoreVersion.SERIAL_VERSION_UID; /** * openId */ private final Object principal; /** * 服务提供商的id */ private String providerId; /** * This constructor can be safely used by any code that wishes to create a * &lt;code&gt;UsernamePasswordAuthenticationToken&lt;/code&gt;, as the &#123;@link #isAuthenticated()&#125; * will return &lt;code&gt;false&lt;/code&gt;. * */ public OpenIdAuthenticationToken(Object principal, String providerId) &#123; super(null); this.principal = principal; this.providerId = providerId; setAuthenticated(false); &#125; /** * This constructor should only be used by &lt;code&gt;AuthenticationManager&lt;/code&gt; or * &lt;code&gt;AuthenticationProvider&lt;/code&gt; implementations that are satisfied with * producing a trusted (i.e. &#123;@link #isAuthenticated()&#125; = &lt;code&gt;true&lt;/code&gt;) * authentication token. * * @param principal * @param authorities */ public OpenIdAuthenticationToken(Object principal, Collection&lt;? extends GrantedAuthority&gt; authorities) &#123; super(authorities); this.principal = principal; // must use super, as we override super.setAuthenticated(true); &#125; @Override public Object getCredentials() &#123; return null; &#125; @Override public Object getPrincipal() &#123; return this.principal; &#125; public String getProviderId() &#123; return providerId; &#125; @Override public void setAuthenticated(boolean isAuthenticated) throws IllegalArgumentException &#123; if (isAuthenticated) &#123; throw new IllegalArgumentException( &quot;Cannot set this token to trusted - use constructor which takes a GrantedAuthority list instead&quot;); &#125; super.setAuthenticated(false); &#125; @Override public void eraseCredentials() &#123; super.eraseCredentials(); &#125;&#125; 然后是过滤器 OpenIdAuthenticationFilter1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586public class OpenIdAuthenticationFilter extends AbstractAuthenticationProcessingFilter &#123; /** * 请求中的openId */ private String openIdParameter = SecurityConstants.DEFAULT_PARAMETER_NAME_OPENID; /** * 请求中的providerId */ private String providerIdParameter = SecurityConstants.DEFAULT_PARAMETER_NAME_PROVIDERID; /** * 只支持post请求 */ private boolean postOnly = true; public OpenIdAuthenticationFilter() &#123; super(new AntPathRequestMatcher(SecurityConstants.DEFAULT_LOGIN_PROCESSING_URL_OPENID, &quot;POST&quot;)); &#125; @Override public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException, IOException, ServletException &#123; if (postOnly &amp;&amp; !request.getMethod().equals(&quot;POST&quot;)) &#123; throw new AuthenticationServiceException( &quot;Authentication method not supported: &quot; + request.getMethod()); &#125; String openId = obtainOpenId(request); String providerId = obtainProviderId(request); if (openId == null) &#123; openId = &quot;&quot;; &#125; if (providerId == null) &#123; providerId = &quot;&quot;; &#125; openId = openId.trim(); providerId = providerId.trim(); // 创建token对象 OpenIdAuthenticationToken openIdAuthenticationToken = new OpenIdAuthenticationToken(openId, providerId); // 设置一些请求的详细信息 setDetails(request, openIdAuthenticationToken); return this.getAuthenticationManager().authenticate(openIdAuthenticationToken); &#125; protected void setDetails(HttpServletRequest request, OpenIdAuthenticationToken authRequest) &#123; authRequest.setDetails(authenticationDetailsSource.buildDetails(request)); &#125; protected String obtainOpenId(HttpServletRequest request) &#123; return request.getParameter(openIdParameter); &#125; protected String obtainProviderId(HttpServletRequest request) &#123; return request.getParameter(providerIdParameter); &#125; public void setOpenIdParameter(String openIdParameter) &#123; Assert.hasText(openIdParameter, &quot;Username parameter must not be empty or null&quot;); this.openIdParameter = openIdParameter; &#125; public void setPostOnly(boolean postOnly) &#123; this.postOnly = postOnly; &#125; public final String getOpenIdParameter() &#123; return openIdParameter; &#125; public String getProviderIdParameter() &#123; return providerIdParameter; &#125; public void setProviderIdParameter(String providerIdParameter) &#123; this.providerIdParameter = providerIdParameter; &#125;&#125; 最后是具体的验证 OpenIdAuthenticationProvider12345678910111213141516171819202122232425262728293031323334353637383940@Datapublic class OpenIdAuthenticationProvider implements AuthenticationProvider &#123; private SocialUserDetailsService userDetailsService; private UsersConnectionRepository usersConnectionRepository; @Override public Authentication authenticate(Authentication authentication) throws AuthenticationException &#123; OpenIdAuthenticationToken authenticationToken = (OpenIdAuthenticationToken) authentication; // 通过 providerId 和 openId 去 usersConnection 表里面查询 Set&lt;String&gt; providerUserIds = new HashSet&lt;&gt;(); providerUserIds.add((String) authenticationToken.getPrincipal()); Set&lt;String&gt; userIds = usersConnectionRepository.findUserIdsConnectedTo(authenticationToken.getProviderId(), providerUserIds); if(CollectionUtils.isEmpty(userIds) || userIds.size() != 1) &#123; throw new InternalAuthenticationServiceException(&quot;无法获取用户信息&quot;); &#125; // 通过id查询用户信息 String userId = userIds.iterator().next(); UserDetails user = userDetailsService.loadUserByUserId(userId); if (user == null) &#123; throw new InternalAuthenticationServiceException(&quot;无法获取用户信息&quot;); &#125; // 组装认证成功后的token对象 OpenIdAuthenticationToken authenticationResult = new OpenIdAuthenticationToken(user, user.getAuthorities()); authenticationResult.setDetails(authenticationToken.getDetails()); return authenticationResult; &#125; @Override public boolean supports(Class&lt;?&gt; authentication) &#123; return OpenIdAuthenticationToken.class.isAssignableFrom(authentication); &#125;&#125; 搞定之后还需要一个配置类, 作用是使这个filter生效, 新建 OpenIdAuthenticationSecurityConfig123456789101112131415161718192021222324252627282930313233343536@Componentpublic class OpenIdAuthenticationSecurityConfig extends SecurityConfigurerAdapter&lt;DefaultSecurityFilterChain, HttpSecurity&gt; &#123; @Autowired private AuthenticationSuccessHandler myAuthenticationSuccessHandler; @Autowired private AuthenticationFailureHandler myAuthenticationFailureHandler; @Autowired private SocialUserDetailsService userDetailsService; @Autowired private UsersConnectionRepository usersConnectionRepository; @Override public void configure(HttpSecurity http) throws Exception &#123; // 过滤器的配置 OpenIdAuthenticationFilter openIdAuthenticationFilter = new OpenIdAuthenticationFilter(); // 设置AuthenticationManager openIdAuthenticationFilter.setAuthenticationManager(http.getSharedObject(AuthenticationManager.class)); // 设置登录成功失败的处理 openIdAuthenticationFilter.setAuthenticationSuccessHandler(myAuthenticationSuccessHandler); openIdAuthenticationFilter.setAuthenticationFailureHandler(myAuthenticationFailureHandler); // Provider的配置 OpenIdAuthenticationProvider openIdAuthenticationProvider = new OpenIdAuthenticationProvider(); openIdAuthenticationProvider.setUserDetailsService(userDetailsService); openIdAuthenticationProvider.setUsersConnectionRepository(usersConnectionRepository); http.authenticationProvider(openIdAuthenticationProvider) .addFilterAfter(openIdAuthenticationFilter, UsernamePasswordAuthenticationFilter.class); &#125;&#125; 最后加到资源服务器的配置中去12345678910111213141516171819@Configuration@EnableResourceServerpublic class MyResourcesServerConfig extends ResourceServerConfigurerAdapter &#123; @Autowired private OpenIdAuthenticationSecurityConfig openIdAuthenticationSecurityConfig; @Override public void configure(HttpSecurity http) throws Exception &#123; // ... 省略部分代码 http // ... 省略部分代码 .and() .apply(openIdAuthenticationSecurityConfig) // ... 省略部分代码 ; &#125;&#125; 测试最后启动项目,做一下测试,如图: 通过传进去的 openId, 成功获取到了Token 授权码模式流程图如下: 这个模式,app请求qq或微信去获取授权码,然后将授权码交给我们自己的第三方client,由第三方client带着授权码去qq或微信申请令牌,然后发放令牌给第三方应用,然后读取用户数据.然后第三方应用重新生成自己的令牌返回给app. 流程大致就是这样 测试首先把项目的依赖转到web上面,然后请求第三方登录, 在代码中打个断点,如下: 然后这步获取到code的时候停掉项目,切换到app的依赖上去,然后再重新请求他的回调,这时候会去获取到第三方的token,然后做个重定向,因为在 web 环境中,授权成功之后就是重定向的,但是在 app 环境中,应该是返回给App一个token, 所以这里要把第三方授权成功之后的动作修改一下,让他走我们自己的成功处理器 core 项目中新建 SocialAuthenticationFilterPostProcessor12345678public interface SocialAuthenticationFilterPostProcessor &#123; /** * SocialAuthenticationFilter 请求处理 * @param socialAuthenticationFilter */ void process(SocialAuthenticationFilter socialAuthenticationFilter);&#125; 然后 app 项目中,需要一个实现类, AppSocialAuthenticationFilterPostProcessor:1234567891011@Componentpublic class AppSocialAuthenticationFilterPostProcessor implements SocialAuthenticationFilterPostProcessor &#123; @Autowired private AuthenticationSuccessHandler myAuthenticationSuccessHandler; @Override public void process(SocialAuthenticationFilter socialAuthenticationFilter) &#123; socialAuthenticationFilter.setAuthenticationSuccessHandler(myAuthenticationSuccessHandler); &#125;&#125; 这里就是让第三方认证成功之后,走我们自己的成功处理, 浏览器的配置中不需要去实现,让他走默认的重定向就可以 MySpringSocialConfigurer 需要做一些修改,如下:12345678910111213141516171819202122@Datapublic class MySpringSocialConfigurer extends SpringSocialConfigurer &#123; private String filterProcessesUrl; private SocialAuthenticationFilterPostProcessor socialAuthenticationFilterPostProcessor; public MySpringSocialConfigurer(String filterProcessesUrl) &#123; this.filterProcessesUrl = filterProcessesUrl; &#125; @Override protected &lt;T&gt; T postProcess(T object) &#123; SocialAuthenticationFilter filter = (SocialAuthenticationFilter) super.postProcess(object); filter.setFilterProcessesUrl(filterProcessesUrl); if (socialAuthenticationFilterPostProcessor != null) &#123; socialAuthenticationFilterPostProcessor.process(filter); &#125; return (T) filter; &#125;&#125; 最后,总的配置 SocialConfig 中,注入进去,赋值给 MySpringSocialConfigurer 里面的 SocialAuthenticationFilterPostProcessor:123456789101112131415161718@Configuration@EnableSocial@Order(10)public class SocialConfig extends SocialConfigurerAdapter &#123; @Autowired(required = false) private SocialAuthenticationFilterPostProcessor socialAuthenticationFilterPostProcessor; @Bean public SpringSocialConfigurer securitySocialConfigurer()&#123; String filterProcessesUrl = securityProperties.getSocial().getFilterProcessesUrl(); MySpringSocialConfigurer mySpringSocialConfigurer = new MySpringSocialConfigurer(filterProcessesUrl); mySpringSocialConfigurer.signupUrl(securityProperties.getBrowser().getSignUpUrl()); mySpringSocialConfigurer.setSocialAuthenticationFilterPostProcessor(socialAuthenticationFilterPostProcessor); return mySpringSocialConfigurer; &#125;&#125; 最后是测试, 还是刚才那个流程,先切到web环境中请求code,然后切回app ,请求刚才的回调地址 本文代码传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-25-用户名密码登录重构]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-25-%E7%94%A8%E6%88%B7%E5%90%8D%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95%E9%87%8D%E6%9E%84%2F</url>
    <content type="text"><![CDATA[述上文中,了解了 Spring Security OAuth 的一个大致的流程,本文将我们之前写的用户名密码登录的方式重新修改一下,让他登录之后也返回一个token, 后续请求都通过token来请求 实现思路回顾一下之前的流程图: 我们在用户名密码等登陆认证完成之后,会生成一个已认证的 Authentication 对象, 看一下图中的 Authentication 这里,也就是说我们再搞到一个 OAuth2Request 对象,就可以用他后面的逻辑去处理生成了,大致流程图如下 这里就是在登录成功之后,在登录成功的处理器里面,从请求中获取到basic client的信息,然后去获取 ClientDetails 的信息, 虚线框住的部分是需要我们自己去实现的 具体实现我们最终是希望拿到一个 OAuth2Request 对象, 如图,得先通过 ClientDetailsService 获取 ClientDetails ,所以第一步得先从请求中把 ClientId 拿出来 之前我们的请求头中有 Authorization:Basic dGVzdEFwcGlkOnRlc3RTZWNyZXQ= 我们只要把这个拿出来做解析就可以拿到 clientId 了 BasicAuthenticationFilter 中有一段解析的代码, 如下: 具体的解码方法代码如下: 把这两段代码直接复制到我们的成功处理器里面,然后做一些修改即可,如下:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980@Slf4j@Component(&quot;myAuthenticationSuccessHandler&quot;)public class MyAuthenticationSuccessHandler extends SavedRequestAwareAuthenticationSuccessHandler &#123; @Autowired private ObjectMapper objectMapper; @Autowired private ClientDetailsService clientDetailsService; @Autowired private AuthorizationServerTokenServices authorizationServerTokenServices; @Autowired private SecurityProperties securityProperties; @Override public void onAuthenticationSuccess(HttpServletRequest request, HttpServletResponse response, Authentication authentication) throws IOException, ServletException &#123; log.info(&quot;用户登录成功...&quot;); String header = request.getHeader(&quot;Authorization&quot;); if (header == null || !header.startsWith(&quot;Basic &quot;)) &#123; throw new UnapprovedClientAuthenticationException(&quot;请在header中传入client信息&quot;); &#125; String[] tokens = extractAndDecodeHeader(header, request); assert tokens.length == 2; String clientId = tokens[0]; String clientSecret = tokens[1]; // 拿到client的信息之后,去构建clientDetails ClientDetails clientDetails = clientDetailsService.loadClientByClientId(clientId); // 验证 if (clientDetails == null) &#123; throw new UnapprovedClientAuthenticationException(&quot;clientId错误&quot;); &#125; else if (!StringUtils.equals(clientDetails.getClientSecret(), clientSecret)) &#123; throw new UnapprovedClientAuthenticationException(&quot;clientSecret不匹配&quot;); &#125; // 创建 TokenRequest对象 TokenRequest tokenRequest = new TokenRequest(MapUtils.EMPTY_MAP, clientId, clientDetails.getScope(), &quot;custom&quot;); OAuth2Request oAuth2Request = tokenRequest.createOAuth2Request(clientDetails); OAuth2Authentication oAuth2Authentication = new OAuth2Authentication(oAuth2Request, authentication); OAuth2AccessToken accessToken = authorizationServerTokenServices.createAccessToken(oAuth2Authentication); // 返回 response.setContentType(&quot;application/json;charset=UTF-8&quot;); response.getWriter().write(objectMapper.writeValueAsString(accessToken)); &#125; private String[] extractAndDecodeHeader(String header, HttpServletRequest request) throws IOException &#123; byte[] base64Token = header.substring(6).getBytes(&quot;UTF-8&quot;); byte[] decoded; try &#123; decoded = Base64.decode(base64Token); &#125; catch (IllegalArgumentException e) &#123; throw new BadCredentialsException( &quot;Failed to decode basic authentication token&quot;); &#125; String token = new String(decoded, &quot;UTF-8&quot;); int delim = token.indexOf(&quot;:&quot;); if (delim == -1) &#123; throw new BadCredentialsException(&quot;Invalid basic authentication token&quot;); &#125; return new String[] &#123; token.substring(0, delim), token.substring(delim + 1) &#125;; &#125;&#125; 这里主要是先获取 client 的信息,然后一步一步往下创建对象就可以了 资源服务器的配置现在项目里的一些登录路径什么的都没有做配置,所以在资源服务器的配置类里还需要做一些配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465@Configuration@EnableResourceServerpublic class MyResourcesServerConfig extends ResourceServerConfigurerAdapter &#123; @Autowired private SecurityProperties securityProperties; @Autowired private UserDetailsService userDetailsService; @Autowired private DataSource dataSource; @Autowired private SmsAuthenticationSecurityConfig smsAuthenticationSecurityConfig; @Autowired private ValidateCodeSecurityConfig validateCodeSecurityConfig; @Autowired private SpringSocialConfigurer securitySocialConfigurer; @Autowired private AuthenticationSuccessHandler myAuthenticationSuccessHandler; @Autowired private AuthenticationFailureHandler myAuthenticationFailureHandler; @Override public void configure(HttpSecurity http) throws Exception &#123; // web网页登录的配置 http.formLogin() .loginPage(SecurityConstants.DEFAULT_UNAUTHENTICATION_URL) .loginProcessingUrl(SecurityConstants.DEFAULT_LOGIN_PROCESSING_URL_FORM) .successHandler(myAuthenticationSuccessHandler) .failureHandler(myAuthenticationFailureHandler); http .apply(validateCodeSecurityConfig) .and() .apply(smsAuthenticationSecurityConfig) .and() .apply(securitySocialConfigurer) .and() .authorizeRequests() // 匹配的是登录页的话放行 .antMatchers( SecurityConstants.DEFAULT_UNAUTHENTICATION_URL, SecurityConstants.DEFAULT_LOGIN_PROCESSING_URL_MOBILE, SecurityConstants.DEFAULT_VALIDATE_CODE_URL_PREFIX + &quot;/*&quot;, securityProperties.getBrowser().getLoginPage(), SecurityConstants.DEFAULT_SIGN_UP_URL, securityProperties.getBrowser().getSignUpPage(), SecurityConstants.GET_SOCIAL_USER_URL, securityProperties.getBrowser().getSession().getSessionInvalidUrl(), &quot;/user/register&quot; ) .permitAll() // 授权请求. anyRequest 就表示所有的请求都需要权限认证 .anyRequest().authenticated() .and() .csrf().disable() ; &#125;&#125; 直接先把浏览器的配置拷过来,然后去掉一些浏览器特有的配置,后期再改 测试启动项目,访问用户名密码登录的接口 /authentication/form ,效果如下: 可以看到,这里已经成功返回token了 短信验证码登录如果之前的验证码是放到 redis 中的话,短信登录现在也是可以正常使用了的,如果是使用的 session 的方式,那还需要做一些修改,因为在App的请求中是没有cookie的,也就没有JSESSIONID, 也就获取不到session, 具体修改这里就不说了 本文代码传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-24-Spring Security OAuth核心源码]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-24-Spring-Security-OAuth%E6%A0%B8%E5%BF%83%E6%BA%90%E7%A0%81%2F</url>
    <content type="text"><![CDATA[述上文中,实现了认证服务器和资源服务器,四种 OAuth 的授权也可以使用了,但是现在仅限于 Spring Security OAuth 给出的4种授权模式,我们之前写的 用户名密码认证, 短信认证, 第三方社交账号认证,这都是不支持的, 那如何让我们自己写的认证方式也按照 OAuth 这样返回一个token, 下面首先来了解一下 Spring Security OAuth 的一些核心的代码 流程 如图(绿色部分是model, 蓝色部分是接口,以及他们各自的默认实现类) TokenEndPoint: 流程入口点, 用来处理获取令牌的请求 ClientDeatilsService: 读取第三方应用信息,根据传过来的clientId,读取client相应的一些配置信息 ClientDetails: 封装第三方应用的信息 TokenRequest: 封装了请求中的其他参数, 比如 grant_type, scope 等等, TokenRequest 包含了 ClientDetails TokenGranter: 封装了4种授权模式的实现,根据 grant_type 去找一个具体的实现, 执行完毕后会生成 OAuthRequest 和 Authentication OAuthRequest: 包含了 ClientDetails 和 TokenRequest Authentication: 封装的授权用户的信息,是通过 UserdetailsService 查询出来的,最终 OAuthRequest 和 Authentication 会组成一个 OAuth2Authentication 对象 OAuth2Authentication: 通过这个对象可以知道,是哪个用户在哪个应用请求授权,走的是哪种授权模式等等这些信息 AuthorizationServerTokenServices: 最终生成令牌的地方,这个对象引用了 TokenStore 负责令牌的存取,还有 TokenEnhancer 令牌的增强器,对生成之后的令牌做一些处理 源码根据上面的流程一步一步去看一下代码, 首先是 TokenEndPoint 如下: 然后这个类最后会调用 TokenGranter, 看下这个的代码: 这里也就是做了个验证,然后调用 AuthorizationServerTokenServices 代码如下: 这里才是真正创建token的地方, 然后创建token的方法是 整个大致的流程就是这样, 一些具体的代码可以自己点进去看一下]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-23-实现标准的OAuth服务提供商]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-23-%E5%AE%9E%E7%8E%B0%E6%A0%87%E5%87%86%E7%9A%84OAuth%E6%9C%8D%E5%8A%A1%E6%8F%90%E4%BE%9B%E5%95%86%2F</url>
    <content type="text"><![CDATA[述本文开始将构建一个 OAuth 服务提供商, 主要就是两大块, 认证服务器和资源服务器, 之后的代码都写在 app 的项目里,所以先把demo项目中的 web 的依赖改成 app 的依赖12345&lt;dependency&gt; &lt;groupId&gt;com.security&lt;/groupId&gt; &lt;artifactId&gt;security-example-app&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; 先把项目启动一下, 修改一些启动过程中可能会出现的问题 问题一123456789101112***************************APPLICATION FAILED TO START***************************Description:Field authenticationFailureHandler in com.security.example.core.validate.filter.ValidateCodeFilter required a bean of type &apos;org.springframework.security.web.authentication.AuthenticationFailureHandler&apos; that could not be found.Action:Consider defining a bean of type &apos;org.springframework.security.web.authentication.AuthenticationFailureHandler&apos; in your configuration. 这个就是验证码的处理器 ValidateCodeFilter 需要一个 AuthenticationFailureHandler 就是认证失败处理器 我们之前的自定义的认证成功失败处理器都是放在 web 项目里面的,因为 app 和 web 的请求成功失败可能不一样,所以先复制一份到 app 项目中来,之后再做修改 复制完成之后再次启动 问题二123456789101112***************************APPLICATION FAILED TO START***************************Description:Field passwordEncoder in com.security.example.demo.controller.TestController required a bean of type &apos;org.springframework.security.crypto.password.PasswordEncoder&apos; that could not be found.Action:Consider defining a bean of type &apos;org.springframework.security.crypto.password.PasswordEncoder&apos; in your configuration. 少了一个 PasswordEncoder, 这个之前也是写在了 web 项目里面,这个是通用的,所以挪到core项目里面 SecurityConfig 中 : 12345678@Configuration@EnableConfigurationProperties(&#123;SecurityProperties.class&#125;)public class SecurityConfig &#123; @Bean public PasswordEncoder passwordEncoder()&#123; return new BCryptPasswordEncoder(); &#125;&#125; 再次启动就ok了 认证服务器实现app 项目中,新建一个类 MyAuthorizationServerConfig, 代码如下:1234@Configuration@EnableAuthorizationServerpublic class MyAuthorizationServerConfig &#123;&#125; 这就ok了, 一个注解就实现了 重启项目, 这注意一下控制台的打印, 重点如下: 首先是分配了一个默认的 clientId 和 secret ,因为我们没有配置,所以每次启动的时候会为我们自动生成一个 下面图中,就是加了一些关于认证的请求 这里的 clientId 和 secret 可以自己做一下配置, 以免每次重启项目都要去修改, 在 application.yml 中,如下:12345security: oauth2: client: client-id: testAppid client-secret: testSecret 几种授权模式的实现Spring Security OAuth 是实现的标准的 OAuth2 协议,所以认证请求的参数什么的,在 OAuth2 的官网上可以找到, 传送门 授权码模式最常用的授权模式,之前我们接QQ微信的时候就是授权码模式 第一步,获取 code 授权码 请求地址1http://localhost:8080/oauth/authorize?response_type=code&amp;client_id=testAppid&amp;redirect_uri=http://www.example.com&amp;scope=all 这里有几个参数: response_type: 固定是code client_id: 上面配置的client-id redirect_uri: 回调地址 scope: 作用域 请求,如下图: 这里可以看到,要输入用户名和密码,这里输入用户名和密码, 最终会交给我们的 MyUserDetailsService 去验证 输入用户名密码,点击登录,如下: 这里返回一个 403 ,没有权限,原因是 用户必须有一个 ROLE_USER 这样的角色,在 MyUserDetailsService 中,返回用户信息的时候加一下:12345678return new org.springframework.security.core.userdetails.User( user.getId().toString(), user.getPassword(), user.getEnable(), true, true, !user.getLocked(), AuthorityUtils.commaSeparatedStringToAuthorityList(&quot;admin,ROLE_USER&quot;)); 重启项目,再次请求授权码的页面, 如下: 这里就是个用户授权页了, 有同意和拒绝,点击同意,如下: 这里会跳转到回调地址,然后携带了code参数, 之后就可以拿code去交换token了 换取 token 的请求地址是 POST /oauth/token ，这里用 postMan 或者其他工具去测试, 请求如下:这里是需要填写我们配置的 client-id 和 client-secret 这里的参数也是固定的 grant_type: 固定 authorization_code ,表示授权码模式 client_id: 配置的clientId code: 前面获取到的授权码 redirect_uri: 回调地址,和获取授权码时候传的地址是一样的 scope: 作用域 点击请求,返回如下:1234567&#123; &quot;access_token&quot;: &quot;8fac95e3-8f50-498d-9dd5-449517e5ff5c&quot;, &quot;token_type&quot;: &quot;bearer&quot;, &quot;refresh_token&quot;: &quot;182f9e9a-fabf-46f1-94a0-780519239045&quot;, &quot;expires_in&quot;: 43199, &quot;scope&quot;: &quot;all&quot;&#125; 密码模式和授权码模式的请求地址是一样的, 参数有些不同, 如下: 这里的 grant_type 是 password 表示是密码模式,然后传的参数是 username 和 password, 就是服务提供商的用户名和密码,这里的返回值也和上面的是一样的 注意看一下我这里的返回的token,两种模式的请求,返回token是一样的, 在固定的时间段内, 不管请求多少次,返回的token都是相同的 剩余的两种模式不常用,就不演示了 资源服务器的实现和认证服务器一样, 也是一个注解就实现, 新建类 MyResourcesServerConfig12345@Configuration@EnableResourceServerpublic class MyResourcesServerConfig &#123; &#125; 重启项目,随便访问一个接口,效果如下: 这里是因为我们没有把token传过去 重新获取一下token, 然后放在请求头中, 格式是: Authorization : token_type access_token ,如图: token 传过去之后就可以访问接口了 这里要注意,现在的token生成是放在内存中的, 所以每次重启项目之后都要重新请求一下 总结熟悉常用两种授权模式的流程,以及传递的参数等, 获取到token之后,通过请求头传递 本文代码传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-22-Spring Security OAuth 简介]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-22-Spring-Security-OAuth-%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[述之前的 Spring Social 中,我们是去访问别人的 OAuth 服务,在我们自己的系统中,客户端访问服务还是基于 session 的,结构大致如下: 如果是前后端分离开发,再按上面这种方式开发就比较繁琐,而且安全性和客户体验差, 一旦 JSESSIONID 泄露,那可能就会被伪造身份,获取用户的信息等, 而且有些前端技术是不支持cookie的,比如小程序 现在大多数的开发都是前后端分离的,一个后端的服务可能会给不同的地方调用, 前端, App, Wap 等等, 基本上都是基于 Token 的这种 OAth 认证, 客户端通过参数或者请求头中携带token, 服务端进行验证从而决定是否能访问服务器资源 Spring Security OAuthSpring Security OAuth 封装了服务提供商大部分的操作, 我们之前的 Spring Social 是封装了客户端和服务提供商交互的流程 我们要实现的服务提供商的大致是这样的 其中 Spring Security OAuth 已经提供了4中授权模式,我们只需要将之前实现的自定义的认证方式改造一下就可以了 token 的生成 Spring Security OAuth 也提供了默认的实现,至于认证,就是在 Spring security 过滤器链上加了一个过滤器 OAuth2AuthencationProcessingFilter]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-21-退出登录]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-21-%E9%80%80%E5%87%BA%E7%99%BB%E5%BD%95%2F</url>
    <content type="text"><![CDATA[述退出登录的功能,Spring Security 也提供了默认的实现, 接口是 /logout, 直接访问 http://www.pinzhi365.com/logout 接口效果如下: 这里会挑战到 /authentication/require?logout 这个请求里面,原因是默认的注销完成之后是跳转到登录页的,而我们的登录页是配置的 /authentication/require 后面的 logout 参数表示是从注销跳转过来的 默认的退出登录的处理逻辑如下: 使当前 session 失效 清除与当前用户相关的 remember-me 记录 清空当前的 SecurityContext 重定向到登录页 基本配置在 BrowserSecurityConfig 中可以对退出登录做一些常用的配置123456.and() .logout() .logoutUrl(&quot;/singnOut&quot;) .logoutSuccessUrl(&quot;signOut.html&quot;) .logoutSuccessHandler() .deleteCookies(&quot;JSESSIONID&quot;) logoutUrl(): 退出登录请求的 url ,通过这个配置可以替换默认的 /logout logoutSuccessUrl(): 退出成功后的跳转路径 logoutSuccessHandler(): 退出成功后调用的控制器,这个和 logoutSuccessUrl() 是互斥的,配置了handler之后上面的方法就不生效了 deleteCookies(): 删除客户端的指定的 cookie logoutSuccessHandler 配置下面重点看一下 logoutSuccessHandler 的配置,有时候我们的退出登录的逻辑还想加一些额外的功能,比如记录日志等等, 就可以实现 LogoutSuccessHandler新建类 MyLogoutSuccessHandler 代码如下:123456789101112131415161718192021222324252627@Slf4jpublic class MyLogoutSuccessHandler implements LogoutSuccessHandler &#123; private String logOutSuccessUrl; public MyLogoutSuccessHandler(String logOutSuccessUrl) &#123; this.logOutSuccessUrl = logOutSuccessUrl; &#125; private ObjectMapper objectMapper = new ObjectMapper(); @Override public void onLogoutSuccess(HttpServletRequest request, HttpServletResponse response, Authentication authentication) throws IOException, ServletException &#123; log.info(&quot;注销成功.....&quot;); // 判断用户有没有配置注销成功页, 有的话做跳转,没有的话返回json提示 if (StringUtils.isBlank(logOutSuccessUrl)) &#123; Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(1); map.put(&quot;message&quot;, &quot;注销成功&quot;); response.setContentType(&quot;application/json;charset=UTF-8&quot;); response.getWriter().write(objectMapper.writeValueAsString(map)); &#125; else &#123; response.sendRedirect(logOutSuccessUrl); &#125; &#125;&#125; 这里就是判断了一下,有没有指定注销成功后的跳转,如果有就重定向,没有的话就返回 json 格式的提示信息 在 BrowserSecurityBeanConfig 中,把这个类注入到Spring容器中.12345@Bean@ConditionalOnMissingBean(LogoutSuccessHandler.class)public LogoutSuccessHandler logoutSuccessHandler()&#123; return new MyLogoutSuccessHandler(securityProperties.getBrowser().getLogOutSuccessUrl());&#125; 这里就需要在浏览器的配置中加一个1234/** * 注销成功后的跳转路径 */private String logOutSuccessUrl; 这里不需要给默认值,然后 BrowserSecurityConfig 最终的配置如下: 1234567@Autowiredprivate LogoutSuccessHandler logoutSuccessHandler;.and() .logout() .logoutUrl(securityProperties.getBrowser().getLogOutUrl()) .logoutSuccessHandler(logoutSuccessHandler) 本文代码传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-20-Session管理]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-20-Session%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[述在之前我们已经实现了三种方式的用户登录: 用户名密码登录 短信验证码登录 第三方社交网站登录 前面两种是使用表单提交的方式登录,后面一种是走 OAuth 流程登录, 但是不管是哪种类型的登陆,最后的用户认证信息都会放到 session 中去 对于session的一些配置,比如失效时间, 失效后的处理等该如何配置,下面来看一下 session失效时间session的超时时间可以在 application.yml 中,加入以下配置:123server: session: timeout: 10 这里的单位是秒, 设置10秒, 在系统实际的运行过程中,10秒过后其实并不会失效 看下源码, 他这里会做一个分钟的转换,如果小于1分钟,默认就是1分钟 如果不做任何配置的话,默认是30分钟的 session超时配置首先看一下简单的实现 在浏览器的配置 BrowserSecurityConfig 中,加入以下配置:123.and().sessionManagement().invalidSessionUrl(&quot;/session/invalid&quot;) 这里是配置 session 失效后跳转的 url, 这个可以是一个页面,或者是一个接口,这个需要自己去实现,记得加到不需要授权的请求中去 1234567@GetMapping(&quot;/session/invalid&quot;)@ResponseStatus(HttpStatus.UNAUTHORIZED)public Map sessionInvalid() &#123; Map&lt;String, Object&gt; result = new HashMap&lt;&gt;(); result.put(&quot;message&quot;, &quot;session已失效,请重新登录&quot;); return result;&#125; session并发控制很多情况下,系统中,一个用户只能在一个地方登录,在两个地方登录的话会把之前的踢下线,或者说是限制登录,下面看一下这两种情况应该如何处理 第一种是新的客户端登录后,旧的客户端踢掉,下线,需要在 BrowserSecurityConfig 中加入以下配置 1234 // 限制同一个用户只能有一个session登录.maximumSessions(1)//.expiredUrl(&quot;/session/expired&quot;).expiredSessionStrategy(new MySessionInformationExpiredStrategy()) 这里有两种解决方案,第一个 expiredUrl() 方法,可以跳转到一个请求里面去,还有一种是 expiredSessionStrategy() 方法,这里可以配置一个策略类, 看一下这个类的代码:12345678public class MySessionInformationExpiredStrategy implements SessionInformationExpiredStrategy &#123; @Override public void onExpiredSessionDetected(SessionInformationExpiredEvent event) throws IOException, ServletException &#123; // 该对象能获取到访问失效前的url地址 event.getResponse().setContentType(&quot;application/json;charset=UTF-8&quot;); event.getResponse().getWriter().write(&quot;session并发登录&quot;); &#125;&#125; 这里要继承 SessionInformationExpiredStrategy 然后重写 onExpiredSessionDetected() ,方法的参数中可以拿到之前的访问失效前的请求信息 再来看下第二种的并发解决方案,配置如下:12.maximumSessions(1) .maxSessionsPreventsLogin(true) 第一个配置是,限制用户只能有一个session登录,然后下面的 .maxSessionsPreventsLogin(true) 表示当 session 达到最大后,阻止后续登录的行为 代码重构上面代码中,配置都是直接写死在配置类里面了, 这些配置都是可以由调用方去控制的,所以下面来重构一下代码,把这些都做成可配置的 首先需要写一个配置类 SessionProperties ,因为这些东西都是可以由调用放去决定的12345678910111213141516171819@Datapublic class SessionProperties &#123; /** * session失效时跳转的地址 */ private String sessionInvalidUrl = SecurityConstants.DEFAULT_SESSION_INVALID_URL; /** * 同一个用户在系统中的最大session数，默认1 */ private int maximumSessions = 1; /** * 达到最大session时是否阻止新的登录请求，默认为false，不阻止，新的登录会将老的登录失效掉 */ private boolean maxSessionsPreventsLogin; &#125; 然后把 SessionProperties 加到浏览器的配置中去 新建一个类 AbstractSessionStrategy 代码如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960@Slf4jpublic class AbstractSessionStrategy &#123; /** * 跳转的url */ private String destinationUrl; /** * 重定向策略 */ private RedirectStrategy redirectStrategy = new DefaultRedirectStrategy(); /** * 跳转前是否创建新的session */ private boolean createNewSession = true; private ObjectMapper objectMapper = new ObjectMapper(); public AbstractSessionStrategy(String invalidSessionUrl) &#123; Assert.isTrue(UrlUtils.isValidRedirectUrl(invalidSessionUrl), &quot;url must start with &apos;/&apos; or with &apos;http(s)&apos;&quot;); this.destinationUrl = invalidSessionUrl; &#125; protected void onSessionInvalid(HttpServletRequest request, HttpServletResponse response) throws IOException &#123; if (createNewSession) &#123; request.getSession(); &#125; Object result = buildResponseContent(request); response.setStatus(HttpStatus.UNAUTHORIZED.value()); response.setContentType(&quot;application/json;charset=UTF-8&quot;); response.getWriter().write(objectMapper.writeValueAsString(result)); &#125; private Object buildResponseContent(HttpServletRequest request) &#123; String message = &quot;session已失效&quot;; if(isConcurrency())&#123; message = message + &quot;,有可能是并发登录导致的&quot;; &#125; Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(1); map.put(&quot;message&quot;, message); return map; &#125; /** * session失效是否是并发导致的 * @return */ protected boolean isConcurrency() &#123; return false; &#125; public void setCreateNewSession(boolean createNewSession) &#123; this.createNewSession = createNewSession; &#125;&#125; 这里主要是 onSessionInvalid 方法,就是session失效后的返回 然后下面有两个子类, 第一个是session过期的配置:12345678910111213public class DefaultInvalidSessionStrategy extends AbstractSessionStrategy implements InvalidSessionStrategy &#123; public DefaultInvalidSessionStrategy(String invalidSessionUrl) &#123; super(invalidSessionUrl); &#125; @Override public void onInvalidSessionDetected(HttpServletRequest request, HttpServletResponse response) throws IOException, ServletException &#123; onSessionInvalid(request, response); &#125;&#125; 然后是一个被踢下线的配置:1234567891011121314151617public class DefaultExpiredSessionStrategy extends AbstractSessionStrategy implements SessionInformationExpiredStrategy &#123; public DefaultExpiredSessionStrategy(String invalidSessionUrl) &#123; super(invalidSessionUrl); &#125; @Override public void onExpiredSessionDetected(SessionInformationExpiredEvent eventØ) throws IOException, ServletException &#123; onSessionInvalid(eventØ.getRequest(), eventØ.getResponse()); &#125; @Override protected boolean isConcurrency() &#123; return true; &#125;&#125; 把这两个类通过 @Bean 注入到 Spring 容器中, 新建 BrowserSecurityBeanConfig 代码如下:12345678910111213141516171819@Configurationpublic class BrowserSecurityBeanConfig &#123; @Autowired private SecurityProperties securityProperties; @Bean @ConditionalOnMissingBean(InvalidSessionStrategy.class) public InvalidSessionStrategy invalidSessionStrategy()&#123; return new DefaultInvalidSessionStrategy(securityProperties.getBrowser().getSession().getSessionInvalidUrl()); &#125; @Bean @ConditionalOnMissingBean(SessionInformationExpiredStrategy.class) public SessionInformationExpiredStrategy sessionInformationExpiredStrategy()&#123; return new DefaultExpiredSessionStrategy(securityProperties.getBrowser().getSession().getSessionInvalidUrl()); &#125; &#125; 这里也是加一个 @ConditionalOnMissingBean 也可以交给调用方去实现 最后 BrowserSecurityConfig 的配置如下:123456789101112131415161718@Autowiredprivate SessionInformationExpiredStrategy sessionInformationExpiredStrategy;@Autowiredprivate InvalidSessionStrategy invalidSessionStrategy;@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; // ... 省略其他配置 .and() .sessionManagement() .invalidSessionStrategy(invalidSessionStrategy) .maximumSessions(securityProperties.getBrowser().getSession().getMaximumSessions()) .maxSessionsPreventsLogin(securityProperties.getBrowser().getSession().isMaxSessionsPreventsLogin()) .expiredSessionStrategy(sessionInformationExpiredStrategy) .and() // ... 省略其他配置&#125; 集群环境的session管理在生产环境中,项目部署一般都是集群部署的, 假如说我现在有个项目,部署了两个实例,分别在A服务器上和B服务器上, 这时候一个登录请求经过网关的复载均衡发送到了A服务器上面, 然后用户登录之后,session的信息是记录在了A服务器上,然后用户又发送一个其他的请求,经过网关,将请求转发到了B服务器上面,但是B服务器拿不到session的信息, 所以B服务器会认为用户没有登录,然后引导用户再去登录 在集群的环境中,session一般都是放到一个公共的地方,不管有多少服务都是从一个地方去取session, 下面看一下如何实现这样的配置 我们之前有引入过 spring-session 的配置,然后配置文件中的配置是:123spring: session: store-type: none none,就表示是使用原生j2ee单机服务器session这种模式,就是我们之前一直用的 然后他支持的几种方式如下: 最常用的就是用 redis 做 session 管理,因为所有请求都需要拿session,然后 redis 中的key本来就可以设置过期时间,不需要我们自己去维护 配置方式直接修改 application.yml 中的配置123spring: session: store-type: redis 然后根据自己的环境,加入redis的配置就好了 启动项目,登录在redis中就会生成以 spring:session开头的数据 本文代码传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-19-绑定和解绑处理]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-19-%E7%BB%91%E5%AE%9A%E5%92%8C%E8%A7%A3%E7%BB%91%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[述在使用第三方登录时,有一个常见的场景就是用户登录完成之后,再去绑定/解绑第三方社交账号 绑定也是需要走一下 OAuth 流程,用户授权完成之后,将第三方的用户信息与当前系统的用户信息做一个绑定, 这里和登录不同的地方是, 登录是不知道当前用户信息的,需要先拿到第三方用户信息然后去 userconnection 表里查询系统的用户信息, 而绑定是用户已经登录之后的操作,也就是说已经知道当前业务系统中的用户了,然后授权之后跟当前登录的用户做绑定 解绑操作的话其实就是删除 userconnection 表里的一条数据 Spring Social 对这种场景提供了默认的支持,下面来看一下如何使用 绑定Spring Social 提供了一个 ConnectController 里面可以查询到当前用户的所有的绑定信息,源码如下: 这里就是从数据库里面找到所有的绑定信息,然后放到Model里面,最后返回一个 connectView(), 这个方法代码如下: 这里就是返回了一个 connect/status 的视图, 但是这个视图 Social 并没有提供,需要我们自己去实现,所以我们只需要实现这个视图就可以了 connect/status 获取绑定状态新建类 ConnectionStatusView 代码如下:123456789101112131415161718192021222324@Component(&quot;connect/status&quot;)public class ConnectionStatusView extends AbstractView &#123; @Autowired private ObjectMapper objectMapper; @Override protected void renderMergedOutputModel(Map&lt;String, Object&gt; map, HttpServletRequest request, HttpServletResponse response) throws Exception &#123; // 拿到所有的绑定信息 Map&lt;String, List&lt;Connection&lt;?&gt;&gt;&gt; connections = (Map&lt;String, List&lt;Connection&lt;?&gt;&gt;&gt;) map.get(&quot;connectionMap&quot;); // 最终结果是 [第三方providerId:是否绑定] Map&lt;String, Boolean&gt; result = new HashMap&lt;&gt;(connections.size()); for (String key : connections.keySet()) &#123; result.put(key, CollectionUtils.isNotEmpty(connections.get(key))); &#125; // 返回给页面 response.setContentType(&quot;application/json;charset=UTF-8&quot;); response.getWriter().write(objectMapper.writeValueAsString(result)); &#125;&#125; 这里我们只需要给前端返回一个是否绑定的标识就好了 可以启动项目请求 connect 看下效果 绑定的实现绑定同样 Social 也提供了一个接口 POST /connect//{providerId} 源码如下:这里其实就是重定向到第三方的授权页面, 然后第三方授权成功后的回调是 这个方法主要是往 userconnection 表里面添加数据,绑定完成之后最终也会返回一个视图,名字是 connect/{providerId}Connected 在实现之前,首先写一个html页面用来做绑定123&lt;form action=&quot;/connect/weChat&quot; method=&quot;post&quot;&gt; &lt;button type=&quot;submit&quot;&gt;绑定微信&lt;/button&gt;&lt;/form&gt; 只需要一个表单即可,这里首先是前半段 /connect 这个是固定的, 后面的 weChat 是 providerId 然后把视图实现一下, 新建类 ConnectedView123456789101112131415161718public class ConnectView extends AbstractView &#123; @Autowired private ObjectMapper objectMapper = new ObjectMapper(); @Override protected void renderMergedOutputModel(Map&lt;String, Object&gt; model, HttpServletRequest request, HttpServletResponse response) throws Exception &#123; Map&lt;String, Object&gt; result = new HashMap&lt;&gt;(1); response.setContentType(&quot;application/json;charset=UTF-8&quot;); result.put(&quot;message&quot;, &quot;绑定成功&quot;); response.getWriter().write(objectMapper.writeValueAsString(result)); &#125;&#125; 这里就是返回了一个绑定成功的信息, 然后这个类并没有加 @Component 注解, 因为所有的第三方绑定成功都是可以重用的 已微信为例,现在视图的名称应该是 connect/weChatConnected ,这个视图我们可以通过 @Bean 的方式注入到 Spring 容器中去, 在 WeChatAutoConfig 中加入以下内容12345@Bean(&quot;connect/weChatConnected&quot;)@ConditionalOnMissingBean(name = &quot;weChatConnectView&quot; )public View weChatConnectView()&#123; return new ConnectView();&#125; 这样就可以了 @ConditionalOnMissingBean 的用法和之前是一样的 这时候可以启动项目,先登陆,然后进去绑定页面,点绑定微信,然后授权,如果返回是我们视图中的信息就成功了, 同时 userconnection 表里面会多一条信息 解绑实现解绑的接口是 DELETE /connect/{providerId} 他其实就是从 userconnection 中删除一条数据,然后最后他也会返回一个视图,名称是 connect/{providerId}Connect 这里这个视图是可以和上面的绑定的视图用一个, 修改 ConnectView ,代码如下:1234567891011121314151617@Overrideprotected void renderMergedOutputModel(Map&lt;String, Object&gt; model, HttpServletRequest request, HttpServletResponse response) throws Exception &#123; Map&lt;String, Object&gt; result = new HashMap&lt;&gt;(1); response.setContentType(&quot;application/json;charset=UTF-8&quot;); if (model.get(&quot;connection&quot;) == null) &#123; result.put(&quot;message&quot;, &quot;解绑成功&quot;); &#125; else &#123; result.put(&quot;message&quot;, &quot;绑定成功&quot;); &#125; response.getWriter().write(objectMapper.writeValueAsString(result));&#125; 然后 WeChatAutoConfig 修改如下:12345@Bean(&#123;&quot;connect/weChatConnected&quot;,&quot;connect/weChatConnect&quot;&#125;)@ConditionalOnMissingBean(name = &quot;weChatConnectView&quot; )public View weChatConnectView()&#123; return new ConnectView();&#125; 这里绑定和解绑就是判断一下,有没有 connection, 然后 @Bean 注入的时候,注入两个就ok了 上面例子都是以微信为例, QQ的可以自己实现一下 本文代码传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-18-开发微信登录]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-18-%E5%BC%80%E5%8F%91%E5%BE%AE%E4%BF%A1%E7%99%BB%E5%BD%95%2F</url>
    <content type="text"><![CDATA[述上文中实现了整个QQ登录的流程,本文来实现以下微信的授权登录, 总体逻辑是和QQ一样的,部分地方有所区别 ServiceProvider构建构建 ServiceProvider 需要 API 和 OAuth2Operations 微信API新建接口 WeChat12345678public interface WeChat &#123; /** * 获取微信的用户信息 * @return &#123;@link WeixinUserInfo&#125; */ WeixinUserInfo getUserInfo(String openId);&#125; 这里和QQ有所区别, QQ 是获取到token之后,再去获取openId,然后再去获取微信的用户信息, 而微信在获取token的时候,就会把 openId 返回来,所以这里直接传过来就好了 然后是实现类 WeChatImpl1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Slf4jpublic class WeChatImpl extends AbstractOAuth2ApiBinding implements WeChat &#123; private ObjectMapper objectMapper = new ObjectMapper(); /** * 获取用户信息的url */ private static final String URL_GET_USER_INFO = &quot;https://api.weixin.qq.com/sns/userinfo?openid=&quot;; public WeChatImpl(String accessToken) &#123; super(accessToken, TokenStrategy.ACCESS_TOKEN_PARAMETER); &#125; /** * 默认注册的StringHttpMessageConverter字符集为ISO-8859-1，而微信返回的是UTF-8的，所以覆盖了原来的方法。 */ @Override protected List&lt;HttpMessageConverter&lt;?&gt;&gt; getMessageConverters() &#123; List&lt;HttpMessageConverter&lt;?&gt;&gt; messageConverters = super.getMessageConverters(); messageConverters.remove(0); messageConverters.add(new StringHttpMessageConverter(Charset.forName(&quot;UTF-8&quot;))); return messageConverters; &#125; @Override public WeixinUserInfo getUserInfo(String openId) &#123; String url = URL_GET_USER_INFO + openId; String responseStr = getRestTemplate().getForObject(url, String.class); log.info(&quot;获取微信信息的用户返回数据:&#123;&#125;&quot;, responseStr); if(StringUtils.contains(responseStr, &quot;errcode&quot;)) &#123; return null; &#125; WeixinUserInfo userInfo = null; try &#123; userInfo = objectMapper.readValue(responseStr, WeixinUserInfo.class); &#125; catch (IOException e) &#123; log.info(&quot;微信用户信息转换失败:&#123;&#125;&quot;, e); &#125; return userInfo; &#125;&#125; 这里和QQ大致一样, 重写了一个方法是 getMessageConverters(), 设置了一下编码格式,否则可能返回来是乱码 API 到这里就完成了,然后是 OAuth2Operations OAuth2Operations创建 WeChatOAuth2Template 然后继承 OAuth2Template123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126@Slf4jpublic class WeChatOAuth2Template extends OAuth2Template &#123; private String clientId; private String clientSecret; private String accessTokenUrl; private ObjectMapper objectMapper = new ObjectMapper(); /** * 刷新token的url */ private static final String REFRESH_TOKEN_URL = &quot;https://api.weixin.qq.com/sns/oauth2/refresh_token&quot;; public WeChatOAuth2Template(String clientId, String clientSecret, String authorizeUrl, String accessTokenUrl) &#123; super(clientId, clientSecret, authorizeUrl, accessTokenUrl); setUseParametersForClientAuthentication(true); this.clientId = clientId; this.clientSecret = clientSecret; this.accessTokenUrl = accessTokenUrl; &#125; /** * 构建获取授权码的请求。也就是引导用户跳转到微信的地址。 */ @Override public String buildAuthenticateUrl(OAuth2Parameters parameters) &#123; String url = super.buildAuthenticateUrl(parameters); url = url + &quot;&amp;appid=&quot; + clientId + &quot;&amp;scope=snsapi_login&quot;; return url; &#125; @Override public String buildAuthorizeUrl(OAuth2Parameters parameters) &#123; return buildAuthenticateUrl(parameters); &#125; /** * 微信返回的contentType是html/text，添加相应的HttpMessageConverter来处理。 */ @Override protected RestTemplate createRestTemplate() &#123; RestTemplate restTemplate = super.createRestTemplate(); restTemplate.getMessageConverters().add(new StringHttpMessageConverter(Charset.forName(&quot;UTF-8&quot;))); return restTemplate; &#125; /** * 获取accesstoken * @param authorizationCode * @param redirectUri * @param additionalParameters * @return */ @Override public AccessGrant exchangeForAccess(String authorizationCode, String redirectUri, MultiValueMap&lt;String, String&gt; additionalParameters) &#123; // 拼接url StringBuilder accessTokenRequestUrl = new StringBuilder(); accessTokenRequestUrl.append(&quot;?appid=&quot;).append(clientId); accessTokenRequestUrl.append(&quot;&amp;secret=&quot;).append(clientSecret); accessTokenRequestUrl.append(&quot;&amp;code=&quot;).append(authorizationCode); accessTokenRequestUrl.append(&quot;&amp;grant_type=authorization_code&quot;); accessTokenRequestUrl.append(&quot;&amp;redirect_uri=&quot;).append(redirectUri); return getAccessToken(accessTokenRequestUrl); &#125; /** * 刷新token的方法 * @param refreshToken * @param additionalParameters * @return */ @Override public AccessGrant refreshAccess(String refreshToken, MultiValueMap&lt;String, String&gt; additionalParameters) &#123; StringBuilder refreshTokenUrl = new StringBuilder(REFRESH_TOKEN_URL); refreshTokenUrl.append(&quot;?appid=&quot;+clientId); refreshTokenUrl.append(&quot;&amp;grant_type=refresh_token&quot;); refreshTokenUrl.append(&quot;&amp;refresh_token=&quot;+refreshToken); return getAccessToken(refreshTokenUrl); &#125; private AccessGrant getAccessToken(StringBuilder accessTokenRequestUrl) &#123; log.info(&quot;获取微信token,请求:&#123;&#125;&quot;, accessTokenRequestUrl); String url = accessTokenUrl + accessTokenRequestUrl; String responseStr = getRestTemplate().getForObject(url , String.class); log.info(&quot;获取微信token,返回:&#123;&#125;&quot;, responseStr); // 转成一个map Map&lt;String, Object&gt; result = null; try &#123; result = objectMapper.readValue(responseStr, Map.class); &#125; catch (IOException e) &#123; log.error(&quot;获取微信token,转换失败:&#123;&#125;&quot;, e); &#125; // 返回错误码时直接返回空 if(StringUtils.isNotBlank(MapUtils.getString(result, &quot;errcode&quot;)))&#123; String errcode = MapUtils.getString(result, &quot;errcode&quot;); String errmsg = MapUtils.getString(result, &quot;errmsg&quot;); throw new RuntimeException(&quot;获取access token失败, errcode:&quot; + errcode + &quot;, errmsg:&quot; + errmsg); &#125; // 构建 AccessGrant 对象返回 WeChatAccessGrant weChatAccessGrant = new WeChatAccessGrant( MapUtils.getString(result, &quot;access_token&quot;), MapUtils.getString(result, &quot;scope&quot;), MapUtils.getString(result, &quot;refresh_token&quot;), MapUtils.getLong(result, &quot;expires_in&quot;) ); weChatAccessGrant.setOpenId(MapUtils.getString(result, &quot;openid&quot;)); return weChatAccessGrant; &#125;&#125; 这里重写的方法比较多,主要是获取token的方法, 在标准的 OAuth 流程中, APPID和 appSecret的参数名是 client_id 和 client_secret 这个是 OAuth2Template 里面写死的参数名, 但是在微信的请求中,参数名是 appId 所以我们只能重写一下她的获取token的方法 这里用到的一个实体类是 WeChatAccessGrant 这个是我们自定义的,继承自 AccessGrant 因为微信返回来的数据多了一个 openId1234567891011121314@Datapublic class WeChatAccessGrant extends AccessGrant &#123; private String openId; public WeChatAccessGrant()&#123; super(&quot;&quot;); &#125; public WeChatAccessGrant(String accessToken, String scope, String refreshToken, Long expiresIn) &#123; super(accessToken, scope, refreshToken, expiresIn); &#125;&#125; ServiceProviderAPI 和 OAuth2Operations 都有了,就可以构建 ServiceProvider 了,新建 WeChatServiceProvider 1234567891011121314151617181920public class WeChatServiceProvider extends AbstractOAuth2ServiceProvider&lt;WeChat&gt; &#123; /** * 微信获取授权码的url */ private static final String URL_AUTHORIZE = &quot;https://open.weixin.qq.com/connect/qrconnect&quot;; /** * 微信获取accessToken的url */ private static final String URL_ACCESS_TOKEN = &quot;https://api.weixin.qq.com/sns/oauth2/access_token&quot;; public WeChatServiceProvider(String appId, String appSecret) &#123; super(new WeChatOAuth2Template(appId, appSecret, URL_AUTHORIZE, URL_ACCESS_TOKEN)); &#125; @Override public WeChat getApi(String accessToken) &#123; return new WeChatImpl(accessToken); &#125;&#125; 和 QQ 基本类似 ApiAdapter构建 ConnectionFactory 需要 ServiceProvider 和 ApiAdapter , 下面看一下 ApiAdapter ,新建 WeChatAdapter1234567891011121314151617181920212223242526272829303132333435public class WeChatAdapter implements ApiAdapter&lt;WeChat&gt; &#123; private String openId; public WeChatAdapter() &#123;&#125; public WeChatAdapter(String openId)&#123; this.openId = openId; &#125; @Override public boolean test(WeChat api) &#123; return true; &#125; @Override public void setConnectionValues(WeChat api, ConnectionValues values) &#123; WeixinUserInfo userInfo = api.getUserInfo(openId); values.setProviderUserId(userInfo.getOpenid()); values.setImageUrl(userInfo.getHeadimgurl() != null ? userInfo.getHeadimgurl() : null); values.setProfileUrl(null); values.setDisplayName(userInfo.getNickname()); &#125; @Override public UserProfile fetchUserProfile(WeChat api) &#123; return null; &#125; @Override public void updateStatus(WeChat api, String message) &#123; &#125;&#125; 这里和QQ基本都是一样的,也不做过多解释 ConnectionFactory新建 WeChatConnectionFactory 代码如下:12345678910111213141516171819202122232425262728293031323334353637383940414243public class WeChatConnectionFactory extends OAuth2ConnectionFactory&lt;WeChat&gt; &#123; public WeChatConnectionFactory(String providerId, String appId, String appSecret) &#123; super(providerId, new WeChatServiceProvider(appId, appSecret), new WeChatAdapter()); &#125; @Override public Connection&lt;WeChat&gt; createConnection(AccessGrant accessGrant) &#123; return new OAuth2Connection&lt;WeChat&gt;(getProviderId(), extractProviderUserId(accessGrant), accessGrant.getAccessToken(), accessGrant.getRefreshToken(), accessGrant.getExpireTime(), getOAuth2ServiceProvider(), getApiAdapter(extractProviderUserId(accessGrant))); &#125; @Override public Connection&lt;WeChat&gt; createConnection(ConnectionData data) &#123; return new OAuth2Connection&lt;WeChat&gt;(data, getOAuth2ServiceProvider(), getApiAdapter(data.getProviderUserId())); &#125; /** * 获取第三方用户的openId * 由于微信的openId是和accessToken一起返回的，所以在这里直接根据accessToken设置providerUserId即可，不用像QQ那样通过QQAdapter来获取 */ @Override protected String extractProviderUserId(AccessGrant accessGrant) &#123; if(accessGrant instanceof WeChatAccessGrant) &#123; return ((WeChatAccessGrant)accessGrant).getOpenId(); &#125; return null; &#125; /** * 把openid传给 WeChatAdapter * @param providerUserId * @return */ private ApiAdapter&lt;WeChat&gt; getApiAdapter(String providerUserId) &#123; return new WeChatAdapter(providerUserId); &#125; private OAuth2ServiceProvider&lt;WeChat&gt; getOAuth2ServiceProvider() &#123; return (OAuth2ServiceProvider&lt;WeChat&gt;) getServiceProvider(); &#125;&#125; 这里跟QQ不同的地方也是因为 微信的openId是和accessToken一起返回的,所以在这里直接根据accessToken设置providerUserId即可，不用像QQ那样通过QQAdapter来获取 配置类最后是微信的配置类, 新建 WeChatProperties123456789@Datapublic class WeChatProperties extends SocialProperties &#123; /** * 第三方id，用来决定发起第三方登录的url，默认是 weixin。 */ private String providerId = &quot;weixin&quot;;&#125; 这里也和QQ是一样的,然后放到 SocialProperties 里面 最后是微信的自动配置 新建 WeChatAutoConfig12345678910111213@Configuration@ConditionalOnProperty(prefix = &quot;core.security.social.weChat&quot;, name = &quot;app-id&quot;)public class WeChatAutoConfig extends SocialAutoConfigurerAdapter &#123; @Autowired private SecurityProperties securityProperties; @Override protected ConnectionFactory&lt;?&gt; createConnectionFactory() &#123; WeChatProperties weChat = securityProperties.getSocial().getWeChat(); return new WeChatConnectionFactory(weChat.getProviderId(), weChat.getAppId(), weChat.getAppSecret()); &#125;&#125; 这里也和QQ是一样的 配置文件配置在 application.yml 中配置微信的appid等信息1234567core: security: social: weChat: app-id: xxx app-secret: xxx providerId: xxx 页面配置加入微信登录的跳转1&lt;a href = &apos;/qqLogin/weChat&apos;&gt;微信登录&lt;/a&gt; 这里的前缀还是 qqLogin 因为我们配置的 filterProcessesUrl 是 /qqLogin 到这里全部的功能就完成了 总结 API 部分: 拿到token之后获取用户信息的接口 OAuth2Template 部分: 通过 appid 等信息去获取token WeChatServiceProvider: 把 OAuth2Template 用到的一些 clientId, clientSecret 等等信息传过去 WeChatConnectionFactory 创建 Connection 对象,以及指定api适配器 WeChatAutoConfig 读取用户的配置 本文代码传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-17-处理注册逻辑]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-17-%E5%A4%84%E7%90%86%E6%B3%A8%E5%86%8C%E9%80%BB%E8%BE%91%2F</url>
    <content type="text"><![CDATA[述上文中,最后QQ授权成功,获取到用户信息之后,是跳转到了一个 /sign/up 的请求上去, 下面首先看一下为什么会跳转到注册页面上去 首先,按照 security 的登录流程, 上文中获取到了 QQ 信息,然后组成一个未认证的 SocialAuthenticationToken 对象, 然后之后就是通过 AuthenticationManager 找到对应的 Provider 来做具体的验证, 在 social 中, Provider 的具体实现是 SocialAuthenticationProvider , 部分源码如下: 这里就是 SocialAuthenticationProvider 中的具体的认证流程, 首先会拿到 Connection 对象,也就是我们获取回来的第三方用户信息, 然后调用了一个 toUserId() 方法,这个方法会从数据库的 userconnection 表中,查询这个第三方用户关联的我们业务系统中的id,但是我们现在这个表中并没有数据,所以会返回空, 然后抛出 BadCredentialsException 异常,标识该用户还没有在我们的业务系统中绑定 然后再上层的 SocialAuthenticationFilter 代码如下: 这里会捕获到认证时候抛出来的 BadCredentialsException 异常,然后判断有没有配置登录页,有的话,就会抛出异常,跳转到注册页 问题已经找到了,下面看一下如何去解决,以及做一些配置 设置注册页面的路径注册页默认的跳转路径是 /signUp, 需要把这个做成一个可配置项,这个配置在之前的 SocialConfig 中1234567@Beanpublic SpringSocialConfigurer securitySocialConfigurer()&#123; String filterProcessesUrl = securityProperties.getSocial().getFilterProcessesUrl(); MySpringSocialConfigurer mySpringSocialConfigurer = new MySpringSocialConfigurer(filterProcessesUrl); mySpringSocialConfigurer.signupUrl(securityProperties.getBrowser().getSignUpUrl()); return mySpringSocialConfigurer;&#125; 在我们自定义的 MySpringSocialConfigurer 中设置, 是从配置中读取过来的 所以在 BrowserProperties 中,需要加上注册的跳转路径:1234/** * 跳转注册的路径 */private String signUpUrl = &quot;/signUp&quot;; 默认的是 /signUp, 我们这里可以提供一个默认的处理方式, web 项目中,写一个 SignUpController,代码如下:12345678910111213141516171819202122@RestController@RequestMapping(&quot;/signUp&quot;)public class SignUpController &#123; private RedirectStrategy redirectStrategy = new DefaultRedirectStrategy(); @Autowired private SecurityProperties securityProperties; @RequestMapping(produces = &quot;text/html&quot;) public void signUpHtml(HttpServletRequest request, HttpServletResponse response) throws IOException &#123; redirectStrategy.sendRedirect(request, response, securityProperties.getBrowser().getSignUpPage()); &#125; @RequestMapping @ResponseStatus(HttpStatus.PRECONDITION_REQUIRED) public Map&lt;String, Object&gt; signUpJson() &#123; Map&lt;String, Object&gt; resultMap = new HashMap&lt;&gt;(1); resultMap.put(&quot;message&quot;, &quot;用户未注册,请引导用户到注册页面....&quot;); return resultMap; &#125;&#125; 这里和之前的 /authentication/require 一样,判断是哪里来的请求,做出不同的跳转, 然后这里跳转的登录页也做成可配置的,同时提供一个默认的页面, 这个默认的页面就提示一下用户配置自己的注册就可以了,因为每个调用方的注册可能都是不一样的,所以由调用方自己实现就好了在demo项目中加入配置1234core: security: browser: signUpPage: /mySignUp.html 然后提供一个注册页面,如下: 这里提供了一个注册和一个绑定的按钮, 因为用户可能之前就有这个网站的账号,只是第一次使用第三方登录,所以就需要一个绑定的接口 最后在 BrowserSecurityConfig 中,记得把登录的跳转路径和登录页面的路径加到不需要权限里面去 这时候,启动项目,然后QQ登录,授权之后跳转到的应该是注册的页面,如下: 获取第三方用户信息在注册/绑定的时候,可能会用到第三方的用户信息, 比如显示第三方用户的昵称或者头像等信息,这些信息可以通过 social 提供的一个工具 ProviderSignInUtils 来获取到,这个工具类再后面做绑定的时候也会用到 首先需要做一下配置, 在 SocialConfig 加入以下代码:1234@Beanpublic ProviderSignInUtils providerSignInUtils(ConnectionFactoryLocator connectionFactoryLocator)&#123; return new ProviderSignInUtils(connectionFactoryLocator, getUsersConnectionRepository(connectionFactoryLocator));&#125; 之后就可以通过注入来使用这个工具类了 web项目里新建一个获取 social 用户信息的 controller,如下:1234567891011121314151617181920@RestControllerpublic class SocialUserController &#123; @Autowired private ProviderSignInUtils providerSignInUtils; @GetMapping(SecurityConstants.GET_SOCIAL_USER_URL) private SocialUserInfo getSocialUser(HttpServletRequest request)&#123; SocialUserInfo userInfo = new SocialUserInfo(); Connection&lt;?&gt; connection = providerSignInUtils.getConnectionFromSession(new ServletWebRequest(request)); userInfo.setProviderId(connection.getKey().getProviderId()); userInfo.setProviderUserId(connection.getKey().getProviderUserId()); userInfo.setNickname(connection.getDisplayName()); userInfo.setHeadimg(connection.getImageUrl()); return userInfo; &#125;&#125; 然后把这个路径也加到不需要权限的路径里面去, 启动项目访问效果如下: 这个原理就是从session中去拿 Connection 对象, 看本文第二张图, social 会在重定向注册路径之前,把获取 Connection 先存在session里面 注册逻辑处理上面的注册页面中,表单是发送到了 /user/register 这个请求里,所以我们还需要这个接口,来处理用户登录的具体逻辑,注册逻辑还是由调用方去实现,所以还是写到 demo 项目中去123456789101112131415@Slf4j@RestController@RequestMapping(&quot;/user&quot;)public class UserController &#123; @Autowired private ProviderSignInUtils providerSignInUtils; @PostMapping(&quot;/register&quot;) public void register(User user, HttpServletRequest request)&#123; // ..省略和数据库的交互 // 最终要拿到业务系统中的 用户的唯一标识 providerSignInUtils.doPostSignUp(String.valueOf(1L), new ServletWebRequest(request)); &#125;&#125; 这里不管是注册还是绑定, 最终都会拿到一个业务系统中的唯一标识,然后通过调用 providerSignInUtils 的 doPostSignUp() 方法来完成注册的,这个方法会往 userconnection 表里添加数据,把业务系统的用户和第三方的用户做关联,我这里先写死用户id是1的数据了 最后要把 /user/register 添加到不需要权限的路径里面, 这个其实是调用方自定义的路径,正常来说不应该去配到 BrowserSecurityConfig 里,但是这里为了方便,先这样写,后面再去优化 这次启动项目,使用QQ登录,然后授权,跳转到注册页面之后,点注册,这时候再userconnection表中就会有一条记录123userId providerId providerUserId rank displayName profileUrl imageUrl accessToken secret refreshToken expireTime ------ ----------- -------------------------------- ------ ----------- ---------- ----------------------------------------------------------------------------- -------------------------------- ------ -------------------------------- ---------------1 callback.do 1B815954CDDAA9E29193E3E185FB293F 1 周。 (NULL) http://thirdqq.qlogo.cn/g?b=oidb&amp;k=cKomQts7QiascqicHy2ZY4lw&amp;s=40&amp;t=1554802302 1582672832E0F1AA99B95059A8A72DA8 (NULL) 4645CB01B0C04604E2B1AB6F0CEC7E5C 1574064303538 这次已经绑定了业务系统中的id, 下次登录的时候,就会通过 providerId 和 providerUserId 查询到业务系统的id,然后进入 SocialUserDetailsService 通过业务系统的id去查询用户,就不会再跳转到登录页面了 踩坑记录这里我在运行的过程中有遇到一个问题,就是 SocialConfig 中配置的 UsersConnectionRepository 没有生效,导致去数据库查询的时候用了它的默认实现 InMemoryUsersConnectionRepository ,然后就导致每次去数据库查询都查不到, 原因是类加载的时候顺序出了点问题, 这个 SocialConfig 先注册了，没等 JdbcUsersConnectionRepository 注册就结束了,导致 UsersConnectionRepository 只能使用默认的实现了 我这里解决方案是 在 SocialConfig 类上加一个 @Order(10) 注解,让他晚点注册 不需要注册的场景上面的场景中,是用户去第三方登录,然后我们业务系统中没有对应的用户,就跳转注册页面让用户去注册,其实在很多的系统中都是第三方可以直接登录的,并不需要去创建业务系统中的用户,这样的话用户体验会更好 这里的实现原理,就是在登录的时候,发现业务系统中没有对应的用户,就我们后台去给他生成一个用户 首先来看一个方法,是 SocialAuthenticationProvider 中,调用 toUserId() 去查询用户id的方法里面的 findUserIdsWithConnection(),代码如下: 这里首先会去数据库里面查询,如果没有查到的话,那就是没有这个用户,他底下有个判断,是在没有查询到,而且 connectionSignUp 不为空的情况下, 去调用 connectionSignUp 的 execute() 方法,然后返回一个新用户的id, 创建完成后把新用户的id返回去 connectionSignUp 是一个接口,里面之后一个 execute() 方法,我们要做的就是实现这个接口 这个实现也应该由调用方去决定是否去实现,所以 也写在demo项目中, 新建 MyConnectionSignUp 代码如下:12345678910111213141516@Componentpublic class MyConnectionSignUp implements ConnectionSignUp &#123; @Autowired private UserService userService; @Override public String execute(Connection&lt;?&gt; connection) &#123; // 这里应该去根据自己的业务去创建一个用户,并返回用户的唯一标识 User user = new User(6L, connection.getDisplayName(), &quot;123456&quot;, false, true); userService.insert(user); return user.getId().toString(); &#125;&#125; 这里能拿到的就是 Connection 对象,也就是第三方用户的一些信息,然后可以结合自己的业务,创建一个新的用户,反正最终要返回业务系统中的用户唯一标识 最后需要在构建 UsersConnectionRepository 的时候把 ConnectionSignUp 的实现设置进去,在 SocialConfig 中代码如下:123456789101112@Autowired(required = false)private ConnectionSignUp connectionSignUp; @Overridepublic UsersConnectionRepository getUsersConnectionRepository(ConnectionFactoryLocator connectionFactoryLocator) &#123; JdbcUsersConnectionRepository jdbcUsersConnectionRepository = new JdbcUsersConnectionRepository(dataSource, connectionFactoryLocator, Encryptors.noOpText()); // 设置表的前缀// jdbcUsersConnectionRepository.setTablePrefix(&quot;&quot;); jdbcUsersConnectionRepository.setConnectionSignUp(connectionSignUp); return jdbcUsersConnectionRepository;&#125; 这里需要注意的是 @Autowired 要设置成非必须的,因为调用方可能并不会去实现这个接口 测试以上配置完成之后,在 UserConnection 表中,把之前测试的数据清空掉,然后重启项目授权,这时候就不会去跳转注册页去了,而且会直接在 UserConnection 表里面生成一条新的数据 本文代码传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-16-开发QQ登录(下)]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-16-%E5%BC%80%E5%8F%91QQ%E7%99%BB%E5%BD%95-%E4%B8%8B%2F</url>
    <content type="text"><![CDATA[述上文中,整个QQ登录的流程的代码已经写完了,可以启动项目试一下, 点QQ登录,效果如下: 可以看下QQ的文档, 就是回调地址不合法 这只是一个问题,在运行代码的过程中还会有几个问题,下面来看一下 回调地址上面的问题就是回调地址不对, 在QQ互联里面,需要在自己应用中去配置一下回调地址,这里要注意,我们QQ登录跳转的请求是 /auth/qq 然后这个地址和回调的地址是一个,就是说登录跳转了 /auth/qq ,那么回调地址也是 /auth/qq 我现在在QQ互联里面配的回调是 http://www.pinzhi365.com/qqLogin/callback.do ,那么我跳转认证和回调的地址都应该是 /qqLogin/callback.do 而不是 /auth/qq 但是在 social 中 /auth 这段地址是 SocialAuthenticationFilter 中写死的, 那我们就需要去把他的配置给覆盖掉,下面看下如何实现在此之前,还有一件事,就是在本地开发环境中,需要把这个域名映射到本地的项目,这个需要在 host 文件中,加入以下内容1127.0.0.1 www.pinzhi365.com 把项目的端口改成80 自定义social的url我们之前在 SocialConfig 中配置了 SpringSocialConfigurer 这个类,然后在浏览器的配置中引入了, 点进去这个类看一下 它里面有一个 configure() 方法,在最后,把 SocialAuthenticationFilter 加到了security的过滤器链上面, 在此之前 调用了一个 postProcess() 方法,去做了一些操作, 我们可以重写这个方法,然后加一些默认的操作 新建一个 MySpringSocialConfigurer 继承 SpringSocialConfigurer ,代码如下:123456789101112131415public class MySpringSocialConfigurer extends SpringSocialConfigurer &#123; private String filterProcessesUrl; public MySpringSocialConfigurer(String filterProcessesUrl) &#123; this.filterProcessesUrl = filterProcessesUrl; &#125; @Override protected &lt;T&gt; T postProcess(T object) &#123; SocialAuthenticationFilter filter = (SocialAuthenticationFilter) super.postProcess(object); filter.setFilterProcessesUrl(filterProcessesUrl); return (T) filter; &#125;&#125; 这里首先重写了一个 postProcess() 方法, 这个object参数其实就是 social 的过滤器, 所以首先拿到父类的处理结果,然后再加入我们自己的 url ,从构造里面传过来 然后 SocialConfig 中,要把之前的 SpringSocialConfigurer 替换成我们自己的 MySpringSocialConfigurer 1234567891011121314@Configuration@EnableSocialpublic class SocialConfig extends SocialConfigurerAdapter &#123; @Autowired private SecurityProperties securityProperties; @Bean public SpringSocialConfigurer securitySocialConfigurer()&#123; String filterProcessesUrl = securityProperties.getSocial().getFilterProcessesUrl(); MySpringSocialConfigurer mySpringSocialConfigurer = new MySpringSocialConfigurer(filterProcessesUrl); return mySpringSocialConfigurer; &#125;&#125; 这个路径是通过配置传过来的, 在 SocialProperties 中需要加一个属性1234/** * social 要拦截的请求地址 */private String filterProcessesUrl = &quot;/auth&quot;; 默认还是 /auth ,但是在我们上面配置的回调中,是 /qqLogin 所以需要在配置文件中加入以下配置:12345678core: security: social: filterProcessesUrl: /qqLogin qq: app-id: xxx app-secret: xxxxxxx providerId: callback.do 先是 filterProcessesUrl 配置成 /qqLogin 然后 providerId 配置为 callback.do, 这样回调配置就ok了, 在页面中的请求地址也是 /qqLogin/callback.do1&lt;a href = &apos;/qqLogin/callback.do&apos;&gt;QQ登录&lt;/a&gt; 配置完上面的之后,启动项目,访问 www.pinzhi365.com/login.html ,然后使用QQ登录,效果如下: 现在,就是授权的界面,点击头像登录,这个时候会调回到登录页, 在控制台中会有这样的一段输出12019-08-19 18:00:03.851 INFO 10952 --- [p-nio-80-exec-4] c.s.e.web.controller.BrowserController : 引发跳转的请求是:http://www.pinzhi365.com/signin 这里可以看到,他并没有去获取到QQ的用户信息,而是到了一个/signup的请求 登录流程下面先不去管QQ登录, 先看一下social的登录流程 总体的流程和之前的登录是一样的, 首先是拦截器拦截特定的登录请求, 然后通过 ConnectionFactory 创建一个没有经过认证的 Authentication 对象, 然后就是通过 AuthenticationProvider 去验证,去调用 SocialUserDetailsService 然后校验,最后返回一个已认证的 Authentication 在这个过程中 SocialAuthenticationService 会完成整个 OAuth2 的流程, 他会去调用我们的 ConnectionFactory 让然后里面有 ServiceProvider 等,最后拿到第三方的用户信息之后, 创建出来一个 Connection 对象, 封装成一个 SocialAuthenticationToken 回到上面的问题中, 我们在网站授权完成之后,条转到的是一个 /signup 的请求, 也就是再授权完成回调的时候出的问题,这个时候 OAuth2 的流程还没有走完, 也就是说问题出在 SocialAuthenticationService 中 看一下 OAuth2AuthenticationService 的源码, 这个类是继承 SocialAuthenticationService 的 这个 getAuthToken() 方法就是去获取第三方的授权的, 首先它会判断请求中有没有code这个参数, 因为我们系统中发起第三方登录请求和回调的地址都是一样的 , 所以有 code 参数的话,就是回调的,没有的话就是我们发起登录的 上面是没有 code 参数的处理, 最后会抛出一个异常, social 捕获到这个异常之后,就会重定向到QQ的授权页面 下面的是有 code 参数的处理, 这里会调用我们的 ConnectionFactory 创建 Connection 最后返回 SocialAuthenticationToken 对象 我们授权之后,QQ重定向回来,也就是说, 上面的问题是创建 SocialAuthenticationToken 的过程中出现了问题 在回调的时候打个断点看一下出的问题 如图这里的异常是:1Could not extract response: no suitable HttpMessageConverter found for response type [interface java.util.Map] and content type [text/html] 这里 QQ 返回的信息的 content type 是 [text/html] 的, 这里是使用 ConnectionFactory 去调用 OAuthOperations 的 exchangeForAccess() 方法出现的问题, 这里的 OAuthOperations 我们使用的是默认的 OAuth2Template 看一下 OAuth2Template 中的 exchangeForAccess() 方法,如下: 这个方法最后调用了一个 postForAccessGrant() 方法 这个方法就是用他自己的 RestTemplate 调用第三方的接口,然后将返回来的数据转成一个 Map 对象,这就需要返回来的数据是一个json格式的数据,contentType 是 application/json ,但是我们上面QQ返回来的是 text/html 的, 所以就会报错,捕获到异常之后返回一个空 在上层的 SocialAuthenticationFilter 中,判断token是否为空, 是空的话也返回空,如下图: 最终,会调用 social 的失败处理器,会重定向到一个默认的登录页面 这个默认的页面就是 /signup 问题定位到之后,看一下如何解决 自定义 OAuth2Template先看一下 OAuth2Template 中的 createRestTemplate() 方法, 如下: 这里其实就是他自己的 RestTemplate 没有加处理 text/html 的 converter,那么我们需要写一个自定义的 RestTemplate ,让他可以去处理 text/html 的返回 还有一个问题, 他的 RestTemplate 拿到返回值以后,会转成一个Map类型的数据,然后调用了一个 extractAccessGrant() 方法,组成一个 AccessGrant 对象返回,代码如下: 但是在QQ的返回,其实是下面这样的一个字符串:1access_token=FE04************************CCE2&amp;expires_in=7776000&amp;refresh_token=88E4************************BE14 然后我们需要把这个拆出来.组合成一个 AccessGrant 对象返回去 最后再看一下 exchangeForAccess() 方法 这里有个属性是 useParametersForClientAuthentication ,只有这个属性是true的时候,请求参数里面才会带上 client_id 和 client_secret 这两个参数是QQ要求必传的,所以我们还需把 useParametersForClientAuthentication 设置成true 好了,要做的功能就是这几个,然后新建一个 QQOAuth2Template 代码如下:12345678910111213141516171819202122232425262728293031323334@Slf4jpublic class QQOAuth2Template extends OAuth2Template &#123; public QQOAuth2Template(String clientId, String clientSecret, String authorizeUrl, String accessTokenUrl) &#123; super(clientId, clientSecret, authorizeUrl, accessTokenUrl); // 设置 useParametersForClientAuthentication 为true setUseParametersForClientAuthentication(true); &#125; @Override protected RestTemplate createRestTemplate() &#123; // 拿到父类创建的结果 RestTemplate restTemplate = super.createRestTemplate(); restTemplate.getMessageConverters().add(new StringHttpMessageConverter(Charset.forName(&quot;UTF-8&quot;))); return restTemplate; &#125; @Override protected AccessGrant postForAccessGrant(String accessTokenUrl, MultiValueMap&lt;String, String&gt; parameters) &#123; String responseStr = getRestTemplate().postForObject(accessTokenUrl, parameters, String.class); log.info(&quot;获取accessToken的响应:&#123;&#125;&quot;, responseStr); // 返回格式是 access_token=FE04************************CCE2&amp;expires_in=7776000&amp;refresh_token=88E4************************BE14 // 拆分组装成 AccessGrant String[] items = StringUtils.splitByWholeSeparatorPreserveAllTokens(responseStr, &quot;&amp;&quot;); String accessToken = StringUtils.substringAfterLast(items[0], &quot;=&quot;); Long expiresIn = new Long(StringUtils.substringAfterLast(items[1], &quot;=&quot;)); String refreshToken = StringUtils.substringAfterLast(items[2], &quot;=&quot;); return new AccessGrant(accessToken, null, refreshToken, expiresIn); &#125;&#125; restTemplate 中,添加了处理 text/html 的处理器 重写了 postForAccessGrant() 自定义解析QQ的返回值 构造中设置 useParametersForClientAuthentication 为true ServiceProvider 修改在我们的 QQServiceProvider 中把之前的 OAuth2Template 替换成我们自己的1234public QQServiceProvider(String appId, String secret) &#123; super(new QQOAuth2Template(appId, secret, URL_AUTHORIZE, URL_ACCESS_TOKEN)); this.appId = appId;&#125; 最后启动项目,用QQ登录, 控制台输出如下:12345678910111213142019-08-19 22:48:35.253 INFO 3160 --- [p-nio-80-exec-2] c.s.e.c.a.social.qq.api.QQImpl : QQ获取用户信息结果:&#123; &quot;ret&quot;: 0, &quot;msg&quot;: &quot;&quot;, &quot;is_lost&quot;:0, &quot;nickname&quot;: &quot;周。&quot;, &quot;gender&quot;: &quot;男&quot;, &quot;province&quot;: &quot;浙江&quot;, &quot;city&quot;: &quot;杭州&quot;, // ... 省略部分信息&#125;2019-08-19 22:48:35.282 INFO 3160 --- [p-nio-80-exec-3] c.s.e.c.v.filter.ValidateCodeFilter : 进入验证码校验的filter.....2019-08-19 22:48:35.296 INFO 3160 --- [p-nio-80-exec-6] c.s.e.c.v.filter.ValidateCodeFilter : 进入验证码校验的filter.....2019-08-19 22:48:35.316 INFO 3160 --- [p-nio-80-exec-6] c.s.e.web.controller.BrowserController : 引发跳转的请求是:http://www.pinzhi365.com/signup 这里这次会跳转到一个 /signup 的请求上去,就是默认的注册请求,这个之后再说 总结回顾下登录的流程: 访问第三方登录的请求,没有携带code参数 social 会重定向到第三方的授权页面 用户授权完毕之后, 第三方会回调原地址, 并且携带code参数 social 检测到了code 参数之后, 调用 QQImpl 去交换 accessToken 走自定义的 OAuthTemplate 去拿到token 组装返回一个 AccessGrant 对象 获取用户的信息,组装成未认证的 Authentication 对象 最后就是 security 的认证流程了 本文代码传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-15-开发QQ登录(中)]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-15-%E5%BC%80%E5%8F%91QQ%E7%99%BB%E5%BD%95-%E4%B8%AD%2F</url>
    <content type="text"><![CDATA[述上文中实现了 ServiceProvider 的部分, 然后剩下就是 ConnectionFactory 创建 Connection 的部分还有与数据量交互的部分 ConnectionFactory 构建适配器实现要构建 ConnectionFactory 就需要 ServiceProvider 和一个 ApiAdapter ,ServiceProvider 已经有了,那下面先把 ApiAdapter 实现一下 ApiAdapter 的作用,就是把我们获取回来的第三方的数据,和 social 的标准的数据做一个适配, 新建类 QQAdapter: 123456789101112131415161718192021222324252627282930313233343536public class QQAdapter implements ApiAdapter&lt;QQ&gt; &#123; /** * 测试当前的API是否可以访问 * @param api * @return */ @Override public boolean test(QQ api) &#123; return true; &#125; /** * 适配,把 ConnectionValues 需要的数据set进去 * @param api * @param values */ @Override public void setConnectionValues(QQ api, ConnectionValues values) &#123; QQUserInfo qqUserInfo = api.getUserInfo(); values.setDisplayName(qqUserInfo.getNickname()); values.setImageUrl(qqUserInfo.getFigureurl_qq_1()); // 用户在服务商的唯一id values.setProviderUserId(qqUserInfo.getOpenId()); // 用户主页, QQ没有 values.setProfileUrl(null); &#125; @Override public UserProfile fetchUserProfile(QQ api) &#123; return null; &#125; @Override public void updateStatus(QQ api, String message) &#123; &#125; 首先这个类实现了一个 ApiAdapter 接口,泛型是QQ ,这里的泛型就是当前适配器适配的 api 的类型,我们这里要是适配的就是QQ 第一个方法 test 是要去服务提供商是否可以使用,这里就直接返回true了 然后是 setConnectionValues() 方法, 这个就是最主要的方法,作用就是把我们api返回来的数据,设置到 ConnectionValues 里面去fetchUserProfile() 这个方法后面再说 updateStatus() 这个在QQ登录也不需要不用管 ConnectionFactory 实现到这里,我们的 ServiceProvider 和一个 ApiAdapter 就都有了,这里就可以去构建 ConnectionFactory 了, 新建一个 QQConnectionFactory ,代码如下:1234567891011121314public class QQConnectionFactory extends OAuth2ConnectionFactory&lt;QQ&gt; &#123; /** * 连接工厂构造 * @param providerId 服务商的唯一id * @param appId * @param appSecret */ public QQConnectionFactory(String providerId, String appId, String appSecret) &#123; super(providerId, new QQServiceProvider(appId, appSecret), new QQAdapter()); &#125;&#125; 这里先继承 OAuth2ConnectionFactory 然后写了一个构造, providerId 是服务提供商的id,这个是后面我们自定义去配置的, 然后把 ServiceProvider 和 ApiAdapter 都传过去就好了 数据库层的实现ConnectionFactory 已经实现了,然后 Connection 就交给 ConnectionFactory 去创建, 最后还剩下的就是数据库层的实现, 这个直接做个配置就好了 首先,数据库需要一个表,sql如下:12345678910111213CREATE TABLE UserConnection (userId VARCHAR(255) NOT NULL, providerId VARCHAR(255) NOT NULL, providerUserId VARCHAR(255), rank INT NOT NULL, displayName VARCHAR(255), profileUrl VARCHAR(512), imageUrl VARCHAR(512), accessToken VARCHAR(512) NOT NULL, secret VARCHAR(512), refreshToken VARCHAR(512), expireTime BIGINT, PRIMARY KEY (userId, providerId, providerUserId));CREATE UNIQUE INDEX UserConnectionRank ON UserConnection(userId, providerId, rank); 这里表名是固定的 UserConnection 这个不能改,但是可以加一个自定义的前缀, 比如test_UserConnection 表建好之后,就是代码里面的配置,新建类 SocialConfig:12345678910111213141516171819202122@Configuration@EnableSocialpublic class SocialConfig extends SocialConfigurerAdapter &#123; @Autowired private DataSource dataSource; /** * 配置 JdbcUsersConnectionRepository * @param connectionFactoryLocator * @return */ @Override public UsersConnectionRepository getUsersConnectionRepository(ConnectionFactoryLocator connectionFactoryLocator) &#123; JdbcUsersConnectionRepository jdbcUsersConnectionRepository = new JdbcUsersConnectionRepository(dataSource, connectionFactoryLocator, Encryptors.noOpText()); // 设置表的前缀// jdbcUsersConnectionRepository.setTablePrefix(&quot;&quot;); return jdbcUsersConnectionRepository; &#125; &#125; 这里继承 SocialConfigurerAdapter 然后重写 getUsersConnectionRepository() 方法 这里的参数是 ConnectionFactoryLocator 是用来查询对应的 ConnectionFactory 的,因为我们集成的服务提供商可能有多个, 所以需要找到对应的 ConnectionFactory 然后再看一下构造,前两个就不说了,最后一个 Encryptors 是配置一个加密的方式,是 noOpText() 表示不做加密处理 然后 setTablePrefix() 方法是设置表的前缀, 如果建表的时候有前缀,那这里就写自己的前缀就好 UserDetailService 改造上面完成之后, QQ登录的流程就算完了, 接下来还有一些配置 之前我们写了一个自定义的 UserDetailService 是 MyUserDetailsService, 这里面只有一个通过用户名去查找的, 但是第三方登录之后,我们只能通过 userconnection 表 通过 providerId 然后还有 第三方的 openId 去拿到我们业务数据库的 userid, 所以这里还需要一个通过 userId 去查询用户的方法 social提供了他自己的 UserDetailService 是 SocialUserDetailsService,在之前的 MyUserDetailsService 上实现一下,修改后如下:1234567891011121314151617181920212223242526@Component@Slf4jpublic class MyUserDetailsService implements UserDetailsService, SocialUserDetailsService &#123; @Autowired private UserService userService; @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; // ... 省略部分代码 &#125; @Override public SocialUserDetails loadUserByUserId(String userId) throws UsernameNotFoundException &#123; // 根据id去查询用户信息 User user = userService.getByUserId(Long.valueOf(userId)); return new SocialUser(user.getName(), user.getPassword(), user.getEnable(), true, true, !user.getLocked(), AuthorityUtils.commaSeparatedStringToAuthorityList(&quot;admin&quot;)); &#125;&#125; 配置把用到的一些变量,写到配置类里面, 首先新建 QQProperties 然后继承 SocialProperties123456@Datapublic class QQProperties extends SocialProperties &#123; private String providerId = &quot;qq&quot;;&#125; SocialProperties 只有两个属性 appId 和 appSecret ,然后自定义一个供应商id ,这里给个默认值是 qq 然后再写一个 SocialProperties 12345678@Datapublic class SocialProperties &#123; /** * QQ的配置 */ private QQProperties qq = new QQProperties();&#125; 最后放到总的配置中 SecurityProperties1234567891011@Data@ConfigurationProperties(prefix = &quot;core.security&quot;)public class SecurityProperties &#123; // ... 省略其他配置 /** * social 的配置 */ private SocialProperties social = new SocialProperties();&#125; 之后,在 application.yml 中,配置好这些:123456core: security: social: qq: app-id: xxxxx app-secret: xxxxx 配置 ConnectionFactory配置类写好之后, 把这些配置给到之前写好的 QQConnectionFactory, 新建一个 QQAutoConfig ,代码如下:12345678910111213@Configuration@ConditionalOnProperty(prefix = &quot;core.security.social.qq&quot;, name = &quot;app-id&quot;)public class QQAutoConfig extends SocialAutoConfigurerAdapter &#123; @Autowired private SecurityProperties securityProperties; @Override protected ConnectionFactory&lt;?&gt; createConnectionFactory() &#123; QQProperties qq = securityProperties.getSocial().getQq(); return new QQConnectionFactory(qq.getProviderId(), qq.getAppId(), qq.getAppSecret()); &#125;&#125; 这里要注意 @ConditionalOnProperty(prefix = &quot;core.security.social.qq&quot;, name = &quot;app-id&quot;) 这个注解的作用是 只有找到前缀是 core.security.social.qq 然后名称是 name 的配置有值之后才会生效 浏览器配置web环境如果想使用 social 的话,还需要把 social 的过滤器加到 security 的过滤器链中去,也就是要把 social 的配置引用过去,首先在 SocialConfig 中,加入以下配置:1234@Beanpublic SpringSocialConfigurer securitySocialConfigurer()&#123; return new SpringSocialConfigurer();&#125; 然后在浏览器的配置中 BrowserSecurityConfig ,就可以注入 用 apply() 方法引入了, 如下:123456789101112131415161718192021@Configurationpublic class BrowserSecurityConfig extends AbstractChannelSecurityConfig &#123; @Autowired private SpringSocialConfigurer securitySocialConfigurer; @Override protected void configure(HttpSecurity http) throws Exception &#123; // web网页登录的配置 applyPasswordAuthenticationConfig(http); http .apply(validateCodeSecurityConfig) .and() .apply(smsAuthenticationSecurityConfig) .and() .apply(securitySocialConfigurer) // ... 省略其他代码 &#125;&#125; 页面配置最后页面中需要一个三方登录的按钮1&lt;a href = &apos;auth/qq&apos;&gt;QQ登录&lt;/a&gt; auth/qq 这个路径,首先 auth 是social的拦截器要拦截前缀, 然后 qq 是我们设置的 providerId 总结从 ServiceProvider 开始配置然后 ConnectionFactory 的配置, 然后是数据库方面的配置, 按照之前的图,一步一步搞好就ok 掌握 @ConditionalOnProperty 的用法 本文代码传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-14-开发QQ登录(上)]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-14-%E5%BC%80%E5%8F%91QQ%E7%99%BB%E5%BD%95-%E4%B8%8A%2F</url>
    <content type="text"><![CDATA[述本文开始,实现一个第三方登录,以QQ为例,首先需要去QQ互联里面,申请开发者认证,然后创建应用,搞到AppId,和AppKey 实现流程按照上文中的实现流程,先是构建 ServiceProvider ,然后是 ConnectionFactory 然后 Connection 最后交互数据库 这里第三方登录可能在APP和web环境下都会用到,所以写在core项目中 ServiceProvider的实现API部分首先 ServiceProvider 里面需要两个东西, 一个是 API 一个是 OAuth2Operations OAuth2Operations 先用 Spring Social 的默认实现 OAuth2Template ,所以现在我们需要搞定 API 部分 首先新建一个获取QQ用户信息的接口.如下:12345678910public interface QQ &#123; /** * 获取用户信息 * @return &#123;@link QQUserInfo&#125; */ QQUserInfo getUserInfo() throws IOException; &#125; 实体类 QQUserInfo 的代码如下:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192@Datapublic class QQUserInfo &#123; /** * 返回码 */ private String ret; /** * 如果ret&lt;0，会有相应的错误信息提示，返回数据全部用UTF-8编码。 */ private String msg; /** * */ private String openId; /** * 不知道什么东西，文档上没写，但是实际api返回里有。 */ private String is_lost; /** * 用户在QQ空间的昵称。 */ private String nickname; /** * 性别。 如果获取不到则默认返回”男” */ private String gender; /** * 省(直辖市) */ private String province; /** * 市(直辖市区) */ private String city; /** * 出生年月 */ private String year; /** * 星座 */ private String constellation; /** * 大小为30×30像素的QQ空间头像URL。 */ private String figureurl; /** * 大小为50×50像素的QQ空间头像URL。 */ private String figureurl_1; /** * 大小为100×100像素的QQ空间头像URL。 */ private String figureurl_2; /** * 大小为40×40像素的QQ头像URL。 */ private String figureurl_qq_1; /** * 大小为100×100像素的QQ头像URL。需要注意，不是所有的用户都拥有QQ的100×100的头像，但40×40像素则是一定会有。 */ private String figureurl_qq_2; /** * 也是头像 */ private String figureurl_qq; /** * 头像类型 */ private String figureurl_type; /** * 标识用户是否为黄钻用户（0：不是；1：是）。 */ private String is_yellow_vip; /** * 标识用户是否为黄钻用户（0：不是；1：是） */ private String vip; /** * 黄钻等级 */ private String yellow_vip_level; /** * 黄钻等级 */ private String level; /** * 标识是否为年费黄钻用户（0：不是； 1：是） */ private String is_yellow_year_vip;&#125; 然后需要一个实现类 QQImpl,代码如下:12345678910111213141516171819202122232425262728293031323334353637383940414243444546@Slf4jpublic class QQImpl extends AbstractOAuth2ApiBinding implements QQ &#123; public static final String URL_GET_OPENID = &quot;https://graph.qq.com/oauth2.0/me?access_token=%s&quot;; public static final String URL_GET_USER_INFO = &quot;https://graph.qq.com/user/get_user_info?oauth_consumer_key=%s&amp;openid=%s&quot;; private String appId; private String openId; private ObjectMapper objectMapper = new ObjectMapper(); public QQImpl(String accessToken, String appId)&#123; super(accessToken, TokenStrategy.ACCESS_TOKEN_PARAMETER); this.appId = appId; // 有了Appid 和 token之后去取openid String url = String.format(URL_GET_OPENID, accessToken); String result = getRestTemplate().getForObject(url, String.class); log.info(&quot;QQ获取用户openid结果:&#123;&#125;&quot;, result); // 返回格式是 callback( &#123;&quot;client_id&quot;:&quot;YOUR_APPID&quot;,&quot;openid&quot;:&quot;YOUR_OPENID&quot;&#125; ) 字符串所以需要截取一下 this.openId = StringUtils.substringBetween(result, &quot;\&quot;openid\&quot;:\&quot;&quot;, &quot;\&quot;&#125;&quot;); &#125; @Override public QQUserInfo getUserInfo() &#123; // 有了openid之后就可以获取用户信息了 String url = String.format(URL_GET_USER_INFO, appId, openId); String result = getRestTemplate().getForObject(url, String.class); log.info(&quot;QQ获取用户信息结果:&#123;&#125;&quot;, result); // 转成对象返回 QQUserInfo qqUserInfo = null; try &#123; qqUserInfo = objectMapper.readValue(result, QQUserInfo.class); // 设置openid到用户信息里 qqUserInfo.setOpenId(openId); &#125; catch (IOException e) &#123; e.printStackTrace(); throw new RuntimeException(&quot;获取QQ用户信息失败&quot;); &#125; return qqUserInfo; &#125;&#125; 首先这里继承了 AbstractOAuth2ApiBinding 这个就是API的默认实现, 首先获取QQ用户需要 Token, appid 和 openId openId需要我们自己去取, 也就是这个类构造里面写的 来看下 AbstractOAuth2ApiBinding 的代码: 他这里的参数一个是accessToken, 一个是RestTemplate, 这俩个都是个全局的变量,所以后面我们不能使用依赖注入, 否则会有线程安全的问题 然后 accessToken 用来放token, 这个类会为我们获取用户信息做一些默认的处理,就是 构造中的 super(accessToken, TokenStrategy.ACCESS_TOKEN_PARAMETER); 我们子类调用了他的这个构造, 第二个参数表示,将token当做参数传递, 如果不传的话,默认会将 token 放到请求头里面 RestTemplate 就是用来发送Http请求到第三方 在构造中拿到openId之后,就可以使用我们的获取用户的方法,去跟QQ交互,获取用户的信息了. 服务提供商实现API的部分已经实现了,现在再用一个默认的 OAuth2Template 就可以组成 ServiceProvider 了 新建一个 QQServiceProvider 继承 ServiceProvider 的默认实现 AbstractOAuth2ServiceProvider 123456789101112131415161718192021222324public class QQServiceProvider extends AbstractOAuth2ServiceProvider&lt;QQ&gt; &#123; /** * 认证链接 */ private static final String URL_AUTHORIZE = &quot;https://graph.qq.com/oauth2.0/authorize&quot;; /** * 获取token的链接 */ private static final String URL_ACCESS_TOKEN = &quot;https://graph.qq.com/oauth2.0/token&quot;; private String appId; public QQServiceProvider(String appId, String secret) &#123; super(new OAuth2Template(appId, secret, URL_AUTHORIZE, URL_ACCESS_TOKEN)); this.appId = appId; &#125; @Override public QQ getApi(String accessToken) &#123; return new QQImpl(accessToken, appId); &#125;&#125; 这里会重写一个父类的方法是 getApi ,就把我们刚才写的 API 的实现返回就好 然后上面的构造是指定了一个 OAuth2Operations ,用了他的默认实现 OAuth2Template 到这儿服务提供商的开发就完成了,对照上文中的图去看比较容易理解. 本文代码传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-13-OAuth2协议及Spring Social简介]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-13-OAuth2%E5%8D%8F%E8%AE%AE%E5%8F%8ASpring-Social%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[述上文中,已经实现了短信登录的方式, 常见的登录方式还有第三方登录,比如集成微信或者QQ登录, 第三方登录我们可以使用 Spring Social 去开发, 这里首先得要了解一下 OAuth2 协议 OAuth协议简介以微信登录为例,使用微信登录后,通常我们需要去微信拿用户的信息, 最常见的方式就是微信去询问用户是否授权,然后用户同意授权之后,才能获取到微信的数据,当然这只是一个大概的流程,下面来详细看一下在这个过程中,我们的应用与微信还有用户都做了哪些事情 如图: 用户访问我们的应用 将用户导向第三方的认证服务器,然后获取用户的同意 用户同意之后,第三方回调我们的接口,并且传过来一个授权码 我们的应用拿到授权码之后,去第三方请求token 第三方返回token 携带token去请求第三方的资源服务器 获取到数据 这个是最常见的授权码模式 (Authorization code) ,OAuth提供了4中授权方式: 授权码模式 Authorization code 简化模式 implicit 密码模式 resource owner Password credentials 客户端模式 client credentials 名词定义 Client: 客户端,即我们自己的应用 Provider: 服务提供商,即微信或者QQ等等 Resource Owner: 资源所有者,即用户 Authorization Server: 认证服务器 Resource Server: 资源服务器 Spring Social上面的例子中,最后一步就可以拿到第三方的用户信息,在Spring Security中,用户认证成功就是将一个已认证的 Authentication 对象放到 SecurityContext 中 Socail 其实和我们之前的用户名密码登录,或者短信登录是一样的,只不过是一种不同的登录方式,在 Spring Socail 会在 Spring Security 的过滤器链中加一个过滤器 SocialAuthenticationFilter ,如图: 这里拦截到所有的用户第三方登录的请求,然后走完第三方登录的流程 用到的一些类首先来看一幅图 Spring Socail 集成第三方登录大概就是用到了这些东西, 基本分为三部分, 连接, 第三方应用还有数据库部分 ServiceProviderServiceProvider 就是服务提供商(比如微信,QQ)的抽象, 然后默认实现是 AbstractOAuth2ServiceProvider ,里面有两个属性 OAuth2Operations 和 API OAuth2Operations 就封装了标准的oauth2协议流程,就算是最从用户访问我们的应用到去认证服务器中请求数据(上面图中的 1-8 步) ,他的默认实现是 OAuth2Template 然后是 API, 这是一个抽象的方法需要我们自己去实现,因为每个提供商返回的数据都不一样,所以针对不同的提供商要做不同的实现,他也提供了一个默认的实现是 AbstractOAuth2ApiBinding 总之就是封装了一些跟服务提供商相关的行为 ConnectionConnection是用来封装上面获取到的用户信息的, 通过 ConnectionFactory 创建, 获取用户信息肯定需要跟服务提供商交互,所以 ConnectionFactory 中包含了 ServiceProvider Connection 的默认实现是 OAuth2Connection ,它的数据结构是固定的,但是每个服务提供方返回来的数据都不一样,所以这里需要 ApiAdapter 去做适配 UsersConnectionRepository最后一部分就是数据库的部分, 我们通过前面几步获取到了第三放的用户信息之后, 需要与我们的业务系统中的用户做关联 所以需要一个业务系统的用户与第三方用户对应的表, 这里就要用到 UsersConnectionRepository 去操作数据库了, 具体的实现是 JdbcUsersConnectionRepository 开发第三方登录流程从上图中可以看到, 要实现一个第三方登录就需要一个 Connection ,这个要通过 ConnectionFactory 去构建, ConnectionFactory 中又需要 ServiceProvider 和 ApiAdapter 所以先从 ServiceProvider 开始,先实现 API 部分, 然后 OAuth2Operations 用它默认的实现, 然后再是 ApiAdapter 有了这两个之后就能构建 ConnectionFactory 了, 然后创建 Connection 最后通过 UsersConnectionRepository 操作数据库做关联 总结 了解OAuth协议的一个流程 了解 Spring Social 的用法]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-12-短信登录配置及验证逻辑重构]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-12-%E7%9F%AD%E4%BF%A1%E7%99%BB%E5%BD%95%E9%85%8D%E7%BD%AE%E5%8F%8A%E9%AA%8C%E8%AF%81%E9%80%BB%E8%BE%91%E9%87%8D%E6%9E%84%2F</url>
    <content type="text"><![CDATA[述上文中,已经把短信登录流程的代码写好了,但是这些都没有加到 Spring Security 中, 所以并没有生效, 下面看一下如何将我们自定义的这个登录流程配置到 Spring Security 中去 配置这个登录方式,可能会在多个环境下用到,所以这里把配置类写到core项目里面去, 新建类 SmsAuthenticationSecurityConfig :123456789101112131415161718192021222324252627282930@Componentpublic class SmsAuthenticationSecurityConfig extends SecurityConfigurerAdapter&lt;DefaultSecurityFilterChain, HttpSecurity&gt; &#123; @Autowired private AuthenticationSuccessHandler myAuthenticationSuccessHandler; @Autowired private AuthenticationFailureHandler myAuthenticationFailureHandler; @Autowired private UserDetailsService userDetailsService; @Override public void configure(HttpSecurity http) throws Exception &#123; // 过滤器的配置 SmsAuthenticationFilter smsAuthenticationFilter = new SmsAuthenticationFilter(); // 设置AuthenticationManager smsAuthenticationFilter.setAuthenticationManager(http.getSharedObject(AuthenticationManager.class)); // 设置登录成功失败的处理 smsAuthenticationFilter.setAuthenticationSuccessHandler(myAuthenticationSuccessHandler); smsAuthenticationFilter.setAuthenticationFailureHandler(myAuthenticationFailureHandler); // Provider的配置 SmsAuthenticationProvider smsAuthenticationProvider = new SmsAuthenticationProvider(); smsAuthenticationProvider.setUserDetailsService(userDetailsService); http.authenticationProvider(smsAuthenticationProvider) .addFilterAfter(smsAuthenticationFilter, UsernamePasswordAuthenticationFilter.class); &#125;&#125; 首先继承了 SecurityConfigurerAdapter ,然后重写 configure(HttpSecurity http) 方法,里面首先配置过滤器, 设置 AuthenticationManager, 还有登录成功失败的处理器 下面就是我们自己的 Provider 的配置, 最后是把这个 Provider 加到 AuthenticationManager 里面去, 再把我们的过滤器加到 UsernamePasswordAuthenticationFilter 的后面去 这个配置是在core里面的配置,web环境并没有, 然后我们需要把这段配置引到web环境中去, BrowserSecurityConfig 中, 修改如下:123456789@Autowiredprivate SmsAuthenticationSecurityConfig smsAuthenticationSecurityConfig;@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; // ... 省略其他diamante .csrf().disable() .apply(smsAuthenticationSecurityConfig);&#125; 这里重点就是 .apply() 方法,他可以把其他地方的配置也引过来,就相当于在这个方法里写一样 常量配置我们系统中有一些常量,比如说 登录的url, 参数名, 等等. 这些常量都可以放在一个地方统一的管理. core项目里面 新建一个类 SecurityConstants1234567891011121314151617181920212223242526272829303132333435363738394041public class SecurityConstants &#123; /** * 验证图片验证码时，http请求中默认的携带图片验证码信息的参数的名称 */ public static final String DEFAULT_PARAMETER_NAME_CODE_IMAGE = &quot;image&quot;; /** * 验证短信验证码时，http请求中默认的携带短信验证码信息的参数的名称 */ public static final String DEFAULT_PARAMETER_NAME_CODE_SMS = &quot;sms&quot;; /** * 默认的处理验证码的url前缀 */ public static final String DEFAULT_VALIDATE_CODE_URL_PREFIX = &quot;/code&quot;; /** * 当请求需要身份认证时，默认跳转的url */ public static final String DEFAULT_UNAUTHENTICATION_URL = &quot;/authentication/require&quot;; /** * 默认的用户名密码登录请求处理url */ public static final String DEFAULT_LOGIN_PROCESSING_URL_FORM = &quot;/authentication/form&quot;; /** * 默认的手机验证码登录请求处理url */ public static final String DEFAULT_LOGIN_PROCESSING_URL_MOBILE = &quot;/authentication/mobile&quot;; /** * 默认登录页面 */ public static final String DEFAULT_LOGIN_PAGE_URL = &quot;/login.html&quot;; /** * 发送短信验证码 或 验证短信验证码时，传递手机号的参数的名称 */ public static final String DEFAULT_PARAMETER_NAME_MOBILE = &quot;phoneNo&quot;;&#125; 然后把我们之前直接写的都替换掉就好了,之后有什么常量也都写在这个里面 验证逻辑重构之前一直没有写验证逻辑,图形验证码和短信验证码的验证逻辑其实是一样的,所以这个验证也是可以封装起来的 ValidateCodeType修改首先是 ValidateCodeType ,加一个方法,就是通过枚举类型,获取参数名称,比如是图片类型,那枚举就是IMAGE,通过方法拿到参数名,就是我们上面常量中配置的 image, 修改后的代码如下:123456789101112131415161718192021222324252627282930public enum ValidateCodeType implements Serializable &#123; /** * 短信验证码 */ SMS&#123; @Override public String getParamNameOnValidate() &#123; return SecurityConstants.DEFAULT_PARAMETER_NAME_CODE_SMS; &#125; &#125;, /** * 图片验证码 */ IMAGE&#123; @Override public String getParamNameOnValidate() &#123; return SecurityConstants.DEFAULT_PARAMETER_NAME_CODE_IMAGE; &#125; &#125;, ; /** * 校验时从请求中获取的参数的名字 * @return */ public abstract String getParamNameOnValidate();&#125; ValidateCodeProcessorHolder然后新建一个类 ValidateCodeProcessorHolder ,这个类用来通过验证码类型获取验证码发送器1234567891011121314151617181920212223@Componentpublic class ValidateCodeProcessorHolder &#123; /** * 收集系统中的所有&#123;@link ValidateCodeProcessor&#125; 的实现 */ @Autowired private Map&lt;String, ValidateCodeProcessor&gt; validateCodeProcessors; public ValidateCodeProcessor findValidateCodeProcessor (ValidateCodeType type) &#123; return findValidateCodeProcessor(type.name().toLowerCase()); &#125; public ValidateCodeProcessor findValidateCodeProcessor(String type) &#123; String name = type.toLowerCase() + ValidateCodeProcessor.class.getSimpleName(); ValidateCodeProcessor processor = validateCodeProcessors.get(name); if (processor == null) &#123; throw new ValidateCodeException(&quot;验证码处理器&quot; + name + &quot;不存在&quot;); &#125; return processor; &#125;&#125; 这里主要就是收集所有的 ValidateCodeProcessorHolder 然后通过类型获取具体的处理器 这里,我们去 map 中取处理器的key是 type + ValidateCodeProcessor.class.getSimpleName() 所以说.我们的图片验证码处理器 应该改为 imageValidateCodeProcessor,然后短信验证码应该改为 smsValidateCodeProcessor ValidateCodeProcessor然后是 ValidateCodeProcessor, 这个类中之前只有一个发送验证码的方法,然后现在再加一个校验的方法123456/** * 校验验证码 * @param request * @throws ServletRequestBindingException */void validate(HttpServletRequest request) throws ServletRequestBindingException; AbstractValidateCodeProcessorAbstractValidateCodeProcessor 需要去实现我们上面的验证的具体的逻辑 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * 根据请求的url获取校验码的类型 * @param request * @return ValidateCodeType */private ValidateCodeType getValidateCodeType(HttpServletRequest request) &#123; String type = StringUtils.substringBefore(getClass().getSimpleName(), &quot;CodeProcessor&quot;); return ValidateCodeType.valueOf(type.toUpperCase());&#125;@Overridepublic void validate(HttpServletRequest request) throws ServletRequestBindingException &#123; // 获取验证的类型 ValidateCodeType processorType = getValidateCodeType(request); // 获取redis中存的key的值 String redisKey = getRedisKey(request); // 从缓存中拿出来 C codeInCache = (C) redisTemplate.opsForValue().get(redisKey); // 然后是请求中的验证码 String codeInRequest; try &#123; codeInRequest = ServletRequestUtils.getStringParameter(request, processorType.getParamNameOnValidate()); &#125; catch (ServletRequestBindingException e) &#123; throw new ValidateCodeException(&quot;获取验证码的值失败&quot;); &#125; if (StringUtils.isBlank(codeInRequest)) &#123; throw new ValidateCodeException(&quot;验证码的值不能为空&quot;); &#125; if (codeInCache == null) &#123; throw new ValidateCodeException(&quot;验证码不存在&quot;); &#125; if (codeInCache.expired()) &#123; redisTemplate.delete(redisKey); throw new ValidateCodeException(&quot;验证码已过期&quot;); &#125; if (!StringUtils.equalsIgnoreCase(codeInCache.getCode(), codeInRequest)) &#123; throw new ValidateCodeException(&quot;验证码错误&quot;); &#125; // 最后通过的话也从缓存清除掉 redisTemplate.delete(redisKey);&#125; 首先通过是哪个发送器,来确定类型. 然后还是之前的验证步骤 ValidateCodeFilter最后是 filter 的修改,修改后的代码如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105@Data@Slf4j@Component(&quot;validateCodeFilter&quot;)public class ValidateCodeFilter extends OncePerRequestFilter implements InitializingBean &#123; @Autowired private ValidateCodeProcessorHolder validateCodeProcessorHolder; @Autowired private AuthenticationFailureHandler authenticationFailureHandler; @Autowired private RedisTemplate redisTemplate; @Autowired private SecurityProperties securityProperties; /** * 存放所有需要校验验证码的url */ private Map&lt;String, ValidateCodeType&gt; urlMap = new HashMap&lt;&gt;(); /** * 用来匹配url */ private AntPathMatcher pathMatcher = new AntPathMatcher(); @Override public void afterPropertiesSet() throws ServletException &#123; super.afterPropertiesSet(); // 首先,登录请求必须验证.直接放进去 urlMap.put(SecurityConstants.DEFAULT_LOGIN_PROCESSING_URL_FORM, ValidateCodeType.IMAGE); urlMap.put(SecurityConstants.DEFAULT_LOGIN_PROCESSING_URL_MOBILE, ValidateCodeType.SMS); // 不同的表单做不同的处理 addUrlToMap(securityProperties.getCode().getImageCode().getUrls(), ValidateCodeType.IMAGE); addUrlToMap(securityProperties.getCode().getSmsCode().getUrls(), ValidateCodeType.SMS); &#125; /** * 从配置中拆出路径来放到url里面 * @param urlString * @param type */ protected void addUrlToMap(String urlString, ValidateCodeType type) &#123; if (StringUtils.isBlank(urlString)) &#123; return; &#125; String[] urls = StringUtils.splitByWholeSeparatorPreserveAllTokens(urlString, &quot;,&quot;); for (String url : urls) &#123; urlMap.put(url, type); &#125; &#125; @Override protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException &#123; log.info(&quot;进入验证码校验的filter.....&quot;); try &#123; // 这里做验证 validate(request); &#125; catch (ValidateCodeException exception) &#123; // 验证失败调用验证失败的处理 authenticationFailureHandler.onAuthenticationFailure(request, response, exception); // 直接返回 不走下一个过滤器了 return; &#125; doFilter(request, response, filterChain); &#125; private void validate(HttpServletRequest request) throws ServletRequestBindingException &#123; ValidateCodeType type = getValidateCodeType(request); if (type == null)&#123; return; &#125; log.info(&quot;校验请求[&#123;&#125;]中的验证码,验证码类型是[&#123;&#125;]&quot;, request.getRequestURI(), type); validateCodeProcessorHolder.findValidateCodeProcessor(type).validate(request); &#125; /** * 获取校验码的类型，如果当前请求不需要校验，则返回null * @param request * @return */ private ValidateCodeType getValidateCodeType(HttpServletRequest request) &#123; ValidateCodeType result = null; if (StringUtils.equalsIgnoreCase(request.getMethod(), &quot;get&quot;)) &#123; return null; &#125; Set&lt;String&gt; urls = urlMap.keySet(); for (String url : urls) &#123; if (pathMatcher.match(url, request.getRequestURI())) &#123; result = urlMap.get(url); &#125; &#125; return result; &#125;&#125; 这里的 urlMap 对应的是他们的 要拦截的url和对应的验证码类型,总体的逻辑还是没有变的,只是处理两个类型的验证码 发送验证码的接口的修改对 ValidateCodeController 做了一点改动,如下:1234567891011121314@RestController@Slf4jpublic class ValidateCodeController &#123; @Autowired private ValidateCodeProcessorHolder validateCodeProcessorHolder; @GetMapping(SecurityConstants.DEFAULT_VALIDATE_CODE_URL_PREFIX + &quot;/&#123;type&#125;&quot;) public void getCode(HttpServletRequest request, HttpServletResponse response, @PathVariable String type) throws IOException, ServletRequestBindingException &#123; ValidateCodeProcessor processor = validateCodeProcessorHolder.findValidateCodeProcessor(type); processor.createCode(request, response); &#125;&#125; 登录页面修改常量配置中,默认接收的图片验证码参数名是 image 然后短信的是 sms ,页面对应的input框里面需要把参数的名字对应上 到这儿两种类型登录就都可以使用了,可以启动项目看下效果 首先,路径更新成了常量,然后这里获取验证码处理器就用了我们上面的 ValidateCodeProcessorHolder 配置重构先来看一下我们现在的配置类 BrowserSecurityConfig 这里可以看到,我们的配置都是左右都是可以分成几类的,然后这里就可以都拆分出来,就跟最后的 短信登录的配置一样,单独抽出来,然后这里通过 apply() 方法去引用 我们现在这个类是浏览器的配置类,然后之后后APP的配置的话,有通用的地方也是直接 apply() 连过去就好了 AbstractChannelSecurityConfig首先新建一个配置类,里面放网页登录的配置,如下:1234567891011121314151617public class AbstractChannelSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Autowired protected AuthenticationSuccessHandler myAuthenticationSuccessHandler; @Autowired protected AuthenticationFailureHandler myAuthenticationFailureHandler; protected void applyPasswordAuthenticationConfig(HttpSecurity http) throws Exception &#123; http.formLogin() .loginPage(SecurityConstants.DEFAULT_UNAUTHENTICATION_URL) .loginProcessingUrl(SecurityConstants.DEFAULT_LOGIN_PROCESSING_URL_FORM) .successHandler(myAuthenticationSuccessHandler) .failureHandler(myAuthenticationFailureHandler); &#125;&#125; ValidateCodeSecurityConfig再新建一个配置类放验证码的配置,如下:1234567891011@Component(&quot;validateCodeSecurityConfig&quot;)public class ValidateCodeSecurityConfig extends SecurityConfigurerAdapter&lt;DefaultSecurityFilterChain, HttpSecurity&gt; &#123; @Autowired private Filter validateCodeFilter; @Override public void configure(HttpSecurity http) throws Exception &#123; http.addFilterBefore(validateCodeFilter, UsernamePasswordAuthenticationFilter.class); &#125;&#125; 总的配置修改最后 BrowserSecurityConfig 里面把这两个引进来, 首先 BrowserSecurityConfig 需要继承 AbstractChannelSecurityConfig 这个里面放着用户名密码的配置, 是必须要有的 ,修改后的代码如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263@Configurationpublic class BrowserSecurityConfig extends AbstractChannelSecurityConfig &#123; @Autowired private SecurityProperties securityProperties; @Autowired private UserDetailsService userDetailsService; @Autowired private DataSource dataSource; @Autowired private SmsAuthenticationSecurityConfig smsAuthenticationSecurityConfig; @Autowired private ValidateCodeSecurityConfig validateCodeSecurityConfig; @Bean public PasswordEncoder passwordEncoder()&#123; return new BCryptPasswordEncoder(); &#125; @Bean public PersistentTokenRepository persistentTokenRepository() &#123; JdbcTokenRepositoryImpl tokenRepository = new JdbcTokenRepositoryImpl(); tokenRepository.setDataSource(dataSource); return tokenRepository; &#125; @Override protected void configure(HttpSecurity http) throws Exception &#123; // web网页登录的配置 applyPasswordAuthenticationConfig(http); http .apply(validateCodeSecurityConfig) .and() .apply(smsAuthenticationSecurityConfig) .and() .rememberMe() .tokenRepository(persistentTokenRepository()) .tokenValiditySeconds(securityProperties.getBrowser().getRememberMeSeconds()) .userDetailsService(userDetailsService) .and() .authorizeRequests() // 匹配的是登录页的话放行 .antMatchers( SecurityConstants.DEFAULT_UNAUTHENTICATION_URL, SecurityConstants.DEFAULT_LOGIN_PROCESSING_URL_MOBILE, SecurityConstants.DEFAULT_VALIDATE_CODE_URL_PREFIX + &quot;/*&quot;, securityProperties.getBrowser().getLoginPage()) .permitAll() // 授权请求. anyRequest 就表示所有的请求都需要权限认证 .anyRequest().authenticated() .and() .csrf().disable() ; &#125;&#125; 浏览器的配置就直接写在了这里,其他可以通用的配置就抽出去 总结首先是一个验证码的重构, 通过验证码的类型去判断是那种处理,这样只需要一个filter就可以做这两种判断,之后如果再有别的类型也很好扩展 配置的重构主要就是把一些公用的配置全部抽出来,通过 apply() 的方式去引入 本文代码传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-11-实现短信登录流程]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-11-%E5%AE%9E%E7%8E%B0%E7%9F%AD%E4%BF%A1%E7%99%BB%E5%BD%95%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[述上文中实现了验证码发送接口的开发,以及重构, 本文再来看一下如何实现短信验证码的登录 先来回顾一下之前的用户名密码的登录认证流程 然后我们要做一个短信认证的流程,就仿照这个流程来就ok了 基本思路 跟之前的用户名密码的登录流程是一样的 提供一个校验验证码的filter 跟他的 UsernamePasswordAuthenticationFilter 一样我们写一个 SmsAuthenticationFilter 这里会创建一个 SmsAuthenticationToken 交给 AuthenticationManager 去找认证器,所以我们这里需要提供一个 SmsAuthenticationProvider 实现自己的认证 最后调用 UserDetailsService 返回用户的认证对象 基本流程就是这样,然后图中橘色部分的是需要我们新建自己去写的 代码实现首先第一个校验验证码的Filter,这个先跳过,最后再写 SmsAuthenticationFilter这个和 UsernamePasswordAuthenticationFilter 的作用其实是一样的,就是组装一个未认证的token对象, 这个token对象也需要我们自己去创建 新建两个类, SmsAuthenticationFilter 和 SmsAuthenticationToken 首先把这个token先建起来, 名称 SmsAuthenticationToken , 仿照 UsernamePasswordAuthenticationToken 直接把他的代码都复制过来然后改改1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class SmsAuthenticationToken extends AbstractAuthenticationToken &#123; private static final long serialVersionUID = SpringSecurityCoreVersion.SERIAL_VERSION_UID; private final Object principal; /** * This constructor can be safely used by any code that wishes to create a * &lt;code&gt;UsernamePasswordAuthenticationToken&lt;/code&gt;, as the &#123;@link #isAuthenticated()&#125; * will return &lt;code&gt;false&lt;/code&gt;. * */ public SmsAuthenticationToken(Object principal) &#123; super(null); this.principal = principal; setAuthenticated(false); &#125; /** * This constructor should only be used by &lt;code&gt;AuthenticationManager&lt;/code&gt; or * &lt;code&gt;AuthenticationProvider&lt;/code&gt; implementations that are satisfied with * producing a trusted (i.e. &#123;@link #isAuthenticated()&#125; = &lt;code&gt;true&lt;/code&gt;) * authentication token. * * @param principal * @param authorities */ public SmsAuthenticationToken(Object principal, Collection&lt;? extends GrantedAuthority&gt; authorities) &#123; super(authorities); this.principal = principal; // must use super, as we override super.setAuthenticated(true); &#125; @Override public Object getCredentials() &#123; return null; &#125; @Override public Object getPrincipal() &#123; return this.principal; &#125; @Override public void setAuthenticated(boolean isAuthenticated) throws IllegalArgumentException &#123; if (isAuthenticated) &#123; throw new IllegalArgumentException( &quot;Cannot set this token to trusted - use constructor which takes a GrantedAuthority list instead&quot;); &#125; super.setAuthenticated(false); &#125; @Override public void eraseCredentials() &#123; super.eraseCredentials(); &#125;&#125; 和 UsernamePasswordAuthenticationToken 的区别就是去掉了 credentials 这个属性,这个是存密码的 我们这里没用就去掉了 同理 SmsAuthenticationFilter 中仿照 UsernamePasswordAuthenticationFilter 的代码, 直接全复制过来改改就可以了,如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899public class SmsAuthenticationFilter extends AbstractAuthenticationProcessingFilter &#123; /** * 表单中传过来的参数名称 */ public static final String SPRING_SECURITY_FORM_PHONE_NO_KEY = &quot;phoneNo&quot;; private String phoneNoParameter = SPRING_SECURITY_FORM_PHONE_NO_KEY; /** * 是否只支持POST请求 */ private boolean postOnly = true; public SmsAuthenticationFilter() &#123; // 设置表单提交的路径 super(new AntPathRequestMatcher(&quot;/authentication/mobile&quot;, &quot;POST&quot;)); &#125; @Override public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException &#123; if (postOnly &amp;&amp; !request.getMethod().equals(&quot;POST&quot;)) &#123; throw new AuthenticationServiceException( &quot;Authentication method not supported: &quot; + request.getMethod()); &#125; String phoneNo = obtainPhoneNo(request); if (phoneNo == null) &#123; phoneNo = &quot;&quot;; &#125; phoneNo = phoneNo.trim(); SmsAuthenticationToken authRequest = new SmsAuthenticationToken(phoneNo); // 设置一些请求的详细信息 setDetails(request, authRequest); return this.getAuthenticationManager().authenticate(authRequest); &#125; /** * Enables subclasses to override the composition of the username, such as by * including additional values and a separator. * * @param request so that request attributes can be retrieved * * @return the username that will be presented in the &lt;code&gt;Authentication&lt;/code&gt; * request token to the &lt;code&gt;AuthenticationManager&lt;/code&gt; */ protected String obtainPhoneNo(HttpServletRequest request) &#123; return request.getParameter(phoneNoParameter); &#125; /** * Provided so that subclasses may configure what is put into the authentication * request&apos;s details property. * * @param request that an authentication request is being created for * @param authRequest the authentication request object that should have its details * set */ protected void setDetails(HttpServletRequest request, SmsAuthenticationToken authRequest) &#123; authRequest.setDetails(authenticationDetailsSource.buildDetails(request)); &#125; /** * Sets the parameter name which will be used to obtain the username from the login * request. * * @param phoneNoParameter the parameter name. Defaults to &quot;phoneNo&quot;. */ public void setPhoneNoParameter(String phoneNoParameter) &#123; Assert.hasText(phoneNoParameter, &quot;phoneNo parameter must not be empty or null&quot;); this.phoneNoParameter = phoneNoParameter; &#125; /** * Defines whether only HTTP POST requests will be allowed by this filter. If set to * true, and an authentication request is received which is not a POST request, an * exception will be raised immediately and authentication will not be attempted. The * &lt;tt&gt;unsuccessfulAuthentication()&lt;/tt&gt; method will be called as if handling a failed * authentication. * &lt;p&gt; * Defaults to &lt;tt&gt;true&lt;/tt&gt; but may be overridden by subclasses. */ public void setPostOnly(boolean postOnly) &#123; this.postOnly = postOnly; &#125; public final String getPhoneNoParameter() &#123; return phoneNoParameter; &#125;&#125; 就是去掉了 UsernamePasswordAuthenticationFilter 中的用户名和密码,改成手机号了 SmsAuthenticationProvider这里是具体的认证,新建类 SmsAuthenticationProvider ,代码如下:123456789101112131415161718192021222324252627282930313233@Datapublic class SmsAuthenticationProvider implements AuthenticationProvider &#123; private UserDetailsService userDetailsService; @Override public Authentication authenticate(Authentication authentication) throws AuthenticationException &#123; SmsAuthenticationToken authenticationToken = (SmsAuthenticationToken) authentication; // 根据手机号去取密码 UserDetails userDetails = userDetailsService.loadUserByUsername((String) authenticationToken.getPrincipal()); if (userDetails == null) &#123; throw new InternalAuthenticationServiceException(&quot;无法获取用户信息&quot;); &#125; // 如果找到用户信息了,就给一个新的认证过的token SmsAuthenticationToken SmsAuthenticationSuccessToken = new SmsAuthenticationToken(userDetails, userDetails.getAuthorities()); // 请求的详细信息从旧的哪里拿出来放进去 SmsAuthenticationSuccessToken.setDetails(authenticationToken.getDetails()); return SmsAuthenticationSuccessToken; &#125; /** * 判断传进来的token (authentication对象) 是否支持处理 * @param authentication * @return */ @Override public boolean supports(Class&lt;?&gt; authentication) &#123; return SmsAuthenticationToken.class.isAssignableFrom(authentication); &#125;&#125; 这里就是根据未认证的token里面的信息(这里是手机号), 通过 UserDetailsService 去查询用户的信息,如果没有查到就抛出一个异常,如果找到了,就新建一个已经认证的过的token,然后返回回去 supports() 方法,就是判断传进来的这个token,我们这个类能不能处理 总结认证流程: ValidateCodeFilter 去验证验证码 SmsAuthenticationFilter 这个filter去创建一个未经过认证的token 上面未认证的token对象会通过 AuthenticationManager 找到支持处理这个token的类 SmsAuthenticationProvider 去调用 UserDetailsService 去查询用户的信息 最后返回一个已认证的token 这里只要搞清楚认证的流程, 就仿照去实现一套就好了 本文代码传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-10-验证码发送代码重构]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-10-%E9%AA%8C%E8%AF%81%E7%A0%81%E5%8F%91%E9%80%81%E4%BB%A3%E7%A0%81%E9%87%8D%E6%9E%84%2F</url>
    <content type="text"><![CDATA[述上文中实现了验证码的发送,先后的发送的代码是这样的 这两个发送的方法其实都是一样的. 只是最后一个是返回图片给页面,一个是给用户发送验证码, 那这两个方法其实可以抽象出来,封装成一个, 下面看一下这些逻辑该如何去重构 验证码发送器图片验证码或者短信验证码的逻辑如下: 生成一个验证码 放到缓存里面去 返回给用户 他们其实可以封装成以下三个方法 生成验证码对象的方法,这里返回的可能是短信验证码对象,也可能是图片验证码对象 放到缓存里面, 不同的验证码的key是不一样的 发送给用户, 图片时直接返回给页面, 然后短信是调用第三方的接口 这里就可以抽象出来,首先,写一个接口, ValidateCodeProcessor :123456789101112public interface ValidateCodeProcessor &#123; /** * 生成验证码,放到缓存,以及发送 * @param request * @param response * @throws ServletRequestBindingException * @throws IOException */ void createCode(HttpServletRequest request, HttpServletResponse response) throws ServletRequestBindingException, IOException;&#125; 通过这个 createCode 方法里面把上面的三步都实现了, 但是第三步 返回给用户,这个步骤是不一样的,所以这个第三步我们可以写一个抽象的方法,交给各个子类去实现 抽象实现首先得有一个抽象类实现上面的接口, AbstractValidateCodeProcessor :123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899@Slf4jpublic abstract class AbstractValidateCodeProcessor&lt;C extend BaseCode&gt; implements ValidateCodeProcessor &#123; @Autowired private RedisTemplate redisTemplate; /** * 收集系统中的所有 &#123;@link ValidateCodeGenerator&#125; 的实现 */ @Autowired private Map&lt;String, ValidateCodeGenerator&gt; validateCodeGenerators; @Override public void createCode(HttpServletRequest request, HttpServletResponse response) throws ServletRequestBindingException, IOException &#123; // 生成 C validateCode = generate(request); // 放缓存 saveCache(request, validateCode); // 发送给用户 send(request, response, validateCode); &#125; /** * 生成验证码 * @param request * @return */ public C generate(HttpServletRequest request) &#123; // 获取验证码类型 String type = getValidateCodeType(request).name().toLowerCase(); // 获取验证码生成器的名称 String generatorName = type + &quot;CodeGenerator&quot;; // 从 validateCodeGenerators 这个Map中取出 ValidateCodeGenerator generator = validateCodeGenerators.get(generatorName); if (generator == null) &#123; throw new ValidateCodeException(&quot;验证码生成器 &quot; + generatorName + &quot; 不存在&quot;); &#125; return (C)generator.generate(request); &#125; /** * 获取存入缓存的key * @param request * @return */ private String getRedisKey(HttpServletRequest request) throws ServletRequestBindingException &#123; StringBuilder redisKey = new StringBuilder(); // 获取验证码类型 ValidateCodeType validateCodeType = getValidateCodeType(request); redisKey.append(Constants.VALIDATE_CODE_KEY_PREFIX) .append(validateCodeType.name().toLowerCase()) .append(&quot;:&quot;); // 判断是哪种类型的验证码 if (validateCodeType.equals(ValidateCodeType.IMAGE)) &#123; // 图片的 redisKey.append(request.getSession().getId()); &#125; else if(validateCodeType.equals(ValidateCodeType.SMS))&#123; // 短信的 redisKey.append(ServletRequestUtils.getRequiredStringParameter(request, &quot;phoneNo&quot;)); &#125; return redisKey.toString(); &#125; /** * 保存到缓存里面 * @param request * @param validateCode */ private void saveCache(HttpServletRequest request, C validateCode) throws ServletRequestBindingException &#123; // 将随机数存到缓存中 String redisKey = getRedisKey(request); log.info(&quot;将验证码放到缓存中,redisKey:&#123;&#125;&quot;, redisKey); redisTemplate.opsForValue().set(redisKey, validateCode); &#125; /** * 根据请求的url获取校验码的类型 * @param request * @return ValidateCodeType */ private ValidateCodeType getValidateCodeType(HttpServletRequest request) &#123; String type = StringUtils.substringAfter(request.getRequestURI(), &quot;/code/&quot;); return ValidateCodeType.valueOf(type.toUpperCase()); &#125; /** * 具体的发送验证码的方法, 交给子类去实现 * @param request * @param response * @param validateCode * @throws IOException * @throws ServletRequestBindingException */ protected abstract void send(HttpServletRequest request, HttpServletResponse response, C validateCode) throws IOException, ServletRequestBindingException;&#125; 这个代码比较多,首先来看一下,我们上面注入了一个 private Map&lt;String, ValidateCodeGenerator&gt; validateCodeGenerators; 这样的一个Map, 这个的作用就是, Spring 会查询容器里面的,所有的 ValidateCodeGenerator 的实现, 然后注入到这个Map里面, key 的值首先会取 @Compent(&quot;&quot;) 注解里面指定的名字,如果没有的话,就是默认的 在我们现在的项目中, ValidateCodeGenerator 的实现有两个, 一个是图片验证生成器,一个是短信验证码生成器 所以容器启动后,这个Map里面会有这两个实现 然后再看一下方法, 首先是 createCode() 方法,这里面就是我们上面的三个步骤 然后再具体看下里面的这三个方法 验证码生成方法generate() 方法 这里首先用到了一个 getValidateCodeType() 方法, 我们后面controller中验证码的接口会合并成一个,通过前端传类型来判断是什么请求, 所以这里就是通过请求来判断是哪种类型的验证码,返回是个枚举类型 ValidateCodeType 代码如下:1234567891011121314public enum ValidateCodeType implements Serializable &#123; /** * 短信验证码 */ SMS, /** * 图片验证码 */ IMAGE, ;&#125; 前端请求是/code/{type} ,然后传 sms ,或者 image 两种 然后我们上面的Map会把所有的验证码生成器加载到, 图片的生成器key是 imageCodeGenerator 短信的是 smsCodeGenerator 所以这里就可以通过前端传过来的 type 去决定用哪种类型的生成器了 取到生成器后调用生成方法,返回 放入缓存方法saveCache() ,这个没什么好多说的, 用到了一个 getRedisKey() 方法, 这里根据不同的验证码类型写不同的key最后放到缓存里面就可以了 发送方法send() 这个方法只写了一个抽象的方法,具体去由子类的发送器去实现, 所以我们就还需要两个发送的实现类 发送功能的实现首先是图片的发送类, 新建 ImageCodeProcessor 然后继承 AbstractValidateCodeProcessor 重写发送的方法,代码如下:123456789@Component(&quot;imageCodeProcessor&quot;)public class ImageCodeProcessor extends AbstractValidateCodeProcessor&lt;ImageCode&gt; &#123; @Override protected void send(HttpServletRequest request, HttpServletResponse response, ImageCode imageCode) throws IOException &#123; ImageIO.write(imageCode.getImage(), &quot;JPEG&quot;, response.getOutputStream()); &#125;&#125; 再建一个短信的发送器,代码如下:123456789101112@Component(&quot;smsCodeProcessor&quot;)public class SmsCodeProcessor extends AbstractValidateCodeProcessor&lt;SmsCode&gt; &#123; @Autowired private SmsCodeSender smsCodeSender; @Override protected void send(HttpServletRequest request, HttpServletResponse response, SmsCode validateCode) throws IOException, ServletRequestBindingException &#123; String phoneNo = ServletRequestUtils.getRequiredStringParameter(request, &quot;phoneNo&quot;); smsCodeSender.send(phoneNo, validateCode.getCode()); &#125;&#125; 接口修改现在发送验证码的逻辑封装完了,controller中的接口就可以合并成一个了,代码如下:1234567891011121314151617@RestController@Slf4jpublic class ValidateCodeController &#123; /** * 收集系统中的所有 &#123;@link ValidateCodeProcessor&#125; 的实现 */ @Autowired private Map&lt;String, ValidateCodeProcessor&gt; validateCodeProcessors; @GetMapping(&quot;code/&#123;type&#125;&quot;) public void getCode(HttpServletRequest request, HttpServletResponse response, @PathVariable String type) throws IOException, ServletRequestBindingException &#123; ValidateCodeProcessor processor = validateCodeProcessors.get(type + &quot;CodeProcessor&quot;); processor.createCode(request, response); &#125;&#125; 首先是注入了一个Map,收集所有的 ValidateCodeProcessor 就是验证码发送器,然后调用发送的方法 配置类修改最后 BrowserSecurityConfig 中之要把接口 code/{type} 加入到不需要权限里面去, 之前配置了 /code/image 改成 /code/* 就好了 验证逻辑上面的修改完成之后, 之前的 ValidateCodeFilter 可能会报错, 所以先注释掉 validate() 方法,先保证验证码可以成功发送 测试启动项目,访问登录页, 然后发一个验证码,控制台看一下输出,如下: 发送逻辑就都ok了, 关于验证的逻辑之后再做 总结重点就是用 @Autowired 注入一个Map的用法, 最后看一下发送验证码功能重构后的类图: 本文代码传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-9-实现短信验证码接口]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-9-%E5%AE%9E%E7%8E%B0%E7%9F%AD%E4%BF%A1%E9%AA%8C%E8%AF%81%E7%A0%81%E6%8E%A5%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[述很多情况下,一个网站登录需要支持短信登录,就是输入手机号,发送验证码,然后用验证码去登录,这也是一种很常见的登录方式, 但是 Spring Security 并没有给我们提供一个短信验证码的登录的方式,这个就需要我们自己去实现了. 本文先来配置一个发送短信的接口,以及做一些验证码这块的代码的重构 验证码实体类重构我们现在有一个图片验证码的类,现在还需要一个短信验证码的类,他们的差别就是一个有图片,一个没有图片所以这个可以写一个验证码的父类,然后以后再有什么验证码都可以继承这个类,反正共有的属性就是验证码和过期时间 这里新建了一个 BaseCode 代码如下:12345678910111213141516171819202122@Data@AllArgsConstructorpublic class BaseCode implements Serializable &#123; /** * 验证码 */ private String code; /** * 过期时间 */ private LocalDateTime expireTime; /** * 验证码是否过期 * @return */ public boolean expired()&#123; return LocalDateTime.now().isAfter(expireTime); &#125;&#125; 然后之前的 ImageCode 继承这个类, 修改后代码如下:12345678910111213141516171819@Datapublic class ImageCode extends BaseCode &#123; /** * 图片对象 */ private transient BufferedImage image; /** * 构造 * @param image 图片对象 * @param code 验证码 * @param expireIn 过期秒数 */ public ImageCode (BufferedImage image, String code, int expireIn)&#123; super(code, LocalDateTime.now().plusSeconds(expireIn)); this.image = image; &#125;&#125; 还需要一个验证码的类,然后这里新建一个 SmsCode ,这个只需要用到父类的属性就ok了,代码如下:12345678910111213@Datapublic class SmsCode extends BaseCode &#123; /** * 构造 * @param code 验证码 * @param expireIn 过期秒数 */ public SmsCode(String code, int expireIn) &#123; super(code, LocalDateTime.now().plusSeconds(expireIn)); &#125;&#125; 然后之前的验证码生成器的接口也需要改一下:12345678910public interface ValidateCodeGenerator &#123; /** * 生成图形验证码 * @param request * @return */ BaseCode generate(HttpServletRequest request);&#125; 返回类型改成父类的类型 短信发送的实现之前的图形验证码,我们是直接返回到了页面上给用户,这里的手机验证码的话,就需要去调用第三方的验证码去实现了, 但是每个应用用的第三方都不同, 所以这里我们需要一个可覆盖的发送方法, 就跟图形验证码的生成一样, 我们提供一个默认的方式,然后应用层可以去替换 新建一个接口 SmsCodeSender ,代码如下:123456789public interface SmsCodeSender &#123; /** * 发送短信的接口 * @param phoneNo * @param code */ void send(String phoneNo, String code);&#125; 然后我们需要一个默认的实现. 新建 DefaultSmsCodeSender 实现上面的接口,代码如下:123456789@Slf4jpublic class DefaultSmsCodeSender implements SmsCodeSender &#123; @Override public void send(String phoneNo, String code) &#123; log.info(&quot;调用第三方短信接口,目标手机号:[&#123;&#125;], 验证码:[&#123;&#125;]&quot;, phoneNo, code); &#125;&#125; 这里就不去做真正的实现了,只在控制台打印一下就ok 然后因为这个可以在应用层替换,所以还需要做一下配置,在配置类 ValidateCodeBeanConfig 中加入以下配置12345@Bean@ConditionalOnMissingBean(SmsCodeSender.class)public SmsCodeSender smsCodeSender()&#123; return new DefaultSmsCodeSender();&#125; 这个和之前的图形验证码类似,都加了一个 @ConditionalOnMissingBean 注解, 不同的是这里是指定的类, 图形验证码指定的是名字 这里 @ConditionalOnMissingBean(SmsCodeSender.class) 作用就是,先去容器里面找,有没有 SmsCodeSender 的实现, 没有的话, 再用我们下面配置的 DefaultSmsCodeSender 短信验证码配置类短信验证码的可配置项,应该是过期时间和验证码长度,还有拦截请求路径这三个, 所以这里也需要一个配置类, 因为图形验证码也有这三个属性, 所以这两个的配置类又可以继承同一个 首先,搞一个父类,里面放通用的,就是过期时间和长度和要拦截的路径, 名称是 BaseCodeProperties :1234567891011121314151617@Datapublic class BaseCodeProperties &#123; /** * 验证码字符个数 */ private int codeCount = 4; /** * 过期秒数 */ private int expireIn = 60; /** * 要拦截的请求 逗号隔开 */ private String urls;&#125; 然后图形验证码要继承这个类, ImageCodeProperties 修改如下:12345678910111213@Datapublic class ImageCodeProperties extends BaseCodeProperties&#123; /** * 图片的宽度 */ private int width = 160; /** * 图片的高度 */ private int height = 40;&#125; 然后是短信验证码的配置,新建 SmsCodeProperties 代码如下:123456@Datapublic class SmsCodeProperties extends BaseCodeProperties&#123; public SmsCodeProperties()&#123; setCodeCount(6); &#125;&#125; 构造设置了短信验证码长度默认是6 最后 ValidateCodeProperties 中, 加入一个短信验证码的配置12345678910111213@Datapublic class ValidateCodeProperties &#123; /** * 图片验证码配置 */ private ImageCodeProperties imageCode = new ImageCodeProperties(); /** * 短信验证码配置 */ private SmsCodeProperties smsCode = new SmsCodeProperties();&#125; 短信验证码生成器之前有了图片验证码生成器, 这里还需要一个短信验证码的生成器, 新建类 SmsCodeGenerator ,然后实现 ValidateCodeGenerator 接口12345678910111213@Component(&quot;smsCodeGenerator&quot;)public class SmsCodeGenerator implements ValidateCodeGenerator &#123; @Autowired private SecurityProperties securityProperties; @Override public BaseCode generate(HttpServletRequest request) &#123; String code = RandomStringUtils.randomNumeric(securityProperties.getCode().getSmsCode().getCodeCount()); return new SmsCode(code, securityProperties.getCode().getSmsCode().getExpireIn()); &#125;&#125; 这里就是生成了一个随机数 新增短信验证码的接口这些配置都完成之后,我们还需要一个短信验证码的接口,供前端使用, 由于验证码生成器的接口有修改(返回改成了验证码父类),所以这个controller修改后的代码如下:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@RestController@Slf4jpublic class ValidateCodeController &#123; @Autowired private RedisTemplate redisTemplate; @Autowired private ValidateCodeGenerator imageCodeGenerator; @Autowired private ValidateCodeGenerator smsCodeGenerator; @Autowired private SmsCodeSender smsCodeSender; @GetMapping(&quot;code/image&quot;) public void getImageCode(HttpServletRequest request, HttpServletResponse response) throws IOException &#123; // 根据随机数生成数字 ImageCode imageCode = (ImageCode)imageCodeGenerator.generate(request); // 将随机数存到缓存中 String redisKey = Constants.IMAGE_CODE_KEY_PREFIX + request.getSession().getId(); log.info(&quot;将验证码放到缓存中,redisKey:&#123;&#125;&quot;, request.getSession().getId()); redisTemplate.opsForValue().set(redisKey, imageCode); // 将生成的图片写到接口的响应中 ImageIO.write(imageCode.getImage(), &quot;JPEG&quot;, response.getOutputStream()); &#125; @GetMapping(&quot;code/sms&quot;) public void getSmsCode(HttpServletRequest request, HttpServletResponse response) throws IOException, ServletRequestBindingException &#123; // 从请求中获取手机号 String phoneNo = ServletRequestUtils.getRequiredStringParameter(request, &quot;phoneNo&quot;); // 根据随机数生成数字 SmsCode smsCode = (SmsCode) smsCodeGenerator.generate(request); // 将随机数存到缓存中 String redisKey = Constants.LOGIN_SMS_CODE_KEY_PREFIX + phoneNo; log.info(&quot;将验证码放到缓存中,redisKey:&#123;&#125;&quot;, request.getSession().getId()); redisTemplate.opsForValue().set(redisKey, smsCode); // 发送短信验证码 smsCodeSender.send(phoneNo, smsCode.getCode()); &#125;&#125; 最后一步,修改登录页面需要再加一个表单1234567&lt;form action=&quot;/authentication/mobile&quot; method=&quot;post&quot;&gt; &lt;input class=&quot;text&quot; type=&quot;text&quot; value=&quot;13200000000&quot; name=&quot;phoneNo&quot; placeholder=&quot;手机号&quot; required=&quot;&quot;&gt; &lt;input class=&quot;text&quot; type=&quot;text&quot; name=&quot;smsCode&quot; placeholder=&quot;验证码&quot; required=&quot;&quot;&gt; &lt;br&gt; &lt;a href=&quot;/code/sms?phoneNo=13200000000&quot;&gt;发送验证码&lt;/a&gt; &lt;input type=&quot;submit&quot; value=&quot;登录&quot;&gt;&lt;/form&gt; 手机号这里先给个默认值,然后请求发送到 /authentication/mobile 到这里呢,发送验证码的逻辑就完了,下一节再来看下如何去验证 总结其实本节的配置跟图形验证码的生成是一样的,这里主要是把一些通用的配置都抽出来, 重构了一下之前的配置 本文代码传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-8-记住我 功能实现及原理]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-8-%E8%AE%B0%E4%BD%8F%E6%88%91-%E5%8A%9F%E8%83%BD%E5%AE%9E%E7%8E%B0%E5%8F%8A%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[述一个网站往往是登录一次之后,一周内或者一月内都不需要重新登录, 这里就是用到了记住我的功能,不管是浏览器关闭,或者是我们服务端的服务重启,都没有关系,用户都不需要重新去登录, 下面看一下如何去实现这个功能 原理简介如下图: 先来简单的看一下这个流程 用户的认证请求过来之后,首先会进入到 UsernamePasswordAuthenticationFilter 这个过滤器 认证成功之后, 调用 RemeberMeService 根据用户名去生成一个Token 生成Token之后,由指定的 TokenRepository 去把Token写到DB中去,同时写到浏览器的Cookie中 Cookie和DB中有了token之后,当用户再次登录系统,就会经过 RememberMeAuthenticationFilter 从Cookie中读取Token,然后 RemeberMeService 去DB里面去查询,如果有记录会调用 UserDetailsService 然后根据用户名与密码生成认证的信息 RememberMeAuthenticationFilter 这个过滤器的位置如图: 代码实现前端配置原理大致了解了之后,看一下我们的项目中应该如何实现 首先,登录页面需要一个复选框 type 是 checkbox, 然后 name 必须是 remember-me,这个是写死的 ,如下:1&lt;input type=&quot;checkbox&quot; class=&quot;checkbox&quot; name=&quot;remember-me&quot; &gt; DB配置这里的token是放到DB里面的,所以我们还需要在数据库中建一个表,sql如下:123456CREATE TABLE persistent_logins ( username VARCHAR (64) NOT NULL, series VARCHAR (64) PRIMARY KEY, token VARCHAR (64) NOT NULL, last_used TIMESTAMP NOT NULL); 代码中的配置在 BrowserSecurityConfig 中需要做出以下配置 123456789101112131415161718192021222324252627@Autowiredprivate UserDetailsService userDetailsService;@Autowiredprivate DataSource dataSource;@Beanpublic PersistentTokenRepository persistentTokenRepository() &#123; JdbcTokenRepositoryImpl tokenRepository = new JdbcTokenRepositoryImpl(); tokenRepository.setDataSource(dataSource); return tokenRepository;&#125;@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; // ... 省略部分代码 http // ... 省略部分代码 .and() .rememberMe() .tokenRepository(persistentTokenRepository()) .tokenValiditySeconds(securityProperties.getBrowser().getRememberMeSeconds()) .userDetailsService(userDetailsService) // ... 省略部分代码&#125; .tokenValiditySeconds(securityProperties.getBrowser().getRememberMeSeconds()) 这个配置是配置过期秒数, 然后这里需要在浏览器的配置类中加一个属性,如下: 1234/** * 记住我的秒数 */private int rememberMeSeconds = 60 * 60 * 24; 到这里配置就完成了, 可以运行项目试一下效果 源码分析RememberMeAuthenticationFilter 这个过滤器是在 UsernamePasswordAuthenticationFilter 后面的, 也就是说只有登录认证成功之后,才会去做记住我的功能 所以这里先看一下认证成功之后,调用的方法,就是 AbstractAuthenticationProcessingFilter 里面的 successfulAuthentication() 方法 如上图,首次登录成功之后,会调用 rememberMeServices.loginSuccess(request, response, authResult); 点进去看一下 这一步主要就是生产token,分别写到数据库和浏览器的cookie中去 然后看一下下一次登录进来的流程, 首先会进到 RememberMeAuthenticationFilter 中, 然后看一下他的方法 这里主要调用了 rememberMeServices.autoLogin(request, response); 这个自动登录的方法,然后看一下这个方法里面 这里先是从cookie中组装了一个 UserDetails 对象出来,然后check一次, 最后返回认证对象 总结用户首次登录成功之后,生成一个token,分别放到数据库和浏览器的cookie中去 第二次登录认证,先判断此次登录是否有经过认证(从session中找),没有的话,调用自动登录, 然后从cookie中拿token, 通过和数据库中的比对, 组合一个 UserDetails, 然后检查返回,最后放到session中去 本文代码传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-7-可配置图形验证码]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-7-%E5%8F%AF%E9%85%8D%E7%BD%AE%E5%9B%BE%E5%BD%A2%E9%AA%8C%E8%AF%81%E7%A0%81%2F</url>
    <content type="text"><![CDATA[述上文中,实现了一个简单的图形验证码,但是这个是写死的, 图片的长宽,验证码的位数都是写死的,然后我们希望做成一个可配置的,就是由调用方去决定这些可变的参数还有验证码拦截的接口,我们现在是只拦截登录请求,这个也可以做成一个可配置的,由调用方去决定,拦截哪些请求 最后就是把验证码的生成逻辑也做成可配置的,由我们的项目提供一个默认的生成逻辑,然后调用方可以去覆盖的这种 验证码参数可配置跟之前配置过的登录页一样,首先我们的安全模块里面先给一个默认的配置, 然后调用安全模块的项目可以覆盖掉这个配置,最后,验证码的长和宽可以通过前端发的参数覆盖掉应用级的配置(不同的页面可能长宽不一样),如下图: 跟之前的登录页配置是一样的 ,这里我们需要一个放图形验证码的配置的配置类 ImageCodeProperties ,代码如下:12345678910111213141516171819202122@Datapublic class ImageCodeProperties &#123; /** * 图片的宽度 */ private int width = 160; /** * 图片的高度 */ private int height = 40; /** * 验证码字符个数 */ private int codeCount = 4; /** * 过期秒数 */ private int expireIn = 60;&#125; 然后把这个类再包一层,新建一个 ValidateCodeProperties ,因为后面可能还有别的验证码,所以这里再包一层,代码如下:12345678910@Datapublic class ValidateCodeProperties &#123; /** * 图片验证码配置 */ private ImageCodeProperties imageCode = new ImageCodeProperties();&#125; 最后放到总的配置 SecurityProperties 中去,如下:1234567891011121314@Data@ConfigurationProperties(prefix = &quot;core.security&quot;)public class SecurityProperties &#123; /** * 浏览器配置 */ private BrowserProperties browser = new BrowserProperties(); /** * 验证码配置 */ private ValidateCodeProperties code = new ValidateCodeProperties();&#125; 这样在调用方,也就是demo项目中的 application.yml,就可以配置验证码的参数了, 如下:12345core: security: code: imageCode: codeCount: 6 这里就是设置了验证码的长度是6位,我们默认的是4位 最后就是请求级的配置了,修改 ValidateCodeController ,修改后的代码如下:1234567891011121314151617181920212223242526272829303132333435363738@RestController@Slf4jpublic class ValidateCodeController &#123; @Autowired private SecurityProperties securityProperties; @Autowired private RedisTemplate redisTemplate; @GetMapping(&quot;code/image&quot;) public void getImageCode(HttpServletRequest request, HttpServletResponse response) throws IOException &#123; // 根据随机数生成数字 ImageCode imageCode = createImageCode(request); // 将随机数存到缓存中 String redisKey = Constants.IMAGE_CODE_KEY_PREFIX + request.getSession().getId(); log.info(&quot;将验证码放到缓存中,redisKey:&#123;&#125;&quot;, request.getSession().getId()); redisTemplate.opsForValue().set(redisKey, imageCode); // 将生成的图片写到接口的响应中 ImageIO.write(imageCode.getImage(), &quot;JPEG&quot;, response.getOutputStream()); &#125; private ImageCode createImageCode(HttpServletRequest request) &#123; // 长宽先从请求中取 int width = ServletRequestUtils.getIntParameter(request, &quot;width&quot;, securityProperties.getCode().getImageCode().getWidth()); int height = ServletRequestUtils.getIntParameter(request, &quot;height&quot;, securityProperties.getCode().getImageCode().getHeight()); // 过期时间和长度不能通过请求指定 int codeCount = securityProperties.getCode().getImageCode().getCodeCount(); int expire = securityProperties.getCode().getImageCode().getExpireIn(); ValidateCode code = new ValidateCode(width, height, codeCount); return new ImageCode(code.getBuffImg(), code.getCode(), expire); &#125;&#125; 这里主要是修改了 createImageCode() 方法,长和宽先从request中去取, 取不到的话,就从配置中取,就是我们之前写死的参数,都通过配置去拿 测试到这儿,验证码的基本参数可配置就修改完成了,最后可以在页面中拿验证码的请求中加上参数,如下:1&lt;img src=&quot;/code/image?width=200&quot;&gt; 这里请求中指定了验证码的长度是200, 然后上面 application.yml 中指定了验证码的长度是6, 然后启动项目,看下效果,如图: 可以看到,不论是请求的配置还是应用级的配置都生效了 验证码拦截接口可配置在我们之前的 ValidateCodeFilter 中,是只拦截了一个 /authentication/from 这个登录的请求,我们在其他请求中,也可能需要用到图片验证码,所以这里的拦截可以做成一个可配置的拦截,下面来看一下具体的实现 首先在 ImageCodeProperties 增加一个属性1234/** * 要拦截的请求 逗号隔开 */private String urls; 这样的话,在应用中的 application.yml 中,就可以配置了, 如下:12345core: security: code: imageCode: urls: /user/**,/test/* 这里我们使用了通配符去匹配 然后修改过滤器 ValidateCodeFilter 中的判断, 部分修改后的代码如下:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465@Data@Slf4jpublic class ValidateCodeFilter extends OncePerRequestFilter implements InitializingBean &#123; private AuthenticationFailureHandler authenticationFailureHandler; private RedisTemplate redisTemplate; private SecurityProperties securityProperties; /** * 要拦截的URL */ private Set&lt;String&gt; urls = new HashSet&lt;&gt;(); /** * 用来匹配url */ private AntPathMatcher pathMatcher = new AntPathMatcher(); @Override public void afterPropertiesSet() throws ServletException &#123; super.afterPropertiesSet(); // 登录是必须拦截的,直接加进去 urls.add(&quot;/authentication/from&quot;); // 如果没有配置的话,return掉 if (StringUtils.isEmpty(securityProperties.getCode().getImageCode().getUrls()))&#123; return; &#125; String[] configUrls = StringUtils.splitByWholeSeparatorPreserveAllTokens(securityProperties.getCode().getImageCode().getUrls(), &quot;,&quot;); for (String configUrl : configUrls) &#123; urls.add(configUrl); &#125; &#125; @Override protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException &#123; // 验证是否需要拦截 boolean action = false; for (String url : urls) &#123; if (pathMatcher.match(url, request.getRequestURI())) &#123; action = true; &#125; &#125; if (action) &#123; try &#123; // 这里做验证 validate(request); &#125; catch (ValidateCodeException exception) &#123; // 验证失败调用验证失败的处理 authenticationFailureHandler.onAuthenticationFailure(request, response, exception); // 直接返回 不走下一个过滤器了 return; &#125; &#125; doFilter(request, response, filterChain); &#125; // ...省略 validate() 方法 &#125; 这里首先是实现了一个 InitializingBean 目的是为了在其他的参数都组装完毕之后,初始化urls的值然后重写了 afterPropertiesSet() 方法,里面把配置中的urls取出来,加到set集合中去 后面修改了doFilterInternal() 方法中的判断逻辑, 因为我们使用了通配符的这种,所以用到了 AntPathMatcher 去做验证 最后需要在配置类 BrowserSecurityConfig 中修改配置, 如下:123456789101112131415 @Overrideprotected void configure(HttpSecurity http) throws Exception &#123; ValidateCodeFilter validateCodeFilter = new ValidateCodeFilter(); validateCodeFilter.setAuthenticationFailureHandler(myAuthenticationFailureHandler); validateCodeFilter.setRedisTemplate(redisTemplate); // 本章加入的两个配置 validateCodeFilter.setSecurityProperties(securityProperties); validateCodeFilter.afterPropertiesSet(); // formLogin 表示表单认证 http .addFilterBefore(validateCodeFilter, UsernamePasswordAuthenticationFilter.class) // ... 省略其他配置&#125; 就是把过滤器里面需要用到的 securityProperties 传了进去,然后调用了初始化属性的方法 测试上面的配置都ok之后,就可以启动项目看下效果了, 我们在 application.yml 中配置的是 /user/**,/test/* 首先是 /user/me 接口,如下: 然后是 /test/1 接口,如下: 可覆盖的图形验证码逻辑我们上面生成的图形验证码,逻辑是写死的,只能按照我们写死的逻辑来,现在我们想实现的就是,我们这个生成图片验证码的逻辑作为一个默认的方式,然后调用方可以自己去重写一个逻辑来覆盖我们的这个逻辑,下面看一下如何实现: 首先需要声明一个接口 ValidateCodeGenerator ,代码如下:12345678910public interface ValidateCodeGenerator &#123; /** * 生成图形验证码 * @param request * @return */ ImageCode generate(HttpServletRequest request); &#125; 然后需要一个实现类,把实现图形验证码的代码都放在里面,新建 ImageCodeGenerator, 代码如下:1234567891011121314151617181920@Datapublic class ImageCodeGenerator implements ValidateCodeGenerator &#123; private SecurityProperties securityProperties; @Override public ImageCode generate(HttpServletRequest request) &#123; // 长宽先从请求中取 int width = ServletRequestUtils.getIntParameter(request, &quot;width&quot;, securityProperties.getCode().getImageCode().getWidth()); int height = ServletRequestUtils.getIntParameter(request, &quot;height&quot;, securityProperties.getCode().getImageCode().getHeight()); // 过期时间和长度不能通过请求指定 int codeCount = securityProperties.getCode().getImageCode().getCodeCount(); int expire = securityProperties.getCode().getImageCode().getExpireIn(); ValidateCode code = new ValidateCode(width, height, codeCount); return new ImageCode(code.getBuffImg(), code.getCode(), expire); &#125;&#125; 这里就是把controller中的逻辑都复制过来了,然后controller中的代码需要修改一下,如下:12345678910111213141516171819202122232425262728@RestController@Slf4jpublic class ValidateCodeController &#123; @Autowired private SecurityProperties securityProperties; @Autowired private RedisTemplate redisTemplate; @Autowired private ValidateCodeGenerator imageCodeGenerator; @GetMapping(&quot;code/image&quot;) public void getImageCode(HttpServletRequest request, HttpServletResponse response) throws IOException &#123; // 根据随机数生成数字 ImageCode imageCode = imageCodeGenerator.generate(request); // 将随机数存到缓存中 String redisKey = Constants.IMAGE_CODE_KEY_PREFIX + request.getSession().getId(); log.info(&quot;将验证码放到缓存中,redisKey:&#123;&#125;&quot;, request.getSession().getId()); redisTemplate.opsForValue().set(redisKey, imageCode); // 将生成的图片写到接口的响应中 ImageIO.write(imageCode.getImage(), &quot;JPEG&quot;, response.getOutputStream()); &#125; &#125; 就是把我们刚刚创建的生成器注入进来,这里调用就ok 到这儿,我们已经把验证码的生成逻辑放到一个接口的实现里面去了, 那如何把这个接口的实现改成可配置的,这里还需要加一个配置类,新建 ValidateCodeBeanConfig ,代码如下:1234567891011121314@Configurationpublic class ValidateCodeBeanConfig &#123; @Autowired private SecurityProperties securityProperties; @Bean @ConditionalOnMissingBean(name = &quot;imageCodeGenerator&quot;) public ValidateCodeGenerator imageCodeGenerator()&#123; ImageCodeGenerator codeGenerator = new ImageCodeGenerator(); codeGenerator.setSecurityProperties(securityProperties); return codeGenerator; &#125;&#125; 这里就是把我们的 ImageCodeGenerator 交给 Spring 去管理, 这里的重点是 @ConditionalOnMissingBean(name = &quot;imageCodeGenerator&quot;) 这个注解的意思是,首先先会在容器中找一个名字是 imageCodeGenerator 的bean,找到的话就用找到的,找不到的话,再用我们下面方法中实例化的那个bean 有了这个注解就可以实现应用层的重写覆盖了 到这里就可以先看一下效果了, 因为我们并没有在应用层重写生成的逻辑,所以还是用的默认的逻辑,如下: 应用层覆盖实现逻辑最后我们在应用层,也就是 demo 项目中,重新写一个图片验证码的生成器,新建一个类,实现我们的 ValidateCodeGenerator 接口,代码如下:12345678910@Slf4j@Component(&quot;imageCodeGenerator&quot;)public class MyImageCodeGenerator implements ValidateCodeGenerator &#123; @Override public ImageCode generate(HttpServletRequest request) &#123; log.info(&quot;这里实现应用层的验证码生成....&quot;); // 此处省略 return null; &#125;&#125; 具体的生成逻辑这里就不写了,主要是@Component(&quot;imageCodeGenerator&quot;)这个注解,这里的名字要和上面配置的 @ConditionalOnMissingBean(name = &quot;imageCodeGenerator&quot;) 名字对应, 然后启动项目看下效果 控制台输出如下: 这里已经进入到我们的应用层中的逻辑了,因为这儿返回的是空所以会报空指针 总结这里,我们的一个可配置的图形验证码的逻辑就实现了, 主要是以下几点 实现 InitializingBean 接口,重写 afterPropertiesSet() 方法可以在其他的参数都组装完毕之后,做一些初始化的逻辑 使用 @Bean 注入一个bean的时候可以 @ConditionalOnMissingBean 来做一个条件,如果条件不满足的情况下,才使用 @Bean 注入 本文代码传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-6-添加图形验证码]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-6-%E6%B7%BB%E5%8A%A0%E5%9B%BE%E5%BD%A2%E9%AA%8C%E8%AF%81%E7%A0%81%2F</url>
    <content type="text"><![CDATA[述前面的文章中主要实现了登录认证的功能,本文来在登录的时候加个图形验证码,然后做成一个可配置的图形验证码,先来看一下图形验证码该如何实现 图形验证码 首先我们要生成一个图片验证码 这个验证码需要放到session中或者缓存中,用来跟用户输入的做验证 然后还要把这个验证加到我们的认证流程中去 然后将生成图片的接口接到前端的页面上去 这就是一个简单的图形验证码的步骤,下面先来看一下如何实现 图形验证码Model首先我们需要一个实体类来放这个图形验证码的信息, 这些就都放在 core 项目中,因为web或者app都可能用的到, 新建类 ImageCode ,代码如下:123456789101112131415161718192021222324252627282930313233343536373839@Datapublic class ImageCode implements Serializable &#123; /** * 图片对象 * transient 序列化的时候这个字段不会被序列化 */ private transient BufferedImage image; /** * 验证码 */ private String code; /** * 过期时间 */ private LocalDateTime expireTime; /** * 验证码是否过期 * @return */ public boolean expired()&#123; return LocalDateTime.now().isAfter(expireTime); &#125; /** * 构造 * @param imageCode 图片对象 * @param code 验证码 * @param expireIn 过期秒数 */ public ImageCode (ImageCode imageCode, String code, int expireIn)&#123; this.image = image; this.code = code; this.expireTime = LocalDateTime.now().plusSeconds(expireIn); &#125;&#125; 生成验证码前端需要调用我们的接口,拿到验证码,所以这里需要写一个接口来获取验证码,如下:1234567891011121314151617181920212223242526@RestControllerpublic class ValidateCodeController &#123; private static final int EXPIRE = 60; @Autowired private RedisTemplate redisTemplate; @GetMapping(&quot;code/image&quot;) public void getImageCode(HttpServletRequest request, HttpServletResponse response) throws IOException &#123; // 根据随机数生成数字 ImageCode imageCode = createImageCode(); // 将随机数存到缓存中 String redisKey = Constants.IMAGE_CODE_KEY_PREFIX + request.getSession().getId(); redisTemplate.opsForValue().set(redisKey, imageCode); // 将生成的图片写到接口的响应中 ImageIO.write(imageCode.getImage(), &quot;JPEG&quot;, response.getOutputStream()); &#125; private ImageCode createImageCode() &#123; ValidateCode code = new ValidateCode(); return new ImageCode(code.getBuffImg(), code.getCode(), EXPIRE); &#125;&#125; 这里是把验证码放到了redis中, 关于redis的用法这里不多做介绍,当然也可以放到session里面去 , 生成图片验证码的话,网上有很多种方式 ,随便复制一直过来就好,最后拼成 ImageCode 对象就ok了 然后前端的页面也需要改一下,加入如下代码 如下:123&lt;input class=&quot;text&quot; type=&quot;text&quot; name=&quot;imageCode&quot; placeholder=&quot;验证码&quot; required=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;/code/image&quot;&gt; 一个验证码的输入框和图片标签, 图片的src就是我们上面接口的路径 最后在配置里面把这个接口加到不需要验证权限里面去 12345.antMatchers( &quot;/authentication/require&quot;, &quot;/code/image&quot;, securityProperties.getBrowser().getLoginPage()).permitAll() 现在就可以启动项目看下效果了,如下: 验证验证码生成完了之后,就还差验证了,我们需要一个过滤器,来放在 UsernamePasswordAuthenticationFilter 这个过滤器前面, 就是先验证验证码,然后再验证 用户名密码 这里我们要写一个过滤器, 新建一个类 ValidateCodeFilter,代码如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657@Data@Slf4jpublic class ValidateCodeFilter extends OncePerRequestFilter &#123; private AuthenticationFailureHandler authenticationFailureHandler; private RedisTemplate redisTemplate; @Override protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException &#123; if (StringUtils.equals(&quot;/authentication/from&quot;, request.getRequestURI()) &amp;&amp; StringUtils.equalsIgnoreCase(request.getMethod(), &quot;post&quot;)) &#123; try &#123; // 这里做验证 validate(request); &#125; catch (ValidateCodeException exception) &#123; // 验证失败调用验证失败的处理 authenticationFailureHandler.onAuthenticationFailure(request, response, exception); // 直接返回 不走下一个过滤器了 return; &#125; &#125; doFilter(request, response, filterChain); &#125; private void validate(HttpServletRequest request) throws ServletRequestBindingException &#123; // 缓存中拿出验证码来 String redisKey = Constants.IMAGE_CODE_KEY_PREFIX + request.getSession().getId(); log.info(&quot;从缓存里取验证码,redisKey:&#123;&#125;&quot;, redisKey); ImageCode code = (ImageCode) redisTemplate.opsForValue().get(redisKey); // 参数中拿出验证码来,作比较 String codeInRequest = ServletRequestUtils.getStringParameter(request, &quot;imageCode&quot;); if (StringUtils.isBlank(codeInRequest)) &#123; throw new ValidateCodeException(&quot;验证码的值不能为空&quot;); &#125; if (code == null) &#123; throw new ValidateCodeException(&quot;验证码不存在&quot;); &#125; if (code.expired()) &#123; redisTemplate.delete(Constants.IMAGE_CODE_KEY_PREFIX + request.getSession().getId()); throw new ValidateCodeException(&quot;验证码已过期&quot;); &#125; if (!StringUtils.equalsIgnoreCase(code.getCode(), codeInRequest)) &#123; throw new ValidateCodeException(&quot;验证码错误&quot;); &#125; // 最后通过的话也从缓存清除掉 redisTemplate.delete(Constants.IMAGE_CODE_KEY_PREFIX + request.getSession().getId()); &#125;&#125; 看一下代码,继承 OncePerRequestFilter 表示这个过滤器只会被执行一次, 首先是判断,只处理 /authentication/from 这个请求, 然后必须是post请求, 然后是 下面是做验证, catch里面要捕获 ValidateCodeException ,这个异常是自定义的异常,然后如果捕获到异常了,那就掉失败处理器把这个异常传给失败处理器,然后直接return掉,就不走下面的逻辑了 然后是 ValidateCodeException 的代码,如下:1234567public class ValidateCodeException extends AuthenticationException &#123; public ValidateCodeException(String msg) &#123; super(msg); &#125;&#125; 这里就继承 ValidateCodeException 然后写个构造就ok了 错误信息处理在我们自定义的失败处理器上,修改一下, 只返回错误信息,那些堆栈信息什么的,就不返回了, MyAuthenticationFailureHandler 中修改,代码如下:12345678910111213141516@Overridepublic void onAuthenticationFailure(HttpServletRequest request, HttpServletResponse response, AuthenticationException exception) throws IOException, ServletException &#123; log.info(&quot;认证失败...&quot;); if (LoginResponseType.JSON.equals(securityProperties.getBrowser().getLoginResponseType())) &#123; // 只返回错误消息 Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(1); map.put(&quot;message&quot;, exception.getMessage()); response.setContentType(&quot;application/json;charset=UTF-8&quot;); response.getWriter().write(objectMapper.writeValueAsString(map)); &#125; else &#123; super.onAuthenticationFailure(request, response, exception); &#125;&#125; 配置过滤器使其生效最后一步,就是把这个过滤器加到 UsernamePasswordAuthenticationFilter 的前面 修改 BrowserSecurityConfig ,部分代码如下:1234567891011121314@Autowiredprivate RedisTemplate redisTemplate;@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; ValidateCodeFilter validateCodeFilter = new ValidateCodeFilter(); validateCodeFilter.setAuthenticationFailureHandler(myAuthenticationFailureHandler); validateCodeFilter.setRedisTemplate(redisTemplate); // formLogin 表示表单认证 http .addFilterBefore(validateCodeFilter, UsernamePasswordAuthenticationFilter.class) // ... 省略其他配置 &#125; 先是实例化了我们刚创建的 ValidateCodeFilter, 然后设置了失败处理器和 redisTemplete,最后在下面加到 UsernamePasswordAuthenticationFilter 前面就ok了 测试上面的配置完成之后,启动项目做一下测试 首先看一下输入一个错误的验证码,效果如下: 再看一下输入一个正确的,如下: 总结本文主要就是加入了一个自定义的验证, 以及如何在 security 的过滤器链上加一个过滤器, 要继承 OncePerRequestFilter 表示这个过滤器只会被执行一次, 然后再配置类中通过 .addFilterBefore(),方法来指定过滤器的位置 本文代码已上传github,传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-5-认证流程梳理]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-5-%E8%AE%A4%E8%AF%81%E6%B5%81%E7%A8%8B%E6%A2%B3%E7%90%86%2F</url>
    <content type="text"><![CDATA[述前面的文章中首先分析了认证流程的源码,然后我们又实现了自己的登录流程以及一些自定义的处理,下面再来梳理一下这些认证的流程 认证这里我们之前就已经分析过认证流程了,这里再坐下补充以及回顾,先看下图: 首先是进到了 UsernamePasswordAuthenticationFilter, 然后这个里面生成了一个没有经过认证的 Authentication 对象, 然后是调用 AuthenticationManager 找到处理当前登录请求的 AuthenticationProvider, 然后通过 UserDetailsService 做用户的验证(密码等), 然后组装返回一个已认证的 Authentication 对象 这部分的源码之前已经说过了,这里就不再看了 认证成功/失败处理在 UsernamePasswordAuthenticationFilter 返回 Authentication 对象之后,会进入到 AbstractAuthenticationProcessingFilter 这个 filter 里面, 看一下源码 然后这里主要是判断是否登录成功, 在catch块里面都是调用的认证失败的处理, 然后最后是调用的认证成功的处理 分别看一下这两个方法的源码 这里每个方法的最后一行,就是去调用我们的自定义的成功/失败的处理 认证结果如何在多个请求之间共享认证完成之后,那认证结果是如何在多个请求中共享的,一般都是在登录成功之后放到session里面,我们详细的看一下这个认证成功后调用的方法 SecurityContext可以看到这里把认证结果放到了 SecurityContext 里面,先来看下面这个图: 认证成功之后,先把认证结果放到了一个 SecurityContext 里面,这个是个接口,实际的实现类是 SecurityContextImpl,看一下源码 里面包装了Authentication 主要就是重写了 equals 还有 hashCode, 保证Authentication的唯一性 SecurityContextHolder然后,是把这个放到了一个 SecurityContextHolder 里面,这个类实际上对 ThreadLocal 的一个封装，可以在不同方法之间进行通信,我们可以简单理解为线程级别的一个全局变量.因此可以在同一个线程中的不同方法中获取到认证信息 SecurityContextPersistenceFilter最后一步是经过 SecurityContextPersistenceFilter 这个过滤器, 这个也是 security 过滤器链中的一个,位置如下 这个过滤器的作用就是 当一个请求来的时候,它会将session中的值传入到该线程中,当请求返回的时候,它会判断该请求线程是否有 SecurityContext, 如果有它会将其放入到session中,因此保证了请求结果可以在不同的请求之间共享 获取认证信息最后来看一下如何获取用户的认证信息,这个有以下几种方式去拿 第一种新建接口,代码如下:1234@GetMapping(&quot;/me&quot;)public Object me()&#123; return SecurityContextHolder.getContext().getAuthentication();&#125; 第二种1234@GetMapping(&quot;/me&quot;)public Object getMeDetail(Authentication authentication)&#123; return authentication;&#125; 第三种1234@GetMapping(&quot;/me&quot;)public Object me(@AuthenticationPrincipal UserDetails userDetails)&#123; return userDetails;&#125; 效果第一种和第二种方式返回的结果都是一样的,都是返回的 Authentication 对象,如下: 第三种方式是只返回了 UserDetails 对象,如下: 总结本文主要是介绍了认证结果共享的原理,就是在认证成功之后放到了一个 SecurityContext 里面,最后通过最外层的过滤器 SecurityContextPersistenceFilter 放到session里面,从而实现认证结果共享 最后就是几种通过接口获取认证后的结果的方法 代码已经上传github,传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-4-自定义登录成功失败处理]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-4-%E8%87%AA%E5%AE%9A%E4%B9%89%E7%99%BB%E5%BD%95%E6%88%90%E5%8A%9F%E5%A4%B1%E8%B4%A5%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[述上文中,我们实现了一个自定义的可配置的登录页,效果就是首先一个请求过来,如果需要认证的话,就跳转到认证的界面,然后认证完成之后,再跳转到最开始访问的页面 在实际的环境中,我们登录成功之后可能并不是去跳转页面, 在前后端分离的环境中一般都是返回一个json格式的用户信息,然后登录失败也是返回具体的原因给前端 下面就来看一下如何实现自定义的登录成功/失败后返回json数据 自定义登录成功处理登录成功处理只需要去实现 MyAuthenticationSuccessHandler 接口, 这个接口很简单,里面就一个方法,看一下源码 这里的参数有一个 Authentication 对象,我们之前看认证处理源码的时候,可以看到认证完成之后就会返回这个对象,然后在我们自定义的实现里面,就把这个对象转成json返回去 新建一个类 MyAuthenticationSuccessHandler 实现这个接口,重写里面的方法,具体代码如下1234567891011121314151617@Slf4j@Component(&quot;myAuthenticationSuccessHandler&quot;)public class MyAuthenticationSuccessHandler implements AuthenticationSuccessHandler &#123; @Autowired private ObjectMapper objectMapper; @Override public void onAuthenticationSuccess(HttpServletRequest request, HttpServletResponse response, Authentication authentication) throws IOException, ServletException &#123; log.info(&quot;用户登录成功...&quot;); response.setContentType(&quot;application/json;charset=UTF-8&quot;); response.getWriter().write(objectMapper.writeValueAsString(authentication)); &#125;&#125; 这里就是直接把 Authentication 这个对象转成json字符串然后返回去了,然后这个类加了 @Component(&quot;myAuthenticationSuccessHandler&quot;) 这个注解,交给spring去管理,然后起了个名字,下面的配置要用到 事件写好之后还需要把这个handler加到 security 的配置中去,才能让他生效,修改 BrowserSecurityConfig 部分代码如下:12345678910111213@Autowiredprivate AuthenticationSuccessHandler myAuthenticationSuccessHandler;@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; http.formLogin() .loginPage(&quot;/authentication/require&quot;) .loginProcessingUrl(&quot;/authentication/from&quot;) .successHandler(myAuthenticationSuccessHandler) .and() // ... 省略其他代码&#125; 先把我们刚刚创建的 MyAuthenticationSuccessHandler 注入进去,然后通过 successHandler() 方法去设置 测试配置完成之后,启动项目,随便访问一个接口,然后登录,返回值如下: 可以看到返回的就是一串json字符串,格式化一下,如下:12345678910111213141516171819&#123; &quot;authorities&quot;: [&#123;&quot;authority&quot;: &quot;admin&quot;&#125;], &quot;details&quot;: &#123; &quot;remoteAddress&quot;: &quot;0:0:0:0:0:0:0:1&quot;, &quot;sessionId&quot;: &quot;CAD61C0C6DB0DE0C5FDF4893061C1247&quot; &#125;, &quot;authenticated&quot;: true, &quot;principal&quot;: &#123; &quot;password&quot;: null, &quot;username&quot;: &quot;zhangsan&quot;, &quot;authorities&quot;: [&#123;&quot;authority&quot;: &quot;admin&quot;&#125;], &quot;accountNonExpired&quot;: true, &quot;accountNonLocked&quot;: true, &quot;credentialsNonExpired&quot;: true, &quot;enabled&quot;: true &#125;, &quot;credentials&quot;: null, &quot;name&quot;: &quot;zhangsan&quot;&#125; 这里就包括了用户的权限,用户名,以及我们认证返回的那个 UserDetail 的信息, 这个基本都能看得懂 就不做过多解释了 自定义登录失败处理登录失败和成功是类似的,只不过实现的接口不一样, 这里要实现的是 AuthenticationFailureHandler 这个接口,看一下源码 里面也是只有一个方法,唯一的区别是这里的参数是 AuthenticationException ,这个就是认证失败的异常,然后我们在自定义的实现里面,把这个对象也转成json返回给前端 新建类 MyAuthenticationFailureHandler ,代码如下: 123456789101112131415@Slf4j@Component(&quot;myAuthenticationFailureHandler&quot;)public class MyAuthenticationFailureHandler implements AuthenticationFailureHandler &#123; @Autowired private ObjectMapper objectMapper; @Override public void onAuthenticationFailure(HttpServletRequest request, HttpServletResponse response, AuthenticationException exception) throws IOException, ServletException &#123; log.info(&quot;认证失败...&quot;); response.setContentType(&quot;application/json;charset=UTF-8&quot;); response.getWriter().write(objectMapper.writeValueAsString(exception)); &#125;&#125; 然后,还是在 BrowserSecurityConfig 中配置一下,修改代码如下: 123456789101112131415@Autowiredprivate AuthenticationFailureHandler myAuthenticationFailureHandler;@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; // formLogin 表示表单认证 http.formLogin() .loginPage(&quot;/authentication/require&quot;) .loginProcessingUrl(&quot;/authentication/from&quot;) .successHandler(myAuthenticationSuccessHandler) .failureHandler(myAuthenticationFailureHandler) .and() // ... 省略其他配置 &#125; 和上面成功的配置基本是一样的 测试完成之后,做一下测试,启动项目,随便访问一个接口,然后输入一个错误的密码,返回如下: 返回值简化一下,如下:1234567&#123; &quot;cause&quot;: null, &quot;stackTrace&quot;:[] &quot;localizedMessage&quot;: &quot;坏的凭证&quot;, &quot;message&quot;: &quot;坏的凭证&quot;, &quot;suppressed&quot;: []&#125; stackTrace 里面是一些java的错误堆栈信息,这个不看,然后其他的就是登陆失败的原因返回去了 到这儿,自定义的登录成功/失败的处理都完成了 升级改造通过上面的配置,我们在任何时候都是返回的json数据给前端, 那可能实际使用中就是想直接跳转请求,而不是返回json数据 我们这里可以做成一个可配置的,就跟之前的登录页面一样,由调用服务的一方去决定是返回json还是去跳转页面, 下面看一下如何实现 配置类改造还是和登录页一样,在上文中的浏览器配置类里面加一个配置,如下:1234/** * 登录返回类型 */private LoginResponseType loginType = LoginResponseType.JSON; 这里用到了一个我们自己定义的枚举, LoginResponseType, 代码如下:123456789101112131415public enum LoginResponseType implements Serializable &#123; /** * 跳转请求 */ REDIRECT, /** * json返回 */ JSON, ;&#125; 里面之后两个,一个跳转一个json返回,然后配置类中给的默认的是返回json的 handler改造配置类搞好之后,再来改造之前的两个handler, 先看登录成功的这个 MyAuthenticationFailureHandler , 我们之前实现的 AuthenticationFailureHandler. 现在 改成继承 SavedRequestAwareAuthenticationSuccessHandler 这个是 Spring Security 默认的成功处理 ,然后我们重写里面的方法,具体代码如下: 123456789101112131415161718192021222324@Slf4j@Component(&quot;myAuthenticationSuccessHandler&quot;)public class MyAuthenticationSuccessHandler extends SavedRequestAwareAuthenticationSuccessHandler &#123; @Autowired private ObjectMapper objectMapper; @Autowired private SecurityProperties securityProperties; @Override public void onAuthenticationSuccess(HttpServletRequest request, HttpServletResponse response, Authentication authentication) throws IOException, ServletException &#123; log.info(&quot;用户登录成功...&quot;); if (LoginResponseType.JSON.equals(securityProperties.getBrowser().getLoginResponseType())) &#123; response.setContentType(&quot;application/json;charset=UTF-8&quot;); response.getWriter().write(objectMapper.writeValueAsString(authentication)); &#125; else &#123; super.onAuthenticationSuccess(request, response, authentication); &#125; &#125;&#125; 这里就是加了个判断,如果配置是json的话,就返回json,如果不是,那就调用父类的处理方式,就是 Spring Security 默认的成功处理 再看下失败的处理, 在 MyAuthenticationFailureHandler 中我们是实现的 AuthenticationFailureHandler 这个接口,然后现在改成继承 SimpleUrlAuthenticationFailureHandler , 然后里面方法和上面一样,也是加个判断就ok了, 代码如下: 12345678910111213141516171819202122232425@Slf4j@Component(&quot;myAuthenticationFailureHandler&quot;)public class MyAuthenticationFailureHandler extends SimpleUrlAuthenticationFailureHandler &#123; @Autowired private ObjectMapper objectMapper; @Autowired private SecurityProperties securityProperties; @Override public void onAuthenticationFailure(HttpServletRequest request, HttpServletResponse response, AuthenticationException exception) throws IOException, ServletException &#123; log.info(&quot;认证失败...&quot;); if (LoginResponseType.JSON.equals(securityProperties.getBrowser().getLoginResponseType())) &#123; response.setContentType(&quot;application/json;charset=UTF-8&quot;); response.getWriter().write(objectMapper.writeValueAsString(exception)); &#125; else &#123; super.onAuthenticationFailure(request, response, exception); &#125; &#125;&#125; 同样的,如果配置的不是返回json就调用父类的处理方式 上面的内容处理完成之后,就可以在demo项目中的 application.yml 中去配置了,如下:1234core: security: browser: loginResponseType: REDIRECT 测试配置都完成之后,来测试看下效果,application.yml 中,先不做任何配置, 直接用默认的就是json的,然后启动项目,效果如下: 然后是认证失败的,如下: 最后把配置加上,重启,改成Security 默认的处理方式,来看下效果,如下: 总结本文主要实现了自定义的登录成功/失败处理 分别通过实现 AuthenticationSuccessHandler 和 AuthenticationFailureHandler 这两个接口实现 Spring Security 默认的实现是 SavedRequestAwareAuthenticationSuccessHandler 和 SimpleUrlAuthenticationFailureHandler 可以通过继承这两个类来重写默认的处理方式 代码已经上传github,传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-3-自定义登录页]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-3-%E8%87%AA%E5%AE%9A%E4%B9%89%E7%99%BB%E5%BD%95%E9%A1%B5%2F</url>
    <content type="text"><![CDATA[述上文中实现了用户自定义的登录认证,但是用的登录页面是默认的登录页,在实际开发中,当然不可能用这种登录页,所以还是需要我们自己去搞一个登录的页面的, 还有一个问题就是,现在大部分开发都是前后端分离的, 如果是针对App的接口的话,我们直接返回一个html页面是不可以的,应该是返回一个json串给前端,然后前端去引导用户到登录页 然后还有一个问题,先来看一下现有的项目结构 ┌security-example-app├security-example-web├security-example-core└security-example-demo 这里core是放一些核心的代码,然后web是针对浏览器的配置,app是针对app的配置,demo是用来调用上面的服务测试的一个工程然后我们还要实现的一个功能是,如果用户在demo中配置了登录页,就用用户配置的登录页,如果没有,我们给一个默认的登录页 主要功能 自定义登录页面 区分是浏览器请求,还是其他请求, 如果是浏览器请求的话,就返回登录页面,如果是其他的,就返回json信息 配置可以被调用服务者覆盖的一个登录页 自定义登录页首先先来看一下如何实现自定义的登录页,首先随便写一个登录的页面,或者网上下载一个,复制到web工程里面,作为我们默认的登录页 如图,我随便找了一个登录页放到项目里 resources/static,这个目录也不需要额外的做什么配置,springboot默认配置好的 然后在之前创建的 BrowserSecurityConfig 里面,来配置默认的登录页,修改后的代码如下:12345678910111213141516171819202122232425@Configurationpublic class BrowserSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Bean public PasswordEncoder passwordEncoder()&#123; return new BCryptPasswordEncoder(); &#125; @Override protected void configure(HttpSecurity http) throws Exception &#123; // formLogin 表示表单认证 http.formLogin() .loginPage(&quot;/login.html&quot;) .loginProcessingUrl(&quot;/authentication/from&quot;) .and() // 授权请求. anyRequest 就表示所有的请求都需要权限认证 .authorizeRequests() // 匹配的是登录页的话放行 .antMatchers(&quot;/login.html&quot;).permitAll() .anyRequest().authenticated() .and().csrf().disable(); &#125;&#125; 简单的看一下这里的代码 .loginPage(&quot;/login.html&quot;)指定了登录的跳转页面 .loginProcessingUrl(&quot;/authentication/from&quot;) 配置要处理的登录请求,这个要和html表单的 action 对应,默认是/login .antMatchers(&quot;/login.html&quot;).permitAll() 这里配置是配置登录页不需要权限,否则跳转登录页的时候也做权限验证,那就进不去了 .and().csrf().disable(); 这个配置是防止csrf攻击的,先关掉,否则会报错,具体原因后面再说 配置都ok之后,启动项目,访问一个接口,就能看到我们自己的登录页了 区分请求来源自定义登录页面完成之后,再看一下如何区分是浏览器发起的请求,还是其他发起的请求,然后先来看一下大致的思路,如图: 首先请求过来之后,如果需要认证,就跳转到一个我们自定义的controller,然后在这个controller中去判断请求的来源,做出不同的响应 上面我们请求到达需要认证的话,就直接跳转到登录页了,然后现在我们写一个controller去区分,他是来自网页的请求,还是app的或者其他地方的 创建一个controller, BrowserController 代码如下:1234567891011121314151617181920212223242526272829303132333435@Slf4j@RestController@RequestMapping(&quot;/authentication/require&quot;)public class BrowserController &#123; private RequestCache requestCache = new HttpSessionRequestCache(); private RedirectStrategy redirectStrategy = new DefaultRedirectStrategy(); /** * 需要身份认证的时候先跳转到这里 * @param request * @param response * @return */ @RequestMapping(produces = &quot;text/html&quot;) public void requireAuthenticationHtml(HttpServletRequest request, HttpServletResponse response) throws IOException &#123; SavedRequest savedRequest = requestCache.getRequest(request, response); if (savedRequest != null) &#123; String targetUrl = savedRequest.getRedirectUrl(); log.info(&quot;引发跳转的请求是:&quot; + targetUrl); redirectStrategy.sendRedirect(request, response, &quot;/login.html&quot;); &#125; &#125; @RequestMapping @ResponseStatus(HttpStatus.UNAUTHORIZED) public Map&lt;String, Object&gt; requireAuthenticationJson() &#123; Map&lt;String, Object&gt; resultMap = new HashMap&lt;&gt;(1); resultMap.put(&quot;message&quot;, &quot;认证失败,请登录...&quot;); return resultMap; &#125;&#125; 上面的方法 就是看请求头里面有没有 text/html, 有的话就进, 没有的话就进到了下面的方法,下面的方法中返回了一个 401 的状态码,就表示没有认证,然后是一个json的提示信息上面的方法中,有一个 RequestCache security会把当前的请求放到这个session里面,然后 RedirectStrategy 是一个重定向的工具类, 这个方法就是要重定向一个登录页, 这里是直接跳转到了 /login.html 最后再把配置类中的登录页的路径修改掉,如下:123456789101112131415@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; // formLogin 表示表单认证 http.formLogin() .loginPage(&quot;/authentication/require&quot;) .loginProcessingUrl(&quot;/authentication/from&quot;) .and() .authorizeRequests() // 匹配的是登录页的话放行 .antMatchers(&quot;/authentication/require&quot;, &quot;/login.html&quot;).permitAll() // 授权请求. anyRequest 就表示所有的请求都需要权限认证 .anyRequest().authenticated() .and().csrf().disable();&#125; 可覆盖登录页配置最后一个问题, 我们上面的登录页是写死的, 我们要提供一个可配置的能力,就是在demo项目中可以覆盖掉我们现在的登录页, 使用用户自定义的登录页 这里就可以用配置类去解决了,这里我在core工程中创建了以下三个类12345678910@Data@ConfigurationProperties(prefix = &quot;core.security&quot;)public class SecurityProperties &#123; /** * 浏览器配置 */ private BrowserProperties browser = new BrowserProperties();&#125; 123456@Datapublic class BrowserProperties &#123; private String loginPage = &quot;/login.html&quot;;&#125; 1234@Configuration@EnableConfigurationProperties(&#123;SecurityProperties.class&#125;)public class SecurityConfig &#123;&#125; 第一个是一个配置的总类,设置了前缀是 core.security, 里面又放了一个浏览器的配置类,因为可能还有其他的配置, 最后一个是用来启用这个配置类的 这些配置完成之后,我们就可以在demo项目中的 application.yml 中注入这些属性的值了 比如上面的登录页路径,在 application.yml 中加入以下配置:1234core: security: browser: loginPage: /myLogin.html 然后demo项目中也需要一个登录的html页面,我直接复制过来,修改了title和名称,区分一下. 测试到这里本文需要实现的功能就都完成了,然后做一下测试看看实际的效果 先来看一下两个登录页,首先是我们的web项目提供的,如下: 然后是用户自定义的: 这里的title和文件名是不一样的(文件名相同的话,不用配置就会直接覆盖的) 然后先用默认的试一下,也就是在 application.yml 里面不做任何配置,启动项目,然后访问一个接口,效果如下: 可以看到,不加配置的时候是跳转到了我们默认的页面的 然后再加上自定义的配置,重启项目再访问一次,这时候就会跳转到我们的自定义的登录页 最后用postMan再访问一次,看一下返回的是什么 这里不是从浏览器进去的请求,所以返回的是一个json的提醒,还有一个401的状态码. 总结主要是做了自定义的登录页跳转的配置, 然后是根据不同环境返回不同的信息,最后实现了动态配置登录页,源码已经上传github,传送门]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-2-自定义用户登录]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-2-%E8%87%AA%E5%AE%9A%E4%B9%89%E7%94%A8%E6%88%B7%E7%99%BB%E5%BD%95%2F</url>
    <content type="text"><![CDATA[述上一个案例中,我们简单的实现了用户的登录,用的是Spring Security的默认的用户名和密码, 实际情况中,我们肯定是要从数据库里面去获取用户信息,然后接下来就看一下具体的要如何实现 大致流程登录就是用户名,密码还有一些其他的校验, 上文中对认证源码有了一些大致的了解,我们自定义登录主要就是实现从数据库里面去,回顾一下这个部分的源码: 这里调用了一个 retrieveUser(...) 这个方法, 返回的是一个UserDetails, 这个需要我们自己去实现 UserDetailsService, 在这个里面去查询数据库 首先,看一下 UserDetailsService,这个类的源码, 如下: 很简单,就是一个接口,里面有一个根据用户名去获取用户信息的方法,然后我们新建一个类,实现这个接口,然后重写方法就可以,这里返回的是一个 UserDetails 对象,这里直接用了security提供的一个user对象,当然也可以自己实现,来看一下他里面的几个属性 首先是个权限的集合,然后是密码,用户名,是否非过期, 是否非锁定,凭证是否非过期,是否启用. 这下面四个boolean值,只要有一个设置为false,就不能登录 手动实现用户验证然后再看一下我们的代码,新建类,实现 UserDetailsService ,具体代码如下: 1234567891011121314151617181920212223242526@Componentpublic class MyUserDetailsService implements UserDetailsService &#123; @Autowired private UserService userService; @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; // 这里是模拟从数据库查询用户 User user = userService.findUserByName(username); if (user == null) &#123; throw new UsernameNotFoundException(&quot;用户不存在&quot;); &#125; // 查询出来,返回去 return new org.springframework.security.core.userdetails.User( username, user.getPassword(), user.getEnable(), true, true, !user.getLocked(), AuthorityUtils.commaSeparatedStringToAuthorityList(&quot;admin&quot;)); &#125;&#125; 先是根据用户名从数据库里面查询出用户信息来,然后根据自己的情况把4个Boolean值赋值,然后最后一个集合是权限,这里先随便给个值 PasswordEncoder在Spring Security中, 验证密码是否正确, 这个事情是交给Spring Security去完成的, 我们只需要按照用户名去查询这个用户就ok了,最后查询出来的信息给到 UserDetails 里面,交给 Spring Security 去验证 我们在数据库中存储的密码都是经过加密的,所以我们还需要告诉 Spring Security 我们的加密方式 ,就是 PasswordEncoder, 先看一下他的源码 这个也是一个接口,里面就两个方法, 一个是加密的方法,这个可以在用户注册的时候给密码加密, 然后是一个匹配的方法, 需要传入原始密码,和加密后的密码, 然后返回是否匹配, 这个东西我们是可以自己实现的,比如常用的MD5等, 这里他还提供了很多默认的实现,这里就先用他提供的 BCryptPasswordEncoder 在前面案例创建的 BrowserSecurityConfig 中,加入下面的配置 1234@Beanpublic PasswordEncoder passwordEncoder()&#123; return new BCryptPasswordEncoder();&#125; 这样密码验证器就生效了 测试最后看一下我在 UserServiceImpl 中的代码1234567891011121314151617181920212223242526272829@Servicepublic class UserServiceImpl implements UserService &#123; private static Map&lt;String, User&gt; userMap = new HashMap&lt;&gt;(3); /** * 初始化数据,模拟从数据库获取数据 */ static&#123; // 密码是经过加密的 都是 123456 通过 BCryptPasswordEncoder 加密 User user1 = new User(1L, &quot;zhangsan&quot;, &quot;$2a$10$W17jDdV96PUBlVx/PL6zDuaK3rv9uH.DKLwkFaD6lhOPkv3GD5sEu&quot;, false, true); User user2 = new User(2L, &quot;lisi&quot;, &quot;$2a$10$W17jDdV96PUBlVx/PL6zDuaK3rv9uH.DKLwkFaD6lhOPkv3GD5sEu&quot;, false, true); User user3 = new User(3L, &quot;lockedtest&quot;, &quot;$2a$10$W17jDdV96PUBlVx/PL6zDuaK3rv9uH.DKLwkFaD6lhOPkv3GD5sEu&quot;, true, true); User user4 = new User(4L, &quot;enabletest&quot;, &quot;$2a$10$W17jDdV96PUBlVx/PL6zDuaK3rv9uH.DKLwkFaD6lhOPkv3GD5sEu&quot;, false, false); userMap.put(user1.getName(), user1); userMap.put(user2.getName(), user2); userMap.put(user3.getName(), user3); userMap.put(user4.getName(), user4); &#125; @Override public User findUserByName(String username) &#123; User user = userMap.get(username); return user; &#125;&#125; 这里应该是要从数据库查询,我这里就模拟了几个用户,密码都是通过 BCryptPasswordEncoder 加密好的,加密这步应该是放到注册里面的 然后启动项目,看一下效果, 访问随便一个接口,我们上面可以试试锁定的用户,还有失效用户登录,还有密码错误的时候,返回分别如下: 然后输入正确的密码,就可以访问到接口了 总结关于自定义认证主要做了以下两件事: 手动实现 UserDetailsService, 跟数据库交互做登录验证 然后就是 PasswordEncoder 自定义密码加密解密的方法,然后配置到项目里面]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-1-基本原理]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-1-%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[述前文中,简单的使用了一下Spring Security,做了基本的用户名密码的认证,下面来大致的看一下,登录这个过程,Spring Security做了哪些事情 基本原理Spring Security的认证授权等处理是经过一系列的filter实现的,大致流程图如下: 一个请求进来,先是经过一系列的security的过滤器链 UsernamePasswordAuthenticationFilter从图上来看,第一个进入的filter是 UsernamePasswordAuthenticationFilter, 就是用户名密码认证, 图中,这个下面是一个 BasicAuthenticationFilter, 这两个是只走一个的,一个是表单的认证,一个是basic的认证, 通常情况我们都是通过用户名密码去做认证的, 所以这里只看 UsernamePasswordAuthenticationFilter, 部分源码如下: 这一步主要就是从请求中获取了用户名和密码,然后生成一个token,就是 UsernamePasswordAuthenticationToken 对象, 看一下他的构造 然后这里下一步 return 的这一步,这里获取了一个 AuthenticationManager,这是一个接口,他有很多的实现,这里是用的是 ProviderManager ,点进去看一下具体调用的方法 这一层主要是判断一下,传进来的这个token是不是支持, 我们传过去的是一个 UsernamePasswordAuthenticationToken, 还可以能有其他的登录方式,比如是第三方登录等 然后进到这个实际处理认证的方法中看一下,代码如下: 前面这部分就是个简单的 去从缓存里面找用户 然后是下面部分的代码: 这里才是用户名密码的检查,最终会返回一个认证之后的 Authentication 对象 这个方法才是主要的认证方法,总结一下主要干了这几件事: 去调用自己实现的UserDetailsService,返回UserDetails,就是源码里面的 retrieveUser(...) 这个方法 然后是做了两次用户检查, 一次是预检查主要是帐号是否被冻结,是否过期等 第二次检查是密码的验证,这里会调用 PasswordEncoder ,具体可以点进去看一下 最后又做了一次检查,是检查UserDetails是否可用 都检查完了之后,返回经过认证的 Authentication 到这儿 UsernamePasswordAuthenticationFilter 就走完了 FilterSecurityInterceptor然后再看一下上面那个过滤器链的图, 最后一级是 FilterSecurityInterceptor, 这个拦截器,他主要的功能就是做一个最终的判断, 比如我们的 UsernamePasswordAuthenticationFilter 通过了,但是没有权限去访问的话,也会被打回去,这个过滤器如果放行了,那请求就到了我们接口了,简单看一下这个类的代码 这里的这个doFilter()方法,其实就是调到了我们项目里面的controller层了 ExceptionTranslationFilter最后再来看一下 ExceptionTranslationFilter 这个类,先看一下源码, 关键代码如下: 他其实就是调下一级的filter,然后主要的逻辑都是在catch里面捕获异常, 他就是个最终的异常处理,捕获到 FilterSecurityInterceptor 里面的异常,然后根据不同的异常做出不同的处理, 比如用户名密码不对,就返回一个登录页面等 整个认证的大致流程就是这样,先大概的了解一下, 之后再详细介绍]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security-0-简介及简单使用]]></title>
    <url>%2F2019%2F09%2F10%2FSpring-Security-0-%E7%AE%80%E4%BB%8B%E5%8F%8A%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[简介Spring Security是一个强大的和高度可定制的身份验证和访问控制框架 Spring Security着重于为Java应用程序提供身份验证和授权。身份验证是为用户建立一个他所声明的主体的过程（主体一般式指用户，设备或可以在你系统中执行动作的其他系统）。授权指的是一个用户能否在你的应用中执行某个操作，在到达授权判断之前，身份的主体已经由身份验证过程建立了。这些概念是通用的，并不是Spring Security特有的。 在一个web服务中.如果没有身份认证的话,那就是只要知道接口地址就可以调用,在实际的项目中肯定是不可能的,都需要做用户的认证,也就是登录的操作,登录完成之后,再调用接口的时候,还需要判断这个用户有没有权限,如果这个用户没有权限的话,那也是不能调,使用Spring Security就可以实现对接口的认证和权限的一个控制 Spring Security主要实现了以下三件事: 认证 授权 攻击防护,防止伪造身份 简单使用首先,需要引入security的依赖,如下:12345&lt;!-- serurity --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt; 引入依赖之后,项目就会自动加入一个身份的验证,我们请求项目中的一个接口,效果如下: 这里会提示输入用户名和密码,这时候我们是只引入了一个依赖,还什么都没有配置,所以就是默认的用户名和密码, 默认的用户名是:user, 然后默认的密码可以在我们项目的启动日志里面找到,如下: 这里登陆之后就可以访问接口了 那如果说我们引入security的依赖之后,并不想去让他做认证,可以在 application.yml 中关闭,代码如下:123security: basic: enabled: false 简单配置在我们上面的例子中,请求如果没有登录的话,是在浏览器里面弹出个框,然后输入用户名和密码,实际的应用中,不可能用这种方式去登录的,一般是用一个登录的页面来进行登录的操作,下面来看做一下简单的配置,实现登录页登录 首先需要建一个类 BrowserSecurityConfig, 针对浏览器的一些配置,写在这个类里面,然后继承 WebSecurityConfigurerAdapter, spring提供的一个适配器, 然后重写里面的 configure(HttpSecurity http) 这个方法,具体代码如下: 1234567891011121314@Configurationpublic class BrowserSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(HttpSecurity http) throws Exception &#123; // formLogin 表示表单认证 http.formLogin() .and() // 授权请求. anyRequest 就表示所有的请求都需要权限认证 .authorizeRequests() .anyRequest() .authenticated(); &#125;&#125; 配置完成之后,再来看一下效果,访问接口,效果如下: 这里就是一个默认的表单登录页面,还是和上面一样,输入用户名和密码就可以访问到接口了 本文用到的一些api,都传到了github上面,之后的文章到基于这个初始的项目 传送门 然后还有本文的这些代码也传了上去,传送门 然后之后文章中所对应的代码的分支名,都和文章标题的下标相同]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot-全局异常处理及自定义异常]]></title>
    <url>%2F2019%2F08%2F03%2FSpring-Boot-%E5%85%A8%E5%B1%80%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%E5%8F%8A%E8%87%AA%E5%AE%9A%E4%B9%89%E5%BC%82%E5%B8%B8%2F</url>
    <content type="text"><![CDATA[述在日常开发过程中,我们使用Spring Boot,如果程序中抛出了一个异常,那我们在网页中看到的内容可能如下:返回的是一个默认的错误页面,那如果我们是用的postMan等工具去请求接口呢, 看到的信息会是一个json字符串,如下: 那他是怎么实现不同的环境返回不同的内容呢? 下面来看一下Spring Boot的异常处理的默认实现 Spring Boot的默认异常处理Spring Boot的默认异常处理在 BasicErrorController 这个类里面,下面看一下这个类的源码:关键的代码就是这两个方法,这里有两个接口, 上面的接口中标注了 produces 是 text/html,就是说如果是网页请求的话(网页请求头中有text/html),就走这个接口,最后返回的是一个ModelAndView,就是一个错误页面 对于返回错误页面, 其中还调用了一个非常重要的方法: this.resolveErrorView(...) 方法, 源码我就不带大家看了, 他的作用就是根据 HTTP 状态码来去找错误页面, 如 500 错误会去找 /error/500.html, 403 错误回去找 /error/403.html, 如果找不到则再找 /error/4xx.html 或 /error/5xx.html 页面. 还找不到的话, 则会去找 /error.html 页面, 如果都没有配置, 则会使用 Spring Boot 默认的页面 然后在下面的接口中,加了注解@ResponseBody,表示返回的是一个json,所以我们在postMan中的请求返回是一个json 自定义异常看完了Spring Boot的默认异常处理后,再来思考这样一个问题,比如我们需要抛出一个用户不存在的异常,然后把用户id返回给前端,这就需要我们自己来定义一个异常,代码如下:1234567891011@Datapublic class UserNotExistException extends RuntimeException &#123; private Long id; public UserNotExistException(Long id) &#123; super(&quot;用户不存在&quot;); this.id = id; &#125;&#125; 首先继承RuntimeException,然后写一个构造,调用super传入异常的message,然后还可以赋值一些自定义的属性,比如说这里的id 异常定义好了之后,下一步就是在出现异常的时候把异常给抛出去,Spring Boot默认的异常处理是不可以的,就需要我们自己做全局异常处理 自定义全局异常处理新建一个全局异常处理器 GlobalExceptionHandler,代码如下:12345678910111213@ControllerAdvicepublic class GlobalExceptionHandler &#123; @ExceptionHandler(UserNotExistException.class) @ResponseBody @ResponseStatus(HttpStatus.INTERNAL_SERVER_ERROR) public Map&lt;String, Object&gt; handleUserNotExistException(UserNotExistException ex) &#123; Map&lt;String, Object&gt; result = new HashMap&lt;&gt;(2); result.put(&quot;id&quot;, ex.getId()); result.put(&quot;message&quot;, ex.getMessage()); return result; &#125;&#125; 首先类上面的注解是 @ControllerAdvice, 然后下面是自定义异常的一些返回信息,@ResponseBody表示返回是json格式的数据,@ResponseStatus(HttpStatus.INTERNAL_SERVER_ERROR)表示返回的http状态码是500, 方法里面返回了异常信息,以及id. 每次自定义了新的异常之后,都需要加到这个类里面来 测试上面的内容都搞定之后,进行一个简单的测试,比如有以下这么一个接口:123456@GetMapping(&quot;&#123;id:\\d+&#125;&quot;)public User getInfo(@PathVariable Long id)&#123; log.info(&quot;查询id是[&#123;&#125;]的数据.....&quot;, id); throw new UserNotExistException(id);// return new User();&#125; 就是直接抛出了我们自定义的异常,然后下面访问接口看一下效果,如下: 可以看到,返回的就是我们的自定义异常信息了]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring-自定义注解校验字段格式]]></title>
    <url>%2F2019%2F08%2F03%2FSpring-%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B3%A8%E8%A7%A3%E6%A0%A1%E9%AA%8C%E5%AD%97%E6%AE%B5%E6%A0%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[述在我们日常编码中,服务端需要校验很多字段的格式,比如是不是为空,字段长度等等各种验证,这些验证如果写到我们的代码逻辑中的话,每次开始都需要做大量的校验,而且还有可能在不同的方法中校验的逻辑相同,那就可能会复制大量相同的代码,看着很烦 java给我们提供了很多验证的注解,比如下面这个model123456789101112131415@Datapublic class User implements Serializable &#123; private Long id; @Length(min=0, max=20) private String name; @NotBlank private String password; private Integer sex; private String phoneNo;&#125; 这里就规定了说,传过来的 name的长度最大是20,最小是0, 密码不能是空, 如果不满足的话直接就返回了,就不需要我们再写这些验证的逻辑代码了. 但是有时候需要一些其他的通用校验,那也可以自定义一个这样的注解来实现,比如说要验证一个字段格式必须是手机号,下面看一下具体如何实现 自定义注解首先我们需要创建一个注解,名称是PhoneNo,具体代码如下:123456789101112@Target(&#123;ElementType.METHOD, ElementType.FIELD&#125;)@Retention(RetentionPolicy.RUNTIME)@Constraint(validatedBy = PhoneNoValidator.class)public @interface PhoneNo &#123; String message() default &quot;手机号格式有误&quot;; Class&lt;?&gt;[] groups() default &#123; &#125;; Class&lt;? extends Payload&gt;[] payload() default &#123; &#125;;&#125; 先来看一下上面的几个注解: @Target({ElementType.METHOD, ElementType.FIELD}): 用来定义这个注解能加在哪里,我们这里就是说可以加在方法上,也可以加在字段上面 @Retention(RetentionPolicy.RUNTIME): 表示是一个运行时的注解 @Constraint(validatedBy = PhoneNoValidator.class): 表示具体的验证逻辑类,是PhoneNoValidator 下面有个方法是message(),用来表示验证不通过后返回的信息,默认的是手机号格式有误 具体验证逻辑这里还需要一个具体的验证类,就是我们上面指定的 PhoneNoValidator.class, 代码如下 :12345678910111213141516171819202122232425262728@Slf4jpublic class PhoneNoValidator implements ConstraintValidator&lt;PhoneNo, Object&gt; &#123; /** * 手机号码正则表达式 */ public static final String REG_PHONE = &quot;^((13[0-9])|(15[^4,\\D])|(14[57])|(17[0])|(17[7])|(18[0,0-9]))\\d&#123;8&#125;$&quot;; /** * 验证器的初始化工作 * @param constraintAnnotation */ @Override public void initialize(PhoneNo constraintAnnotation) &#123; log.info(&quot;进入手机号码验证器....&quot;); &#125; /** * 验证的具体逻辑 * @param value * @param context * @return */ @Override public boolean isValid(Object value, ConstraintValidatorContext context) &#123; return Pattern.matches(REG_PHONE, (String)value); &#125;&#125; 这里首先是实现了ConstraintValidator&lt;PhoneNo, Object&gt; 接口,这里的PhoneNo,就是我们注解的名称,然后这里需要重写两个方法, initialize() 和 isValid(), 前面的是一个初始化的方法, 就是说,在验证方法被执行之前需要做的事情 然后是下面的验证的具体逻辑,我们上面就是写了个正则去验证手机号,最后返回的是 true/false, 对应验证通过/不通过 扩展这个类中,可以使用@Autowired来注入SpringBean,比如我们需要验证这个字段在数据库里面有没有,那就需要注入具体的service来跟数据库进行交互了 这里这个类是不需要加@Component注解的,只要实现了 ConstraintValidator&lt;PhoneNo, Object&gt; 这个接口,Spring就会自动管理这个类的 具体使用上面我们注解和具体的逻辑都写好了,那再来看一下如何去使用, 跟java提供的那些注解的用法是一样的,如下:1234567891011121314@Datapublic class User implements Serializable &#123; private Long id; private String name; private String password; private Integer sex; @PhoneNo private String phoneNo;&#125; 这里如果手机号格式不正确的话,就会返回默认的错误信息, 如果需要自定义的话就在注解后面加个message就好了, 比如: @PhoneNo(message = &quot;自定义信息&quot;)]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-14-心跳与空闲检测]]></title>
    <url>%2F2019%2F07%2F02%2FNetty-14-%E5%BF%83%E8%B7%B3%E4%B8%8E%E7%A9%BA%E9%97%B2%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[述连接假死现象: 在某一端(服务端或者客户端)看来底层TCP连接已经断开,但是应用程序并没有捕获到,因此认为这条连接仍然是存在的,从TCP层面来说,只有收到四次握手数据包或者一个RST数据包,连接的状态表示已断开. 引发的问题: 对于服务端来说,因为每条连接都耗费CPU和内存资源,大量假死的连接逐渐耗光服务器的资源,最终导致性能逐渐下降,程序奔溃;对于客户端来说,连接假死造成发送数据超时,影响用户体验. 出现假死的原因可能一以下几种: 应用程序出现线程堵塞,无法进行数据读写 客户端或者服务端网络相关设备出现故障,比如网卡,机房故障 公网丢包,公网环境相对内网而言容易出现丢包、网络抖动等现象,如果在一段时间内用户接入网络连续出现丢包现象,则对客户端来说数据一直发送不出去,服务端接收不到客户端的数据,连接一直耗着. 下面来看一下如何解决这个问题 服务端空闲检测空闲检测就是服务端每隔一段时间,就去检测这段时间内是否有数据读写,简单来说,就是检测一下有没有收到客户端发过来的数据 使用 Netty 自带的 IdleStateHandler 就可以实现这个功能,下面看下具体如何实现 12345678910111213141516@Slf4jpublic class IMIdleStateHandler extends IdleStateHandler &#123; private static final int READER_IDLE_TIME = 15; public IMIdleStateHandler()&#123; super(READER_IDLE_TIME, 0, 0, TimeUnit.SECONDS); &#125; @Override protected void channelIdle(ChannelHandlerContext ctx, IdleStateEvent evt) throws Exception &#123; log.info(&quot;&#123;&#125;秒内未读到数据,连接关闭&quot;, READER_IDLE_TIME); ctx.channel().close(); &#125;&#125; 新建一个类,然后继承IdleStateHandler类, 构造函数调用父类,这里传入了4个参数: 第一个: 读空闲时间,在这段时间内如果没有数据读到,就表示连接假死 第二个: 写空闲时间,在这段时间内如果没有写数据,就表示连接假死 第三个: 读写空闲时间,在这段时间内如果没有产生数据读或写,就表示连接假死 第四个: 时间单位 我们上面的代码就表示,15秒内如果没有读到数据,就表示连接假死连接假死之后会回调下面的channelIdle(),这里就是打印数据,然后关闭channel 最后把这个handler添加到服务端的第一个123456789101112.childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; @Override protected void initChannel(NioSocketChannel ch) &#123; log.info(&quot;取出childAttr属性:&#123;&#125;&quot;, ch.attr(CommonConfig.CLIENT_KEY).get()); ch.pipeline() .addLast(new IMIdleStateHandler()) .addLast(new Spliter()) .addLast(PacketCodecHandler.INSTANCE) // .... &#125;&#125;); 这里放到第一个是因为,如果这个handler在后面,那前面的handler处理数据完毕,或者出错了就不会再往后传递了,那我们的IMIdleStateHandler最终就收不到数据了,就会误判 客户端发送心跳上面我们实现了服务端的空闲检测,就是每隔一段时间读取channel里面的数据,读不到就是连接假死 这里有个问题就是,客户端在这段时间确实是没有发送数据,但是客户端是正常状态,并没有假死,那这种情况就需要客户端每隔一段时间去给服务端发一个心跳数据包 这里也是新建一个handler,给服务端发送心跳的数据包,具体代码如下:123456789101112131415161718public class HeartBeatTimerHandler extends ChannelInboundHandlerAdapter &#123; private static final int HEARTBEAT_INTERVAL = 5; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; super.channelActive(ctx); &#125; private void scheduleSendHeartBeat(ChannelHandlerContext ctx)&#123; ctx.executor().schedule(() -&gt; &#123; if (ctx.channel().isActive()) &#123; ctx.writeAndFlush(new HeartBeatRequestPacket()); scheduleSendHeartBeat(ctx); &#125; &#125;, HEARTBEAT_INTERVAL, TimeUnit.SECONDS); &#125;&#125; 这里也就是一个定时任务,隔五秒给服务端发送一个心跳数据包,通常空闲检测时间要比发送心跳时间的两倍要长一些,排除偶发的公网抖动防止误判 服务端回复心跳与客户端空闲检测上面的操作做完之后是解决了服务端的空闲检测问题,服务端这个时候是能够在一定时间段之内关掉假死的连接,释放连接的资源了,但是对于客户端来说,我们也需要检测到假死的连接 方法和服务端是一样的,也是在客户端pipeline的最前面,加入IMIdleStateHandler12345678910.handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; socketChannel.pipeline() .addLast(new IMIdleStateHandler()) .addLast(new Spliter()) // ... ; &#125;&#125;); 然后服务端也需要定时的给客户端发送心跳数据, 然后服务端再添加一个HeartBeatRequestHandler,放在AuthHandler前面,因为这个是可以不用登录的123456789101112131415.childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; @Override protected void initChannel(NioSocketChannel ch) &#123; log.info(&quot;取出childAttr属性:&#123;&#125;&quot;, ch.attr(CommonConfig.CLIENT_KEY).get()); ch.pipeline() .addLast(new IMIdleStateHandler()) .addLast(new Spliter()) .addLast(PacketCodecHandler.INSTANCE) .addLast(LoginRequestHandler.INSTANCE) .addLast(HeartBeatRequestHandler.INSTANCE) .addLast(AuthHandler.INSTANCE) .addLast(IMHandler.INSTANCE); &#125;&#125;); HeartBeatRequestHandler的代码如下:12345678910111213@ChannelHandler.Sharablepublic class HeartBeatRequestHandler extends SimpleChannelInboundHandler&lt;HeartBeatRequestPacket&gt; &#123; public static final HeartBeatRequestHandler INSTANCE = new HeartBeatRequestHandler(); private HeartBeatRequestHandler() &#123; &#125; @Override protected void channelRead0(ChannelHandlerContext ctx, HeartBeatRequestPacket requestPacket) &#123; ctx.writeAndFlush(new HeartBeatResponsePacket()); &#125;&#125; 这里就是简单的回复一个响应的数据包 完整代码已上传github,传送门]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-13-性能优化处理]]></title>
    <url>%2F2019%2F07%2F02%2FNetty-13-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[述我们在实现完一系列的功能之后,会有很多的handler通过pipeline加到channel中去, 下面看一下如何去优化这些handler 优化第一步:共享handler首先来看下服务端的代码: 指令相关的handler很多,netty在这里的逻辑是:每次有新连接到来的时候,都会调用ChannelInitializer 的 initChannel() 方法,然后这里指令相关的 handler 都会被 new 一次 这里的每一个指令handler的内部都没有成员变量的,也就是无状态的,这里就可以使用单例模式,不需要每次都new对象,提高效率,下面看一下具体如何实现 以LoginRequestHandler为例,看一下如何修改,代码如下:1234567891011121314@Slf4j@ChannelHandler.Sharablepublic class LoginRequestHandler extends SimpleChannelInboundHandler&lt;LoginRequestPacket&gt; &#123; public static final LoginRequestHandler INSTANCE = new LoginRequestHandler(); protected LoginRequestHandler() &#123; &#125; @Override protected void channelRead0(ChannelHandlerContext channelHandlerContext, LoginRequestPacket loginRequestPacket) throws Exception &#123; // ... 省略具体逻辑 &#125;&#125; 这里首先需要加一个`@ChannelHandler.Sharable`注解,表示这个handler是要被多个channel共享的,不加这个会报错 然后就是把构造堵死,用INSTANCE变量获取对象 最后添加到channel用.addLast(LoginRequestHandler.INSTANCE) 这里有一个叫Spliter的handler,他内部实现是与每个 channel 有关,每个 Spliter 需要维持每个 channel 当前读到的数据,也就是说他是有状态的,所以这个类是没法去改的 优化第二步: 压缩 handler下面看一下服务端编解码用的这两个handler 解码: PacketDecoder 继承了 ByteToMessageDecoder 编码: PacketEncoder 继承了 MessageToByteEncoder 这两个类其实可以合并到一起,继承MessageToMessageCodec就ok了,新建一个类,代码如下:1234567891011121314151617181920@ChannelHandler.Sharablepublic class PacketCodecHandler extends MessageToMessageCodec&lt;ByteBuf, Packet&gt; &#123; public static final PacketCodecHandler INSTANCE = new PacketCodecHandler(); protected PacketCodecHandler()&#123;&#125; @Override protected void encode(ChannelHandlerContext channelHandlerContext, Packet packet, List&lt;Object&gt; list) throws Exception &#123; ByteBuf byteBuf = channelHandlerContext.alloc().ioBuffer(); PacketCodeC.INSTANCE.encode(byteBuf, packet); list.add(byteBuf); &#125; @Override protected void decode(ChannelHandlerContext channelHandlerContext, ByteBuf byteBuf, List&lt;Object&gt; list) throws Exception &#123; list.add(PacketCodeC.INSTANCE.decode(byteBuf)); &#125;&#125; 这个编解码的类也可以做成单例模式,这里用了我们之前的PacketCodeC这个类. 然后把这个编解码的类添加到服务端就好了12345678910ch.pipeline() .addLast(new Spliter()) .addLast(PacketCodecHandler.INSTANCE) .addLast(LoginRequestHandler.INSTANCE) .addLast(AuthHandler.INSTANCE) .addLast(MessageRequestHandler.INSTANCE) .addLast(CreateGroupRequestHandler.INSTANCE) .addLast(LogoutRequestHandler.INSTANCE); &#125;&#125;); 优化第三步:缩短事件传播路径这里有一个问题, pipeline 链中,绝大部分是指令相关的handler,按现在的写法的话,等指令越来越多,我们要一个个通过addLast()加进去,然后 handler 链越来越长,在事件传播过程中性能损耗会被逐渐放大, 下面看一下如何解决这个问题 合并平行 handler在我们的应用中,数据过来然后解码出来之后,只会在一个handler上执行处理,我们可以把这么多的指令 handler 压缩为一个 handler, 代码如下:1234567891011121314151617181920@ChannelHandler.Sharablepublic class IMHandler extends SimpleChannelInboundHandler&lt;Packet&gt; &#123; public static final IMHandler INSTANCE = new IMHandler(); private Map&lt;Byte, SimpleChannelInboundHandler&lt;? extends Packet&gt;&gt;handlerMap; private IMHandler()&#123; handlerMap = new HashMap&lt;&gt;(); handlerMap.put(Command.MESSAGE_REQUEST, MessageRequestHandler.INSTANCE); handlerMap.put(Command.CREATE_GROUP_REQUEST, CreateGroupRequestHandler.INSTANCE); handlerMap.put(Command.LOGOUT_REQUEST, LogoutRequestHandler.INSTANCE); &#125; @Override protected void channelRead0(ChannelHandlerContext channelHandlerContext, Packet packet) throws Exception &#123; handlerMap.get(packet.getCommand()).channelRead(channelHandlerContext,packet); &#125;&#125; 这里就是建一个总的 handler ,然后这个handler也是无状态的,也可以做成单例模式,然后写一个map,把所有的指令处理器都加到这个map里面,然后再channelRead0()这个方法里面通过指令找到对应的handler去做具体的处理 最后把这个总的handler加到服务端就好了,代码如下:123456ch.pipeline() .addLast(new Spliter()) .addLast(PacketCodecHandler.INSTANCE) .addLast(LoginRequestHandler.INSTANCE) .addLast(AuthHandler.INSTANCE) .addLast(IMHandler.INSTANCE); 更改事件传播源如果OutBound类型的handler用的比较多的话,写数据可以使用ctx.writeAndFlush()来减短事件传播路径 ctx.writeAndFlush()ctx.writeAndFlush()是从pipeline链的当前节点开始往前找到第一个 outBound 类型的 handler ,把对象往前进行传播,如果这个对象确认不需要经过其他 outBound 类型的 handler 处理,就使用这个方法 上图中的这个InBound在处理完逻辑之后,调用ctx.writeAndFlush()就可以直接一口气把对象送到 codec 中编码,然后写出去 ctx.channel().writeAndFlush()再来看一下ctx.channel().writeAndFlush(),它是从 pipeline 链中的最后一个 outBound 类型的 handler 开始,把对象往前进行传播,如果你确认当前创建的对象需要经过后面的 outBound 类型的 handler,那么就调用此方法 传播路径如图,ctx.channel().writeAndFlush()会从最后一个 outBound 类型的handler往前逐个传递,路径是比ctx.channel().writeAndFlush()长的 所以在我们的程序中,没有改造编码器之前,必须调用ctx.channel().writeAndFlush(),但是经过改造之后,这个编码器既属于inBound类型又属于outBound类型,所以可以放到pipeline链的前面了 优化第四步: 减少阻塞主线程的操作我们的程序中通常会涉及到数据库或者网络等一些很耗时的操作,这些操作是不能放到handler里面的,只要有一个 channel 的一个 handler 中的 channelRead0() 方法阻塞了 NIO 线程,最终都会拖慢绑定在该 NIO 线程上的其他所有的 channel 所以,对于耗时的一些逻辑,都丢到业务线程池去处理 统计程序处理时长我们通常统计程序的处理时长就是方法的开始前后输出一个时间,然后相减,但是如果writeAndFlush()方法在nio线程中执行的话,他是一个异步的操作,调用之后会立即返回,但是其实他是没有执行完的, 如果要判断writeAndFlush()是否执行完毕的话,应该怎么做呢? writeAndFlush()方法会返回一个ChannelFuture,可以给他添加一个监听器,Listener, 去监听writeAndFlush()的执行结果,然后统计耗时即可 Netty 里面很多方法都是异步的操作,在业务线程中如果要统计这部分操作的时间,都需要使用监听器回调的方式来统计耗时,如果在 NIO 线程中调用,就不需要这么干 完整代码已上传github,传送门]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-12-channelHandler热拔插]]></title>
    <url>%2F2019%2F07%2F02%2FNetty-12-channelHandler%E7%83%AD%E6%8B%94%E6%8F%92%2F</url>
    <content type="text"><![CDATA[述前面的代码中,我们的客户端登录之后,存了一个标记,表示当前的channel是已登录的状态,然后每次发现系的时候都需要去验证一次用户是否登录,代码如下: 这里我们也可以在服务端加一个handler去验证用户是否登录 登录验证handler新建一个handler,代码如下:123456789101112public class AuthHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; if (!LoginUtil.hasLogin(ctx.channel())) &#123; ctx.channel().close(); &#125; else &#123; super.channelRead(ctx, msg); &#125; &#125; &#125; 这里这个类继承自ChannelInboundHandlerAdapter,然后重写了channelRead()方法,也就是说,这个handler能处理所有类型的数据包 在这个channelRead()中,判断客户端是否登录,没有登录的话就直接关闭连接(实际生产环境可能逻辑要复杂些),如果登录了的话就将数据传给以一个handler去处理 然后再NettyServer中把这个handler加进去,代码如下:1234567891011121314.childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; @Override protected void initChannel(NioSocketChannel ch) &#123; log.info(&quot;取出childAttr属性:&#123;&#125;&quot;, ch.attr(CommonConfig.CLIENT_KEY).get()); ch.pipeline() .addLast(new Spliter()) .addLast(new PacketDecoder()) .addLast(new LoginRequestHandler()) .addLast(new AuthHandler()) .addLast(new MessageRequestHandler()) .addLast(new PacketEncoder()); &#125;&#125;); 这样MessageRequestHandler以及后面的handler都已经经过了AuthHandler的过滤,后续所有的handler都不用担心身份认证这个问题 热拔插这里会有一个问题,就是客户端登录之后每次发送消息都会经过这个验证,其实只要客户端的连接未断开,客户端只要登录成功过,就不需要再去校验了,也就是说,我们只在第一次消息发过来的时候判断一下就好了, 这里就可以使用 pipeline 的热插拔机制 handler 其实可以看做是一段功能相对聚合的逻辑,然后通过 pipeline 把这些一个个小的逻辑聚合起来,串起一个功能完整的逻辑链.既然可以把逻辑串起来,也可以做到动态删除一个或多个逻辑 修改AuthHandler,修改后如下:12345678910111213141516171819202122@Slf4jpublic class AuthHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; if (!LoginUtil.hasLogin(ctx.channel())) &#123; ctx.channel().close(); &#125; else &#123; ctx.pipeline().remove(this); super.channelRead(ctx, msg); &#125; &#125; @Override public void handlerRemoved(ChannelHandlerContext ctx) throws Exception &#123; if (LoginUtil.hasLogin(ctx.channel())) &#123; log.info(&quot;当前连接登录验证完毕，无需再次验证, AuthHandler 被移除&quot;); &#125; else &#123; log.info(&quot;无登录验证，强制关闭连接!&quot;); &#125; &#125;&#125; 就是权限验证通过之后,就把自己移除,这里的remove(this)就指的是移除AuthHandler, 移除之后,后续的数据过来服务端就不会再校验是否登录了 然后还重写了handlerRemoved(),就是handler被移除的时候的回调,打印信息 分别启动服务端客户端看一下控制台输出 客户端: 服务端: 完整代码已上传github,传送门]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-11-channelHandler的生命周期]]></title>
    <url>%2F2019%2F07%2F02%2FNetty-11-channelHandler%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%2F</url>
    <content type="text"><![CDATA[述我们之前在使用channelHanndler的时候,只重写了channelRead()和channelActive()这两个方法,它里面还有一些其他的方法, 这些方法的回调顺序就是channelHandler的生命周期 案例新建一个handler,名称是LifeCyCleTestHandler,具体代码如下:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@Slf4jpublic class LifeCycleTestHandler extends ChannelInboundHandlerAdapter &#123; @Override public void handlerAdded(ChannelHandlerContext ctx) throws Exception &#123; log.info(&quot;逻辑处理器被添加....&quot;); super.handlerAdded(ctx); &#125; @Override public void channelRegistered(ChannelHandlerContext ctx) throws Exception &#123; log.info(&quot;channel 绑定到线程(NioEventLoop): channelRegistered()&quot;); super.channelRegistered(ctx); &#125; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; log.info(&quot;channel准备就绪:channelActive()&quot;); super.channelActive(ctx); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; log.info(&quot;channel有数据可读:channelRead()&quot;); super.channelRead(ctx, msg); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; log.info(&quot;某次数据读取完毕.....channelReadComplete()&quot;); super.channelReadComplete(ctx); &#125; @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; log.info(&quot;channel被关闭:channelInactive()&quot;); super.channelInactive(ctx); &#125; @Override public void channelUnregistered(ChannelHandlerContext ctx) throws Exception &#123; log.info(&quot;channel取消线程(NioEventLoop)的绑定:channelUnregistered()&quot;); super.channelUnregistered(ctx); &#125; @Override public void handlerRemoved(ChannelHandlerContext ctx) throws Exception &#123; log.info(&quot;逻辑处理器被移除:handlerRemoved()&quot;); super.handlerRemoved(ctx); &#125; &#125; 重写了一部分方法,然后每个方法被调用的时候都打印了一段信息,然后把这个事件继续往下传播,最后把这个handler添加到服务端的第一个位置ch.pipeline().addLast(new LifeCycleTestHandler()),运行客户端,服务端的输出如下: 可以看到这里的ChannelHandler的顺序如下:handlerAdded() –&gt; channelRegistered() –&gt; channelActive() –&gt; channelRead() –&gt; channelReadComplete() 每个回调方法的具体含义如下: handlerAdded(): 当检测到新的连接之后,调用ch.pipeline().addLast(new LifeCycleTestHandler())之后的回调,表示当前的channel中已经成功添加了一个逻辑处理器 channelRegistered(): 表示当前的 channel 的所有的逻辑处理已经和某个 NIO 线程建立了绑定关系 channelActive(): 当 channel 的 pipeline 中已经添加完所有的 handler 以及绑定好一个NIO线程之后,这条连接算是真正激活了,接下来就会回调这个方法 channelRead(): 客户端向服务端每次发来数据之后,都会回调这个方法,表示有数据可读 channelReadComplete(): 服务端每次读完一次完整的数据之后,都会回调这个方法,表示数据读取完毕 然后客户端的连接关闭(channel被关闭),之后再看服务端的控制台输出: 执行顺序如下:channelInactive() –&gt; channelUnregistered() –&gt; handlerRemoved() 这里跟新建的时候其实是相反的 channelInactive(): 表示这条连接被关闭了 channelUnregistered(): 表明与这条连接对应的 NIO 线程移除掉对这条连接的处理 handlerRemoved(): 最后把这条连接上的所有逻辑处理器全部移除掉 channelHandler生命周期图 ChannelInitializer实现原理服务端启动的部分代码如下:123456789101112.childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; @Override protected void initChannel(NioSocketChannel ch) &#123; ch.pipeline() .addLast(new LifeCycleTestHandler()) .addLast(new Spliter()) .addLast(new PacketDecoder()) .addLast(new LoginRequestHandler()) .addLast(new MessageRequestHandler()) .addLast(new PacketEncoder()); &#125;&#125;); 这里是通过.childHandler()方法,给连接设置一个handler(ChannelInitializer),然后在ChannelInitializer的initChannel()方法里,拿到channel对应的pipeline,往里面添加我们的handler 看一下ChannelInitializer的源码,关键部分如下:123456789101112131415161718192021222324protected abstract void initChannel(C ch) throws Exception;public final void channelRegistered(ChannelHandlerContext ctx) throws Exception &#123; // ... initChannel(ctx); // ...&#125;public void handlerAdded(ChannelHandlerContext ctx) throws Exception &#123; // ... if (ctx.channel().isRegistered()) &#123; initChannel(ctx); &#125; // ...&#125;private boolean initChannel(ChannelHandlerContext ctx) throws Exception &#123; if (initMap.putIfAbsent(ctx, Boolean.TRUE) == null) &#123; initChannel((C) ctx.channel()); // ... return true; &#125; return false;&#125; 这里定义了一个抽象方法initChannel(),这个方法由我们自己去实现,服务端启动流程实现逻辑是往Pipeline里面添加Handler处理器链 handlerAdded()和channelRegistered()方法都尝试调用initChannel()方法,initChannel()使用putIfAbsent()防止initChannel()调用多次. handlerAdded() 与 handlerRemoved()通常可以用在一些资源的申请和释放 channelActive() 与 channelInActive()这两个方法表明的含义是TCP连接的建立与释放,通常可以用于统计单机的连接数, 在channelActive()方法里面还可以过滤客户端连接IP黑白名单. channelRead()用于服务端根据自定义协议来进行拆包等. channelReadComplete()之前写数据的时候都是用writeAndFlush()这个方法,这个方法效率不是很高,可以把这个方法换成write(),然后在channelReadComplete()里面调用ctx.channel().flush()方法进行批量刷新.]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-10-案例-粘包拆包]]></title>
    <url>%2F2019%2F07%2F02%2FNetty-10-%E6%A1%88%E4%BE%8B-%E7%B2%98%E5%8C%85%E6%8B%86%E5%8C%85%2F</url>
    <content type="text"><![CDATA[述先来看一个简单的例子,服务端向客户端发送1000条数据,然后服务端打印出来代码如下 拆包粘包示例客户端代码: 服务端代码: 然后通过channel.pipeline().addLast()把这两个handler添加到对应的端,最后运行看服务端的输出 这里存在以下三种类型的输出: 正常的字符串输出 多个字符串 “粘在一起”, 我们定义这种 ByteBuf 为粘包 一个字符串被”拆开”,形成一个破碎的包,我们定义这种 ByteBuf 为半包 为什么会有粘包半包现象我们在应用层是用的netty,但是操作系统是只认TCP协议的, netty用的ByteBuf来发送数据,到了操作系统还是字节流的,然后数据再到了netty应用层面,再组装成ByteBuf,这里的 ByteBuf 与客户端按顺序发送的 ByteBuf 可能是不对等的 所以我们需要在客户端自定义协议来组装数据包,这个过程叫粘包,然后服务端根据这个协议去组装数据包,这个过程叫拆包 拆包原理在没有netty的情况下,拆包的话就是不断的去从TCP的缓冲区读取数据,每次读取完成之后都需要判断是不是一个完整的包 如果当前读取到的数据不够一个完整的数据包,就保留这些数据,然后继续从缓冲区里面读取 如果当前读取到的加上已经读取的数据,足够拼成一个完整的数据包的话,就将当前读取的数据加上上次读取的数据,拼成一个完整的数据包传给业务层,然后多余的数据保留起来供下次读取用 Netty自带的拆包器我们自己实现拆包的话,会很麻烦,直接用Netty自带的拆包器就可以了 固定长度的拆包器 FixedLengthFrameDecoder适用于长度固定的数据包 行拆包器 LineBasedFrameDecoder每个数据包之间用换行符进行分割话,可以用这个拆包器 分隔符拆包器 DelimiterBasedFrameDecoder更上面的行拆包器差不多,只不过我们可以自定义分隔符 基于长度域拆包器 LengthFieldBasedFrameDecoder最通用的一种拆包器,只要你的自定义协议中包含长度域的字段,就可以使用这个拆包器 LengthFieldBasedFrameDecoder重点就是这个基于长度域拆包器 我们之前定义的数据包协议是这样的: 关于拆包,我们需要知道以下两点: 长度域在数据包的什么位置,或者说长度域相对整个数据包的偏移量是多少,这里就是4+1+1+1=7 数据包中长度域的长度是多少,这里就是4 有了这两点,就可以构造一个拆包器了1new LengthFieldBasedFrameDecoder(Integer.MAX_VALUE, 7, 4); 第一个参数是数据包的最大长度,第二个是长度域的偏移量,第三个是长度域的长度 使用的话就是channel.pipeline().addLast(new LengthFieldBasedFrameDecoder(Integer.MAX_VALUE, 7, 4));,添加到第一个位置 拒绝非本协议的连接我们的数据包开头是有一个魔数的,作用就是尽早屏蔽非本协议的客户端,通常在第一个handler处理这个逻辑,下面看一下这个功能具体该如何实现,新建类Sliter,代码如下:123456789101112131415161718192021222324252627public class Spliter extends LengthFieldBasedFrameDecoder &#123; /** * 长度域的偏移量 */ private static final int LENGTH_FIELD_OFFSET = 7; /** * 长度域的长度 */ private static final int LENGTH_FIELD_LENGTH = 4; public Spliter() &#123; super(Integer.MAX_VALUE, LENGTH_FIELD_OFFSET, LENGTH_FIELD_LENGTH); &#125; @Override protected Object decode(ChannelHandlerContext ctx, ByteBuf in) throws Exception &#123; // 屏蔽非本协议的客户端 if (in.getInt(in.readerIndex()) != PacketCodeC.MAGIC_NUMBER)&#123; ctx.channel().close(); return null; &#125; return super.decode(ctx, in); &#125;&#125; 这里只需要继承一下LengthFieldBasedFrameDecoder,然后写个构造,然后重写decode()方法就可以了 这里的decode()的第二个参数in,每次传递进来都是一个数据包的开头,所以直接用这个in来判断和魔数是不是相等就知道是不是这个协议的连接了 最后把这个类通过ch.pipeline().addLast(new Spliter());加到第一个位置,替换掉上面刚才创建的那个LengthFieldBasedFrameDecoder 这样如果非本协议的数据过来,能尽早判断,关闭连接节省资源 至此.服务端和客户端的pipeline结构是: 完整代码已上传gitHub,传送门]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-9-案例-构建客户端与服务端pipeline]]></title>
    <url>%2F2019%2F07%2F02%2FNetty-9-%E6%A1%88%E4%BE%8B-%E6%9E%84%E5%BB%BA%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E6%9C%8D%E5%8A%A1%E7%AB%AFpipeline%2F</url>
    <content type="text"><![CDATA[述上文中,对pipeline和channelHandler和有了一个基本的了解,以及两种channelHandler的执行顺序. 在之前的案例中我们都是用if-else去判断数据包的类型, 那本文将通过使用channelHandler的方式,重构之前的代码 Netty 内置了很多开箱即用的 ChannelHandler.下面,我们通过一些 Netty 内置的 ChannelHandler 来逐步构建我们的 pipeline ChannelInboundHandlerAdapter 与 ChannelOutboundHandlerAdapter这两个适配器是比较常用的分别用于实现ChannelInboundHandler和ChannelOutboundHandler接口的,这样我们继承他们之后只需要重写我们关心的方法 这两个类我们一般重写的就是ChannelInboundHandler的channelRead()方法和ChannelOutboundHandler的write()方法. 分别来看一下他们的源码 ChannelInboundHandler的channelRead()方法代码如下:这个方法的作用就是接收上一个的handler的输出,这里的msg就是上个handler的输出,默认情况下会通过fireChannelRead()这个方法直接把上一个handler的输出结果传递到下一个handler 再来看一下ChannelOutboundHandler的write()的源码: 默认情况下也会把对象传递给下一个outBound节点,只不过传播顺序和上面的inboundHandler是相反的 ByteToMessageDecoder我们往pipeline添加的第一个handler中的channelRead()方法中,msg对象其实就是ByteBuf,服务端在接收到数据后需要先把msg强转成ByteBuf,然后再解码,转成java对象 每一次收到msg,都需要解码,这就需要一大堆重复的代码,所以netty提供了一个父类ByteToMessageDecoder,来专门做解码这个事儿,下面看一下具体的用法 新建类PacketDecoder,代码如下:12345678public class PacketDecoder extends ByteToMessageDecoder &#123; @Override protected void decode(ChannelHandlerContext channelHandlerContext, ByteBuf byteBuf, List&lt;Object&gt; list) throws Exception &#123; list.add(PacketCodeC.INSTANCE.decode(byteBuf)); &#125;&#125; 这里继承ByteToMessageDecoder然后重写decode方法,这里的参数传过来直接就是ByteBuf对象了,所以我们不需要强转了,只要把解码后的对象塞到这个list里面就好了,就可以自动实现结果往下一个 handler 进行传递,这样,我们就实现了解码的逻辑 handler 我们用的netty版本是4.1.6.Final,默认情况下ByteBuf是用的堆外内存,之前有说过这个内存需要去手动释放的,但是我们之前的操作都没有去释放这个内存,随着程序的运行,就可能造成内存泄露, 但是我们在使用ByteToMessageDecoder的情况下,就完全不用担心这个问题了,Netty会自动进行内存的释放 SimpleChannelInboundHandler我们之前处理数据包的时候,需要写一大堆的if-else去判断是哪种类型的数据包,从而做出相应的处理 然后我们又说了可以通过给pipeline添加多个inBoundHandler来解决这个问题,大致逻辑如下:12345if (packet instanceof XXXPacket) &#123; // ...处理&#125; else &#123; ctx.fireChannelRead(packet); &#125; 这里就又有一个问题, 每个handler中都要去写一个if-else,然后还要手动去传递无法处理的对象(上面else中的代码)这也是一大堆的重复代码, 所以netty抽象出了一个SimpleChannelInboundHandler对象,类型判断和对象传递的活都自动帮我们实现了,而我们可以专注于处理我们所关心的指令即可,下面来看一下具体如何使用 新建类LoginRequestHandler,代码如下:12345678public class LoginRequestHandler extends SimpleChannelInboundHandler&lt;LoginRequestPacket&gt; &#123; @Override protected void channelRead0(ChannelHandlerContext channelHandlerContext, LoginRequestPacket loginRequestPacket) throws Exception &#123; // 具体登录的逻辑 &#125; &#125; 我们只需要去继承SimpleChannelInboundHandler,然后给定一个泛型,处理那种数据包就给定哪种泛型就好,然后重写channelRead0,直接在这个里面去写业务逻辑就可以了,也不用强转,也不用手动去给下一个handler去传递对象了 MessageToByteEncoder在我们的请求处理完之后,一般都会给客户端一个响应,我们之前写响应的时候,需要把响应的数据包先编码成ByteBuf然后再通过writeAndFlush()对象去写给客户端, 比如下面这样:12345678910111213public class LoginRequestHandler extends SimpleChannelInboundHandler&lt;LoginRequestPacket&gt; &#123; @Override protected void channelRead0(ChannelHandlerContext channelHandlerContext, LoginRequestPacket loginRequestPacket) throws Exception &#123; // ...具体登录的逻辑 LoginResponsePacket loginResponsePacket = new LoginResponsePacket(); loginResponsePacket.setXXX(); // ... ByteBuf responseByteBuf = PacketCodeC.INSTANCE.encode(ctx.alloc(), loginResponsePacket); ctx.channel().writeAndFlush(responseByteBuf); &#125; &#125; 每种数据包给响应都需要先编码,然后writeAndFlush(),这也是一段重复的代码,而且在PacketCodeC.INSTANCE.encode()这个方法里面我们还得手动创建一个ByteBuf对象 Netty 提供了一个特殊的channelHandler来专门处理编码逻辑,我们不需要每一次将响应写到对端的时候调用一次编码逻辑进行编码,也不需要自行创建ByteBuf,这个类叫做MessageToByteEncoder,从字面意思也可以看出,它的功能就是将对象转换到二进制数据,下面来看一下如何使用 新建类PacketEncoder,代码如下:12345678public class PacketEncoder extends MessageToByteEncoder&lt;Packet&gt; &#123; @Override protected void encode(ChannelHandlerContext channelHandlerContext, Packet packet, ByteBuf byteBuf) throws Exception &#123; PacketCodeC.INSTANCE.encode(byteBuf, packet); &#125;&#125; 这里只需要继承MessageToByteEncoder然后给定一个泛型,表示给哪种类型的数据编码,然后重写encode()方法,里面再调用我们PacketCodeC类的编码方法,然后这个方法里面的代码需要修改一下,修改后的代码如下:12345678910111213public void encode(ByteBuf byteBuf, Packet packet) &#123; // 序列化java对象 byte[] bytes = Serializer.DEFAULT.serialize(packet); // 实际编码过程 byteBuf.writeInt(MAGIC_NUMBER); byteBuf.writeByte(packet.getVersion()); byteBuf.writeByte(Serializer.DEFAULT.getSerializerAlgorithm()); byteBuf.writeByte(packet.getCommand()); byteBuf.writeInt(bytes.length); byteBuf.writeBytes(bytes);&#125; 和之前的区别就是不需要我们去手动创建ByteBuf对象了,当我们向 pipeline 中添加了这个编码器之后,我们在指令处理完毕之后就只需要 writeAndFlush java 对象即可,像这样 12345678910@Slf4jpublic class LoginRequestHandler extends SimpleChannelInboundHandler&lt;LoginRequestPacket&gt; &#123; @Override protected void channelRead0(ChannelHandlerContext channelHandlerContext, LoginRequestPacket loginRequestPacket) throws Exception &#123; // ..省略具体登录逻辑 channelHandlerContext.channel().writeAndFlush(loginResponsePacket); &#125;&#125; 总结上面这是一个客户端的大概逻辑,然后客户端跟服务端都差不多,结构图如下 对应的代码 服务端:123456789serverBootstrap .childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; protected void initChannel(NioSocketChannel ch) &#123; ch.pipeline().addLast(new PacketDecoder()); ch.pipeline().addLast(new LoginRequestHandler()); ch.pipeline().addLast(new MessageRequestHandler()); ch.pipeline().addLast(new PacketEncoder()); &#125; &#125;); 客户端:12345678910bootstrap .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel ch) &#123; ch.pipeline().addLast(new PacketDecoder()); ch.pipeline().addLast(new LoginResponseHandler()); ch.pipeline().addLast(new MessageResponseHandler()); ch.pipeline().addLast(new PacketEncoder()); &#125; &#125;); 完整代码已上传gitHub,传送门]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-8-pipeline 和 channelHandler]]></title>
    <url>%2F2019%2F07%2F02%2FNetty-8-pipeline-%E5%92%8C-channelHandler%2F</url>
    <content type="text"><![CDATA[述上面两个案例中,我们接收到数据包之后,都是先转成Packet类型,然后再去判断是哪种类型,最后做出相应的处理,这里就会有一个问题,随着业务的增加,channelRead()中需要判断的类型越来越多,就需要写很多的if-else来判断,这个方法的代码就会越来越多 另外,每次发指令数据包都要手动调用编码器编码成ByteBuf,对于这类场景的编码优化,我们能想到的办法自然是模块化处理,不同的逻辑放置到单独的类来处理,最后将这些逻辑串联起来,形成一个完整的逻辑处理链. Netty 中的 pipeline 和 channelHandler 正是用来解决这个问题的:它通过责任链设计模式来组织代码逻辑,并且能够支持逻辑的动态添加和删除,Netty 能够支持各类协议的扩展,比如 HTTP,Websocket,Redis,靠的就是 pipeline 和 channelHandler pipeline 和 channelHandler netty整个框架中,一个连接对应一个Channel,这条Channel所有的处理逻辑都在一个ChannelPipeline对象里面,ChannelPipeline是一个双向链表结构,和Channel是一对一的关系 ChannelPipeline里面每个节点都是一个ChannelHandlerContext对象,这个对象能够拿到和 Channel 相关的所有的上下文信息,然后这个对象包着一个重要的对象,那就是逻辑处理器 ChannelHandler channelHandler先来看一下channelHandler的类图 可以看到channelHandler有两大子接口,分别是ChannelInboundHandler和ChannelOutBoundHandler ChannelInboundHandler: 这个接口主要是用来处理读数据的逻辑,比如,我们在一端读到一段数据,首先要解析这段数据,然后对这些数据做一系列逻辑处理,最终把响应写到对端,在开始组装响应之前的所有的逻辑,都可以放置在 ChannelInboundHandler 里处理,它的一个最重要的方法就是 channelRead(). ChannelOutBoundHandler: 这个接口是用来处理写数据的逻辑,它是定义我们一端在组装完响应之后,把数据写到对端的逻辑,比如,我们封装好一个 response 对象,接下来我们有可能对这个 response 做一些其他的特殊逻辑,然后,再编码成 ByteBuf,最终写到对端,它里面最核心的一个方法就是 write(). 这两个接口分别有对应的默认实现,ChannelInboundHandlerAdapter和ChanneloutBoundHandlerAdapter它们分别实现了两大接口的所有功能,默认情况下会把读写事件传播到下一个 handler ChannelInboundHandler 的事件传播下面通过一个案例看一下ChannelInboundHandler的事件传播 新建三个ChannelInboundHandler,分别是InBoundHandlerA,InBoundHandlerB,InBoundHandlerC,代码分别如下:12345678910@Slf4jpublic class InBoundHandlerA extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; log.info(&quot;InBoundHandlerA: &#123;&#125;&quot;, msg); super.channelRead(ctx, msg); &#125; &#125; 12345678910@Slf4jpublic class InBoundHandlerB extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; log.info(&quot;InBoundHandlerB: &#123;&#125;&quot;, msg); super.channelRead(ctx, msg); &#125;&#125; 12345678910@Slf4jpublic class InBoundHandlerC extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; log.info(&quot;InBoundHandlerC: &#123;&#125;&quot;, msg); super.channelRead(ctx, msg); &#125;&#125; 建好都重写一下channelRead()方法,打印当前的handler信息,然后调用父类的channelRead()方法. 然后是NettyServer中把这三个类加进去,NettyServer.java中部分代码如下:123456789101112.childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; @Override protected void initChannel(NioSocketChannel ch) &#123; log.info(&quot;取出childAttr属性:&#123;&#125;&quot;, ch.attr(CommonConfig.CLIENT_KEY).get());// ch.pipeline().addLast(new ServerHandler()); ch.pipeline().addLast(new InBoundHandlerA()); ch.pipeline().addLast(new InBoundHandlerB()); ch.pipeline().addLast(new InBoundHandlerC()); &#125;&#125;); 这里就是通过addLast()方法去添加inBoundHandler,然后顺序是A-&gt;B-&gt;C,最后启动服务端和客户端,看一下服务端的控制台输出,如下:这里输出的顺序和我们addLast()的顺序是一样的 在每个inBoundHandler中的channelRead()方法最后都调用了父类的channelRead(),而父类的channelRead()会自动调用下一个inBoundHandler的channelRead()方法,并且会把当前inBoundHandler里处理完毕的对象传递到下一个inBoundHandler 结论inBoundHandler的执行顺序与我们通过addLast()方法添加的顺序保持一致 ChannelOutboundHandler 的事件传播和上面的例子一样,这次我们先新建三个outBoundHandler,分别是OutBoundHandlerA,OutBoundHandlerB,OutBoundHandlerC,然后都继承ChannelOutboundHandlerAdapter,重写write()方法,write()方法中输出以下当前的信息,然后调用父类的write()方法,具体代码如下:12345678910@Slf4jpublic class OutBoundHandlerA extends ChannelOutboundHandlerAdapter &#123; @Override public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception &#123; log.info(&quot;OutBoundHandlerA: &#123;&#125;&quot;, msg); super.write(ctx, msg, promise); &#125;&#125; 123456789@Slf4jpublic class OutBoundHandlerB extends ChannelOutboundHandlerAdapter &#123; @Override public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception &#123; log.info(&quot;OutBoundHandlerB: &#123;&#125;&quot;, msg); super.write(ctx, msg, promise); &#125;&#125; 12345678910@Slf4jpublic class OutBoundHandlerC extends ChannelOutboundHandlerAdapter &#123; @Override public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception &#123; log.info(&quot;OutBoundHandlerC: &#123;&#125;&quot;, msg); super.write(ctx, msg, promise); &#125;&#125; 然后再NettyServer中把这三个加进去,部分代码如下:123456789101112131415.childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; @Override protected void initChannel(NioSocketChannel ch) &#123; log.info(&quot;取出childAttr属性:&#123;&#125;&quot;, ch.attr(CommonConfig.CLIENT_KEY).get());// ch.pipeline().addLast(new ServerHandler()); ch.pipeline().addLast(new InBoundHandlerA()); ch.pipeline().addLast(new InBoundHandlerB()); ch.pipeline().addLast(new InBoundHandlerC()); ch.pipeline().addLast(new OutBoundHandlerA()); ch.pipeline().addLast(new OutBoundHandlerB()); ch.pipeline().addLast(new OutBoundHandlerC()); &#125;&#125;); 最后,要想触发outBoundHandler中的write()方法,就必须向客户端写数据, 所以我们修改一下InBoundHandlerC中的channelRead(),最后想客户端写数据,代码如下:12345678910@Slf4jpublic class InBoundHandlerC extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; log.info(&quot;InBoundHandlerC: &#123;&#125;&quot;, msg); ctx.channel().writeAndFlush(msg); &#125;&#125; 然后启动服务端,客户端看一下控制台输出,如下:我们通过addLast()方法添加的顺序是A-&gt;B-&gt;C,然后最后控制台输出的顺序是相反的 结论outBoundHandler的执行顺序与我们通过addLast()方法添加的顺序相反 传递事件其实说白了就是把本 handler 的处理结果传递到下一个 handler 继续处理 pipeline 的结构及执行顺序结构图如下:不管我们定义的是哪种类型的 handler, 最终它们都是以双向链表的方式连接，这里实际链表的节点是 ChannelHandlerContext 执行顺序图如下:虽然两种类型的 handler 在一个双向链表里,但是这两类 handler 的分工是不一样的,inBoundHandler 的事件通常只会传播到下一个 inBoundHandler,outBoundHandler 的事件通常只会传播到下一个 outBoundHandler,两者相互不受干扰. 完整代码已上传到github, 传送门]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-7-案例-客户端与服务端收发消息]]></title>
    <url>%2F2019%2F07%2F02%2FNetty-7-%E6%A1%88%E4%BE%8B-%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E6%9C%8D%E5%8A%A1%E7%AB%AF%E6%94%B6%E5%8F%91%E6%B6%88%E6%81%AF%2F</url>
    <content type="text"><![CDATA[述上文中实现了客户端登录的案例,主要就是创建数据包,编码,写到服务端,然后解码,验证,再编码发送到客户端,客户端最后再解码 本文再来实现一个客户端收发消息的案例,具体功能如下:在控制台输入一串文字,然后按回车,客户端校验登录状态,然后把这个消息传给服务端,服务端收到之后打印出来,并向客户端发送一条消息,客户端收到之后打印 其实跟上个登录的案例的流程是差不多的,下面来看一下具体的实现 收发消息数据包上面我们在登录的时候是创建了一个数据包LoginRequestPacket,每次请求就把数据包编码发送过去,然后服务端给个响应的LoginResponsePacket数据包 这里我们收发消息也是一样的,需要两个数据包,一个客户端发送消息的数据包,一个服务端响应的数据包,代码如下: 客户端发送消息的数据包:1234567891011@Datapublic class MessageRequestPacket extends Packet &#123; private String message; @Override public Byte getCommand() &#123; return Command.MESSAGE_REQUEST; &#125;&#125; 服务端响应的数据包:1234567891011@Datapublic class MessageResponsePacket extends Packet &#123; private String message; @Override public Byte getCommand() &#123; return Command.MESSAGE_RESPONSE; &#125;&#125; 然后就是Command类里面的命令标识,添加以下两个:123456789/** * 客户端发送消息命令 */Byte MESSAGE_REQUEST = 3 ;/** * 服务端响应发送消息命令 */Byte MESSAGE_RESPONSE = 4; 最后记得在PacketCodeC类里面的packetTypeMap这个map,把新建的这两个数据包put进去:12packetTypeMap.put(MESSAGE_REQUEST, MessageRequestPacket.class);packetTypeMap.put(MESSAGE_RESPONSE, MessageResponsePacket.class); 登录状态记录客户端每次发送消息的时候,都需要去判断客户端是否已经登录了,这里就需要一个登录状态的标识,这个标识可以绑定在channel里面,通过channel.attr(xxx).set(xx)的方式,然后用的时候就可以取出来判断有没有登录 所以这里先要定义一个登录成功的标识,新建Attributes接口,代码如下:12345678public interface Attributes &#123; /** * 客户端是否登录的标识 */ AttributeKey&lt;Boolean&gt; LOGIN = AttributeKey.newInstance(&quot;login&quot;);&#125; 我们之前客户端登录成功后,服务端会给是否登录成功的响应,然后在客户端登录成功的时候,去把这个标记存到channel里面,ClientHandler中,修改的部分代码如下:1234567891011121314151617181920212223242526@Overridepublic void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; ByteBuf byteBuf = (ByteBuf) msg; // 收到服务端发来的消息后,解码 Packet packet = PacketCodeC.INSTANCE.decode(byteBuf); // 判断是不是登录请求数据包 if (packet instanceof LoginResponsePacket) &#123; LoginResponsePacket loginResponsePacket = (LoginResponsePacket) packet; if (loginResponsePacket.getSuccess()) &#123; log.info(&quot;登录成功....&quot;); // 记录登录成功的标识 LoginUtil.markAsLogin(ctx.channel()); &#125; else &#123; log.info(&quot;登录失败,原因:&#123;&#125;&quot;, loginResponsePacket.getReason()); &#125; &#125; else if (packet instanceof MessageResponsePacket) &#123; MessageResponsePacket messageResponsePacket = (MessageResponsePacket) packet; log.info(&quot;收到了服务端发来的消息:&#123;&#125;&quot;, messageResponsePacket.getMessage()); &#125;&#125; 这里是抽出来一个工具类LoginUtil,如下:1234567891011121314151617181920public class LoginUtil &#123; /** * 标识已经登录 * @param channel */ public static void markAsLogin(Channel channel) &#123; channel.attr(Attributes.LOGIN).set(true); &#125; /** * 判断当前是否有登录的标识(只要标识存在,不管标识的值是什么) * @param channel * @return 是否已经登录 */ public static boolean hasLogin(Channel channel) &#123; return channel.attr(Attributes.LOGIN).get() != null; &#125;&#125; 一个是存登录标识的方法,一个是判断是否已经登录的方法 客户端发送消息给服务端在控制台输入一串文字,然后按回车,客户端校验登录状态,然后把这个消息传给服务端,需求是这样的. 所以我们在客户端连接成功之后,需要开启一个接受用户输入数据,发送到服务端的线程, 在NettyClient中,部分修改的代码如下:1234567891011121314151617181920212223242526272829303132333435363738public static void connect(Bootstrap bootstrap, String host, int port, int retry) &#123; bootstrap.connect(host, port).addListener(future -&gt; &#123; if (future.isSuccess()) &#123; log.info(&quot;连接成功&quot;); // 启动控制台线程 Channel channel = ((ChannelFuture) future).channel(); startConsoleThread(channel); &#125; // ... 省略其他代码 &#125;);&#125; public static void startConsoleThread(Channel channel) &#123; new Thread(() -&gt; &#123; // 线程没有被中断就继续循环 while (!Thread.interrupted()) &#123; // 判断是否已经登录 if (LoginUtil.hasLogin(channel)) &#123; log.info(&quot;当前客户端已经登录,请输入要发送的消息...&quot;); // 然后接收用户输入的数据 Scanner sc = new Scanner(System.in); String line = sc.nextLine(); // 封装到数据包里,然后转码发送给服务端 MessageRequestPacket messageRequestPacket = new MessageRequestPacket(); messageRequestPacket.setMessage(line); // 编码 ByteBuf buffer = PacketCodeC.INSTANCE.encode(channel.alloc(), messageRequestPacket); // 发送 channel.writeAndFlush(buffer); &#125; &#125; &#125;).start();&#125; 这里就是在客户端连接成功之后,开启一个线程然后是判断只要是登录状态,就可以在控制台输入消息,然后从控制台获取到用户输入的内容之后,转成数据包对象,再进行编码,发送 服务端处理消息服务端收到客户端传过来的消息之后打印出来,并向客户端发送一条消息 服务端接收消息还是和之前的一样,在ServerHandler的channelRead()方法里面,代码如下:1234567891011121314151617181920212223@Overridepublic void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; // 收到消息后解码 ByteBuf byteBuf = (ByteBuf) msg; Packet packet = PacketCodeC.INSTANCE.decode(byteBuf); // 判断是不是登录请求数据包 if (packet instanceof LoginRequestPacket) &#123; // ...省略登录的代码 &#125; else if (packet instanceof MessageRequestPacket)&#123; // 处理客户端发过来的消息 MessageRequestPacket messageRequestPacket = (MessageRequestPacket) packet; log.info(&quot;&#123;&#125;:收到客户端发过来的消息:&#123;&#125;&quot;, new Date(), messageRequestPacket.getMessage()); // 然后服务端发消息给客户端 MessageResponsePacket messageResponsePacket = new MessageResponsePacket(); messageResponsePacket.setMessage(&quot;服务端收到了[&quot; + messageRequestPacket.getMessage() + &quot;]这条消息&quot;); ByteBuf responseBuffer = PacketCodeC.INSTANCE.encode(ctx.alloc(), messageResponsePacket); ctx.channel().writeAndFlush(responseBuffer); &#125;&#125; 这里我们客户端发过来的数据包是一个MessageRequestPacket类型的,所以只需要每次判断是哪种类型的数据包,根据类型做出相应的处理就ok了,最后是把响应的数据包MessageResponsePacket传给客户端 客户端接收消息客户端需要接收服务端发过来的消息,也是在ClientHandler的channelRead()方法里面,代码如下:12345678910111213141516@Overridepublic void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; ByteBuf byteBuf = (ByteBuf) msg; // 收到服务端发来的消息后,解码 Packet packet = PacketCodeC.INSTANCE.decode(byteBuf); // 判断是不是登录请求数据包 if (packet instanceof LoginResponsePacket) &#123; // ...省略登录处理的代码 &#125; else if (packet instanceof MessageResponsePacket) &#123; MessageResponsePacket messageResponsePacket = (MessageResponsePacket) packet; log.info(&quot;收到了服务端发来的消息:&#123;&#125;&quot;, messageResponsePacket.getMessage()); &#125;&#125; 这里和服务端接收消息是一毛一样的,都是先转成Packet,然后判断是哪种类型,做出相应的处理即可 测试最后先后运行NettyServer和NettyClient,然后在NettyClient的控制台输入消息,回车,看两个控制台的输出,如下: 客户端: 服务端: 总结此案例流程图如下:图片来源,侵删 完整代码已上传到github, 传送门]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-6-案例-客户端登录]]></title>
    <url>%2F2019%2F07%2F02%2FNetty-6-%E6%A1%88%E4%BE%8B-%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%99%BB%E5%BD%95%2F</url>
    <content type="text"><![CDATA[述上文中,说了客户端服务端的通信协议以及编解码,本文延续上文,实现一个客户端的登录 具体流程就是客户端去发送登录的数据包,然后服务端解码,验证是否可以登录,然后返回给客户端是否登录成功 我们之前有写过一个客户端服务端通信的案例,就是给客户端和服务端启动的时候都添加一个逻辑处理器,写数据就重写channelActive()方法,读数据就重写channelRead()方法,具体的可以回顾之前的文章(Netty-3-客户端服务端通信案例) 准备工作首先,这里改造了一下上文中的PacketCodeC类, 就是用来编解码的类,改成了单例模式的,然后把 ByteBuf 分配器抽取出一个参数,第一个实参是 ctx.alloc() 获取的就是与当前连接相关的 ByteBuf 分配器,建议这样来使用,具体修改后的代码如下:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public class PacketCodeC &#123; private static final int MAGIC_NUMBER = 0x12345678; private final Map&lt;Byte, Class&lt;? extends Packet&gt;&gt; packetTypeMap; private final Map&lt;Byte, Serializer&gt; serializerMap; public static final PacketCodeC INSTANCE = new PacketCodeC(); private PacketCodeC() &#123; packetTypeMap = new HashMap&lt;&gt;(); packetTypeMap.put(LOGIN_REQUEST, LoginRequestPacket.class); serializerMap = new HashMap&lt;&gt;(); Serializer serializer = new JSONSerializer(); serializerMap.put(serializer.getSerializerAlgorithm(), serializer); &#125; /** * 编码 * @param packet * @return ByteBuf */ public ByteBuf encode(ByteBufAllocator byteBufAllocator, Packet packet) &#123; // 创建ByteBuf对象 ByteBuf byteBuf = byteBufAllocator.ioBuffer(); // 序列化java对象 byte[] bytes = Serializer.DEFAULT.serialize(packet); // 实际编码过程 byteBuf.writeInt(MAGIC_NUMBER); byteBuf.writeByte(packet.getVersion()); byteBuf.writeByte(Serializer.DEFAULT.getSerializerAlgorithm()); byteBuf.writeByte(packet.getCommand()); byteBuf.writeInt(bytes.length); byteBuf.writeBytes(bytes); return byteBuf; &#125; /** * 解码 * @param byteBuf * @return Packet */ public Packet decode(ByteBuf byteBuf) &#123; // 跳过魔数 byteBuf.skipBytes(4); // 跳过版本号 byteBuf.skipBytes(1); // 序列化算法标识 byte serializeAlgorithm = byteBuf.readByte(); // 指令 byte command = byteBuf.readByte(); // 数据包长度 int length = byteBuf.readInt(); byte[] bytes = new byte[length]; byteBuf.readBytes(bytes); // 具体数据内容 Class&lt;? extends Packet&gt; requestType = getRequestType(command); Serializer serializer = getSerializer(serializeAlgorithm); if (requestType != null &amp;&amp; serializer != null) &#123; return serializer.deserialize(requestType, bytes); &#125; return null; &#125; private Serializer getSerializer(byte serializeAlgorithm) &#123; return serializerMap.get(serializeAlgorithm); &#125; private Class&lt;? extends Packet&gt; getRequestType(byte command) &#123; return packetTypeMap.get(command); &#125;&#125; 客户端发送登录请求新建一个客户端的逻辑处理器,用来发送登录数据包.代码如下:123456789101112131415161718192021222324@Slf4jpublic class ClientHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; log.info(&quot;&#123;&#125;,客户端开始登录...&quot;, new Date()); // 创建登录的对象 LoginRequestPacket loginRequestPacket = new LoginRequestPacket(); loginRequestPacket.setUsername(&quot;zhangsan&quot;); loginRequestPacket.setPassword(&quot;123456&quot;); loginRequestPacket.setUserId(1); // 编码 ByteBuf byteBuf = PacketCodeC.INSTANCE.encode(ctx.alloc(), loginRequestPacket); // 写到服务端 ctx.channel().writeAndFlush(byteBuf); &#125; &#125; 这里就是先是继承ChannelInboundHandlerAdapter类,然后重写了channelActive()方法,当客户端连接到服务端之后,Netty会回调ClientHandler 的 channelActive()方法,然后,我们就在这个方法里面去构建数据包,然后转码,通过writeAndFlush()方法将数据写到服务端 最后就是在客户端启动的时候把这个逻辑处理器加进去就行了,在NettyCilent.java中添加逻辑处理器,部分代码如下:123456.handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; socketChannel.pipeline().addLast(new ClientHandler()); &#125;&#125;); 到这儿客户端发送数据的操作就完了,接下来是服务端接收数据然后解码,验证是否可以登录 服务端接收数据并验证服务端也是新建一个逻辑处理器,用来接收客户端发来的数据,解码验证,具体代码如下:123456789101112131415161718192021222324252627public class ServerHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; // 收到消息后解码 ByteBuf byteBuf = (ByteBuf) msg; Packet packet = PacketCodeC.INSTANCE.decode(byteBuf); // 判断是不是登录请求数据包 if (packet instanceof LoginRequestPacket) &#123; LoginRequestPacket loginRequestPacket = (LoginRequestPacket) packet; // 校验用户名密码 if (valid(loginRequestPacket)) &#123; // 登录成功 &#125; else &#123; // 登录失败 &#125; &#125; &#125; private boolean valid(LoginRequestPacket loginRequestPacket) &#123; return true; &#125;&#125; 这里也是继承ChannelInboundHandlerAdapter,然后重写了channelRead()方法,netty收到数据之后,会回调这个方法,然后我们解码,校验数据(这里所有情况都返回true了) 然后服务端校验完成之后,要把登录结果返回给客户端 服务端返回校验结果这里就又需要一个服务端响应的数据包了,,代码如下:123456789101112@Datapublic class LoginResponsePacket extends Packet &#123; private Boolean success; private String reason; @Override public Byte getCommand() &#123; return Command.LOGIN_RESPONSE; &#125;&#125; 然后Command中定义登录响应的命令是2:1Byte LOGIN_RESPONSE = 2; 然后就是在ServerHandler中,服务端验证完成之后,新建响应的数据包,返回给客户端,修改后的代码如下:12345678910111213141516171819202122232425262728293031@Overridepublic void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; // 收到消息后解码 ByteBuf byteBuf = (ByteBuf) msg; Packet packet = PacketCodeC.INSTANCE.decode(byteBuf); // 判断是不是登录请求数据包 if (packet instanceof LoginRequestPacket) &#123; LoginRequestPacket loginRequestPacket = (LoginRequestPacket) packet; LoginResponsePacket loginResponsePacket = new LoginResponsePacket(); loginRequestPacket.setVersion(loginRequestPacket.getVersion()); // 校验用户名密码 if (valid(loginRequestPacket)) &#123; // 登录成功 loginResponsePacket.setSuccess(true); &#125; else &#123; // 登录失败 loginResponsePacket.setSuccess(false); loginResponsePacket.setReason(&quot;账号密码错误&quot;); &#125; // 编码返回给客户端 ByteBuf responseBuffer = PacketCodeC.INSTANCE.encode(ctx.alloc(), loginResponsePacket); ctx.channel().writeAndFlush(responseBuffer); &#125;&#125; 客户端接收返回结果客户端接收数据处理也是需要重写channelRead()方法,代码如下(也是在ClientHandler类里):12345678910111213141516171819@Overridepublic void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; ByteBuf byteBuf = (ByteBuf) msg; // 收到服务端发来的消息后,解码 Packet packet = PacketCodeC.INSTANCE.decode(byteBuf); // 判断是不是登录请求数据包 if (packet instanceof LoginResponsePacket) &#123; LoginResponsePacket loginResponsePacket = (LoginResponsePacket) packet; if (loginResponsePacket.getSuccess()) &#123; log.info(&quot;登录成功....&quot;); &#125; else &#123; log.info(&quot;登录失败,原因:&#123;&#125;&quot;, loginResponsePacket.getReason()); &#125; &#125;&#125; 这里还需要注意一点,在PacketCodeC类里面,需要在packetTypeMap这个map里面配置上我们刚刚的响应数据包.1packetTypeMap.put(LOGIN_RESPONSE, LoginResponsePacket.class); 这里就是客户端拿到服务端发过来的数据,然后解码打印,就完成了 最后依次启动服务端和客户端就可以看到效果了,客户端控制台输出如下: 总结具体流程图如下:图片来源,侵删 完整代码已上传到github, 传送门]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-5-客户端与服务端通信协议编解码]]></title>
    <url>%2F2019%2F07%2F02%2FNetty-5-%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E6%9C%8D%E5%8A%A1%E7%AB%AF%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE%E7%BC%96%E8%A7%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[述客户端与服务端的通信协议,就是客户端与服务端事先商量好的,每个二进制数据包中的每一段字节分别代表什么含义 看一下下面这个图: 如图,就是一个数据包,这个数据包中,第一个字节是1,表示是一个指令,比如是登录还是注册,然后下面是用户名,密码,中间有分隔符做分割 服务端收到这个数据包后,就能按这个格式取出用户名密码,然后执行对应的操作,在实际的通信协议设计中,会比这个要复杂 编解码呢,就是客户端按照协议去组装数据发送给服务端,然后服务端也按照协议去解析数据 客户端服务端通信过程如图:客户端服务端的通信过程就是.首先客户端把Java对象按照通信协议转换成二进制数据包,然后通过网络把这段二进制数据包发送到服务端,数据的传输过程由TCP/IP协议负责数据的传输 服务端接收到数据按照协议截取二进制数据包的相应字段包装成Java对象交给应用逻辑处理,服务端处理完毕如果需要返回响应给客户端按照相同的流程进行. 通信协议设计 第一部分: 魔数,通常情况为固定的几个字节 第二部分: 版本号 第三部分: 序列化算法,表示通过哪种序列化方式,把Java对象转为二进制对象,二进制对象又如何转为Java对象 第四部分: 指令,表示具体的操作,这里用一个字节最高能有256种指令 第五部分: 数据部分的长度 第六部分: 具体的数据内容 通常情况下,这样一套标准的协议能够适配大多数情况下的服务端与客户端的通信场景,接下来我们就来看一下我们如何使用 Netty 来实现这套协议. 通信协议实现编码首先调用ByteBuf分配器ByteBufAllocator,创建ByteBuf对象,使用ioBuffer()方法返回适配io读写相关的内存,它会尽可能创建一个直接内存,直接内存可以理解为不受 jvm 堆管理的内存空间,写到 IO 缓冲区的效果更高 第二步,把Java对象序列化成二进制数据包 最后,按照通信协议逐个往ByteBuf对象写入字段即实现编码. 解码解码就是根据通信协议,将上面传过来的ByteBuf对象解析,读取到我们需要的数据 具体实现代码通信数据包首先,我们的服务端客户端都是通过数据包去通信的,然后我们可以把这个数据包抽象出来,代码如下:123456789101112131415@Datapublic abstract class Packet &#123; /** * 协议版本号 */ private Byte version = 1; /** * 获取指令 * @return 指令 */ public abstract Byte getCommand();&#125; 然后,以后用到的数据包就都继承这个抽象类就好了,比如说我们下面用到的登录请求的数据包,代码如下:1234567891011121314@Datapublic class LoginRequestPacket extends Packet&#123; private Integer userId; private String username; private String password; @Override public Byte getCommand() &#123; return Command.LOGIN_REQUEST; &#125;&#125; 这里有一个getCommand()方法,是重写的父类的方法,用来标识是什么命令,Command里面就放了个常量,如下:12345public interface Command &#123; Byte LOGIN_REQUEST = 1;&#125; 序列化工具类我们编解码需要把Java对象转成二进制数据,二进制数据转成Java对象,所以,还需要一个序列化的类,序列化可以有多种实现方式,所以先搞一个接口出来,Serializer,代码如下:12345678910111213141516171819202122232425public interface Serializer &#123; /** * json 序列化 */ byte JSON_SERIALIZER = 1; Serializer DEFAULT = new JSONSerializer(); /** * 序列化算法 */ byte getSerializerAlgorithm(); /** * java 对象转换成二进制 */ byte[] serialize(Object object); /** * 二进制转换成 java 对象 */ &lt;T&gt; T deserialize(Class&lt;T&gt; clazz, byte[] bytes);&#125; 然后这里我们默认就用阿里巴巴的fastjson去序列化,这里给了个默认的DEFAULT,是实例化了一个JSONSerializer,JSONSerializer的代码如下:12345678910111213141516public class JSONSerializer implements Serializer &#123; @Override public byte getSerializerAlgorithm() &#123; return SerializerAlgorithm.JSON; &#125; @Override public byte[] serialize(Object object) &#123; return JSON.toJSONBytes(object); &#125; @Override public &lt;T&gt; T deserialize(Class&lt;T&gt; clazz, byte[] bytes) &#123; return JSON.parseObject(bytes, clazz); &#125;&#125; 这个类就是实现了上面序列化接口,然后一个序列化方法,一个反序列化方法. 最后还有个SerializerAlgorithm,代码如下:12345678public interface SerializerAlgorithm &#123; /** * Json序列化标识 */ byte JSON = 1;&#125; 编解码序列化和数据包的都搞完了,然后就是编解码的方法了,代码如下:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283public class PacketCodeC &#123; private static final int MAGIC_NUMBER = 0x12345678; private static final Map&lt;Byte, Class&lt;? extends Packet&gt;&gt; packetTypeMap; private static final Map&lt;Byte, Serializer&gt; serializerMap; static &#123; packetTypeMap = new HashMap&lt;&gt;(); packetTypeMap.put(LOGIN_REQUEST, LoginRequestPacket.class); serializerMap = new HashMap&lt;&gt;(); Serializer serializer = new JSONSerializer(); serializerMap.put(serializer.getSerializerAlgorithm(), serializer); &#125; /** * 编码 * @param packet * @return ByteBuf */ public ByteBuf encode(Packet packet) &#123; // 创建ByteBuf对象 ByteBuf byteBuf = ByteBufAllocator.DEFAULT.ioBuffer(); // 序列化java对象 byte[] bytes = Serializer.DEFAULT.serialize(packet); // 实际编码过程 byteBuf.writeInt(MAGIC_NUMBER); byteBuf.writeByte(packet.getVersion()); byteBuf.writeByte(Serializer.DEFAULT.getSerializerAlgorithm()); byteBuf.writeByte(packet.getCommand()); byteBuf.writeInt(bytes.length); byteBuf.writeBytes(bytes); return byteBuf; &#125; /** * 解码 * @param byteBuf * @return Packet */ public Packet decode(ByteBuf byteBuf) &#123; // 跳过魔数 byteBuf.skipBytes(4); // 跳过版本号 byteBuf.skipBytes(1); // 序列化算法标识 byte serializeAlgorithm = byteBuf.readByte(); // 指令 byte command = byteBuf.readByte(); // 数据包长度 int length = byteBuf.readInt(); byte[] bytes = new byte[length]; byteBuf.readBytes(bytes); // 具体数据内容 Class&lt;? extends Packet&gt; requestType = getRequestType(command); Serializer serializer = getSerializer(serializeAlgorithm); if (requestType != null &amp;&amp; serializer != null) &#123; return serializer.deserialize(requestType, bytes); &#125; return null; &#125; private Serializer getSerializer(byte serializeAlgorithm) &#123; return serializerMap.get(serializeAlgorithm); &#125; private Class&lt;? extends Packet&gt; getRequestType(byte command) &#123; return packetTypeMap.get(command); &#125;&#125; 这里的packetTypeMap,是放了指令和对应的数据包类,然后serializerMap是存放了序列化的标识和具体的类 然后编解码就ok了 上面这些类的类图如下: 具体实现编解码的完整代码已经上传到了github]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-4-ByteBuf详解]]></title>
    <url>%2F2019%2F07%2F02%2FNetty-4-ByteBuf%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[述上文中,我们服务端与客户端之间的通信的数据都是通过ByteBuf来传输的,本文来详细的看一下这个ByteBuf ByteBuf数据结构ByteBuf数据结构如下图: 从图中来看,ByteBuf是一个字节容器,容器里面的数据一共有三个部分 第一部分是已经丢弃的字节,这部分数据是无效的 第二部分是可读字节,这部分数据是ByteBuf的主体数据,从ByteBuf里面读取的数据都来自这一部分 最后一部分的数据是可写字节,所有写到 ByteBuf 的数据都会写到这一段,虚线表示的是该 ByteBuf 最多还能扩容多少容量 这三部分内容是被两个指针给划分出来的,从左到右依次是读指针(readerIndex)、写指针(writerIndex),然后还有容量capacity表示ByteBuf底层内存的总容量. 数据读写读数据从 ByteBuf 中每读取一个字节, readerIndex 自增1,ByteBuf 里面总共有 writerIndex-readerIndex 个字节可读, 由此可以推论出当 readerIndex 与 writerIndex 相等的时, ByteBuf 不可读 写数据写数据是从 writerIndex 指向的部分开始写,每写一个字节,writerIndex 自增1,直到增到 capacity,这个时候,表示 ByteBuf 已经不可写了 最大容量ByteBuf 里面其实还有一个参数 maxCapacity,当向 ByteBuf 写数据的时候,如果容量不足,那么这个时候可以进行扩容,直到 capacity 扩容到 maxCapacity,超过 maxCapacity 就会报错 相关API容量API1readerIndex() 与 readerIndex(int) 前者表示返回当前的读指针 readerIndex, 后者表示设置读指针 1writeIndex() 与 writeIndex(int) 前者表示返回当前的写指针 writerIndex, 后者表示设置写指针 1markReaderIndex() 与 resetReaderIndex() 前者表示把当前的读指针保存起来，后者表示把当前的读指针恢复到之前保存的值 下面来看两个代码片段:123int readerIndex = buffer.readerIndex();// .. 其他操作buffer.readerIndex(readerIndex); 123buffer.markReaderIndex();// .. 其他操作buffer.resetReaderIndex(); 这两个代码的作用其实是一样的,都是先记录一下读指针当前的位置,然后做完其他操作之后在恢复到记录下来的位置,推荐使用下面的这种方法,不需要额外创建一个变量,常见使用场景为解析自定义协议的数据包. 1markWriterIndex() 与 resetWriterIndex() 这个是针对写指针的,作用和上面的一样 读写API关于 ByteBuf 的读写都可以看作从指针开始的地方开始读写数据 1writeBytes(byte[] src) 与 buffer.readBytes(byte[] dst) writeBytes()表示把字节数组 src 里面的数据全部写到 ByteBuf readBytes()指的是把 ByteBuf 里面的数据全部读取到 dst 这里 dst 字节数组的大小通常等于 readableBytes() ,而 src 字节数组大小的长度通常小于等于 writableBytes() 1writeByte(byte b) 与 buffer.readByte() riteByte() 表示往 ByteBuf 中写一个字节 buffer.readByte() 表示从 ByteBuf 中读取一个字节 类似的还有 writeBoolean()、writeChar()、writeShort()、writeInt()、writeLong()、writeFloat()、writeDouble() 与 readBoolean()、readChar()、readShort()、readInt()、readLong()、readFloat()、readDouble() 与读写 API 类似的 API 还有 getBytes()、getByte() 与 setBytes()、setByte() 系列,唯一的区别就是 get/set 不会改变读写指针,而 read/write 会改变读写指针,这点在解析数据的时候千万要注意 1release() 与 retain() Netty使用堆外内存,堆外内存是不被JVM直接管理的,申请到的内存无法被垃圾回收器直接回收需要手动回收,即申请到的内存必须手工释放,否则会造成内存泄漏. ByteBuf通过引用计数方式管理,如果ByteBuf没有地方被引用到,需要回收底层内存. 默认情况下,当创建完ByteBuf时,其引用为1,然后每次调用retain()方法,引用加1,调用release()方法原理是将引用计数减1,减完发现引用计数为0时,回收ByteBuf底层分配内存. 1slice()、duplicate()、copy() slice()方法会从原始 ByteBuf 中截取一段,这段数据是从 readerIndex 到 writeIndex(上图中的绿色部分,也就是原始ByteBuf的可读部分),同时,返回的新的 ByteBuf 的最大容量 maxCapacity 为原始 ByteBuf 的 readableBytes()简单来说,就是把原始的ByteBuf的可读部分单独截取出来成一个新的ByteBuf,然后最大长度就是原始ByteBuf的可读长度 duplicate()方法会把整个 ByteBuf 都截取出来,包括所有的数据,指针信息 slice()与duplicate()对比: 相同点: 底层内存以及引用计数与原始的ByteBuf共享,即经过slice()或者duplicate()返回的ByteBuf调用write系列方法都会影响到原始的ByteBuf,但是它们都维持着与原始ByteBuf相同的内存引用计数和不同的读写指针 不同点: slice()只截取从readerIndex到writerIndex之间的数据,返回的ByteBuf最大容量被限制到原始ByteBuf的readableBytes(),duplicate()是把整个ByteBuf都与原始的ByteBuf共享 还有一个是copy()方法,slice() 方法与 duplicate() 方法不会拷贝数据,它们只是通过改变读写指针来改变读写的行为,而最后一个方法 copy() 会直接从原始的 ByteBuf 中拷贝所有的信息,包括读写指针以及底层对应的数据,因此,往 copy() 返回的 ByteBuf 中写数据不会影响到原始的 ByteBuf slice()方法与duplicate()方法不会改变原始ByteBuf的引用计数,所以原始的ByteBuf调用release()之后发现引用计数为零的时候开始释放内存,调用这两个方法返回的ByteBuf也会被释放,此时如果再对它们进行读写会报错.因此需要调用一次retain()方法增加引用,表示它们对应的底层的内存多一次引用,引用计数为2,在释放内存的时候需要调用两次 release()方法将引用计数降到零才会释放内存 这三个方法均维护着自己的读写指针,与原始的 ByteBuf 的读写指针无关,相互之间不受影响 1retainedSlice() 与 retainedDuplicate() 这两个方法就是在截取片段的同时增加其内存的引用计数,这两个API等价于以下两端代码:1slice().retain(); 与 duplicate().retain(); 多个 ByteBuf 可以引用同一段内存,通过引用计数来控制内存的释放,遵循谁 retain() 谁 release() 的原则 使用到 slice 和 duplicate 方法的时候,千万要理清内存共享,引用计数共享,读写指针不共享几个概念 最后,一个ByteBuf的例子,放在了gitHub上,传送门]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-3-案例-客户端服务端通信]]></title>
    <url>%2F2019%2F07%2F02%2FNetty-3-%E6%A1%88%E4%BE%8B-%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%9C%8D%E5%8A%A1%E7%AB%AF%E9%80%9A%E4%BF%A1%2F</url>
    <content type="text"><![CDATA[述前面已经了解了netty服务端和客户端的启动流程,本文将通过一个小案例,来了解服务端和客户端是如何通信的 案例功能是,客户端连接成功后,向服务端写一段数据,服务端收到数据之后打印,并向客户端回一段数据,具体实现流程如下: 客户端发送数据到服务端上文中有说,客户端相关的数据读写是通过Bootstrap的handler()方法指定,handler()中创建了一个匿名类ChannelInitializer,重写了initChannel()方法,然后我们现在在这个方法里面加一个逻辑处理器,这个处理器的作用就是负责向服务端写数据,部分代码如下:123456.handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; socketChannel.pipeline().addLast(new FirstClientHandler()); &#125;&#125;); ch.pipeline()返回的是和这条连接相关的逻辑处理链,采用了责任链模式,然后调用addLast()方法添加逻辑处理器FirstClientHandler. FirstClientHandler的代码如下:12345678910111213141516171819202122232425262728@Slf4jpublic class FirstClientHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; log.info(new Date() + &quot;客户端写出数据&quot;); // 1. 获取数据 ByteBuf buffer = getByteBuf(ctx); // 2. 写数据 ctx.channel().writeAndFlush(buffer); &#125; private ByteBuf getByteBuf(ChannelHandlerContext ctx) &#123; // 获取二进制抽象 ByteBuf buffer = ctx.alloc().buffer(); // 准备数据,并指定编码格式 byte[] bytes = &quot;hello,world&quot;.getBytes(Charset.forName(&quot;utf-8&quot;)); // 填充数据到ByteBuf buffer.writeBytes(bytes); return buffer; &#125;&#125; 这个逻辑处理器继承自ChannelInboundHandlerAdapter,然后覆盖了channelActive()方法,这个方法会在客户端连接建立成功之后被调用 客户端连接建立成功之后,调用到channelActive()方法,在这个方法里面,我们编写向服务端写数据的逻辑 写数据的逻辑分为两步,首先我们需要获取一个 netty 对二进制数据的抽象ByteBuf,上面代码中,ctx.alloc() 获取到一个ByteBuf的内存管理器,这个 内存管理器的作用就是分配一个ByteBuf,然后我们把字符串的二进制数据填充到ByteBuf,这样我们就获取到了 Netty 需要的一个数据格式,最后我们调用ctx.channel().writeAndFlush()把数据写到服务端 Netty里面的数据是以ByteBuf为单位的,所有需要写出/读取的数据都得塞到一个ByteBuf,下面在来看看服务端读取数据的流程 服务端读取客户端数据服务端相关数据处理逻辑在ServerBootstrap的childHandler()方法中,该方法中也声明了一个匿名类ChannelInitializer,然后重写了initChannel()方法,同样的也是在这个方法里面加一个逻辑处理器,负责读取客户端传过来的数据,部分代码如下:12345678.childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; @Override protected void initChannel(NioSocketChannel ch) &#123; log.info(&quot;取出childAttr属性:&#123;&#125;&quot;, ch.attr(CommonConfig.CLIENT_KEY).get()); ch.pipeline().addLast(new FirstServerHandler()); &#125;&#125;); 这里方法里面的逻辑是和客户端一样的,先获取服务端关于这条连接的逻辑处理链pipeline,然后添加一个逻辑处理器FirstServerHandler,用来读取客户端发过来的数据 FirstServerHandler的代码如下:123456@Overridepublic void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; ByteBuf byteBuf = (ByteBuf) msg; log.info(new Date() + &quot;服务端读取数据:&#123;&#125;&quot;, byteBuf.toString(Charset.forName(&quot;utf-8&quot;)));&#125; 这个逻辑处理器跟客户端的不同,这里是重写的channelRead()方法,这个方法在接收到客户端发来的数据之后会被回调 这里的msg就是 Netty 里面数据读写的载体,这里需要进行一次强转,转成ByteBuf类型,然后用toString()方法就能获取到发送过来的数据 到现在,客户端发送消息和服务端接收消息都完成了,最后就剩下的是服务端给客户端发送消息了 这里跟客户端发送消息的逻辑是一样的,先创建一个ByteBuf,然后填充二进制数据,最后调用writeAndFlush()方法写出去. 完整代码如下:12345678910111213141516171819202122232425262728293031@Slf4jpublic class FirstServerHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; ByteBuf byteBuf = (ByteBuf) msg; log.info(new Date() + &quot;服务端读取数据:&#123;&#125;&quot;, byteBuf.toString(Charset.forName(&quot;utf-8&quot;))); // 给客户端返回消息 log.info(new Date() + &quot;服务端发送数据.....&quot;); ByteBuf outBuf = getByteBuf(ctx); ctx.channel().writeAndFlush(outBuf); &#125; private ByteBuf getByteBuf(ChannelHandlerContext ctx) &#123; // 获取二进制抽象 ByteBuf buffer = ctx.alloc().buffer(); // 准备数据,并指定编码格式 byte[] bytes = &quot;hello,world from server&quot;.getBytes(Charset.forName(&quot;utf-8&quot;)); // 填充数据到ByteBuf buffer.writeBytes(bytes); return buffer; &#125;&#125; 就是服务端在收到客户端的消息之后,给客户端再发一条数据就ok,发送消息的流程是和客户端的是一样的 测试上面的都搞定之后,先后运行服务端和客户端,然后看一下控制台输出 客户端控制台输出如下:1210:39:04.141 [nioEventLoopGroup-2-1] INFO com.example.netty.netty.NettyClient - 连接成功10:39:04.147 [nioEventLoopGroup-2-1] INFO com.example.netty.handler.FirstClientHandler - Sat Jun 01 10:39:04 CST 2019客户端写出数据 然后服务端控制台输出如下:123456789101110:38:53.238 [nioEventLoopGroup-2-1] INFO com.example.netty.netty.NettyServer - 服务端启动中...............10:38:53.243 [nioEventLoopGroup-2-1] INFO com.example.netty.netty.NettyServer - 端口:8000&gt;绑定成功10:39:04.155 [nioEventLoopGroup-3-1] INFO com.example.netty.netty.NettyServer - 取出childAttr属性:clientValue10:39:04.219 [nioEventLoopGroup-3-1] DEBUG io.netty.util.Recycler - -Dio.netty.recycler.maxCapacityPerThread: 3276810:39:04.220 [nioEventLoopGroup-3-1] DEBUG io.netty.util.Recycler - -Dio.netty.recycler.maxSharedCapacityFactor: 210:39:04.227 [nioEventLoopGroup-3-1] DEBUG io.netty.util.Recycler - -Dio.netty.recycler.linkCapacity: 1610:39:04.228 [nioEventLoopGroup-3-1] DEBUG io.netty.util.Recycler - -Dio.netty.recycler.ratio: 810:39:04.252 [nioEventLoopGroup-3-1] DEBUG io.netty.buffer.AbstractByteBuf - -Dio.netty.buffer.bytebuf.checkAccessible: true10:39:04.256 [nioEventLoopGroup-3-1] DEBUG io.netty.util.ResourceLeakDetectorFactory - Loaded default ResourceLeakDetector: io.netty.util.ResourceLeakDetector@6e9133c710:39:04.270 [nioEventLoopGroup-3-1] INFO com.example.netty.handler.FirstServerHandler - Sat Jun 01 10:39:04 CST 2019服务端读取数据:hello,world10:39:04.270 [nioEventLoopGroup-3-1] INFO com.example.netty.handler.FirstServerHandler - Sat Jun 01 10:39:04 CST 2019服务端发送数据..... 总结 客户端服务端的逻辑处理都是在启动的时候,通过逻辑处理链pipeline添加逻辑处理器,去处理逻辑 客户端连接成功后,会调用逻辑处理器的channelActive()方法 不管是服务端还是客户端,收到数据都会调用逻辑处理器的channelRead()方法 写数据通过调用channel.writeAndFlush()方法,需要把发送的数据塞到ByteBuf对象然后传进去 总体逻辑如下图: 图片来源,侵删 本文完整代码已上传gitHub,传送门]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-2-客户端启动流程]]></title>
    <url>%2F2019%2F07%2F02%2FNetty-2-%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[述上文中说了服务端的启动流程以及一些额外的设置等, 本文再来详细看一下客户端的启动流程 NettyClient解析代码如下:123456789101112131415161718192021222324252627public class NettyClient &#123; public static void main(String[] args) &#123; NioEventLoopGroup workerGroup = new NioEventLoopGroup(); Bootstrap bootstrap = new Bootstrap(); bootstrap // 1.指定线程模型 .group(workerGroup) // 2.指定 IO 类型为 NIO .channel(NioSocketChannel.class) // 3.IO 处理逻辑 .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel ch) &#123; &#125; &#125;); // 4.建立连接 bootstrap.connect(&quot;127.0.0.1&quot;, 80).addListener(future -&gt; &#123; if (future.isSuccess()) &#123; System.out.println(&quot;连接成功!&quot;); &#125; else &#123; System.err.println(&quot;连接失败!&quot;); &#125; &#125;); &#125;&#125; 跟服务端的启动流程是类似的,首先要创建一个引导类Bootstrap,跟服务端的引导类不同,服务端的是ServerBootstrap 线面第一步就是指定线程模型,驱动着连接的数据读写,类似第一篇文章中的IOClient.java类创建的线程 然后指定IO模型是NioSocketChannel,表示是IO模型是NIO, 这里也可以设置成OioSocketChannel,但是一般不会这么做 然后给引导类指定一个handler,主要就是定义连接的业务处理逻辑 最后一步,建立和服务端的连接,传入的参数是ip/域名,和端口号, 然后又加了个监听,监听是否连接成功. 总体大致流程如下:图片来源,侵删 失败重连在网络情况差的情况下,客户端第一次连接可能会连接失败,这个时候我们可能会尝试重新连接,而且要给定重试次数以及每次间隔时间 通常情况下,连接建立失败不会立即重新连接,而是会通过一个指数退避的方式,比如每隔 1 秒、2 秒、4 秒、8 秒，以 2 的幂次来建立连接,然后到达一定次数之后就放弃连接 我们把connect()方法抽出来,实现重新连接,代码如下:123456789101112131415161718public static void connect(Bootstrap bootstrap, String host, int port, int retry) &#123; bootstrap.connect(host, port).addListener(future -&gt; &#123; if (future.isSuccess()) &#123; log.info(&quot;连接成功&quot;); &#125; else if (retry == 0) &#123; log.info(&quot;重试次数已经用完了,连接失败&quot;); &#125; else &#123; // 第几次重连 int order = (CommonConfig.MAX_RETRY - retry) + 1; // 本次重连的间隔 int delay = 1 &lt;&lt; order; log.info(&quot;连接失败,进行第&#123;&#125;次重连&quot;, order); bootstrap.config().group() .schedule(() -&gt; connect(bootstrap, host, port, retry - 1), delay, TimeUnit.SECONDS); &#125; &#125;);&#125; 通过判断连接是否成功以及剩余重试次数,分别执行不同的逻辑. 如果连接成功,就输出连接成功,如果重试次数用完,就放弃,如果连接失败而且重试次数还没有用完的话,就计算下一次重连间隔delay,然后定期重连 最后,定时任务是调用bootstrap.config().group().schedule(),其中bootstrap.config()这个方法返回的是BootstrapConfig,他是对Bootstrap配置参数的抽象,然后bootstrap.config().group() 返回的就是我们在一开始的时候配置的线程模型workerGroup,调workerGroup 的 schedule 方法即可实现定时任务逻辑 客户端启动时的一些其他功能NettyClient.java代码如下:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@Slf4jpublic class NettyClient &#123; public static void main(String[] args) throws InterruptedException &#123; Bootstrap bootstrap = new Bootstrap(); NioEventLoopGroup group = new NioEventLoopGroup(); bootstrap // 1.指定线程模型 .group(group) // 2.指定 IO 类型为 NIO .channel(NioSocketChannel.class) // 3.IO 处理逻辑 .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; &#125; &#125;); bootstrap // attr() 给客户端的channel即NioServerSocketChannel指定一些自定义属性,通过channel.attr()取出该属性,给NioServerSocketChannel维护一个map .attr(CommonConfig.CLIENT_NAME_KEY, CommonConfig.CLIENT_NAME_VALUE) // option() 给连接设置一些 TCP 底层相关的属性 // ChannelOption.CONNECT_TIMEOUT_MILLIS 示连接的超时时间,超过这个时间还是建立不上的话则代表连接失败 .option(ChannelOption.CONNECT_TIMEOUT_MILLIS, 5000) // ChannelOption.SO_KEEPALIVE 表示是否开启 TCP 底层心跳机制,true 为开启 .option(ChannelOption.SO_KEEPALIVE, true) // ChannelOption.TCP_NODELAY 表示是否开启Nagle算法,true表示关闭,false表示开启,通俗地说,如果要求高实时性,有数据发送时就马上发送,就关闭,如果需要减少发送次数减少网络交互,就开启. .option(ChannelOption.TCP_NODELAY, true); // 4. 建立连接 connect(bootstrap, CommonConfig.NETTY_HOST, CommonConfig.NETTY_PORT, CommonConfig.MAX_RETRY); &#125; public static void connect(Bootstrap bootstrap, String host, int port, int retry) &#123; bootstrap.connect(host, port).addListener(future -&gt; &#123; if (future.isSuccess()) &#123; log.info(&quot;连接成功&quot;); &#125; else if (retry == 0) &#123; log.info(&quot;重试次数已经用完了,连接失败&quot;); &#125; else &#123; // 第几次重连 int order = (CommonConfig.MAX_RETRY - retry) + 1; // 本次重连的间隔 int delay = 1 &lt;&lt; order; log.info(&quot;连接失败,进行第&#123;&#125;次重连&quot;, order); bootstrap.config().group() .schedule(() -&gt; connect(bootstrap, host, port, retry - 1), delay, TimeUnit.SECONDS); &#125; &#125;); &#125;&#125; 额外的一些方法及其功能都在代码的注释里了.]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-1-服务端启动详解]]></title>
    <url>%2F2019%2F07%2F02%2FNetty-1-%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%90%AF%E5%8A%A8%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[述上文中通过一个简单的例子,对netty有了一个大致的了解,本文来详细看一下Netty服务端的启动. NettyServer解析代码如下:1234567891011121314151617public class NettyServer &#123; public static void main(String[] args) &#123; NioEventLoopGroup bossGroup = new NioEventLoopGroup(); NioEventLoopGroup workerGroup = new NioEventLoopGroup(); ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap .group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; protected void initChannel(NioSocketChannel ch) &#123; &#125; &#125;); serverBootstrap.bind(8000); &#125;&#125; 和上篇文章中的NettyServer基本是一样的,详细看一下这段代码 首先创建了两个NioEventLoopGroup对象,这两个对象可以看做是传统IO编程模型的两大线程组,bossGroup表示监听端口,accept新连接的线程组,workerGroup表示处理每一条连接的数据读写的线程组,不理解的同学可以看一下上一小节. 用生活中的例子来讲就是,一个工厂要运作,必然要有一个老板负责从外面接活,然后有很多员工,负责具体干活,老板就是bossGroup,员工们就是workerGroup,bossGroup接收完连接,扔给workerGroup去处理. 接下来,我们创建了一个引导类ServerBootstrap,这个类将引导我们进行服务端的启动工作,直接new出来开搞 我们通过.group(bossGroup, workerGroup)给引导类配置两大线程组,这个引导类的线程模型也就定型了 然后,我们指定我们服务端的IO模型为NIO,我们通过.channel(NioServerSocketChannel.class)来指定 IO 模型,当然,这里也有其他的选择,如果你想指定 IO 模型为BIO,那么这里配置上OioServerSocketChannel.class类型即可,当然通常我们也不会这么做,因为Netty的优势就在于NIO. 接着,我们调用childHandler()方法,给这个引导类创建一个ChannelInitializer,这里主要就是定义后续每条连接的数据读写,业务处理逻辑,不理解没关系,在后面我们会详细分析.ChannelInitializer这个类中,我们注意到有一个泛型参数NioSocketChannel,这个类呢,就是 Netty 对 NIO 类型的连接的抽象,而我们前面NioServerSocketChannel也是对 NIO 类型的连接的抽象,NioServerSocketChannel和NioSocketChannel的概念可以和 BIO 编程模型中的ServerSocket以及Socket两个概念对应上 总结启动一个Netty服务端,必须要指定三类属性,分别是线程模型、IO 模型、连接读写处理逻辑,有了这三者,之后在调用bind(8000),我们就可以在本地绑定一个 8000 端口启动起来 图片来源,侵删 自动绑定递增端口首先看下代码,如下:1234567891011121314public static void bind(final ServerBootstrap serverBootstrap, final int port) &#123; serverBootstrap.bind(port) .addListener(new GenericFutureListener&lt;Future&lt;? super Void&gt;&gt;() &#123; @Override public void operationComplete(Future&lt;? super Void&gt; future) throws Exception &#123; if (future.isSuccess()) &#123; log.info(&quot;端口:&#123;&#125;&gt;绑定成功&quot;, port); &#125; else &#123; log.info(&quot;端口:&#123;&#125;&gt;绑定失败&quot;, port); bind(serverBootstrap, port + 1); &#125; &#125; &#125;);&#125; 就是把上面绑定端口的代码抽出来,单独的一个方法,上面的代码中serverBootstrap.bind(8000);是一个异步方法,调用之后立即返回一个ChannelFuture,然后我们给这个ChannelFuture加了一个监听器GenericFutureListener,然后里面重写了operationComplete()方法,里面判断端口是否绑定成功,绑定失败的话,就又调用自身,把端口加1再去尝试绑定 比如说我传的port参数是1000,然后端口1000绑定失败的话,就又去绑定1001,1002…直到绑定成功 服务端启动时的一些其他功能NettyServer代码如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778/** * @author 周泽 * @date Create in 14:14 2019/5/29 * @Description netty服务端 */@Slf4jpublic class NettyServer &#123; public static void main(String[] args) &#123; ServerBootstrap serverBootstrap = new ServerBootstrap(); NioEventLoopGroup boos = new NioEventLoopGroup(); NioEventLoopGroup worker = new NioEventLoopGroup(); serverBootstrap .group(boos, worker) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; @Override protected void initChannel(NioSocketChannel ch) &#123; log.info(&quot;取出childAttr属性:&#123;&#125;&quot;, ch.attr(CommonConfig.CLIENT_KEY).get()); ch.pipeline().addLast(new StringDecoder()); ch.pipeline().addLast(new SimpleChannelInboundHandler&lt;String&gt;() &#123; @Override protected void channelRead0(ChannelHandlerContext ctx, String msg) &#123; log.info(msg); &#125; &#125;); &#125; &#125;); serverBootstrap // handler() 服务端启动过程中的一些逻辑处理,通常情况用不到这个方法 .handler(new ChannelInitializer&lt;ServerSocketChannel&gt;() &#123; @Override protected void initChannel(ServerSocketChannel serverSocketChannel) throws Exception &#123; log.info(&quot;取出attr属性:&#123;&#125;&quot;, serverSocketChannel.attr(CommonConfig.SERVER_NAME_KEY).get()); log.info(&quot;服务端启动中...............&quot;); &#125; &#125;) // attr() 给服务端的channel即NioServerSocketChannel指定一些自定义属性,通过channel.attr()取出该属性,给NioServerSocketChannel维护一个map .attr(CommonConfig.SERVER_NAME_KEY, CommonConfig.SERVER_NAME_VALUE) // childAttr() 给每一条连接指定自定义属性,通过channel.attr()取出该属性,对应的是childHandler .childAttr(CommonConfig.CLIENT_KEY, CommonConfig.CLIENT_VALUE) // childOption() 可以给每条连接设置一些TCP底层相关的属性 // ChannelOption.SO_KEEPALIVE 表示是否开启TCP底层心跳机制,true为开启 .childOption(ChannelOption.SO_KEEPALIVE, true) // ChannelOption.TCP_NODELAY 表示是否开启Nagle算法,true表示关闭,false表示开启,通俗地说,如果要求高实时性,有数据发送时就马上发送,就关闭,如果需要减少发送次数减少网络交互,就开启. .childOption(ChannelOption.TCP_NODELAY, true) // ChannelOption.SO_REUSEADDR 表示端口释放后立即就可以被再次使用,因为一般来说,一个端口释放后会等待两分钟之后才能再被使用 .childOption(ChannelOption.SO_REUSEADDR, true) // option() 给服务端channel设置一些属性 // ChannelOption.SO_BACKLOG 表示系统用于临时存放已完成三次握手的请求的队列的最大长度,如果连接建立频繁,服务器处理创建新连接较慢,适当调大该参数 .option(ChannelOption.SO_BACKLOG, 1024); // 绑定端口 bind(serverBootstrap, CommonConfig.NETTY_PORT); &#125; public static void bind(final ServerBootstrap serverBootstrap, final int port) &#123; serverBootstrap.bind(port) .addListener(new GenericFutureListener&lt;Future&lt;? super Void&gt;&gt;() &#123; @Override public void operationComplete(Future&lt;? super Void&gt; future) throws Exception &#123; if (future.isSuccess()) &#123; log.info(&quot;端口:&#123;&#125;&gt;绑定成功&quot;, port); &#125; else &#123; log.info(&quot;端口:&#123;&#125;&gt;绑定失败&quot;, port); bind(serverBootstrap, port + 1); &#125; &#125; &#125;); &#125;&#125; 额外的一些方法及其功能都在代码的注释里了.从方法名上来看比如说上面的attr(),childAttr(),option(),childOption()…等等这些,可以看出一类是去设置服务端的,还有一类是针对连接的设置 附上面的代码中还用到了一个类CommonConfig,用来放一些常量,如下:1234567891011121314151617181920212223242526272829303132333435363738public class CommonConfig &#123; /** * 服务端ip */ public static final String NETTY_HOST = &quot;127.0.0.1&quot;; /** * 绑定端口 */ public static final int NETTY_PORT = 8000; /** * 尝试重新连接次数 */ public static final int MAX_RETRY = 5; /** * serverNameKey */ public static final AttributeKey&lt;Object&gt; SERVER_NAME_KEY = AttributeKey.newInstance(&quot;serverName&quot;); /** * saerverNameValue */ public static final String SERVER_NAME_VALUE = &quot;nettyServer&quot;; /** * clientKey */ public static final AttributeKey&lt;Object&gt; CLIENT_KEY = AttributeKey.newInstance(&quot;clientKey&quot;); /** * clientValue */ public static final String CLIENT_VALUE = &quot;clientValue&quot;;&#125;]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty-0-Netty简介]]></title>
    <url>%2F2019%2F07%2F02%2FNetty-0-Netty%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[述在了解netty之前,先来看一个简单的案例,一个客户端,一个服务端,客户端每隔两秒发送一个带有时间戳的”hello world”给服务端,服务端收到之后打印. IO编程上面这个案例用传统的IO方式实现代码如下: IOServer:12345678910111213141516171819202122232425262728293031323334353637public class IOServer &#123; public static void main(String[] args) throws Exception &#123; ServerSocket serverSocket = new ServerSocket(8000); // (1) 接收新连接线程 new Thread(() -&gt; &#123; while (true) &#123; try &#123; // (1) 阻塞方法获取新的连接 Socket socket = serverSocket.accept(); // (2) 每一个新的连接都创建一个线程，负责读取数据 new Thread(() -&gt; &#123; try &#123; byte[] data = new byte[1024]; InputStream inputStream = socket.getInputStream(); while (true) &#123; int len; // (3) 按字节流方式读取数据 while ((len = inputStream.read(data)) != -1) &#123; System.out.println(new String(data, 0, len)); &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;).start(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125;&#125; IOClient:1234567891011121314151617181920public class IOClient &#123; public static void main(String[] args) &#123; new Thread(() -&gt; &#123; try &#123; Socket socket = new Socket(&quot;127.0.0.1&quot;, 8000); while (true) &#123; try &#123; socket.getOutputStream().write((new Date() + &quot;: hello world&quot;).getBytes()); socket.getOutputStream().flush(); Thread.sleep(2000); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;).start(); &#125;&#125; 在Server端中,创建了一个serverSocket监听8000端口,然后创建一个线程,线程里面不断调用阻塞方法serverSocket.accept();获取新的连接,当获取到新的连接之后,给每个连接再创建一个新的线程,这个线程负责从该连接中读取数据,读取数据是以字节流的方式 再来看一下客户端,客户端先连接服务端,然后每隔两秒发送一条消息给服务端. 上面的demo,从服务端代码中我们可以看到,在传统的IO模型中,每个连接创建成功之后都需要一个线程来维护,每个线程包含一个while死循环,那么1w个连接对应1w个线程,继而1w个while死循环,这就带来如下几个问题: 线程资源受限: 线程是操作系统中非常宝贵的资源,同一时刻有大量的线程处于阻塞状态是非常严重的资源浪费,操作系统耗不起 线程切换效率低下:单机cpu核数固定,线程爆炸之后操作系统频繁进行线程切换,应用性能急剧下降. 除了以上两个问题,IO编程中,我们看到数据读写是以字节流为单位,效率不高. 为了解决这三个问题,JDK在1.4之后提出了NIO. NIO编程关于NIO相关的文章网上也有很多,这里不打算详细深入分析,下面简单描述一下NIO是如何解决以上三个问题的. 线程资源受限问题NIO编程模型中,新来一个连接不再创建一个新的线程,而是可以把这条连接直接绑定到某个固定的线程,然后这条连接所有的读写都由这个线程来负责,那么他是怎么做到的? 我们用一幅图来对比一下IO与NIO: 阻塞式IO: NIO: 如上图所示,IO模型中,一个连接来了,会创建一个线程,对应一个while死循环,死循环的目的就是不断监测这条连接上是否有数据可以读,大多数情况下,1w个连接里面同一时刻只有少量的连接有数据可读,因此,很多个while死循环都白白浪费掉了,因为读不出啥数据. 而在NIO模型中,他把这么多while死循环变成一个死循环,这个死循环由一个线程控制,那么他又是如何做到一个线程,一个while死循环就能监测1w个连接是否有数据可读的呢?这就是NIO模型中selector的作用,一条连接来了之后,现在不创建一个while死循环去监听是否有数据可读了,而是直接把这条连接注册到selector上,然后通过检查这个selector,就可以批量监测出有数据可读的连接,进而读取数据. 下面我再举个非常简单的生活中的例子说明IO与NIO的区别: 每个小朋友配一个老师.每个老师隔段时间询问小朋友是否要上厕所,如果要上,就领他去厕所,100个小朋友就需要100个老师来询问,并且每个小朋友上厕所的时候都需要一个老师领着他去上,这就是IO模型,一个连接对应一个线程. 所有的小朋友都配同一个老师.这个老师隔段时间询问所有的小朋友是否有人要上厕所,然后每一时刻把所有要上厕所的小朋友批量领到厕所,这就是NIO模型,所有小朋友都注册到同一个老师,对应的就是所有的连接都注册到一个线程,然后批量轮询. 这就是NIO模型解决线程资源受限的方案,实际开发过程中,我们会开多个线程,每个线程都管理着一批连接,相对于IO模型中一个线程管理一条连接,消耗的线程资源大幅减少 线程切换效率低下问题由于NIO模型中线程数量大大降低,线程切换效率因此也大幅度提高 IO读写以字节为单位NIO解决这个问题的方式是数据读写不再以字节为单位,而是以字节块为单位.IO模型中,每次都是从操作系统底层一个字节一个字节地读取数据,而NIO维护一个缓冲区,每次可以从这个缓冲区里面读取一块的数据,这就好比一盘美味的豆子放在你面前,你用筷子一个个夹（每次一个）,肯定不如要勺子挖着吃（每次一批）效率来得高 NIO实现上面案例简单讲完了JDK NIO的解决方案之后,我们接下来使用NIO的方案替换掉IO的方案,我们先来看看，如果用JDK原生的NIO来实现服务端,该怎么做 NIOServer.java代码如下:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980public class NIOServer &#123; public static void main(String[] args) throws IOException &#123; Selector serverSelector = Selector.open(); Selector clientSelector = Selector.open(); new Thread(() -&gt; &#123; try &#123; // 对应IO编程中服务端启动 ServerSocketChannel listenerChannel = ServerSocketChannel.open(); listenerChannel.socket().bind(new InetSocketAddress(8000)); listenerChannel.configureBlocking(false); listenerChannel.register(serverSelector, SelectionKey.OP_ACCEPT); while (true) &#123; // 监测是否有新的连接，这里的1指的是阻塞的时间为1ms if (serverSelector.select(1) &gt; 0) &#123; Set&lt;SelectionKey&gt; set = serverSelector.selectedKeys(); Iterator&lt;SelectionKey&gt; keyIterator = set.iterator(); while (keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if (key.isAcceptable()) &#123; try &#123; // (1) 每来一个新连接，不需要创建一个线程，而是直接注册到clientSelector SocketChannel clientChannel = ((ServerSocketChannel) key.channel()).accept(); clientChannel.configureBlocking(false); clientChannel.register(clientSelector, SelectionKey.OP_READ); &#125; finally &#123; keyIterator.remove(); &#125; &#125; &#125; &#125; &#125; &#125; catch (IOException ignored) &#123; ignored.printStackTrace(); &#125; &#125;).start(); new Thread(() -&gt; &#123; try &#123; while (true) &#123; // (2) 批量轮询是否有哪些连接有数据可读，这里的1指的是阻塞的时间为1ms if (clientSelector.select(1) &gt; 0) &#123; Set&lt;SelectionKey&gt; set = clientSelector.selectedKeys(); Iterator&lt;SelectionKey&gt; keyIterator = set.iterator(); while (keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if (key.isReadable()) &#123; try &#123; SocketChannel clientChannel = (SocketChannel) key.channel(); ByteBuffer byteBuffer = ByteBuffer.allocate(1024); // (3) 读取数据以块为单位批量读取 clientChannel.read(byteBuffer); byteBuffer.flip(); System.out.println(Charset.defaultCharset().newDecoder().decode(byteBuffer) .toString()); &#125; finally &#123; keyIterator.remove(); key.interestOps(SelectionKey.OP_READ); &#125; &#125; &#125; &#125; &#125; &#125; catch (IOException ignored) &#123; ignored.printStackTrace(); &#125; &#125;).start(); &#125;&#125; 相信大部分没有接触过NIO的同学应该会直接跳过代码来到这一行：原来使用JDK原生NIO的API实现一个简单的服务端通信程序是如此复杂! 复杂得我都没耐心解释这一坨代码的执行逻辑(开个玩笑)，我们还是先对照NIO来解释一下几个核心思路 NIO模型中通常会有两个线程,每个线程绑定一个轮询器selector,在我们这个例子中serverSelector负责轮询是否有新的连接,clientSelector负责轮询连接是否有数据可读 服务端监测到新的连接之后,不再创建一个新的线程,而是直接将新连接绑定到clientSelector上,这样就不用IO模型中1w个while循环在死等,参见(1) clientSelector被一个while死循环包裹着,如果在某一时刻有多条连接有数据可读,那么通过 clientSelector.select(1)方法可以轮询出来,进而批量处理,参见(2) 数据的读写以内存块为单位,参见(3) 其他的细节部分,我不愿意多讲,因为实在是太复杂,你也不用对代码的细节深究到底.总之,强烈不建议直接基于JDK原生NIO来进行网络开发,下面是我总结的原因: JDK的NIO编程需要了解很多的概念,编程复杂,对NIO入门非常不友好,编程模型不友好,ByteBuffer的api简直反人类 对NIO编程来说,一个比较合适的线程模型能充分发挥它的优势,而JDK没有给你实现,你需要自己实现,就连简单的自定义协议拆包都要你自己实现 JDK的NIO底层由epoll实现，该实现饱受诟病的空轮训bug会导致cpu飙升100% 项目庞大之后,自行实现的NIO很容易出现各类bug,维护成本较高,上面这一坨代码我都不能保证没有bug 正因为如此,我客户端代码都懒得写给你看了,你可以直接使用上面的IOClient.java与NIOServer.java通信 JDK的NIO犹如带刺的玫瑰,虽然美好,让人向往,但是使用不当会让你抓耳挠腮,痛不欲生,正因为如此,Netty横空出世 Netty那么Netty到底是何方神圣? 用一句简单的话来说就是:Netty封装了JDK的NIO,让你用得更爽,你不用再写一大堆复杂的代码了. 用官方正式的话来说就是:Netty是一个异步事件驱动的网络应用框架,用于快速开发可维护的高性能服务器和客户端. 下面是我总结的使用Netty不使用JDK原生NIO的原因: 使用JDK自带的NIO需要了解太多的概念,编程复杂,一不小心bug横飞 Netty底层IO模型随意切换,而这一切只需要做微小的改动,改改参数,Netty可以直接从NIO模型变身为IO模型 Netty自带的拆包解包,异常检测等机制让你从NIO的繁重细节中脱离出来,让你只需要关心业务逻辑 Netty解决了JDK的很多包括空轮询在内的bug Netty底层对线程,selector做了很多细小的优化,精心设计的reactor线程模型做到非常高效的并发处理 自带各种协议栈让你处理任何一种通用协议都几乎不用亲自动手 Netty社区活跃,遇到问题随时邮件列表或者issue Netty已经历各大RPC框架,消息中间件,分布式通信中间件线上的广泛验证,健壮性无比强大 接下来我们用Netty的版本来重新实现一下本文开篇的功能 案例首先,引入maven依赖12345&lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;4.1.6.Final&lt;/version&gt;&lt;/dependency&gt; 然后,是服务端的实现部分:NettyServer.java:123456789101112131415161718192021222324public class NettyServer &#123; public static void main(String[] args) &#123; ServerBootstrap serverBootstrap = new ServerBootstrap(); NioEventLoopGroup boos = new NioEventLoopGroup(); NioEventLoopGroup worker = new NioEventLoopGroup(); serverBootstrap .group(boos, worker) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; @Override protected void initChannel(NioSocketChannel ch) &#123; ch.pipeline().addLast(new StringDecoder()); ch.pipeline().addLast(new SimpleChannelInboundHandler&lt;String&gt;() &#123; @Override protected void channelRead0(ChannelHandlerContext ctx, String msg) &#123; System.out.println(msg); &#125; &#125;); &#125; &#125;) .bind(8000); &#125;&#125; 这么一小段代码就实现了我们前面NIO编程中的所有的功能,包括服务端启动,接收新连接,打印客户端传来的数据,怎么样,是不是比JDK原生的NIO编程优雅许多? 初学Netty的时候,由于大部分人对NIO编程缺乏经验,因此,将Netty里面的概念与IO模型结合起来可能更好理解 boos对应IOServer.java中的接收新连接线程,主要负责创建新连接 worker对应IOClient.java中的负责读取数据的线程,主要用于读取数据以及业务逻辑处理 然后下面是客户端NIO的实现部分 NettyClient.java:12345678910111213141516171819202122public class NettyClient &#123; public static void main(String[] args) throws InterruptedException &#123; Bootstrap bootstrap = new Bootstrap(); NioEventLoopGroup group = new NioEventLoopGroup(); bootstrap.group(group) .channel(NioSocketChannel.class) .handler(new ChannelInitializer&lt;Channel&gt;() &#123; @Override protected void initChannel(Channel ch) &#123; ch.pipeline().addLast(new StringEncoder()); &#125; &#125;); Channel channel = bootstrap.connect(&quot;127.0.0.1&quot;, 8000).channel(); while (true) &#123; channel.writeAndFlush(new Date() + &quot;: hello world!&quot;); Thread.sleep(2000); &#125; &#125;&#125; 在客户端程序中,group对应了我们IOClient.java中main函数起的线程,剩下的逻辑后面再慢慢了解 使用Netty之后是不是觉得整个世界都美好了,一方面Netty对NIO封装得如此完美,写出来的代码非常优雅,另外一方面,使用Netty之后,网络通信这块的性能问题几乎不用操心,尽情地让Netty榨干你的CPU吧 本文转自:原文地址]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-17-docker-compose使用案例]]></title>
    <url>%2F2019%2F05%2F25%2FDocker-17-docker-compose%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[述上文中,对docker-compose做了一个简单的介绍,以及安装,本文将通过一个简单的案例,来进一步了解docker-compose的使用 本文以jpress(Java版的wordPress)这样一个开源网站的部署为例,来了解docker-compose的使用,我们不必关注jpress的实现,在这里我们只需要将之当作一个普通的web应用即可. 准备工作一共需要两个容器: Tomcat Mysql 然后需要jpress的war包,下载地址 编写DockerfileTomcat容器中,要下载相关的war等,因此这里编写一个Dockerfile来做这个事.在一个空的文件夹下创建Dockerfile,内容如下:1234FROM tomcatADD https://github.com/JpressProjects/jpress/raw/alpha/wars/jpress-web-newest.war /usr/local/tomcat/webapps/RUN cd /usr/local/tomcat/webapps/ \ &amp;&amp; mv jpress-web-newest.war jpress.war 这个Dockerfile主要内容就是: 基于Tomcat镜像构建容器 下载jpress项目的war包到tomcat的webapps目录下 给jpress项目重命名 编写docker-compose.yml在相同的目录下创建docker-compose.yml,内容如下:123456789101112131415161718192021version: &quot;3.1&quot;services: web: build: . container_name: jpress ports: - &quot;8080:8080&quot; volumes: - /usr/local/tomcat/ depends_on: - db db: image: mysql container_name: mysql command: --default-authentication-plugin=mysql_native_password restart: always ports: - &quot;3306:3306&quot; environment: MYSQL_ROOT_PASSWORD: 123 MYSQL_DATABASE: jpress 详解 version: 指定 compose 文件的版本( Compose文件格式有3个版本,分别为1, 2.x 和 3.x目前主流的为 3.x 其支持 docker 1.13.0 及其以上的版本) services: 定义所有的service信息,services下面的第一级别的 key 既是一个 service 的名称(如上面的web,db). build: 指定包含构建上下文的路径,或作为一个对象，该对象具有 context 和指定的 dockerfile 文件以及 args 参数值 container_name: 容器名称 ports: 容器端口映射 volumes: 挂载数据卷 depends_on: 上面的值是db,表示该容器依赖于db容器,在启动的时候,会先启动db容器,然后再启动web容器,这只是启动时机的先后问题,并不是说web容器会等db容器完全启动了才会启动 image: 对于db容器,则使用image来构建,没有使用Dockerfile restart: 重启策略 environment:启动容器时的环境变量,这里配置了数据库root用户的密码以及在启动时创建一个名为jpress的库,environment的配置可以使用字典和数组两种形式 上面的两个文件创建完成之后,就ok了 运行运行的方式有好几种,但是建议使用up这个终极命令,up命令十分强大,它将尝试自动完成包括构建镜像,（重新）创建服务,启动服务,并关联服务相关容器的一系列操作.对于大部分应用都可以直接通过该命令来启动. 默认情况下,docker-compose up启动的容器都在前台,控制台将会同时打印所有容器的输出信息,可以很方便进行调试,通过 Ctrl-C停止命令时,所有容器将会停止,而如果使用docker-compose up -d命令,则将会在后台启动并运行所有的容器.一般推荐生产环境下使用该选项. 进入到docker-compose.yml的目录下,执行以下命令:1docker-compose up -d 启动完成之后,访问http://localhost:8080/jpress,就可以看到jpress的页面了,如下: 停止容器的命令如下:1docker-compose down 踩坑记录在上面执行docker-compose up -d去启动容器的时候,遇到了一个错误,提示如下:1Unsupported config option for services 解决方案参考地址 原因就是我们上面docker-compose.yml中,指定的version,这个version需要跟docker-compose的版本是对应的,否则是无法解析的,具体的对照信息可以去docker-compose的github中去查看,在release中找到自己的docker-compose版本,然后查看可以使用的version]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-16-docker-compose简介与安装]]></title>
    <url>%2F2019%2F05%2F25%2FDocker-16-docker-compose%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[述Docker Compose 是 Docker 官方编排（Orchestration）项目之一,负责快速的部署分布式应用. Compose 定位是 「定义和运行多个 Docker 容器的应用（Defining and running multi-container Docker applications）」 我们知道使用一个 Dockerfile 模板文件，可以让用户很方便的定义一个单独的应用容器。然而，在日常工作中，经常会碰到需要多个容器相互配合来完成某项任务的情况。例如要实现一个 Web 项目，除了 Web 服务容器本身，往往还需要再加上后端的数据库服务容器，甚至还包括负载均衡容器等。 Compose 恰好满足了这样的需求。它允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）。 Compose 中有两个重要的概念: 服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。 项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。 Compose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。 Compose 项目由 Python 编写，实现上调用了 Docker 服务提供的 API 来对容器进行管理。因此，只要所操作的平台支持 Docker API，就可以在其上利用 Compose 来进行编排管理 安装docker-composedocker-compose的github地址在这里 这里使用PIP的方式安装,简单粗暴,首先把pip装上,分别运行以下命令:123456#安装依赖yum -y install epel-release#安装PIPyum -y install python-pip#升级PIPpip install --upgrade pip 安装完pip后,查看版本号验证安装1pip --version 返回如下,就表示安装成功 pip装好之后,就可以装docker-compose了,命令如下:1pip install -U docker-compose==1.24.0 最后的1.24.0是版本号,可以自己进行调整 安装完成之后用docker-compose --version命令去验证是否安装成功,如下: 至此docker-compose就安装成功了]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-15-容器连接]]></title>
    <url>%2F2019%2F05%2F25%2FDocker-15-%E5%AE%B9%E5%99%A8%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[述一般来说,容器启动后,都是通过端口映射使用容器提供的服务,实际上,端口映射只是使用容器服务的一种方式,除了这种方式外,还可以使用容器连接的方式来使用容器服务 例如,有两个容器,一个运行着Spring boot项目,另一个容器运行着mysql数据库,可以通过容器连接使Spring boot直接访问到mysql而不用通过端口映射来访问mysql服务 案例启动两个容器,一个nginx,一个ubuntu,然后在启动nginx的时候,不分配端口映射,然后再启动ubuntu,通过容器连接,在ubuntu中访问nginx,具体步骤如下: 启动nginx容器启动一个nginx容器,但是不分配端口,命令如下:1docker run -d --name nginx1 nginx 如下图: 在不分配端口的情况下,是访问不了的 启动ubuntu容器命令如下:1docker run -dit --name ubuntu --link nginx1:mylink ubuntu bash 使用--link参数建立连接,nginx1是要建立连接的容器,mylink是连接的别名 连接nginx启动成功后,进入容器,命令如下:1docker exec -ti ubuntu bash 查看nginx的方式有两种: 第一种在ubuntu控制台输入env,查看环境变量,如下: 可以看到docker为nginx创建了一系列环境变量.每个前缀变量是MYLINK,这就是刚刚给连接取得别名.开发者可以使用这些环境变量来配置应用程序连接到nginx.该连接是安全、私有的. 然后看图中,可以通过172.17.0.2:80这个地址去访问nginx,在容器中使用curl命令的话,还需要先装一下,执行以下两个命令:12apt updateapt install curl 安装完成之后,就可以访问了,结果如下： 第二种方式查看ubuntu容器的hosts文件,如下: 可以看到,在ubuntu的hosts文件中已经给nginx1取了几个别名,可以直接使用这些别名来访问nginx1,如下:]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-14-数据卷容器]]></title>
    <url>%2F2019%2F05%2F25%2FDocker-14-%E6%95%B0%E6%8D%AE%E5%8D%B7%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[述数据卷容器就是一个专门用来挂载数据卷的容器,该容器主要是供其他容器引用和使用,所谓的数据卷容器,实际上就是一个普通的容器. 创建数据卷容器命令如下:1docker run -tid -v /usr/share/nginx/html/ --name mydata ubuntu 就是运行了一个ubuntu容器,然后挂载了/usr/share/nginx/html/这个目录 引用容器使用如下命令引用数据卷容器:123docker run -tid --volumes-from mydata -p 80:80 --name nginx1 nginx docker run -tid --volumes-from mydata -p 81:80 --name nginx2 nginx 执行完毕后,刚刚运行的两个nginx容器就都挂载了同一个数据卷到usr/share/nginx/html/目录下面,三个容器中,任意一个修改了该目录下的文件,其他两个都能看到变化 可以用docker inspect命令去查看这三个容器的详情,关于数据卷的部分如下: 从图中可以看到,三个容器的数据卷的描述都是一致的 注意事项这里有几点需要注意的地方 可以多次使用--volumes-from参数来从多个容器挂载多个数据卷,还可以从其他已经挂载了容器卷的容器来挂载数据卷. 使用--volumes-from参数所挂载数据卷的容器自身并不需要保持在运行状态 如果删除了挂载的容器(包括mydata、nginx1和nginx2),数据卷并不会被自动删除.如果要删除一个数据卷,必须在删除最后一个还挂载着它的容器时显式使用docker rm -v命令来指定同时删除关联的容器. 数据备份利用数据卷容器可以实现数据的备份与恢复,备份命令如下:1docker run mydata --name worker --privileged=true -v /usr/docker/backup/:/backup/ ubuntu tar cvf /backup/backup.tar /usr/share/nginx/html/ --volumes-from 连接待备份的容器 -v 用来挂载数据卷到容器的/backup/目录下面 最后是个tar命令,将容器中/usr/share/nginx/html目录下的内容备份到/backup目录下的backup.tar文件中,由于已经设置将宿主机的/usr/docker/backup/目录映射到容器的/backup目录,因为备份在容器/backup目录下的压缩文件在当前目录下可以立马看到 执行如下: 可以看到backup.tar就有了已经,然后打开其实就是/usr/share/nginx/html目录及内容,至此备份就完成了 数据恢复步骤如下 创建容器首先需要创建一个容器,这个容器就是要使用恢复的数据的容器,这里就再创建一个nginx容器了,命令如下:1docker run -itd -p 82:80 -v /usr/share/nginx/html/ --name nginx3 nginx 容器名称是nginx3,然后挂载了一个数据卷 恢复创建一个临时容器,如下:1docker run --volumes-from nginx3 -v $(pwd):/backup nginx tar xvf /backup/backup.tar 首先还是使用--volumes-from参数连接上备份容器,即第一步创建出来的nginx3. 然后将当前目录映射到容器的/backup目录下. 然后执行解压操作,将backup.tar文件解压.解压文件位置描述是一个容器内的地址,但是该地址已经映射到宿主机中的当前目录了,因此这里要解压缩的文件实际上就是宿主机当前目录下的文件. 至此,数据恢复就完成了]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-13-数据卷]]></title>
    <url>%2F2019%2F05%2F25%2FDocker-13-%E6%95%B0%E6%8D%AE%E5%8D%B7%2F</url>
    <content type="text"><![CDATA[述数据卷,可以绕过拷贝系统,在多个容器之间,或者容器和宿主机之间共享目录或者文件,数据卷绕过了拷贝系统,可以达到本地磁盘I/O性能 示例以之前的nginx镜像为例,运行一个容器,然后再运行的时候,指定一个数据卷,用-v参数,具体命令如下:1docker run -tid --name nginx -v /usr/share/nginx/html/ -p 80:80 27a188018e18 执行完毕之后,我们就创建一个数据卷并且挂载到容器的/usr/share/nginx/html/目录下面,该目录其实是nginx保存html目录,在这里挂载数据卷,一会我们只需修改本地的映射位置,就可以实现页面的修改了. 使用docker inspect命令来查看一下刚刚创建的容器的具体信息,找到数据卷映射目录,如下:1docker inspect c39c3e362d27 找到下面这个部分: 从上图中可以看到,Docker默认将宿主机的/var/lib/docker/volumes/609df55091699750b86ee1f870e56050d4083a5159c0234360ffe9f903e41b55/_data这个目录作为source目录,然后我们进到这个目录中,看一下文件,如下: 这个目录下面的文件跟容器中的/usr/share/nginx/html/目录下的文件是一致的,这是因为挂载一个空数据卷到容器中的一个非空目录中时这个目录下的文件会被复制到数据卷中,如果挂载一个非空的数据卷到容器中的一个目录中,那么容器中的目录中会显示数据卷中的数据.如果原来容器中的目录中有数据,那么这些原始数据会被隐藏掉 然后现在先访问一下nginx的首页,内容如下: 现在我们把宿主机中,刚才目录下面的index.html修改掉,命令如下:1echo &quot;hello volumes&quot; &gt; index.html 修改完成之后,再回到浏览器中,看一下首页内容,内容已经发生更改了,如下: 使用宿主机目录做数据卷上面我们使用数据卷是系统随机分配的一个目录,一般情况下,需要我们明确的指定宿主机中的一个目录挂载到容器中,方式如下:1docker run -tid --name nginx -v /usr/docker/nginx/html:/usr/share/nginx/html/ -p 80:80 27a188018e18 这样就是将宿主机中的/usr/docker/nginx/html目录挂载到容器的/usr/share/nginx/html/目录下. 接下来只需要修改/usr/docker/nginx/html这个目录下的文件,就可以在nginx中看到效果 这种用法对于开发测试非常方便,不用重新部署,重启容器等.宿主机地址是一个绝对路径 Dockerfile中挂载数据卷如果我们使用了Dockerfile去构建镜像的话,也可以在构建镜像的时候声明数据卷,例如下面这个Dockerfile:1234FROM nginxADD https://www.baidu.com/img/bd_logo1.png /usr/share/nginx/html/RUN echo &quot;hello docker volume!&quot;&gt;/usr/share/nginx/html/index.htmlVOLUME /usr/share/nginx/html/ 通过VOLUME就配置了一个匿名的数据卷,由于没有指定挂载到的宿主机目录,因此会默认挂载到宿主机的/var/lib/docker/volumes下的一个随机名称的目录下,因此Dockerfile中使用VOLUME指令挂载目录和docker run时通过-v参数指定挂载目录的区别在于,run的-v可以指定挂载到宿主机的哪个目录,而Dockerfile的VOLUME不能,其挂载目录由docker随机生成 数据卷的一些常用操作查看所有数据卷命令:1docker volume ls 查看数据卷详情命令:1docker volume inspect &lt;VOLUME NAME&gt; 删除数据卷单个删除:1docker volume rm &lt;VOLUME NAME&gt; 批量删除:1docker volume prune 批量删除的时候,只能删除掉没有在使用中的数据卷,正在使用中的需要停止相关的容器然后再次删除.]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-12-容器网络]]></title>
    <url>%2F2019%2F05%2F25%2FDocker-12-%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[述要构建具有安全的一致行为的web应用程序,可以使用Docker网络特性.根据定义,网络为容器实现了完全隔离.因此,控制应用程序所在的网络很重要,Docker容器网络为您提供了这种控制能力. 默认网络docker提供了三种网络,可以通过docker network ls命令去查看,结果如下: 前面三个就是docker默认提供的,运行容器的时候可以通过--net来指定网络,先来看下这三个默认的网络分别是怎样的 bridge: 表示所有Docker安装中都存在的docker0网络. 除非使用docker run --net=&lt;network&gt;选项另行指定,否则Docker守护进程默认情况下会将容器连接到此网络,在主机上使用ifconfig命令,可以看到此网桥是主机的网络堆栈的一部分 none: 意味着不指定网络,使用这个网络的容器中没有网卡. host: 会将容器加入宿主机所在的网络中,在使用这个网络的容器中其网络配置和宿主机一样 bridge网络详解查看bridge网络的详细信息,命令如下:1docker network inspect bridge 返回值:12345678910111213141516171819202122232425262728293031323334353637383940[ &#123; &quot;Name&quot;: &quot;bridge&quot;, &quot;Id&quot;: &quot;e73fffb9d7ea78c02f9fdcabd33377054ae1f391ed883d8b4c6141f2ee99b101&quot;, &quot;Created&quot;: &quot;2019-04-23T09:36:04.07923852+08:00&quot;, &quot;Scope&quot;: &quot;local&quot;, &quot;Driver&quot;: &quot;bridge&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: &#123; &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: null, &quot;Config&quot;: [ &#123; &quot;Subnet&quot;: &quot;172.17.0.0/16&quot;, &quot;Gateway&quot;: &quot;172.17.0.1&quot; &#125; ] &#125;, &quot;Internal&quot;: false, &quot;Attachable&quot;: false, &quot;Containers&quot;: &#123; &quot;dc37d28c293f8568e396f2c788b43f1fc1e293b88f85232a6c5ae550a099493e&quot;: &#123; &quot;Name&quot;: &quot;registry&quot;, &quot;EndpointID&quot;: &quot;8d31b605406c7aa857366af422eca1e859cbb6610bb4a166c61c893866f006df&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:11:00:02&quot;, &quot;IPv4Address&quot;: &quot;172.17.0.2/16&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125; &#125;, &quot;Options&quot;: &#123; &quot;com.docker.network.bridge.default_bridge&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.enable_icc&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.enable_ip_masquerade&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.host_binding_ipv4&quot;: &quot;0.0.0.0&quot;, &quot;com.docker.network.bridge.name&quot;: &quot;docker0&quot;, &quot;com.docker.network.driver.mtu&quot;: &quot;1500&quot; &#125;, &quot;Labels&quot;: &#123;&#125; &#125;] 看一下返回值中的Containers,就是所有使用这个网络的容器的信息,上面这个返回值里面的容器就是我们上文中搭建docker仓库的那个容器 在这个网络中的容器,互相之间可以通过ip进行通信,Docker不支持默认的bridge网络上的服务发现(automatic service discovery),如果你希望默认的bridge网络上的容器之间能够通过名字互相通信,需要在docker run命令中指定--link标识 默认的docker0网络上支持通过端口映射或是使用–link来进行通信,但这种方法很笨重且容易出错,不建议在以后的应用中使用. 自定义网络为了很好的实现容器之间的网络隔离,可以使用自定义网络,Docker提供了一些网络驱动器(network driver)来方便用户自定义网络,你可以创建bridge network或overlay network,也可以通过创建自己的网络插件. 你可以创建许多网络,Docker支持将一个容器加入多个网络,只有在同一个网络中的容器之间才可以相互通信,而不能跨网络通信. 创建bridge网络创建自定义网络最简单的方式就是创建一个bridge网络.这样创建的网络和之间介绍的docker0网络很像. 命令如下:1docker network create --driver bridge isolated_nw 创建完成之后,容器就可以使用了.在docker run命令中加入参数,--net=isolated_nw就ok了 同时在容器运行过程中,也可以指定网络,命令如下:1docker network connect 网络名/id 容器名/id 我们上面创建的这个网络中,加入这个bridge网络的容器必须在同一个宿主机上,同一个网络中的容器之间可以直接通信,但不能和不同网络的容器通信.其示意图如下: 在用户自定义的bridge网络中,--link是不支持的,如果你希望外部网络可以访问容器中的应用,可以通过对外暴露端口(expose port)的方式完成. 如果你希望在单个宿主机上创建一个小型网络,bridge网络是很有用的.但如果想创建更大的网络,尤其是跨越多个宿主机的网络,那就需要创建overlay网络 overlay网络这里先不做介绍,想了解的同学可以看这里:原文地址]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-11-创建自己的Docker Hub]]></title>
    <url>%2F2019%2F05%2F25%2FDocker-11-%E5%88%9B%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84Docker-Hub%2F</url>
    <content type="text"><![CDATA[述前面我们使用的Docker Hub是由Docker官方提供的,我们也可以搭建自己的Docker Hub,搭建方式也很容器,因为Docker官方已经将Docker注册服务器做成镜像了,我们直接pull下来运行即可 环境搭建拉取镜像命令如下:1docker pull registry 运行容器镜像拉下来之后,就可以运行容器了.命令如下:1docker run -itd --name registry -p 5000:5000 f32a97de94e1 默认情况下,仓库会被创建在容器的/var/lib/registry目录下,可以通过-v参数挂载本地的路径 私有仓库操作容器运行起来之后,就可以使用docker tag来标记一个镜像,然后推送它到仓库,我们上面的仓库地址就是127.0.0.1:5000 先来看一下我们本地有哪些镜像 标记本地镜像下面使用docker tag将zhou/nginx:v3这个镜像标记为127.0.0.1:5000/zhou/nginx:v3,命令如下: 1docker tag zhou/nginx:v3 127.0.0.1:5000/zhou/nginx:v3 这里用到的docker tag命令的语法是这样的:1docker tag IMAGE[:TAG] [REGISTRY_HOST[:REGISTRY_PORT]/]REPOSITORY[:TAG] 上传本地镜像使用docker push上传标记的镜像到仓库,命令如下:1docker push 127.0.0.1:5000/zhou/nginx:v3 执行完毕后,再来看一下本地的镜像: 查看本地仓库中的镜像然后我们可以通过curl去查看仓库中的镜像,命令如下:1curl 127.0.0.1:5000/v2/_catalog 返回如下: 表示我们刚刚上传的镜像已经上传成功了 下载仓库的镜像上传完成之后,就可以从本地的仓库中去下载了 首先我们把本地的先删了,然后再下载试试1docker rmi 127.0.0.1:5000/zhou/nginx:v3 删除完成之后,从本地仓库中拉取镜像,命令如下:1docker pull 127.0.0.1:5000/zhou/nginx:v3 后续的运行容器等操作都和之前的一样的,关于私有仓库就先介绍到这里]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-10-自动化构建]]></title>
    <url>%2F2019%2F05%2F25%2FDocker-10-%E8%87%AA%E5%8A%A8%E5%8C%96%E6%9E%84%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[述上文看了如何把本地的镜像上传到Docker Hub上面,但是如果我们对镜像要进行修改的时候,就得先去改Dockerfile,然后再去build,再push到Docker Hub上面去,修改频繁的情况下,要一种重复这几个操作,这就比较难受了,所以下面来看下自动化构建 自动化构建自动化构建,就是使用Docker Hub连接一个包含Dockerfile文件的Github仓库或者BitBucket仓库,DockerHub则会自动构建镜像,通过这种方式构建出来的镜像会被标记为Automated Build,也称之为受信构建(Trusted Build),这种构建方式构建出来的镜像,使其他人在使用的时候可以自由查看Dockerfile的内容,知道镜像是怎么来的,同时由于构建过程是自动的,所以能确保仓库中的镜像是最新的. 下面来看一下具体怎么操作(这里演示用gitHub了,为什么呢?因为我没用过BitBucket.): 关联GitHub首先登录到Docker Hub,点击右上角头像,然后账号设置,如图: 然后如下图,关联到自己的github账号. 新建仓库gitHub关联好之后,去新建一个仓库,如图 信息填写完成之后点create就好了 新建GitHub仓库DockerHub创建好了之后,去GitHub中也需要创建一个仓库,用来放Dockerfile文件,如下: 这里Dockerfile的内容还是和我们之前的Dockerfile内容一样,如下:123FROM nginxMAINTAINER zhou &quot;zhouze_java@sina.com&quot;RUN echo &apos;hello docker!&apos;&gt;/usr/share/nginx/html/index.html 到这一步,gitHub中的操作就完成了 配置自动构建再进到DockerHub,点击刚刚新建好的仓库进去,然后上面的选项卡切换到Builds,如图: 点击配置进去,然后选择刚刚在git上创建的仓库,然后其他都用默认的就ok,如下: 最后点击Sava And Build就会自动构建了, 如下图,就是在构建中: 等一段时间,就可以看到是构建成功还是失败了,如下: 这样配置完成之后,一旦GitHub仓库中的Dockerfile文件有更新,Docker Hub上的镜像构建就会自动触发,不用人工干预,从而保证镜像始终都是最新的. 然后用户就可以通过docker pull命令去获取镜像了,关于自动化构建就先说到这里]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-9-Docker Hub上传自己的镜像]]></title>
    <url>%2F2019%2F05%2F25%2FDocker-9-Docker-Hub%E4%B8%8A%E4%BC%A0%E8%87%AA%E5%B7%B1%E7%9A%84%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[述类似于GitHub提供的代码托管服务,Docker Hub提供了镜像托管服务,利用Docker Hub我们可以搜索,创建,分享和管理镜像,Docker Hub上的镜像分为两大类,一类是官方镜像,例如我们之前用到的nginx、mysql等,还有一类是普通的用户镜像,普通用户镜像由用户自己上传 由于天朝网络的原因,我们可以使用一些国内公司提供的镜像,比如网易的,本文就使用Docker Hub了. 注册登录打开Docker Hub,去注册一个账号,这里就说了,注册好之后,就可以在命令行去登录了,命令如下: 123456789101112131415docker login``` ![image](http://r.photo.store.qq.com/psb?/V10eEnSd0OPhSW/KeCNd9Yi0ZIpl5u*P05qTsn7MwbXjz1M48tQA1gof1Q!/r/dDABAAAAAAAA) 如图,出现Login Succeeded就表示登录成功了 ### 上传镜像 登录成功之后就可以在Docker Hub上面上传我们的自制镜像了,这里要注意,自制的镜像如果要上传的话,命名必须满足规范,即`namespace/name`格式,其中namespace必须是用户名,以前文中我们创建的Dockerfile为例,我们重新构建一个本地镜像并且上传到Docker Hub, 如图: ![image](http://r.photo.store.qq.com/psb?/V10eEnSd0OPhSW/voYxPzTDdf6z9.59id0xZSqHsk2WcJP4R2*WMLnST7k!/r/dE0BAAAAAAAA) 这里`-t`后面的值替换成自己的`namespace/name`即可 镜像构建完成之后,就可以上传到docker hub上去了,命令如下 docker push 镜像名/id1234567 ![image](http://r.photo.store.qq.com/psb?/V10eEnSd0OPhSW/soD28enic85haqpcHxIFtY8AW7fbsclgh4eiR9RWUqc!/r/dMMAAAAAAAAA) 上传成功后,就可以在docker hub上面看到刚刚上传的镜像了,如图: ![image](http://r.photo.store.qq.com/psb?/V10eEnSd0OPhSW/QlMZGyX.WoS.bj*8K.LlVgf1mYw3c6q6bP5s*I57grU!/r/dLYAAAAAAAAA) 然后其他人如果需要使用的话,就可以直接从Dokcer Hub上面拉下来就好了,命令如下: docker pull namespace/name`]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-8-Dockerfile的指令及使用方法]]></title>
    <url>%2F2019%2F05%2F25%2FDocker-8-Dockerfile%E7%9A%84%E6%8C%87%E4%BB%A4%E5%8F%8A%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[述上文中,简单的使用了一个Dockerfile构建镜像,使用到了三个指令就是FROM,MAINTAINER和RUN,本文在来看一些其他的指令.他们的作用以及使用方式 指令详解COPY 复制文件格式12COPY &lt;源路径&gt;... &lt;目标路径&gt;COPY [&quot;&lt;源路径1&gt;&quot;,... &quot;&lt;目标路径&gt;&quot;] 和RUN指令一样,也是两种格式,一种类似于命令行,一种类似于函数调用,COPY指令将从构建上下文目录中&lt;源路径&gt;的文件/目录复制到新的一层的镜像内的&lt;目标路径&gt;位置. 比如:1COPY package.json /usr/src/app/ &lt;源路径&gt;可以是多个,甚至可以是通配符,其通配符规则要满足 Go 的 filepath.Match 规则,如:12COPY hom* /mydir/COPY hom?.txt /mydir/ &lt;目标路径&gt;可以是容器内的绝对路径,也可以是相对于工作目录的相对路径(工作目录可以用WORKDIR指令来指定).目标路径不需要事先创建,如果目录不存在会在复制文件前先行创建缺失目录. 此外,还需要注意一点,使用 COPY 指令,源文件的各种元数据都会保留.比如读、写、执行权限、文件变更时间等.这个特性对于镜像定制很有用.特别是构建相关文件都在使用 Git 进行管理的时候. ADD 更高级的复制文件格式12ADD &lt;源路径&gt;... &lt;目标路径&gt;ADD [&quot;&lt;源路径&gt;&quot;,... &quot;&lt;目标路径&gt;&quot;] ADD 指令和 COPY 的格式和性质基本一致。但是在 COPY 基础上增加了一些功能。 比如 &lt;源路径&gt; 可以是一个 URL，这种情况下，Docker 引擎会试图去下载这个链接的文件放到 &lt;目标路径&gt; 去。下载后的文件权限自动设置为 600，如果这并不是想要的权限，那么还需要增加额外的一层 RUN 进行权限调整，另外，如果下载的是个压缩包，需要解压缩，也一样还需要额外的一层 RUN 指令进行解压缩。所以不如直接使用 RUN 指令，然后使用 wget 或者 curl 工具下载，处理权限、解压缩、然后清理无用文件更合理。因此，这个功能其实并不实用，而且不推荐使用。 如果 &lt;源路径&gt; 为一个 tar 压缩文件的话，压缩格式为 gzip, bzip2 以及 xz 的情况下，ADD 指令将会自动解压缩这个压缩文件到 &lt;目标路径&gt; 去。 但在某些情况下，如果我们真的是希望复制个压缩文件进去，而不解压缩，这时就不可以使用 ADD 命令了。 另外需要注意的是，ADD 指令会令镜像构建缓存失效，从而可能会令镜像构建变得比较缓慢。 因此在 COPY 和 ADD 指令中选择的时候，可以遵循这样的原则，所有的文件复制均使用 COPY 指令，仅在需要自动解压缩的场合使用 ADD。 在使用该指令的时候还可以加上 –chown=\&lt;user>:\&lt;group> 选项来改变文件的所属用户及所属组。1234ADD --chown=55:mygroup files* /mydir/ADD --chown=bin files* /mydir/ADD --chown=1 files* /mydir/ADD --chown=10:11 files* /mydir/ CMD 设置容器启动时执行的操作指定容器启动时执行的命令,每个Dockerfile只能有一条CMD命令 格式使用 exec 执行,推荐方式:1CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] Shell格式:1CMD &lt;命令&gt; 参数列表格式:1CMD [&quot;参数1&quot;, &quot;参数2&quot;...] 在指定了 ENTRYPOINT 指令后,用 CMD 指定具体的参数。 这里与RUN指令的区别就是,RUN是在构建容器的时候执行的,CMD是在容器运行的时候执行的. ENTRYPOINT 入口点ENTRYPOINT用于给容器配置一个可执行程序,也就是说,每次使用镜像创建容器时,通过 ENTRYPOINT 指定的程序都会被设置为默认程序 格式数组/exec格式,推荐:1ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] Shell格式1ENTRYPOINT command param1 param2 ENTRYPOINT 与 CMD 非常类似,不同的是通过docker run执行的命令不会覆盖 ENTRYPOINT,而docker run命令中指定的任何参数,都会被当做参数再次传递给 ENTRYPOINT,Dockerfile中只允许有一个 ENTRYPOINT 命令,多指定时会覆盖前面的设置,而只执行最后的 ENTRYPOINT 指令. docker run运行容器时指定的参数都会被传递给 ENTRYPOINT ,且会覆盖 CMD 命令指定的参数.如,执行docker run \&lt;image> -d时,-d 参数将被传递给入口点. 也可以通过docker run --entrypoint重写 ENTRYPOINT 入口点.如:可以像下面这样指定一个容器执行程序:1ENTRYPOINT [&quot;/usr/bin/nginx&quot;] 完整构建代码:123456789# Version: 0.0.3FROM ubuntu:16.04MAINTAINER 何民三 &quot;cn.liuht@gmail.com&quot;RUN apt-get updateRUN apt-get install -y nginxRUN echo &apos;Hello World, 我是个容器&apos; \ &gt; /var/www/html/index.htmlENTRYPOINT [&quot;/usr/sbin/nginx&quot;]EXPOSE 80 使用docker build构建镜像,并将镜像指定为 itbilu/test:1docker build -t=&quot;itbilu/test&quot; . 构建完成后,使用itbilu/test启动一个容器:1docker run -i -t itbilu/test -g &quot;daemon off;&quot; 在运行容器时,我们使用了-g &quot;daemon off;&quot;,这个参数将会被传递给 ENTRYPOINT,最终在容器中执行的命令为 /usr/sbin/nginx -g &quot;daemon off;&quot; ENV 设置环境变量这个指令很简单，就是设置环境变量而已，无论是后面的其它指令，如 RUN，还是运行时的应用，都可以直接使用这里定义的环境变量。 格式12ENV &lt;key&gt; &lt;value&gt;ENV &lt;key1&gt;=&lt;value1&gt; &lt;key2&gt;=&lt;value2&gt;... 12ENV VERSION=1.0 DEBUG=on \ NAME=&quot;Happy Feet&quot; 这个例子中演示了如何换行，以及对含有空格的值用双引号括起来的办法，这和 Shell 下的行为是一致的。 ARG构建参数ARG用于指定传递给构建运行时的变量:1ARG &lt;name&gt;[=&lt;default value&gt;] 如，通过ARG指定两个变量：12ARG siteARG build_user=IT笔录 以上我们指定了 site 和 build_user 两个变量，其中 build_user 指定了默认值。在使用 docker build 构建镜像时，可以通过 –build-arg \&lt;varname>=\&lt;value> 参数来指定或重设置这些变量的值。 1docker build --build-arg site=itiblu.com -t itbilu/test . 这样我们构建了 itbilu/test 镜像，其中site会被设置为 itbilu.com，由于没有指定 build_user，其值将是默认值 IT 笔录。 VOLUME 定义匿名卷格式12VOLUME [&quot;&lt;路径1&gt;&quot;, &quot;&lt;路径2&gt;&quot;...] VOLUME &lt;路径&gt; 之前我们说过，容器运行时应该尽量保持容器存储层不发生写操作，对于数据库类需要保存动态数据的应用，其数据库文件应该保存于卷(volume)中，后面的章节我们会进一步介绍 Docker 卷的概念。为了防止运行时用户忘记将动态文件所保存目录挂载为卷，在 Dockerfile 中，我们可以事先指定某些目录挂载为匿名卷，这样在运行时如果用户不指定挂载，其应用也可以正常运行，不会向容器存储层写入大量数据。 1VOLUME /data 这里的 /data 目录就会在运行时自动挂载为匿名卷，任何向 /data 中写入的信息都不会记录进容器存储层，从而保证了容器存储层的无状态化。当然，运行时可以覆盖这个挂载设置。比如：1docker run -d -v mydata:/data xxxx 在这行命令中，就使用了 mydata 这个命名卷挂载到了 /data 这个位置，替代了 Dockerfile 中定义的匿名卷的挂载配置。 EXPOSE 声明端口格式1格式为 EXPOSE &lt;端口1&gt; [&lt;端口2&gt;...] EXPOSE 指令是声明运行时容器提供服务端口，这只是一个声明，在运行时并不会因为这个声明应用就会开启这个端口的服务。在 Dockerfile 中写入这样的声明有两个好处，一个是帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射；另一个用处则是在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口。 要将 EXPOSE 和在运行时使用 -p &lt;宿主端口&gt;:&lt;容器端口&gt; 区分开来。-p，是映射宿主端口和容器端口，换句话说，就是将容器的对应端口服务公开给外界访问，而 EXPOSE 仅仅是声明容器打算使用什么端口而已，并不会自动在宿主进行端口映射。 WORKDIR 指定工作目录格式1WORKDIR &lt;工作目录路径&gt; 使用 WORKDIR 指令可以来指定工作目录（或者称为当前目录），以后各层的当前目录就被改为指定的目录，如该目录不存在，WORKDIR 会帮你建立目录。 之前提到一些初学者常犯的错误是把 Dockerfile 等同于 Shell 脚本来书写，这种错误的理解还可能会导致出现下面这样的错误：12RUN cd /appRUN echo &quot;hello&quot; &gt; world.txt 如果将这个 Dockerfile 进行构建镜像运行后，会发现找不到 /app/world.txt 文件，或者其内容不是 hello。原因其实很简单，在 Shell 中，连续两行是同一个进程执行环境，因此前一个命令修改的内存状态，会直接影响后一个命令；而在 Dockerfile 中，这两行 RUN 命令的执行环境根本不同，是两个完全不同的容器。这就是对 Dockerfile 构建分层存储的概念不了解所导致的错误。 之前说过每一个 RUN 都是启动一个容器、执行命令、然后提交存储层文件变更。第一层 RUN cd /app 的执行仅仅是当前进程的工作目录变更，一个内存上的变化而已，其结果不会造成任何文件变更。而到第二层的时候，启动的是一个全新的容器，跟第一层的容器更完全没关系，自然不可能继承前一层构建过程中的内存变化。 因此如果需要改变以后各层的工作目录的位置，那么应该使用 WORKDIR 指令。 USER 指定当前用户格式1USER &lt;用户名&gt;[:&lt;用户组&gt;] USER 指令和 WORKDIR 相似，都是改变环境状态并影响以后的层。WORKDIR 是改变工作目录，USER 则是改变之后层的执行 RUN, CMD 以及 ENTRYPOINT 这类命令的身份。 当然，和 WORKDIR 一样，USER 只是帮助你切换到指定用户而已，这个用户必须是事先建立好的，否则无法切换。 123RUN groupadd -r redis &amp;&amp; useradd -r -g redis redisUSER redisRUN [ &quot;redis-server&quot; ] 如果以 root 执行的脚本，在执行期间希望改变身份，比如希望以某个已经建立好的用户来运行某个服务进程，不要使用 su 或者 sudo，这些都需要比较麻烦的配置，而且在 TTY 缺失的环境下经常出错。建议使用 gosu 12345678# 建立 redis 用户，并使用 gosu 换另一个用户执行命令RUN groupadd -r redis &amp;&amp; useradd -r -g redis redis# 下载 gosuRUN wget -O /usr/local/bin/gosu &quot;https://github.com/tianon/gosu/releases/download/1.7/gosu-amd64&quot; \ &amp;&amp; chmod +x /usr/local/bin/gosu \ &amp;&amp; gosu nobody true# 设置 CMD，并以另外的用户执行CMD [ &quot;exec&quot;, &quot;gosu&quot;, &quot;redis&quot;, &quot;redis-server&quot; ] HEALTHCHECK 健康检查格式12HEALTHCHECK [选项] CMD &lt;命令&gt;：设置检查容器健康状况的命令HEALTHCHECK NONE：如果基础镜像有健康检查指令，使用这行可以屏蔽掉其健康检查指令 HEALTHCHECK 指令是告诉 Docker 应该如何进行判断容器的状态是否正常 当在一个镜像指定了 HEALTHCHECK 指令后，用其启动容器，初始状态会为 starting，在 HEALTHCHECK 指令检查成功后变为 healthy，如果连续一定次数失败，则会变为 unhealthy。 HEALTHCHECK 支持下列选项： --interval=&lt;间隔&gt;：两次健康检查的间隔，默认为 30 秒； --timeout=&lt;时长&gt;: 健康检查命令运行超时时间，如果超过这个时间，本次健康检查就被视为失败，默认 30 秒； --retries=&lt;次数&gt;：当连续失败指定次数后，则将容器状态视为 unhealthy，默认 3 次。 和 CMD, ENTRYPOINT 一样，HEALTHCHECK 只可以出现一次，如果写了多个，只有最后一个生效。 原文地址,参考文章地址]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-7-Dockerfile的简单使用]]></title>
    <url>%2F2019%2F05%2F25%2FDocker-7-Dockerfile%E7%9A%84%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[述上文中,我们使用commit的方式创建了一个本地的镜像,但是commit方式存在一些问题,比如不够透明化,无法重复,体积较大,为了解决这些问题,可以使用Dockerfile Dockerfile是一个文本文件,其内包含了一条条的指令(Instruction),每一条指令构建一层,因此每一条指令的内容,就是描述该层应当如何构建. 书写规则Dockerfile的指令是忽略大小写的,建议使用大写,使用#作为注释,每一行只支持一条指令,每条指令可以携带多个参数 示例先来看一个简单的例子,首先找个空白目录创建一个名为Dockerfile的文件,命令如下:1touch Dockerfile 编辑文件内容,如下:123FROM nginxMAINTAINER zhou &quot;xxx@sina.com&quot;RUN echo &apos;hello docker!&apos;&gt;/usr/share/nginx/html/index.html 在这个Dockerfile中一共就三行,用到了三个指令,FROM,MAINTAINER和RUN 命令详解FROM指定镜像,这里表示已nginx为基础,在该镜像的基础上构建 MAINTAINER表示镜像作者的信息,这命令可以不写 RUNRUN指令用来修改镜像,算是使用比较频繁的一个指令了,该指令可以用来安装程序、安装库以及配置应用程序等. 一个RUN指令执行会在当前镜像的基础上创建一个新的镜像层,接下来的指令将在这个新的镜像层上执行.这里要注意,不要写很多个RUN,否则会建立很多层,Union FS是有最大层数限制的,比如AUFS,曾经是最大不得超过42层,现在是不得超过 127 层. 可以使用&amp;&amp;将各个所需执行的命令串联起来,而且Dockerfile 支持 Shell 类的行尾添加 \ 的命令换行方式 RUN语句有两种不同的形式:shell格式和exec格式,本案例采用的shell格式,shell格式就像linux命令一样,exec格式则是一个JSON数组,将命令放到数组中即可 构建镜像在刚刚创建的Dockerfile的目录下执行下面的命令去构建镜像:1docker build -t zhou/nginx:v3 . 执行结果如下: 从输出结果中,就能清晰的看到镜像的构建过程. 构建完成之后就可以用这个镜像去运行容器了. 这里构建命令的-t参数,用来指定镜像的命名空间,仓库名以及TAG等信息 命令最后有一个.,指的是镜像构建上下文官方的描述是这样的: Docker采用了C/S架构，分为Docker客户端（Docker可执行程序）与Docker守护进程，Docker客户端通过命令行和API的形式与Docker守护进程进行通信，Docker守护进程则提供Docker服务。因此，我们操作的各种docker命令实际上都是由docker?客户端发送到docker守护进程上去执行。我们在构建一个镜像时，不可避免的需要将一些本地文件拷贝到镜像中，例如上文提到的COPY命令，用户在构建镜像时，需要指定构建镜像的上下文路径（即前文的.）,docker build在获得这个路径之后，会将路径下的所有内容打包，然后上传给Docker引擎。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-6-构建自己的镜像]]></title>
    <url>%2F2019%2F05%2F25%2FDocker-6-%E6%9E%84%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[述上文中,介绍了对于镜像的一些常用的操作,还有镜像和容器的关系等,本文来看一下如何构建一个自己的镜像 创建容器这里我们创建两个nginx的容器,首先就需要有nginx的镜像,如果没有的话可以用以下命令下载一个下来:1docker pull nginx 首先启动一个nginx容器,然后复制一个本地的html进去做首页,如下: 可以看到这里容nginx的首页已经换成是我们自定义的了 然后再启动一个nginx,这次不替换首页, 用的镜像还是和上面的一样,命令如下: 这里可以发现我们第二次创建的这个容器的首页还是默认页面没有发生改变的,所以容器的修改并不能影响镜像 创建本地镜像具体操作如下: 这里主要命令是这个:1docker commit -m &quot;update index.html&quot; --author=&apos;zz&apos; 415fe3874fb7 zhou/nginx:v1 -m: 提交的备注 --author: 表示镜像的作者 415fe3874fb7: 创建镜像所依据的容器id zhou/nginx: 表示仓库名,zhou是名称空间,nginx是镜像名 v1: 表示仓库的tag 如图中,镜像创建完成之后,就可以通过docker images查看到刚刚创建的镜像了. 测试我们用刚刚创建的镜像建一个容器来看一下 如图,可以看到,这里的首页已经是改变过的了 在实际的使用过程中,并不会通过docker commit的方式创建镜像 使用 docker commit 意味着所有对镜像的操作都是黑箱操作，生成的镜像也被称为黑箱镜像，换句话说，就是除了制作镜像的人知道执行过什么命令、怎么生成的镜像，别人根本无从得知。而且，即使是这个制作镜像的人，过一段时间后也无法记清具体在操作的。虽然 docker diff 或许可以告诉得到一些线索，但是远远不到可以确保生成一致镜像的地步。这种黑箱镜像的维护工作是非常痛苦的。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-5-镜像的一些基本操作]]></title>
    <url>%2F2019%2F05%2F25%2FDocker-5-%E9%95%9C%E5%83%8F%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[述前面的文章中说的都是一些针对容器的操作,下面再来看一下镜像,镜像是容器运行的基础,容器是镜像运行后的形态 镜像是一个包含程序运行必要的环境和代码的只读文件,它采用分层的文件系统,将每一层的改变以读写的形式增加到原来的只读文件上 镜像和容器的关系前文中我们已经使用过容器了,我们在创建或者启动一个容器的时候,必须要指定一个镜像的名称或者id,这时镜像所扮演的角色就是容器的模板,不同的镜像可以构造出不同的容器. 我们上文中运行了一个nginx容器,命令如下:1docker run -itd --name nginx -p 80:80 nginx 命令中的最后一个nginx就表示创建这个容器所需要的模板 镜像的体系结构镜像的最底层是一个启动文件系统(bootfs)镜像,bootfs的上层镜像叫做根镜像,一般来说,根镜像是一个操作系统,例如Ubuntu,CentOS等,用户的镜像必须构建于根镜像之上,在根镜像之上,用户可以构建出各种各样的其他镜像 镜像的本质其实就是一系列文件的集合,一层套一层的结构有点类似于Git. 镜像的写时复制机制当docker第一次启动一个容器的时候,初始的读写层是空的,当文件系统发生变化时,这些变化都会应用到这一层之上,比如,如果你想修改一个文件,这个文件首先会从该读写层下面的只读层复制到该读写层,由此,该文件的只读版本依然存在于只读层,只是被读写层的该文件副本隐藏,该机制本被称之为写时复制. 查看本地镜像通过以下命令可以查询到所有的本地镜像:1docker images 返回值详解可以看到这里一共是有5个信息,下面来详细的看一下每个列代表的是什么意思 REPOSITORY仓库名称,仓库一般用来存放同一类型的镜像,仓库的名称由其创建者指定,如果没有指定则为\&lt;none>,一般来说,仓库名称有如下几种不同的形式: [namespace\ubuntu]: 这种仓库名称由命名空间和实际的仓库名组成,中间通过\隔开.当开发者在Docker Hub上创建一个用户时,用户名就是默认的命名空间,这个命令空间是用来区分Docker Hub上注册的不同用户或者组织(类似于GitHub上用户名的作用),如果将自己的镜像上传到Docker Hub上供别人使用,则必须指定命名空间 [ubuntu]:这种只有仓库名,对于这种没有命名空间的仓库名,可以认为其属于顶级命名空间,该空间的仓库只用于官方的镜像,由Docker官方进行管理,但一般会授权给第三方进行开发维护.当然用户自己创建的镜像也可以使用这种命名方式,但是将无法上传到Docker Hub上共享. [daocloud.io/library/redis]: 这种指定url路径的方式,一般用于非Docker Hub上的镜像命名,例如一个第三方服务商提供的镜像或者开发者?自己搭建的镜像中心,都可以使用这种命名方式命名 TAGTAG是镜像的标签,用来区分同一仓库中的不同镜像,默认是latest IMAGE_ID镜像的ID,唯一标识符 CREATED表示镜像创建的时间 SIZE表示镜像的大小 通配符过滤使用docker images可以列出所有的镜像,但是当我们本地镜像太多的时候,还可以通过通配符匹配,如下: 如果想要查看镜像的详细信息,也可以通过以下命令去查看:1docker inspect 镜像ID/名称 下载镜像我们去执行docker run命令的时候,如果本地没有镜像,就会自动去Docker Hub去下载,之前已经演示过了,我们也可以通过命令去Docker Hub上去搜索符合要求的镜像,命令如下:1docker search 镜像名称 返回列详解 NAME: 镜像名称 DESCRIPTION: 描述 STARS: 表示用户对镜像的评分,评分越高越可以放心使用 OFFICIAL: 是否为官方镜像 AUTOMATED: 是否使用了自动构建 在使用docker run运行容器的时候再去下载镜像,速度可能会有点慢,所以我们可以在镜像运行之前先把镜像给下载下来,命令如下:1docker pull 指定的镜像 这里就跟从github上往下拉代码是差不多的 删除镜像删除镜像的命令是docker rmi 镜像id/镜像名称,之前我们有用过,这里就不多说了]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-4-容器的导入导出]]></title>
    <url>%2F2019%2F05%2F25%2FDocker-4-%E5%AE%B9%E5%99%A8%E7%9A%84%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BA%2F</url>
    <content type="text"><![CDATA[述docker的一大优势,就是可移植性,docker容器可以随意的进行导入导出,下面来看一下具体怎么操作 准备工作首先,我们创建一个容器,然后做一些基本的配置,这里以nginx为例,创建一个nginx容器,然后启动,将本地的一个index.html文件上传到容器中去,具体步骤如下 首先创建一个index.html文件,内容如下,路径自己随意,我这里是/usr/docker/nginx/,内容如下:12345678910&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;测试页面&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;Hello Docker!&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; 创建完成之后,运行一个nginx容器,命令如下:1docker run -itd --name nginx -p 80:80 nginx 最后把本地的这个网页copy到容器中去,命令如下:1docker cp /usr/docker/nginx/index.html nginx:/usr/share/nginx/html/ 这时候,访问ip:80,内容如下: 容器导出上面的操作完成之后,就可以通过export命令将容器导出,如下:1docker export 容器id/名称 &gt; 导出的路径 如图,这样就导出完成了 容器导入把我们现在docker中的镜像和容器都删掉然后做导入的操作,删除操作如下: 查看所有镜像的命令是:1docker images 删除镜像的命令是:1docker rmi 镜像id 删除完成之后就可以导入了,命令如下:12cd /usr/docker/nginxcat nginx.tar | docker import - importednginx:ilatest 如下: 导入完成之后,镜像里面就有了,就可以通过docker run去启动容器了]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-3-容器的基本操作II]]></title>
    <url>%2F2019%2F05%2F25%2FDocker-3-%E5%AE%B9%E5%99%A8%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9CII%2F</url>
    <content type="text"><![CDATA[述接着上文,还是来看一些对容器的操作 启动容器在上文中,我们分别使用了docker run和docker create的方式创建了一个容器,前者创建完容器之后就直接启动了,那对于docker create我们可以使用docker start命令来启动,命令如下:1docker start 容器id/容器名称 如图,就是通过容器的id去启动容器的,docker start启动的是一个已经存在的容器,要使用该命令启动一个容器,必须要先知道容器的id或者name 一般来说,第一次启动容器可以直接用docker run以后就用docker start就好了 重启容器容器在运行过程中,会不可避免的出问题,出了问题的时候,需要容器能够自动重启,我们可以在容器启动的时候使用–restart参数实现这一需求. docker的重启策略有以下几种: no: 表示不重启,默认即此 on:failure:[max-retries]: 在容器非正常退出(退出状态非0),才会重启容器,可以指定一个最大重试次数,重启次数达到上限之后会放弃重启 always: 始终重启容器,当docker守护进程启动时,无论容器当时的状态为何,都会尝试重启容器 ubless-stopped: 表示始终重启容器,但是当docker守护进程启动时,如果容器已经停止运行,则不会去重启它 停止容器通过docker stop命令可以停止一个容器1docker stop 容器id/容器名称 删除容器容器停止后,还是存在的,如果我们不需要了,可以通过docker rm去删除一个容器,删除容器只能删除已经停止的容器1docker rm 容器id/容器名 如图,只能删除停止状态的容器 , 如果非要删除一个运行状态的容器的话,可以通过加 -f参数实现,语法如下:1docker rm -f 容器id/容器名 批量删除批量删除命令如下:1docker rm $(docker ps -a -q) docker ps -a -q会列出所有的容器的id,供删除,同样的,这里也是只能删除停止的容器 还有一个命令,可以删除所有的停止状态的容器,如下:1docker container prune 查看容器信息容器创建成功后,可以通过docker inspect查看容器的详细信息,包括容器的id,容器名称,环境变量,运行命令,主机配置,网络配置以及数据卷等信息1docker inspect 容器id/名称 返回值如下: format参数上面返回的数据实在是太多了,看起来也不方便,使用format参数可以只查看用户关心的数据,如下: 查看容器运行状态:1docker inspect -f=&apos;&#123;&#123;.State.Running&#125;&#125;&apos; 容器id/名称 查看容器的id和名称:1docker inspect -f=&apos;&#123;&#123;.Name&#125;&#125; &#123;&#123;.ID&#125;&#125;&apos; 容器id/名称 查看容器内的进程查看某个容器中有哪些正在运行的进程,可以用以下命令:1docker top 容器id/名称 这里要确保查询的容器是启动的 查看容器日志要查看容器的日志可以使用以下命令:1docker logs 容器id/名称 我这里查看了一个redis容器的日志,返回值如下: 详细参数默认情况下只能看到历史日志,不能看到实时的日志,我们可以使用-f参数查看实时日志1docker logs -f 容器id/名称 这样看到的就是实时的日志了,但是日志又太多了,可以使用--tail参数控制日志输出行数,同时可以用-t来显示日志的输出时间,如下:]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-2-容器的基本操作I]]></title>
    <url>%2F2019%2F05%2F25%2FDocker-2-%E5%AE%B9%E5%99%A8%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9CI%2F</url>
    <content type="text"><![CDATA[述前文中简单的了解了一下docker,也在CentOS7的环境下安装了docker,那下面就来看一下docker中的一些基本的操作 查看容器查看正在运行的容器1docker ps 查看所有容器1docker ps -a 查看最近创建的容器查看最近创建的一个容器:1docker ps -l 查看最近创建的x个容器1docker ps -n=x 返回值详解 CONTAINER ID: 容器的唯一ID,是一个64位的十六进制整数,在不会混淆的情况下,可以只采用id的前几位进行标识一个容器 IMAGE: 表示创建容器时所使用的镜像 COMMAND: 表示容器最后运行的命令 CREATED: 创建容器的时间 STATUS: 容器的状态,这里可能显示一个容器的启动时间也可能是容器的关闭时间,具体要看容器当前的状态 PORTS: 容器对外开放的端口 NAMES: 容器的名字,如果不设置,会有一个默认的名字 创建容器docker创建容器有两种方式,一种是创建完成直接启动,一种是先创建,完了再去启动,两种方式其实是差不多的,当执行一个创建命令后,docker首先会去本地路径下找是不是有相应的镜像,如果没有的话就去docker hub上去搜索,搜到了的话先下载下来,然后再用刚下载下来的镜像创建[启动]容器. 容器的文件系统是在只读的镜像文件上添加一层可读可写的文件层,这样可以使在不改变镜像的情况下,只记录改变的数据. 创建一个容器不启动使用以下命令可以创建一个容器但是不启动,创建完成之后容器的状态处于停止状态:1docker create 镜像ID/镜像名 举个例子,比如我们要创建一个nginx的容器,命令如下:1docker create nginx 首先它会先看我本地有没有这个nginx的镜像,发现并没有,然后就去docker hub里面去下载这个镜像,下载的版本是最新的镜像,下载完成之后再用刚下载下来的镜像去创建容器 然后创建完成之后用docker ps -a查看一下,可以看到他的状态是Created,并没有启动,然后名称是一个默认的名称 这里我们也可以指定名称去创建容器,如下:1docker create --name=nginx nginx 这次创建我们本地有了镜像之后,就直接创建了不用再去docker hub去下载了 上面创建了容器之后,并没有去启动,下面再来看下直接创建并启动 创建容器并启动使用docker run命令可以创建并启动一个容器,这个命令又可以启动两种不同模式的容器:后台型容器和交互型容器 后台型容器就是一个在后台运行的容器,不需要和开发者进行交互,而交互型容器则需要接收开发者的输入进行处理给出反馈,在实际使用中,大部分情况都是创建后台型容器,不过在很多时候,后台型容器也不可避免进行交互 后台型容器还是以nginx为例,命令如下:1docker run --name nginx -d -p 8080:80 nginx –name 创建的容器名称 -d 表示容器在后台裕兴 -p 将容器的80端口映射到宿主机的8080端口 nginx 使用的镜像的名字/这里也可以替换成镜像的id 创建好了之后,可以看到他已经运行起来了,这时候我们就可以通过ip和端口访问了,如下: 这就是一个后台型容器的基本创建方式,再来看一下交互型容器 交互型容器比如要创建一个ubuntu容器,开发者可能需要在ubuntu上面输入命令执行相关操作,创建方式如下: 1docker run --name ubuntu -it ubuntu /bin/bash it: i表示开发容器的标准输入(STDIN),t表示告诉docker,为容器创建一个命令行终端 上图中,第一次创建是失败了,拉取镜像超时了,这是因为天朝网络的原因,多试几次就好了 然后拉取下来创建完成之后,进入了一个命令终端,在这个终端里面就可以操作我们刚创建ubuntu了 最后操作完成之后使用exit命令就退出去了,退出的同时,这个容器就关闭了,并不会后台运行,等我们下次再手动启动这个容器的时候就变成了后台运行的了 后台型进行交互我们上面创建的后台型容器如何进行交互呢, 也就是说怎样进入容器内部呢? 可以使用以下命令:1docker exec -it 容器id/容器名称 /bin/bash -ti的意思还是和上面一样的,就不多说了,退出容器用exit命令就退出了]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-1-CentOS7环境下安装与运行]]></title>
    <url>%2F2019%2F05%2F25%2FDocker-1-CentOS7%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%AE%89%E8%A3%85%E4%B8%8E%E8%BF%90%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[前置条件Docker支持以下的CentOS版本: CentOS 7(64-bit) CentOS 6.5(64-bit)或者更高的版本 Docker 运行在 CentOS 7 上,要求系统为64位,系统内核版本为 3.10 以上. Docker 运行在 CentOS-6.5 或更高的版本的 CentOS 上,要求系统为64位,系统内核版本为 2.6.32-431 或者更高版本. 安装首先docker要求系统内核版本为3.10以上,先查看系统版本是否支持docker 查看系统内核版本1uname -r 安装docker1yum -y install docker 验证是否安装成功1docker version 出现版本号等信息则安装成功,如下:12345Client: Version: 1.13.1 API version: 1.26 Package version: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? 设置开启自启动1sudo systemctl enable docker 启动docker1systemctl start docker 常用启动关闭命令启动1systemctl start docker 守护进程重启1sudo systemctl daemon-reload 重启docker服务1systemctl restart docker 或者1sudo service docker restart 关闭docker1service docker stop 或1systemctl stop docker]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-0-什么是Docker]]></title>
    <url>%2F2019%2F05%2F25%2FDocker-0-%E4%BB%80%E4%B9%88%E6%98%AFDocker%2F</url>
    <content type="text"><![CDATA[述如今Docker的使用已经非常普遍了,使用Docker技术可以帮助企业快速水平扩展服务,从而到达弹性部署业务的能力,在如今微服务架构越来越流行的情况下,使用微服务+Docker的完美组合,更加方便微服务架构运维部署落地 那么,到底什么是Docker,为什么要使用Docker,它有什么优势呢? 什么是DockerDocker 是一个开源的应用容器引擎,让开发者可以打包他们的应用以及依赖包到一个可移植的容器中,然后发布到任何流行的Linux 机器上,也可以实现虚拟化.容器是完全使用沙箱机制,相互之间不会有任何接口 为什么要使用Docker容器除了运行其中的应用外,基本不消耗额外的系统资源,使得应用的性能很高,同时系统的开销尽量小, 按照传统虚拟机运行的方式的话10个不同的应用就需要有10个虚拟机,而Docker只需要启动10个隔离的应用即可 Docker主要有以下几个优势: 简化环境管理在我们开发的过程中,经常会遇到代码在开发环境中可以运行,但是一到生产环境就运行不了了,这个问题很常见,对于开发和运维来说,最希望的就是一次创建和配置就可以在任意环境下正常运行 开发者可以使用一个标准的镜像来构建一套开发环境的容器,开发完成之后,运维可以直接用这个容器去部署,这样也就不会出现换了环境代码不能用的情况了 虚拟化更加轻量级说到容器,虚拟化,很多人总会想到虚拟机,想到VMware、VirtualBox等工具，,不同于这些虚拟技术,docker虚拟化更加轻量级 传统的虚拟机都是先虚拟出一个操作系统,然后在操作系统上完成各种各样的配置,这样并不能充分利用物理机的技能,docker是操作系统级别的虚拟技术,它运行在操作系统之上的用户空间,所有的容器都共用一个系统内核甚至公共库,容器引擎提供了进程级别的隔离,让每个容器都像运行在单独的系统之上,但是又能够共享很多底层资源.因此docker更为轻量、快速和易于管理 更轻松的迁移和扩展Docker容器几乎可以在任意平台上运行,包括物理机,虚拟机,服务器,个人电脑等等,这种兼容性可以让用户可以把一个应用程序从一个平台直接迁移到另一个平台上 Docker和虚拟机作为一种轻量级的虚拟化方式,Docker在运行应用上跟传统的虚拟机方式相比更有显著优势: Docker容器很快,启动停止可以在秒级别实现,这相比传统虚拟机方式要快的多 Docker容器对系统资源需求很少,一台主机上可以同时运行数千个docker容器 Docker通过类似Git的操作来方便用户获取,分发,和更新应用镜像,指令简明,学习成本低 Docker通过Dockerfile配置文件来支持灵活的自动化创建和部署机制,提高工作效率 特性 容器 虚拟机 启动速度 秒级 分钟级 硬盘使用 一般为MB 一般为GB 性能 接近原生 弱于 系统支持量 单机支持上千个容器 一般几十个 隔离性 安全隔离 完全隔离 虚拟机是实现了硬件上的虚拟,而Docker则是实现了操作系统级别的虚拟 Docker应用场景 加速本地开发 自动打包和部署应用 创建轻量,私有的PaaS环境 自动化测试和持续集成/部署 部署并扩展web应用,数据库和后端服务器 创建安全沙盒 轻量级的桌面虚拟化 Docker核心组件docker中有三大核心组件: 镜像镜像是一个只读的静态模板,它保存了容器需要的环境和应用的执行代码,可以将镜像看成是容器的代码,当代码运行起来之后,就成了容器,镜像和容器的关系也类似于程序和进程的关系 容器容器是一个运行时环境,是镜像的一个运行状态,它是镜像执行的动态表现. 库库是一个特定的用户存储镜像的目录,一个用户可以建立多个库来保存自己的镜像. 原文地址本系列文章来源]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-Redis-三主三从集群架构搭建]]></title>
    <url>%2F2019%2F04%2F27%2FDocker-Redis-%E4%B8%89%E4%B8%BB%E4%B8%89%E4%BB%8E%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[述上文中,我们搭建了一个一主二仆的一个Redis架构,在实际生产环境中,我们往往需要搭建一个redis集群,那本文来看一下,基于docker搭建一个三主三从的redis集群 环境准备一开始,准备还是用上面文章中拉下来的3.2.9版本的redis,结果在搭建集群的时候出了一些问题,可能是配置的原因,最后也没有深入去研究,所以这里还是用一个新的版本.5.0.0来搭建 然后拉一个redis5.0.0版本的下来1docker pull redis:5.0.0 创建集群配置文件在/usr/docker/redis-cluster下创建一个文件(这个路径是可以自定义的),redis-cluster.tmpl,文件内容如下:123456789101112131415161718#redis端口port $&#123;PORT&#125;#关闭保护模式，允许外网访问protected-mode no# 开启集群模式 cluster-enabled yes#集群配置名cluster-config-file nodes.conf#超时时间 cluster-node-timeout 5000#搭建集群主机的外网ipcluster-announce-ip $&#123;IP&#125;#节点映射端口cluster-announce-port $&#123;PORT&#125;#节点总线端cluster-announce-bus-port 1$&#123;PORT&#125;#持久化模式appendonly yes 创建每个节点的配置文件和文件夹首先需要两个变量,一个是我们的ip地址,一个是刚刚的目录,命令如下:1ip=xxx.xxx.xx.xx 1redisdir=&quot;/usr/docker/redis-cluster&quot; 变量搞定之后,就是循环创建文件夹和文件了,命令如下:12345for port in `seq 7000 7005`; do \ mkdir -p $&#123;redisdir&#125;/$&#123;port&#125;/conf \ &amp;&amp; PORT=$&#123;port&#125; IP=$&#123;ip&#125; envsubst &lt; ./redis-cluster.tmpl &gt; $&#123;redisdir&#125;/$&#123;port&#125;/conf/redis.conf \ &amp;&amp; mkdir -p $&#123;redisdir&#125;/$&#123;port&#125;/data; \done 执行完毕之后,随便打开一个看下,下面这样子,变量都被替换掉就ok了: 创建docker自定义网桥执行命令1docker network create redis-net 查看docker所有的网桥,命令是:1docker network ls 运行各个redis节点ok,上面的步骤完成之后,就可以启动容器了 定义一个初始值变量,用于叠加ip1execsh=&apos;/usr/local/bin/redis-cli --cluster create &apos; 这里这个路径就是这样的,不要去修改 然后循环运行6个容器123456789for port in `seq 7000 7005`; do \ docker run -d -ti -p $&#123;port&#125;:$&#123;port&#125; -p 1$&#123;port&#125;:1$&#123;port&#125; \ -v $&#123;redisdir&#125;/$&#123;port&#125;/conf/redis.conf:/usr/local/etc/redis/redis.conf \ -v $&#123;redisdir&#125;/$&#123;port&#125;/data:/data \ --privileged=true \ --restart always --name redis-$&#123;port&#125; --net redis-net \ --sysctl net.core.somaxconn=1024 redis:5.0.0 redis-server /usr/local/etc/redis/redis.conf; \ execsh=$&#123;execsh&#125;`docker inspect redis-$&#123;port&#125; | grep &quot;IPAddress&quot; | grep --color=auto -P &apos;(\d&#123;1,3&#125;.)&#123;3&#125;\d&#123;1,3&#125;&apos; -o`:$&#123;port&#125;&apos; &apos;done 如下图,就创建成功了 完成后执行下面的命令.往上面声明的那个变量后面追加--cluster-replicas 11execsh=$&#123;execsh&#125;&apos;--cluster-replicas 1&apos; 最后将拼接好的命令打印到控制台,后面进入到redis容器中需要用到1echo $&#123;execsh&#125; 返回值应该是这样的:1/usr/local/bin/redis-cli --cluster create 172.18.0.2:7000 172.18.0.3:7001 172.18.0.4:7002 172.18.0.5:7003 172.18.0.6:7004 172.18.0.7:7005 --cluster-replicas 1 这个返回的数据复制下来,等下要用 查看已启动的容器看一下6个容器是不是都启动了,如下:1docker ps 进入容器创建集群上面我们启动完成之后,都是单独的redis服务,并不是一个集群,然后我们先进入redis-7000这个容器的内部,命令如下:1docker exec -ti redis-7000 /bin/bash 然后执行我们上面复制下来的那个shell脚本1/usr/local/bin/redis-cli --cluster create 172.18.0.2:7000 172.18.0.3:7001 172.18.0.4:7002 172.18.0.5:7003 172.18.0.6:7004 172.18.0.7:7005 --cluster-replicas 1 返回如下: 红色圈住的地方,需要手动输入个yes. 校验是否创建成功在容器内使用redis-cli连接1redis-cli -p 7000 -c -c就表示集群模式 如图,我这里获取数据,他自动跳到了7002节点上面 接下来我们把7002节点先停了 然后,再次进到7000容器内部,再get数据,如下: 7002关闭了之后,又跳到了7005上面了 搭建成功最后再把刚停掉的节点启动起来,至此,一个三主三从的redis集群就搭建好了,参考文章,这里还有一种更方便的方法,可以看看]]></content>
      <categories>
        <category>Docker-Redis</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-Redis-配置主从和哨兵]]></title>
    <url>%2F2019%2F04%2F27%2FDocker-Redis-%E9%85%8D%E7%BD%AE%E4%B8%BB%E4%BB%8E%E5%92%8C%E5%93%A8%E5%85%B5%2F</url>
    <content type="text"><![CDATA[述上文中,我们使用docker安装了redis,下面来配置一个一主二仆的一个主从结构,并且设置哨兵,基于上文中的环境,我们现在已经有一个Redis实例了 环境准备上文中我们是挂载的外部的配置文件,和数据文件,然后我们把这个文件夹复制两个出来,改下名字,如下: 然后修改这两个新的redis中,配置文件的端口号,分别该为6380 6381 修改完成之后,分别运行以下命令:1234docker run -p 6380:6380 --name redis-6380 -v /usr/docker/redis-6380/conf/redis.conf:/etc/redis/redis.conf -v /usr/docker/redis-6380/data:/data --privileged=true -d 3459037fcc3a redis-server /etc/redis/redis.conf --appendonly yesdocker run -p 6381:6381 --name redis-6381 -v /usr/docker/redis-6381/conf/redis.conf:/etc/redis/redis.conf -v /usr/docker/redis-6381/data:/data --privileged=true -d 3459037fcc3a redis-server /etc/redis/redis.conf --appendonly yes 运行完成之后,可以用docker ps来查看是否启动成功 启动好之后,我们就有三个redis的服务了,我这里的ip和端口分别如下:123172.16.12.3 6379 172.16.12.3 6380172.16.12.3 6381 主从配置环境设置好之后,我们把6379设置为Master,然后6380和6381设置为slave 先进入6380的容器内部:1docker exec -it 容器id /bin/bash 使用redis-cli设置主从, 或者是使用远程的redis-cli,执行以下命令:1SLAVEOF 172.16.12.3 6379 6381也这样操作就好了. 这里要注意的是,如果主机设置了访问密码的话,从机的配置中需要加入以下配置:1masterauth 123456 两个从机都设置完成之后,主从就搭建好了, 可以通过info replication来查看Redis的角色等信息 哨兵配置我们现在是有三个redis的容器,然后我们在每个容器中再启动一个哨兵的服务,也就是说,一个容器中有一个redis的服务,还有一个哨兵的服务 这里的三个容器的配置都是一样的,这里只说一个的配置 首先,进入容器内部,命令如下:1docker exec -ti 容器ID /bin/bash 进入根目录1cd / 创建一个哨兵的配置文件1touch sentinel.conf 用vim编辑这个配置文件,这里容器默认是没有安装vim的,需要自己装一个,命令如下:12apt-get updateapt-get install vim –y 安装完成之后,就可以使用vim了,编辑刚创建的配置文件:1vim sentinel.conf 文件内容如下:12345sentinel monitor mymaster 172.16.12.3 6379 1daemonize yeslogfile &quot;/var/log/redis/sentinel_log.log&quot;protected-mode nosentinel auth-pass mymaster 123456 第一行的配置中 mymaster是给监控主机起的名字,这个是自定义的,然后后面是主机的地址和端口,最后的1表示有多少个哨兵认为主机挂掉了,就进行切换 第二行的配置表示哨兵在后台运行 第三行是哨兵的日志文件地址 第四行把保护模式关闭,保护模式如果开启只接受回环地址的ipv4和ipv6地址链接,拒绝外部链接,而且正常应该配置多个哨兵,避免一个哨兵出现独裁情况,如果配置多个哨兵那如果开启也会拒绝其他sentinel的连接.导致哨兵配置无法生效. 第五行如果redis配置了密码,那这里必须配置认证,否则不能自动切换 最后启动哨兵,命令如下:1redis-sentinel /sentinel.conf 至此一个哨兵就配置启动完成了,其他两台的配置是一样的 验证三个哨兵都启动完成之后,可以把master关闭然后去看哨兵的日志,如果进行了重新选举那就ok了]]></content>
      <categories>
        <category>Docker-Redis</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-Redis-安装运行Redis并挂载外部配置文件和数据]]></title>
    <url>%2F2019%2F04%2F27%2FDocker-Redis-%E5%AE%89%E8%A3%85%E8%BF%90%E8%A1%8CRedis%E5%B9%B6%E6%8C%82%E8%BD%BD%E5%A4%96%E9%83%A8%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%92%8C%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[安装运行Redis拉取镜像在国内的镜像市场找到redis, 选好版本,pull下来,传送门 选一个新的,并且稳定的版本,我这里选择的3.2.9版本 执行以下命令,pull镜像1docker pull daocloud.io/library/redis:3.2.9 创建文件夹创建两个文件夹,一个用来放持久化的数据,一个用来放配置文件,如果不把持久化的数据放在服务器本地的话,重启容器,数据就没了.12mkdir /usr/docker/redis/datamkdir /usr/docker/redis/conf data文件夹用来存放数据,conf文件夹用来存放redis的配置文件 配置文件修改在github中搜索redis,找到对应的版本,我在上面是下载的3.2.9版本,所以我在release中找到3.2.9版本,然后下载源码 下载完成之后找到redis.conf这个文件 bind 127.0.0.1,在配置文件中找到这个配置,注释掉(注释掉才可以远程连接redis) protected-mode yes,找到这个配置改为protected-mode no daemonize yes找到这个配置,注释掉 最后保存修改,文件上传到服务器刚刚创建好的/usr/docker/redis/conf目录下面 启动容器命令:1docker run -p 6379:6379 --name redis-6379 -v /usr/docker/redis/conf/redis.conf:/etc/redis/redis.conf -v /usr/docker/redis/data:/data --privileged=true -d 3459037fcc3a redis-server /etc/redis/redis.conf --appendonly yes 参数解析: -p 端口映射 --name 指定容器的名称是redis-6379 -v 映射容器外部的配置文件和目录 --privileged=true 使容器内的root用户具有真正的root用户的权限,不设置可能会启动失败,提示没有权限 -d 3459037fcc3a -d是以后台的方式启动,3459037fcc3a是最开始拉下来的镜像的id redis-server /etc/redis/redis.conf 指定配置文件启动redis-server进程 --appendonly yes 开启数据持久化 启动完成查看状态可以通过以下命令,查看正在运行的容器1docker ps 完成上面的操作全部搞定之后,就可以通过远程去连接了]]></content>
      <categories>
        <category>Docker-Redis</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-19-常见使用场景]]></title>
    <url>%2F2019%2F04%2F27%2FRedis-19-%E5%B8%B8%E8%A7%81%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%2F</url>
    <content type="text"><![CDATA[述通过之前文章的介绍,现在redis也可以基本的使用了,那么到底什么时候该使用redis呢.下面就来说几个redis的常用场景 使用场景计数器数据统计的需求非常普遍，通过原子递增保持计数。例如，点赞数、收藏数、分享数等。 排行榜排行榜按照得分进行排序，例如，展示最近、最热、点击率最高、活跃度最高等等条件的top list。 用于存储时间戳类似排行榜，使用redis的zset用于存储时间戳，时间会不断变化。例如，按照用户关注用户的最新动态列表。 记录用户判定信息记录用户判定信息的需求也非常普遍，可以知道一个用户是否进行了某个操作。例如，用户是否点赞、用户是否收藏、用户是否分享等。 社交列表社交属性相关的列表信息，例如，用户点赞列表、用户收藏列表、用户关注列表等。 缓存热点数据缓存一些热点数据，例如，PC版本文件更新内容、资讯标签和分类信息、生日祝福寿星列表。 对于热点数据，缓存以后可能读取数十万次，因此，对于热点数据，缓存的价值非常大。例如，分类栏目更新频率不高，但是绝大多数的页面都需要访问这个数据，因此读取频率相当高，可以考虑基于 Redis 实现缓存。 队列使用Redis进行会话缓存。例如，将web session存放在Redis中。 会话缓存 Redis 进行会话缓存。例如，将 web session 存放在 Redis 中。 时效性例如验证码只有60秒有效期，超过时间无法使用，或者基于 Oauth2 的 Token 只能在 5 分钟内使用一次，超过时间也无法使用。 访问频率出于减轻服务器的压力或防止恶意的洪水攻击的考虑，需要控制访问频率，例如限制 IP 在一段时间的最大访问量。 交集、并集和差集在某些场景中，例如社交场景，通过交集、并集和差集运算，可以非常方便地实现共同好友，共同关注，共同偏好等社交关系。 最新动态按照时间顺序排列的最新动态，也是一个很好的应用，可以使用 Sorted Set 类型的分数权重存储 Unix 时间戳进行排序。 原文地址]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-18-缓存击穿、穿透、雪崩等问题及解决方案]]></title>
    <url>%2F2019%2F04%2F27%2FRedis-18-%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E3%80%81%E7%A9%BF%E9%80%8F%E3%80%81%E9%9B%AA%E5%B4%A9%E7%AD%89%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[述上文说了redis的内存淘汰策略,下面再来看一下使用缓存的过程中一些常见的问题 我们在使用redis做缓存的时候,一般流程是这样的 请求进来时候首先查询redis判断是否存在缓存且缓存是否过期 若已经存在不过期的缓存则直接获取返回 若缓存不存在或已过期则重新查询数据库并将该数据存到redis中 用代码表示如下:12345678910111213141516171819@Autowiredprivate RedisTemplate redisTemplate;public List&lt;String&gt; getValueBySql(String key)&#123; System.out.println(&quot;这里模拟从数据库中获取数据&quot;); return new ArrayList&lt;&gt;();&#125;public List&lt;String&gt; getCache(String key)&#123; List&lt;String&gt; resultList = (List&lt;String&gt;)redisTemplate.opsForValue().get(key); if(resultList == null || CollectionUtils.isEmpty(resultList))&#123; //若缓存不存在则从数据库获取并设置时间 resultList = getValueBySql(key); redisTemplate.opsForValue().set(key, resultList, 1000, TimeUnit.SECONDS); return resultList; &#125;else&#123; return resultList; &#125;&#125; 缓存击穿概念如上面的经典缓存流程,在整个流程中我们需要先查询redis,在redis没有的时候再去查数据库最后再将数据库返回的数据存到redis中 如果有一些key被超高并发的访问,如果在某个时间点这些key过期了,恰好这个时间有对这个key的大量的并发请求过来,这些请求先redisTemplate.opsForValue().get(key);,发现在缓存里面没有查到数据,这时候,所有的请求都会去访问DB,大并发的请求可能会瞬间把后端DB压垮 解决方案一使用synchronized+双检查机制(适用于单机模式),代码如下:1234567891011121314151617public List&lt;String&gt; getCacheSave(String key)&#123; List&lt;String&gt; resultList = (List&lt;String&gt;)redisTemplate.opsForValue().get(key); if(resultList == null || CollectionUtils.isEmpty(resultList))&#123; //采用synchronized保证一次只有一个请求进入到这个代码块 synchronized (this)&#123; resultList = (List&lt;String&gt;)redisTemplate.opsForValue().get(key); if(CollectionUtils.isEmpty(resultList))&#123; return resultList; &#125; resultList = getValueBySql(key); redisTemplate.opsForValue().set(key, resultList, 1000, TimeUnit.SECONDS); return resultList; &#125; &#125;else&#123; return resultList; &#125;&#125; 上面代码第一个判断保证在缓存有数据时,让查询缓存的请求不必排队,减小了同步的粒度 synchronized (this)保证查询数据库是同步操作,同一时刻只能有一个请求查询数据库 第二个判断保证所有在redis有缓存时,其他请求无需在查意思数据库.若没有这个判断,其他已经等待synchronized 解锁的请求会在请求一次数据库 解决方案二使用互斥锁(适用于分布式模式),图，使用分布式锁保证只有一个线程查询数据库,其他线程采用重试的方式进行获取: 代码如下:12345678910111213141516171819202122232425262728293031323334public List&lt;String&gt; getCacheSave2(String key,int retryCount) throws InterruptedException &#123; List&lt;String&gt; resultList = (List&lt;String&gt;)redisTemplate.opsForValue().get(key); if(CollectionUtils.isEmpty(resultList))&#123; final String mutexKey = key + &quot;_lock&quot;; boolean isLock = (Boolean) redisTemplate.execute(new RedisCallback() &#123; @Override public Object doInRedis(RedisConnection connection) throws DataAccessException &#123; //只在键key不存在的情况下，将键key的值设置为value,若键key已经存在，则 SETNX 命令不做任何动作 //命令在设置成功时返回 1 ， 设置失败时返回 0 return connection.setNX(mutexKey.getBytes(),&quot;1&quot;.getBytes()); &#125; &#125;); if(isLock)&#123; //设置成1秒过期 redisTemplate.expire(mutexKey, 1000, TimeUnit.MILLISECONDS); resultList = getValueBySql(key); redisTemplate.opsForValue().set(key, resultList, 1000, TimeUnit.SECONDS); redisTemplate.delete(mutexKey); &#125;else&#123; //线程休息50毫秒后重试 Thread.sleep(50); retryCount--; System.out.println(&quot;=====进行重试，当前次数:&quot; + retryCount); if(retryCount == 0)&#123; System.out.println(&quot;====这里发邮件或者记录下获取不到数据的日志，并为key设置一个空置防止重复获取&quot;); List&lt;String&gt; list = Lists.newArrayList(&quot;no find&quot;); redisTemplate.opsForValue().set(key, list, 1000, TimeUnit.SECONDS); return list; &#125; return getCacheSave2(key,retryCount); &#125; &#125; return resultList;&#125; 简单地来说,就是在缓存失效的时候(判断拿出来的值为空),不是立即去load db,而是先使用缓存工具的某些带成功操作返回值的操作(比如Redis的SETNX或者Memcache的ADD)去set一个mutex key,当操作返回成功时,再进行load db的操作并回设缓存,否则,就重试整个get缓存的方法 缓存穿透概念缓存穿透是指查询一个一定不存在的数据,按照规则,在缓存中查不到的话,就会去DB查询,这将导致这个不存在的数据每次请求都要到存储层去查询,失去了缓存的意义.在流量大时,可能DB就挂掉了,要是有人利用不存在的key频繁攻击我们的应用,这就是漏洞. 解决方案有很多种方法可以有效地解决缓存穿透问题,最常见的则是采用布隆过滤器,将所有可能存在的数据哈希到一个足够大的bitmap中,一个一定不存在的数据会被这个bitmap拦截掉,从而避免了对底层存储系统的查询压力.另外也有一个更为简单粗暴的方法,如果一个查询返回的数据为空(不管是数据不存在，还是系统故障),我们仍然把这个空结果进行缓存,但它的过期时间会很短,最长不超过五分钟. 缓存雪崩概念缓存雪崩是指在我们设置缓存时采用了相同的过期时间,导致缓存在某一时刻同时失效,请求全部转发到DB,DB瞬时压力过重雪崩. 解决方案缓存失效时的雪崩效应对底层系统的冲击非常可怕.大多数系统设计者考虑用加锁或者队列的方式保证缓存的单线程(进程)写,从而避免失效时大量的并发请求落到底层存储系统上.这里分享一个简单方案就是将缓存失效时间分散开,比如我们可以在原有的失效时间基础上增加一个随机值,比如1-5分钟随机,这样每一个缓存的过期时间的重复率就会降低,就很难引发集体失效的事件. 好了,关于redis使用中可能会出现的几种问题就先介绍到这里,本文参考文章1,参考文章2]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-17-内存淘汰策略]]></title>
    <url>%2F2019%2F04%2F27%2FRedis-17-%E5%86%85%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[述上文介绍了redis中的过期策略,本文再来看一下内存淘汰策略 内存淘汰策略指的是用户存储的一些key可以被Redis主动的删除,从而可能产生读miss的情况,那为什么Redis会有这种功能呢? 假设我现在有一个redis的服务器,内存是2G,随着系统业务的增长,reids中放的数据越来越多,数据大小可能已经超过2G了,但是这时候应用还是可以正常运行的,这是因为OS中的可见内存并不受物理内存的限制,而是虚拟内存,物理内存不够用的话,OS就会从硬盘上划分出一片空间构建虚拟内存,这是OS为我们解决的,但是,不合理的使用内存可能就会发生频繁的swap,频繁swap的代价是很大的,所以,我们还是需要合理的使用内存,尽量不要让操作系统去解决 Redis中的内存淘汰机制就是为了更好地使用内存,用一定的缓存miss来换取内存的使用效率 内存淘汰过程内存淘汰的过程大致如下: 首先客户端发起了需要申请内存的命令,比如说set命令 Redis检查内存的使用情况,如果已使用的内存大于maxmemory,则开始根据用户配置的不同淘汰策略来淘汰内存,从而换取一定的内存 如果上面都没有问题,这个命令就执行成功 内存淘汰策略内存淘汰只是Redis提供的一个功能,为了更好地实现这个功能,必须为不同的应用场景提供不同的策略,内存淘汰策略讲的是为实现内存淘汰我们具体怎么做,要解决的问题包括淘汰键空间如何选择?在键空间中淘汰键如何选择? redis给我们提供了以下几种内存淘汰策略,供用户选择: noeviction: 当内存使用达到阈值的时候,所有引起申请内存的命令会报错 allkeys-lru: 在主键空间中,优先移除最近未使用的key volatile-lru: 在设置了过期时间的键空间中,优先移除最近未使用的key. allkeys-random: 在设置了过期时间的键空间中,随机移除某个key volatile-ttl: 在设置了过期时间的键空间中,具有更早过期时间的key优先移除 redis的默认的内存淘汰策略是noeviction. 这里补充一下主键空间和设置了过期时间的键空间: 举个例子,假设我们有一批键存储在Redis中,则有那么一个哈希表用于存储这批键及其值,如果这批键中有一部分设置了过期时间,那么这批键还会被存储到另外一个哈希表中,这个哈希表中的值对应的是键被设置的过期时间.设置了过期时间的键空间为主键空间的子集. 简单点说,主键空间就是放的我们的数据,设置了过期时间的键空间就是放的key和它的过期时间 配置方式我们可以在redis.conf中设置maxmemory这个值来开启内存淘汰功能,就是最大能使用的内存,maxmemory为0的时候表示我们对Redis的内存使用没有限制 内存淘汰策略的设置则是通过maxmemory-policy去配置,默认就是noeviction,如下:1maxmemory-policy noeviction 如何选择淘汰策略配置我们是知道怎么配了,但是上面那么多的策略要怎么选择呢? 我们需要了解我们的应用请求对于Redis中存储的数据集的访问方式以及我们的诉求是什么.同时Redis也支持Runtime修改淘汰策略,这使得我们不需要重启Redis实例而实时的调整内存淘汰策略. 下面看看几种策略的适用场景: allkeys-lru: 如果我们的应用对缓存的访问符合幂律分布(也就是存在相对热点数据),或者我们不太清楚我们应用的缓存访问分布状况,我们可以选择allkeys-lru策略 allkeys-random:如果我们的应用对于缓存key的访问概率相等,则可以使用这个策略 volatile-ttl: 这种策略使得我们可以向Redis提示哪些key更适合被eviction 另外,volatile-lru策略和volatile-random策略适合我们将一个Redis实例既应用于缓存和又应用于持久化存储的时候,然而我们也可以通过使用两个Redis实例来达到相同的效果,值得一提的是将key设置过期时间实际上会消耗更多的内存,因此我们建议使用allkeys-lru策略从而更有效率的使用内存 LRUredis采用的LRU是非精准的LRU算法,基于抽样实现,由于redis是单线程的,如果采用全量的数据进行LRU计算,会比较消耗CPU资源,因此,redis提供了一个重要参数1maxmemory-samples 5 该参数表示,每次随机选出5个不经常使用的key进行移除,配置的参数值越大,其LRU结果能接近全量LRU的结果,但同时会给CPU带来比较大的开销,因此如果仅仅将redis作为LRU缓存服务使用,建议保持默认配置即可 原文地址]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-16-过期策略]]></title>
    <url>%2F2019%2F04%2F27%2FRedis-16-%E8%BF%87%E6%9C%9F%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[述上文中介绍了redis和spring boot的整合使用,本文再来看一下redis中的过期策略,算是对之前内容的一个补充 过期策略在之前说命令行的时候,我们有给一个key设置过期时间,reids的过期策略就是说,一个key过期了的话redis会怎么处理这个key 过期策略通常有以下三种: 定时过期: 每个设置过期时间的key都创建一个定时器,时间到了就立即清除. 这种策略可以立即清理过期的数据,对内存友好,但是会占用大量的CPU 资源去处理这些过期的数据,所以会影响缓存的响应时间和吞吐量 惰性过期: 只有当访问一个key的时候,才会判断这个key是不是过期了,过期的话就清除掉,该策略可以最大化的节省CPU资源,但是对内存不友好,极端情况下可能会出现一大堆过期的key没有被访问,还留在内存中,占用资源 定期过期: 每隔一定的时间,去扫描一定数量的数据库的expires字典中的一定数量的key,该策略是前面两个策略的一个折中的方案,通过调整定时扫描的时间间隔和每次扫描的限定耗时,可以在不同情况下使得CPU和内存资源达到最优的平衡效果 redis中的过期策略及配置Redis中同时使用了惰性过期和定期过期两种过期策略 定期删除可以通过以下两点去配置: 配置redis.conf中的hz,默认是10(即1秒执行10次,100ms一次,值越大说明频率越高,对redis的性能损耗也越高) 还可以通过配置redis.conf中的maxmemory去配置,当已用内存超过maxmemory限定的时候,就会触发主动清理策略 持久化时对过期key的处理reids持久化是分为RDB和AOF持久化两种,下面分别看一下两种情况是如何处理的 RDB对过期key的处理过期的key对RDB是没有任何影响的,从内存中持久化到RDB文件之前,会先检查key是否过期,过期的key是不进入到RDB文件中的 从RDB恢复到内存数据库中的时候,也会先检查key是否过期,过期的就不导入了 AOF对过期key的处理过期key对AOF也是没有任何影响的, 当key过期还没有被删除,此时进行持久化,该key是不会进入aof文件的,因为没有发生修改的命令 当key过期掉,发生删除操作的时候,程序会向AOF文件追加一条del命令,在将来用aof文件恢复数据的时候,这个过期的key就会被删除掉 还有就是AOF重写,重写的时候会判断key是否过期,过期的key也是不会被重写到aof文件中的]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-15-与Spring Boot整合使用]]></title>
    <url>%2F2019%2F04%2F27%2FRedis-15-%E4%B8%8ESpring-Boot%E6%95%B4%E5%90%88%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[述我们在实际的开发中,通常是需要spring集成redis,这里就以spring boot为例,整合redis,做一些常用的操作 环境准备首先开启redis服务,可以用我们之前创建的集群,或者是开启一个单独的redis服务,并且开启远程连接,还是上篇文章的那三步 然后,新建一个Spring Boot的工程 建好工程之后,引入下面的依赖:12345&lt;!-- redis依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 配置修改application.yml中的配置如下:12345678910111213141516171819202122232425spring: redis: # 单节点的ip # host: # 单节点的端口 # port: cluster: # 集群节点 nodes: 172.16.12.3:7000,172.16.12.3:7001,172.16.12.3:7002,172.16.12.3:7003,172.16.12.3:7004,172.16.12.3:7005 # 连接超时时间 timeout: 10000 # 密码(默认是空) password: # Redis数据库索引(默认是0) database: 0 jedis: pool: # 连接池最大阻塞等待时间(使用负值表示没有限制) max-wait: -1 # 连接池最大连接数(使用负值表示没有限制) max-active: 8 # 连接池中的最大空闲连接 max-idle: 8 # 连接池中的最小空闲连接 min-idle: 0 这里我直接使用上文中建的集群了,单个redis服务的话,直接用host和port设置就ok了,其他不变 StringRedisTemplate和RedisTemplateSpring给了两个模板,一个是StringRedisTemplate一个是RedisTemplate,那具体应该用哪个呢 这个主要根据要存储的数据类型决定,key的话一般都是string,但是value我们可能放string也可能是放一个对象进去 所以,对于k-v都是String类型的话,那直接使用StringRedisTemplate就好了,也不需要什么配置,直接注入就可以使用了,这也是官方建议的 对于k-v是String,Object类型的话就用RedisTemplate,但是还需要自定义配置一下,这个下面再说 StringRedisTemplate的使用这里我写了一个工具类,是一些基本的操作,代码如下:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677@Slf4j@Componentpublic class RedisStringUtil &#123; @Autowired private StringRedisTemplate redisTemplate; /** * 写入redis缓存,没有过期时间 * @param key key * @param value value */ public void set(final String key, String value)&#123; redisTemplate.opsForValue().set(key, value); &#125; /** * 写入redis缓存,过期时间单位是秒 * @param key key * @param value value * @param expire 过期时间(单位是秒) */ public void setex(final String key, String value, Long expire)&#123; redisTemplate.opsForValue().set(key, value, expire, TimeUnit.SECONDS); &#125; /** * 写入redis缓存,过期时间单位是毫秒 * @param key key * @param value value * @param expire 过期时间(单位是毫秒) */ public void psetex(final String key, String value, Long expire)&#123; redisTemplate.opsForValue().set(key, value, expire, TimeUnit.MILLISECONDS); &#125; /** * 读取缓存 * @param key key * @return result */ public String get(final String key) &#123; return redisTemplate.opsForValue().get(key); &#125; /** * 判断某个key是否存在 * @param key key * @return 是否存在 */ public boolean exists(final String key) &#123; return redisTemplate.hasKey(key); &#125; /** * 根据key删除数据 * @param key key */ public void remove(final String key) &#123; if (exists(key)) &#123; redisTemplate.delete(key); &#125; &#125; /** * 批量删除 * @param keys key */ public void remove(final String ... keys)&#123; for (String key : keys) &#123; if (exists(key)) &#123; redisTemplate.delete(key); &#125; &#125; &#125;&#125; 主要就是通过StringRedisTemplate对数据的一些基本操作,然后测试类测试一下:12345678910@Testpublic void test()&#123; boolean result = redisStringUtil.exists(&quot;k1&quot;); log.info(String.valueOf(result)); redisStringUtil.set(&quot;k2&quot;, &quot;v1&quot;); String v2 = redisStringUtil.get(&quot;k2&quot;); log.info(v2);&#125; 控制台输出如下: 122019-04-26 14:19:32.525 INFO 4212 --- [ main] com.redis.example.AppTests : true2019-04-26 14:19:32.540 INFO 4212 --- [ main] com.redis.example.AppTests : v1 可以看到数据能正常添加,查询 RedisTemplate下面再来看一下RedisTemplate的一些操作 spring boot是不支持直接使用RedisTemplate的,需要我们自己做一些配置才可以使用,具体步骤如下 序列化我们需要自己实现RedisSerializer&lt;T&gt;接口来对传入对象进行序列化和反序列化,新建一个RedisObjectSerializer类,代码如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class RedisObjectSerializer implements RedisSerializer&lt;Object&gt; &#123; static final byte[] EMPTY_ARRAY = new byte[0]; @Override public byte[] serialize(Object o) throws SerializationException &#123; if (o == null) &#123; return EMPTY_ARRAY; &#125; ObjectOutputStream oos = null; ByteArrayOutputStream bos = null; try &#123; bos = new ByteArrayOutputStream(); oos = new ObjectOutputStream(bos); oos.writeObject(o); byte[] byt = bos.toByteArray(); return byt; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; &#125; @Override public Object deserialize(byte[] bytes) throws SerializationException &#123; if (isEmpty(bytes)) &#123; return null; &#125; ObjectInputStream ois = null; ByteArrayInputStream bis = null; bis = new ByteArrayInputStream(bytes); try &#123; ois = new ObjectInputStream(bis); Object obj = ois.readObject(); return obj; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; &#125; private boolean isEmpty(byte[] data) &#123; return (data == null || data.length == 0); &#125;&#125; 序列化的类搞定之后,需要在spring中配置一下RedisTemplate,代码如下:123456789101112131415161718@Configurationpublic class RedisConfig &#123; @Bean public RedisTemplate&lt;String, Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) &#123; StringRedisSerializer stringRedisSerializer = new StringRedisSerializer(); RedisObjectSerializer redisObjectSerializer = new RedisObjectSerializer(); RedisTemplate&lt;String, Object&gt; redisTemplate = new RedisTemplate&lt;&gt;(); redisTemplate.setConnectionFactory(redisConnectionFactory); redisTemplate.setStringSerializer(stringRedisSerializer); redisTemplate.setValueSerializer(redisObjectSerializer); return redisTemplate; &#125;&#125; 配置完成之后就是测试环节了,定义一个User类供测试使用,代码如下:1234567891011@Data@AllArgsConstructorpublic class User implements Serializable &#123; private Long id; private String name; private Integer age; &#125; 然后就可以写单元测试来测试了,测试代码如下:12345678910111213@Autowiredprivate RedisTemplate&lt;String, Object&gt; redisTemplate;@Testpublic void test1() &#123; User user = new User(1L, &quot;张三&quot;, 18); redisTemplate.opsForValue().set(&quot;user1&quot;, user); User user1 = (User)redisTemplate.opsForValue().get(&quot;user1&quot;); log.info(&quot;用户名:&#123;&#125;&quot;, user1.getName());&#125; 运行后,控制台输出如下:12019-04-26 16:47:52.319 INFO 20444 --- [ main] com.redis.example.AppTests : 用户名:张三 我们可以在redis可视化工具中看一下这个数据,如下图: 在reids中,他的key和value都是经过序列化的,在到了java程序中,反序列化转成对象 总结对比StringRedisTemplate 和 RedisTemplate的区别主要有以下几点: StringRedisTemplate继承了RedisTemplate RedisTemplate是一个泛型类,而StringRedisTemplate则不是 StringRedisTemplate只能对k-v都是String类型的数据操作,RedisTemplate可以对任何类型的k-v数据操作 他们各自序列化的方式不同,但最终都是得到了一个字节数组,StringRedisTemplate使用的是StringRedisSerializer类;RedisTemplate使用的是JdkSerializationRedisSerializer类.反序列化的时候则是一个得到String,一个得到Object 好了,RedisTemplate就先说到这里 参考文章本文代码地址]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-14-Jedis的基本使用]]></title>
    <url>%2F2019%2F04%2F27%2FRedis-14-Jedis%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[述前面的文章中介绍了一些reids的基本命令,数据结构,持久化方式,还有主从集群等等,我们操作redis的数据的时候一直是在redis-cli里面的,本文就来看一下如何用java去操作redis,java操作reids的方式有很多种,这里首先先介绍的是Jedis 准备工作远程连接reids的前提是,系统关闭防火墙,reids关闭保护模式,还有注释掉redis的ip地址绑定,这些配置基本在前文中已经介绍了,这里再来看一下 关闭防火墙这里以centos7为例,关闭防火墙命令是:1systemctl stop firewalld.service 再补一刀直接禁掉1systemctl disable firewalld.service 关闭保护模式就是redis配置中的protected设置为no,之前一直都是设置的no,如下:1protected-mode no 注释掉redis的ip地址绑定也是在redis.conf中1# bind:127.0.0.1 java客户端连接首先,新建一个工程,然后引入jedis的依赖,如下: 12345&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt; 然后可以写一个方法去测试一下,如下:123456public static void main(String[] args) &#123; Jedis jedis = new Jedis(&quot;172.16.12.3&quot;, 6379); // 如果设置了密码的话,就需要认证 jedis.auth(&quot;123456&quot;); log.info(jedis.ping());&#125; 运行,控制台输出:116:48:40.227 [main] INFO com.redis.example.test.JedisTest - PONG 连接完成之后就可以对redis中的数据进行操作了,jedis方法名称基本是和命令是一样的,如下: 需要用到什么方法,直接通过方法名找就好了,这里就不在多说了 连接池上面的例子中,频繁的创建和销毁连接会影响性能,我们可以采用连接池来部分的解决这个问题,代码如下:12345678910public static void main(String[] args) &#123; GenericObjectPoolConfig config = new GenericObjectPoolConfig(); config.setMaxIdle(20); config.setMaxTotal(100); // ip 端口 超时时间 密码 JedisPool jedisPool = new JedisPool(config, &quot;172.16.12.3&quot;, 6379, 5000, &quot;123456&quot;); Jedis jedis = jedisPool.getResource(); log.info(jedis.ping());&#125; 这样就不会频繁的创建销毁连接了,在JavaSE环境中可以把连接池配置成一个单例模式,如果用了Spring容器的话,可以把连接池交给Spring容器管理 集群连接上面连接的都是单节点的redis,如果我们想连接一个reids集群呢,代码如下:123456789101112131415public static void main(String[] args) &#123; Set&lt;HostAndPort&gt; clusterNodes = new HashSet&lt;&gt;(); clusterNodes.add(new HostAndPort(&quot;172.16.12.3&quot;, 7000)); clusterNodes.add(new HostAndPort(&quot;172.16.12.3&quot;, 7001)); clusterNodes.add(new HostAndPort(&quot;172.16.12.3&quot;, 7002)); clusterNodes.add(new HostAndPort(&quot;172.16.12.3&quot;, 7003)); clusterNodes.add(new HostAndPort(&quot;172.16.12.3&quot;, 7004)); clusterNodes.add(new HostAndPort(&quot;172.16.12.3&quot;, 7005)); JedisCluster jedisCluster = new JedisCluster(clusterNodes); jedisCluster.set(&quot;testKey&quot;, &quot;java&quot;); log.info(jedisCluster.get(&quot;testKey&quot;));&#125; 运行,控制台输出117:13:31.633 [main] INFO com.redis.example.test.JedisTest - java 这里方法名也和redis的命令基本一致,这里就不多说了 jedis就先介绍到这里]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-13-集群介绍]]></title>
    <url>%2F2019%2F04%2F27%2FRedis-13-%E9%9B%86%E7%BE%A4%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[述上文中,我们搭建的是主从结构的redis,那所有的写操作都是在master上的,然后同步更新到slave,所以从Master同步到Slave机器有一定的延迟,当系统很繁忙的时候,延迟问题会更加严重,Slave机器数量的增加也会使这个问题更加严重, 这时候就需要用到集群来提升reids的性能, 下面来具体看一下如何搭建redis集群 集群原理官方给出的集群方案采用无中心结构,每个节点保存数据和整个集群状态,每个节点都和其他所有节点连接,架构图如下: 客户端与redis节点直连,连接集群中的任意一个节点即可,不需要中间的proxy层,集群中的redis节点彼此互联,内部使用二进制协议优化传输速度和带宽 redis集群中内置了16384个哈希槽,当需要在redis集群中放置一个key-value时,reids先对key使用crc16算法算出一个结果,然后把结果对16384求余数,这样每个key都会对应一个编号在0-16383 之间的哈希槽,redis会根据节点数据量大致均等的将哈希槽映射到不同节点 比如说集群又三个节点,槽分布的值如下: server1: 0-5460 server2: 5461-10922 server3: 10923-16383 容错机制选举过程是所有主节点(Master)参与,如果过半数的Master认为故障节点挂了,那就会将这个节点踢出集群,然后这个故障节点对应的slave节点就会升级成Master 集群中每个节点都是只储存部分信息,所以当主节点挂掉之后,又没有从节点替补的情况下,集群就会挂掉,因为它已经不是一个完整的集群了 搭建一个集群需要一台以上的节点,然后我们这里还是用上文中的,三个节点来搭建,然后为了防止这三个节点挂掉而导致集群崩溃,我们还需要给这三个节点都配一个从节点,来保证高可用,那一共就是6个节点 怎样投票投票过程是集群中所有master参与,如果半数以上master节点与master节点通信超过cluster-node-timeout设置的时间,认为当前master节点挂掉. 集群挂掉的条件如果集群任意master挂掉,且当前master没有slave.集群进入fail状态,也可以理解成集群的slot(槽)映射不完整时进入fail状态 如果集群超过半数以上master挂掉,无论是否有slave,集群进入fail状态,当集群不可用时,所有对集群的操作做都不可用 搭建方法这里推荐使用docker的方式搭建,方便也很快,可以参考docker分类下的文章,这里不就不多说了如果用传统的方式搭建的话,可以参考这里:传送门 集群架构集群架构其实有很多种,在实际生产环境中需要根据具体需求来选择不同的集群架构,上面这个只是一个redis官方给出的架构模式]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-12-哨兵模式]]></title>
    <url>%2F2019%2F04%2F27%2FRedis-12-%E5%93%A8%E5%85%B5%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[述上文中,使用了两种方式搭建redis主从,这两种方式无论是哪种,都会存在一个问题,那就是在主机宕机的时候,就会发生群龙无首的情况,那么如果在master挂掉的时候,能够从slave中选举出一个来充当master,就不需要我们去手动重启主机了,这就是哨兵模式 环境准备本文环境基于上文中,如下这样的一个主从结构: 首先我们需要创建一个哨兵的配置文件,名称是sentinel.conf,如果你的三个redis在不同的目录下,那就需要去三个目录下都创建一份哨兵的配置文件,创建命令是:1touch sentinel.conf 然后通过vim编辑,内容如下:12345sentinel monitor mymaster 172.16.12.3 6379 1daemonize yeslogfile &quot;/var/log/sentinel_log.log&quot;protected-mode nosentinel auth-pass mymaster 123456 第一行的配置中 mymaster是给监控主机起的名字,这个是自定义的,然后后面是主机的地址和端口,最后的1表示有多少个哨兵认为主机挂掉了,就进行切换 第二行的配置表示哨兵在后台运行 第三行是哨兵的日志文件地址 第四行把保护模式关闭,保护模式如果开启只接受回环地址的ipv4和ipv6地址链接,拒绝外部链接,而且正常应该配置多个哨兵,避免一个哨兵出现独裁情况,如果配置多个哨兵那如果开启也会拒绝其他sentinel的连接.导致哨兵配置无法生效. 第五行如果redis配置了密码,那这里必须配置认证,否则不能自动切换 三个redis节点都要这样配置,三个sentinel.conf文件的内容除了日志文件的名称要改其他都不需要改. 最后,配置完成之后,可以通过以下命令启动哨兵1redis-sentinel sentinel.conf 按我们上面的配置,配置了后台运行和日志文件的地址,那就可以到去查看哨兵的运行日志.123cd /var/log/cat sentinel_log.log 内容如下: 测试好了哨兵起来了,然后主从关系也搭建好了,如下: 现在的结构是这样的:123Master: 172.16.12.3 6379 Slave: 172.16.12.3 6380Slave: 172.16.12.3 6381 一主二仆,服务都是启动的. 然后现在关闭6379的服务,观察哨兵的日志输出,如下: 从图中可以看到在6379这个master挂掉之后,他把master换成了6381这个服务,然后我们在6381和6380上运行info replication,看一下他们各自的身份,如下: 这时候,6381显示他是个master然后下面有一个slave是6380 最后,我们再把6379启动起来,启动起来之后看一下他的信息,如下: 6379重启之后,他的身份是slave了,并不是master了 好了,哨兵模式就介绍到这里, 如果用的是docker装的redis,在docker环境下配置哨兵的方法请前往docker分类中去查看,有详细说明]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-11-主从复制环境搭建]]></title>
    <url>%2F2019%2F04%2F27%2FRedis-11-%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[述上文中,介绍了两种redis的持久化方式,分别是RDB和AOF的方式,下面再来看看redis中的主从复制.主从配置也是一种数据备份的方案,而且可以在一定程度上提升redis性能,redis的主从复制和关系型数据库中的主从复制类似, 从机可以精确的复制主机上的数据, 配置好主从之后,一方面能够实现读写分离,降低master的压力,另一方面也可以用来数据备份 环境准备我们就在一台服务器上,运行3个redis的实例, 首先,需要找到redis的目录,然后找到配置文件redis.conf,复制三份出来,名称分别是redis-6379.conf,redis-6380.conf,redis-6381.conf, 然后分别打开这三个配置文件,找到一些需要修改的配置改掉,具体有以下几个:12345port 6379pidfile /var/run/redis_6379.pidlogfile &quot;6379.log&quot;dbfilename dump6379.rdbappendfilename &quot;appendonly6379.aof&quot; 找到这几个配置,各自改成各自的配置,就ok了. 然后可以分去指定配置文件去启动,如下命令:123redis-server redis-6379.confredis-server redis-6380.confredis-server redis-6381.conf 这时候就启动了三个reids了,然后如果需要通过redis-cli连接的话,只需要通过-p指定端口号就ok了 也可以通过用docker的方式再启动两个redis,具体方法请前往docker的分类下查看 这里我已经启动了三个redis实例了,然后通过redis-cli连接一下试试, 如下: 我这里的三个实例的地址分别是:123172.16.12.3:6379172.16.12.3:6380172.16.12.3:6381 主从配置假设这三个实例中,6379是主机,也就是master,剩下的6380和6381是从机即slave,那么需要在从机上执行以下命令:12172.16.12.3:6380&gt; slaveof 172.16.12.3 6379OK 12172.16.12.3:6381&gt; slaveof 172.16.12.3 6379OK 也可以在两个从机的redis.conf中加入以下配置构建主从:1slaveof 172.16.12.3 6379 这里有一个需要注意的地方,如果我们的主机设置了密码了, 那么需要在从机的配置中加入以下配置:1masterauth 123456 123456是我主机的密码,不设置的话可能会导致从机连接不到主机 ok,主从搭建好之后,就可以通过命令查看每个实例的状态,如下:1234567891011172.16.12.3:6379&gt; INFO replication# Replicationrole:masterconnected_slaves:2slave0:ip=172.17.0.1,port=6380,state=online,offset=57,lag=1slave1:ip=172.17.0.1,port=6381,state=online,offset=57,lag=1master_repl_offset:57repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:2repl_backlog_histlen:56 在6379上面执行INFO replication,可以看到他的角色是master,然后下面有两个连接到的从机slave0和slave1,他们的ip端口这里都有 然后在从机上执行这个命令,返回如下:1234567891011121314151617172.16.12.3:6381&gt; INFO replication# Replicationrole:slavemaster_host:172.16.12.3master_port:6379master_link_status:upmaster_last_io_seconds_ago:10master_sync_in_progress:0slave_repl_offset:407slave_priority:100slave_read_only:1connected_slaves:0master_repl_offset:0repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0 这里就可以看到6381的角色是从机,而且还显示了他的主机的端口和ip地址 测试配置完成之后,在主机上新添加一条数据:12172.16.12.3:6379&gt; set k1 testv1OK 然后再从机上去get,如下:12172.16.12.3:6380&gt; get k1&quot;testv1&quot; 好了,主机上的数据从机也可以获取的到了 另一种主从结构上面,我们搭建的主从结构是这样的 其实还有一种方式,可以搭建成下面这个样子的: 上图中这种主从搭建方式,只需要在上文环境的基础上,修改6381的master即可,让6381去复制6380的数据,命令如下:12172.16.12.3:6381&gt; slaveof 172.16.12.3 6380OK 执行完毕后,看一下6379的信息:12345678910172.16.12.3:6379&gt; info replication# Replicationrole:masterconnected_slaves:1slave0:ip=172.17.0.1,port=6380,state=online,offset=4033,lag=0master_repl_offset:4033repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:2repl_backlog_histlen:4032 可以看到,他的角色依然是master,然后从机只有一个6380了 那再来看一下6380的信息,如下:123456789101112131415161718172.16.12.3:6380&gt; info replication# Replicationrole:slavemaster_host:172.16.12.3master_port:6379master_link_status:upmaster_last_io_seconds_ago:6master_sync_in_progress:0slave_repl_offset:4131slave_priority:100slave_read_only:1connected_slaves:1slave0:ip=172.17.0.1,port=6381,state=online,offset=155,lag=1master_repl_offset:155repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:2repl_backlog_histlen:154 此时,6380的角色是一个从机,但是他自己还有一个从机,就是6381. 至此,上面这种方式的主从就搭建成功了 注意点 当主机运行了一段时间并且已经存储了数据,这是从机连接上来的话,从机是会复制主机上面的所有数据,而不是从连接的那个时间点开始复制 配置了主从之后,默认情况下,主机可读可写,从机只读不可写,也可以通过修改redis.conf中 slave-read-only的值让从机也可以执行写操作 在整个主从结构运行过程中,如果主机不幸挂掉,重启之后,他依然是主机,主从复制操作也能够继续进行 主从原理 每一个 Redis master 都有一个 replication ID ：这是一个较大的伪随机字符串，标记了一个给定的数据集。每个 master 也持有一个偏移量，master 将自己产生的复制流发送给 slave 时，发送多少个字节的数据，自身的偏移量就会增加多少，目的是当有新的操作修改自己的数据集时，它可以以此更新 slave 的状态。复制偏移量即使在没有一个 slave 连接到 master 时，也会自增，所以基本上每一对给定的Replication ID, offset都会标识一个 master 数据集的确切版本。当 slave 连接到 master 时，它们使用 PSYNC 命令来发送它们记录的旧的 master replication ID 和它们至今为止处理的偏移量。通过这种方式， master 能够仅发送 slave 所需的增量部分。但是如果 master 的缓冲区中没有足够的命令积压缓冲记录，或者如果 slave 引用了不再知道的历史记录（replication ID），则会转而进行一个全量重同步：在这种情况下， slave 会得到一个完整的数据集副本，从头开始 简单来说,就是: slave启动成功连接到master后发送一个sync命令 Master接到命令后启动后台的存盘进程,同时收集所有接收到的用于修改数据集命令 在后台进程执行完毕之后,master将传送整个数据文件到slave,以完成一次完全同步 全量复制:slave服务在接收到数据库文件数据后,将其存盘并加载到内存中 增量复制:Master继续将新的所有收集到的修改命令依次传给slave,完成同步 只要是重新连接master,一次完全同步（全量复制)将被自动执行]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-10-AOF持久化]]></title>
    <url>%2F2019%2F04%2F27%2FRedis-10-AOF%E6%8C%81%E4%B9%85%E5%8C%96%2F</url>
    <content type="text"><![CDATA[述上文中说了快照持久化,那本文再来看一下另外一种持久化的方式,就是AOF(append-only file) AOF持久化AOF持久化是把被执行的命令都写到一个aof文件末尾,在恢复的时候只需要从头到尾执行一遍写命令即可恢复数据. AOF在redis中默认是没有开启的,需要手动配置开启 相关配置开启AOF持久化,需要在redis.conf中,找到 appendonly这个配置,修改为yes,如下:1appendonly yes 还有几个AOF相关的配置,如下:1234567appendfilename &quot;appendonly.aof&quot;# appendfsync alwaysappendfsync everysec# appendfsync nono-appendfsync-on-rewrite noauto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb 这些配置的作用如下: appendfilename: 表示生成的AOF文件的名称 appendfsync: 表示备份的时机,always是每执行一个命令就备份一次,everysec表示每秒备份一次, no表示将备份时机交给操作系统 no-appendfsync-on-rewrite: 表示在对aof文件进行压缩时，是否执行同步操作 appendfsync 备份时机的设置如上面所说,appendfsync一共可以设置三种,我们在实际使用中,首选是everysec,也就是每秒钟备份一次 如果设置always,会严重影响redis的性能,如果设置everysec,最坏的情况也就是会丢失一秒钟的数据 AOF文件的重写与压缩如果我们开启了AOF备份的话,随着系统的运行,AOF文件会越来越大,甚至会把电脑的磁盘跑满, AOF文件的重写与压缩机制可以在一定程度上缓解这个问题 当AOF的备份文件过大的时候,我们可以向redis发送一条bgrewriteaof命令,进行文件重写,如下:12172.16.12.3:6379&gt; bgrewriteaofBackground append only file rewriting started bgrewriteaof的执行原理和我们上文说的bgsave的原理一致,这里我就不再赘述,因此bgsave执行过程中存在的问题在这里也一样存在 bgrewriteaof也可以自动执行,依赖于相关配置中的auto-aof-rewrite-percentage和auto-aof-rewrite-min-size配置 auto-aof-rewrite-percentage 100: 表示当目前aof文件大小超过上一次重写时的aof文件大小的百分之多少时会再次进行重写 auto-aof-rewrite-min-size 64mb:表示如果之前没有重写,则以启动时的AOF文件大小为依据,同时还要求AOF文件的大小至少要大于64M 对比两种持久化的方式就先介绍到这里, 下面将快照持久化(RDB)和AOF持久化做一个比较 命令 RDB AOF 启动优先级 低 高 体积 小 大 恢复速度 快 慢 数据安全性 丢数据 根据策略决定 轻重 重 轻 最佳策略首先,如果reids是只做缓存服务器的话,可以不开启任何持久化 如果你非常关心你的数据,但仍然可以承受数分钟以内的数据丢失,那么你可以只使用 RDB 持久 AOF将Redis执行的每一条命令追加到磁盘中,处理巨大的写入会降低Redis的性能,不知道你是否可以接受 定时生成 RDB 快照(snapshot)非常便于进行数据库备份,并且 RDB 恢复数据集的速度也要比 AOF 恢复的速度要快 Redis 支持同时开启 RDB 和 AOF,系统重启后,Redis 会优先使用 AOF 来恢复数据,这样丢失的数据会最少.]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-9-快照持久化]]></title>
    <url>%2F2019%2F04%2F27%2FRedis-9-%E5%BF%AB%E7%85%A7%E6%8C%81%E4%B9%85%E5%8C%96%2F</url>
    <content type="text"><![CDATA[述redis的持久化有两种方式,分别是快照持久化和AOF,具体使用哪种,需要结合项目的实际情况选择, 下面就先看一下快照持久化. 快照持久化, 就是说,redis在某个时间点上,对内存中的数据创建一个副本文件,副本文件中的数据在redis重启时会被自动加载. 配置方式redis中,快照持久化是默认开启的,在redis.conf中,相关的配置主要有如下几项: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647save 900 1save 300 10save 60 10000# By default Redis will stop accepting writes if RDB snapshots are enabled# (at least one save point) and the latest background save failed.# This will make the user aware (in a hard way) that data is not persisting# on disk properly, otherwise chances are that no one will notice and some# disaster will happen.## If the background saving process will start working again Redis will# automatically allow writes again.## However if you have setup your proper monitoring of the Redis server# and persistence, you may want to disable this feature so that Redis will# continue to work as usual even if there are problems with disk,# permissions, and so forth.stop-writes-on-bgsave-error yes# Compress string objects using LZF when dump .rdb databases?# For default that&apos;s set to &apos;yes&apos; as it&apos;s almost always a win.# If you want to save some CPU in the saving child set it to &apos;no&apos; but# the dataset will likely be bigger if you have compressible values or keys.rdbcompression yes# Since version 5 of RDB a CRC64 checksum is placed at the end of the file.# This makes the format more resistant to corruption but there is a performance# hit to pay (around 10%) when saving and loading RDB files, so you can disable it# for maximum performances.## RDB files created with checksum disabled have a checksum of zero that will# tell the loading code to skip the check.rdbchecksum yes# The filename where to dump the DBdbfilename dump.rdb# The working directory.## The DB will be written inside this directory, with the filename specified# above using the &apos;dbfilename&apos; configuration directive.## The Append Only File will also be created inside this directory.## Note that you must specify a directory here, not a file name.dir ./ 注释删掉,就是以下几项:1234567save 900 1save 300 10save 60 10000stop-writes-on-bgsave-error: yesrdbcompression yesdbfilename dump.rdbdir ./ 配置解析 save: 前三个save的配置分别表示,900秒内至少一个键被更改则进行快照,300秒内至少10个键被更改则进行快照,60秒内至少10000个键被更改则进行快照 stop-writes-on-bgsave-error: 表示在快照创建出错后,是否继续执行写命令 rdbcompression: 表示是否对快照文件进行压缩 dbfilename: 表示生成的快照文件的名字 dir: 表示生成的文件的目录 快照持久化,默认就是开启的,在redis的文件夹下,就可以看到这个快照文件,如下: 如果把这个文件删掉的话,我们之前存的那些key就都没有了 快照持久化的几种方式上面初步了解了快照持久化,那么他是怎么运行的,运行的时机是什么,来详细看一下: 第一种方式在redis运行过程中,我们可以向redis发送一个save命令来创建一个快照, save是一个阻塞命令,也就是说,redis在接收到save命令之后,就开始创建快照,在快照创建完毕之前,其他的请求过来将会被挂起,这个命令用的不是很多,使用如下:12172.16.12.3:6379&gt; saveOK 第二种方式在redis运行过程中,可以通过bgsave命令去创建快照,这个命令不同于save命令的是bgsave会fork一个子进程然后由子进程去创建快照,父进程继续处理客户端的请求,这样就不会导致客户端命令阻塞了, 简单来说就是后台去创建快照,命令如下:12172.16.12.3:6379&gt; bgsaveBackground saving started 第三种方式根据redis.conf的配置去执行快照持久化,就比如默认配置,如下:123save 900 1save 300 10save 60 10000 当配置文件中的条件满足的时候,redis就会自动触发命令进行备份,就比如第一个配置900秒内,有一个key被操作了,就执行快照持久化, 这个配置可以根据自己的实际需求去配置 第四种方式还有一种情况会触发save命令,就是我们在执行shutdown命令的时候,也就是关闭redis服务的时候,也会执行一个save命令,进行备份的操作,并且在备份完成后将服务关闭 第五种方式还有一种特殊情况会触发bgsave命令,就是在主从备份的时候,当从机连接上主机后,会发送一条sync命令来开始一次复制操作,此时主机会开始一次bgsave操作,并且在bgsave操作结束之后,向从机发送快照数据实现数据同步 缺点快照持久化存在一些缺点,比如说save命令会发生阻塞,bgsave虽然不会阻塞,但是fork一个子进程又要耗费资源,在一些极端情况下,fork子进程的时间甚至超过数据备份的时间 定期的持久化也会让我们存在数据丢失的风险,最坏的情况我们可能丢失掉最近一次备份到当下的数据,具体丢失多久的数据,要看我们项目的承受能力,我们可以根据项目的承受能力配饰save参数.]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-8-事务的使用]]></title>
    <url>%2F2019%2F04%2F27%2FRedis-8-%E4%BA%8B%E5%8A%A1%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[述上文中说了redis中的发布与订阅,下面来看一下redis中的事务是如何使用的 redis是一个nosql数据库,那当然也就有事务的功能,但是和关系型数据库中的事务还是有点差异 事务redis中,可以通过MULTI命令开启一个事务,如下:12172.16.12.3:6379&gt; multiOK 这样就开始了一个事务, 然后我们继续去执行命令,如下:123456172.16.12.3:6379&gt; set k1 v1QUEUED172.16.12.3:6379&gt; set k2 v2QUEUED172.16.12.3:6379&gt; set k3 v3QUEUED 这里可以看到,返回值是QUEUED, 这几个命令是都不会被立马执行的,而是放到了一个队列中,然后当所有的命令都输入完成之后, 可以通过exec命令执行事务, 也可以通过discard命令清空队列,如下: 1234172.16.12.3:6379&gt; exec1) OK2) OK3) OK 事务中的异常redis中事务的异常情况总的来说分为两类: 进入队列之前就能发现的错误,比如命令输错 执行EXEC之后才能发现的错误,比如给一个非数字字符加1 对于这两种错误,redis有不同的处理策略 对于第一种错误,服务器会对命令入队失败的情况进行记录,并在客户端调用 EXEC 命令时,拒绝执行并自动放弃这个事务,如下:123456789101112131415172.16.12.3:6379&gt; multiOK172.16.12.3:6379&gt; set k1 v1QUEUED172.16.12.3:6379&gt; set k2 v2QUEUED172.16.12.3:6379&gt; set k3 v3QUEUED172.16.12.3:6379&gt; set k4 v4 kQUEUED172.16.12.3:6379&gt; exec1) OK2) OK3) OK4) (error) ERR syntax error 上面 第四个命令是输的错误的命令,但是他也会先进入队列,然后再事务提交的时候,才会报错,但是其他命令是可以执行成功的 对于第二种情况,redis并没有对它们进行特别处理, 即使事务中有某个/某些命令在执行时产生了错误, 事务中的其他命令仍然会继续执行,如下测试 : 1234567891011172.16.12.3:6379&gt; multiOK172.16.12.3:6379&gt; set k1 v1QUEUED172.16.12.3:6379&gt; incr k1QUEUED172.16.12.3:6379&gt; exec1) OK2) (error) ERR value is not an integer or out of range172.16.12.3:6379&gt; get k1&quot;v1&quot; 不同于关系型数据库,redis中的事务出错时没有回滚,对此,官方的解释如下: Redis 命令只会因为错误的语法而失败（并且这些问题不能在入队时发现），或是命令用在了错误类型的键上面：这也就是说，从实用性的角度来说，失败的命令是由编程错误造成的，而这些错误应该在开发的过程中被发现，而不应该出现在生产环境中。因为不需要对回滚进行支持，所以 Redis 的内部可以保持简单且快速。 watch命令事务中,watch命令可以用来监听一个key,通过监控,可以为redis事务提供(CAS)行为 如果有至少一个被watch监听的key在exec执行之前被修改了,那么整个事务都会被取消,exec返回nil-reply来表示事务已经失败 具体用法看一下下面这个例子: 12345678910111213141516172.16.12.3:6379&gt; set k1 v1OK172.16.12.3:6379&gt; watch k1OK172.16.12.3:6379&gt; set k1 v2OK172.16.12.3:6379&gt; get k1&quot;v2&quot;172.16.12.3:6379&gt; multiOK172.16.12.3:6379&gt; set k1 v3QUEUED172.16.12.3:6379&gt; exec(nil)172.16.12.3:6379&gt; get k1&quot;v2&quot; 看一下上面的命令,就是先设置了一个k1, 然后监听k1, 再下面就是修改了一下k1的值,这个是在事务外面修改的,所以可以修改成功的 然后开启了一个事务,事务里面再去修改k1的值,然后提交事务,发现返回nil,再去获取k1的值,发现并没有修改成功 unwatch命令unwatch命令 可以取消对一个key的监控,如下:1234567891011121314172.16.12.3:6379&gt; get k1&quot;v2&quot;172.16.12.3:6379&gt; watch k1OK172.16.12.3:6379&gt; unwatchOK172.16.12.3:6379&gt; multiOK172.16.12.3:6379&gt; set k1 v1QUEUED172.16.12.3:6379&gt; exec1) OK172.16.12.3:6379&gt; get k1&quot;v1&quot; ok,事务方面就先说这么多]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-7-发布与订阅]]></title>
    <url>%2F2019%2F04%2F27%2FRedis-7-%E5%8F%91%E5%B8%83%E4%B8%8E%E8%AE%A2%E9%98%85%2F</url>
    <content type="text"><![CDATA[述前面,都是说了一些基础数据类型的一些常用的命令,本文来看一下redis中的发布与订阅 发布订阅reids中的发布与订阅类似于生活中的电台, 电台可以在某一个频率上发广播,而我们可以接收任何一个频率的广播 下面来看一下具体是如何使用的, 首先需要打开一个redis-cli,然后连接我们的redis服务. 订阅消息的方式如下:1234567891011172.16.12.3:6379&gt; subscribe c1 c2 c3Reading messages... (press Ctrl-C to quit)1) &quot;subscribe&quot;2) &quot;c1&quot;3) (integer) 11) &quot;subscribe&quot;2) &quot;c2&quot;3) (integer) 21) &quot;subscribe&quot;2) &quot;c3&quot;3) (integer) 3 意思就是接收 c1 c2 c3 这三个频道的消息, 然后我们怎么在这些频道发送消息呢? 再打开一个redis-cli,然后连接redis服务,用来发送消息,如下: 12172.16.12.3:6379&gt; publish c1 &quot;hello redis&quot;(integer) 1 这样就是往c1这个通道里面push了一条消息,这时候,订阅方就可以收到了,会有如下输出:12345678910111213Reading messages... (press Ctrl-C to quit)1) &quot;subscribe&quot;2) &quot;c1&quot;3) (integer) 11) &quot;subscribe&quot;2) &quot;c2&quot;3) (integer) 21) &quot;subscribe&quot;2) &quot;c3&quot;3) (integer) 31) &quot;message&quot;2) &quot;c1&quot;3) &quot;hello redis&quot; 也可以使用模式匹配订阅,比如下面的命令:12345172.16.12.3:6379&gt; psubscribe c*Reading messages... (press Ctrl-C to quit)1) &quot;psubscribe&quot;2) &quot;c*&quot;3) (integer) 1 c*, 就表示所有c开头的通道,然后在消息发送方发几条消息试试,如下:1234172.16.12.3:6379&gt; publish c1 &quot;hello redis from c1&quot;(integer) 1172.16.12.3:6379&gt; publish c2 &quot;hello redis from c2&quot;(integer) 1 发完之后,订阅放的输出如下:12345678910111213172.16.12.3:6379&gt; psubscribe c*Reading messages... (press Ctrl-C to quit)1) &quot;psubscribe&quot;2) &quot;c*&quot;3) (integer) 11) &quot;pmessage&quot;2) &quot;c*&quot;3) &quot;c1&quot;4) &quot;hello redis from c1&quot;1) &quot;pmessage&quot;2) &quot;c*&quot;3) &quot;c2&quot;4) &quot;hello redis from c2&quot; 总结发布订阅功能就跟消息队列其实差不多,在某些场景下还是很好用的, 但是同时也需要注意一些问题 由于网络在传输过程中可能会遭遇断线等意外情况,断线后需要进行重连,然而这会导致断线期间的数据丢失.]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-6-针对散列和有序集合的常用命令]]></title>
    <url>%2F2019%2F04%2F27%2FRedis-6-%E9%92%88%E5%AF%B9%E6%95%A3%E5%88%97%E5%92%8C%E6%9C%89%E5%BA%8F%E9%9B%86%E5%90%88%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[述上文中,介绍了list和set的一些常用的命令,下面再来看一下散列和有序集合的一些常用的命令 散列(hash)和java中的hashmap其实是差不多的,redis其实就是存放的键值对,散列也是存放的键值对,,下面来具体看下一下一些常用的操作 hsethset用来设置key指定的哈希集中指定字段的值, 如果key指定的哈希集不存在,就会创建一个新的哈希集并与key关联,如果字段在哈希集中存在,它将被重写,具体使用如下: 12172.16.12.3:6379&gt; hset myhash field1 hello(integer) 1 这里的返回值: 1: 表示这个field是一个新的字段 0: 表示这个field是原来已经存在的 hgethget用来返回key指定的哈希集中某一个field的值,如下:12172.16.12.3:6379&gt; hget myhash field1&quot;hello&quot; hmsethmset,批量添加的命令,使用如下:12172.16.12.3:6379&gt; hmset myhash field2 world field3 java field4 phpOK hmget是一个批量查询的命令,如下:12345172.16.12.3:6379&gt; hmget myhash field1 field2 field3 field41) &quot;hello&quot;2) &quot;world&quot;3) &quot;java&quot;4) &quot;php&quot; hdel用来删除key指定的哈希集中指定的field,如果这个field不存在,则会被忽略,如下:1234567172.16.12.3:6379&gt; hdel myhash field1(integer) 1172.16.12.3:6379&gt; hmget myhash field1 field2 field3 field41) (nil)2) &quot;world&quot;3) &quot;java&quot;4) &quot;php&quot; hsetnxhsetnx命令,只在key指定的哈希集中,不存在指定的字段时再设置字段的值,如果是已经存在的,则命令无效,如下:1234172.16.12.3:6379&gt; hsetnx myhash field4 c++(integer) 0172.16.12.3:6379&gt; hsetnx myhash field5 c++(integer) 1 hvals用来返回key指定的哈希集中的所有字段的值,如下:12345172.16.12.3:6379&gt; hvals myhash1) &quot;world&quot;2) &quot;java&quot;3) &quot;php&quot;4) &quot;c++&quot; hkeys用来返回key指定的哈希集中的所有字段的名字,如下:12345172.16.12.3:6379&gt; hkeys myhash1) &quot;field2&quot;2) &quot;field3&quot;3) &quot;field4&quot;4) &quot;field5&quot; hgetall返回key指定的哈希集中的所有字段和值,在返回值中,每个字段名的下一个是他的值,返回值的长度是该哈希集大小的两倍,如下:123456789172.16.12.3:6379&gt; hgetall myhash1) &quot;field2&quot;2) &quot;world&quot;3) &quot;field3&quot;4) &quot;java&quot;5) &quot;field4&quot;6) &quot;php&quot;7) &quot;field5&quot;8) &quot;c++&quot; hexists用来判断key对应的哈希集中,指定的field是否存在,如下:1234172.16.12.3:6379&gt; hexists myhash field1(integer) 0172.16.12.3:6379&gt; hexists myhash field2(integer) 1 hincrby用来增加key指定的哈希集中指定字段的数值 如果指定的哈希集不存在,那就会创建一个新的hash并与key关联 如果字段不存在,则字段的值在该操作执行前被设置为0 hincrby命令支持的值的范围限定在64位有符号整数 123456789101112131415161718192021172.16.12.3:6379&gt; hgetall myhash1) &quot;field2&quot;2) &quot;world&quot;3) &quot;field3&quot;4) &quot;java&quot;5) &quot;field4&quot;6) &quot;php&quot;7) &quot;field5&quot;8) &quot;c++&quot;172.16.12.3:6379&gt; hset myhash field6 1(integer) 1172.16.12.3:6379&gt; hincrby myhash field6 9(integer) 10172.16.12.3:6379&gt; hincrby myhash field7 6(integer) 6172.16.12.3:6379&gt; hget myhash field6&quot;10&quot;172.16.12.3:6379&gt; hget myhash field7&quot;6&quot; hincrbyfloat和上面的hincrby用法一样,只不过是可以加float类型的数据1234172.16.12.3:6379&gt; hincrbyfloat myhash field7 1.55&quot;7.55&quot;172.16.12.3:6379&gt; hget myhash field7&quot;7.55&quot; hlen可以获取到key指定的哈希集包含的字段的数量,如下:12345678910172.16.12.3:6379&gt; hkeys myhash1) &quot;field2&quot;2) &quot;field3&quot;3) &quot;field4&quot;4) &quot;field5&quot;5) &quot;field6&quot;6) &quot;field7&quot;172.16.12.3:6379&gt; hlen myhash(integer) 6 hstrlen获取key指定的哈希集中的,指定的field对应的value的长度 ,如果field不存在的话,就返回0,如下: 123456172.16.12.3:6379&gt; hstrlen myhash field2(integer) 5172.16.12.3:6379&gt; hstrlen myhash field7(integer) 4172.16.12.3:6379&gt; hstrlen myhash field8(integer) 0 有序集合(zset)有序集合类似set,但是每个字符串元素都关联到一个score的数值,里面的元素总是通过score进行着排序,因此它是可以检索的一系列元素 zadd往键为key的集合里面添加元素 添加时,可以指定多个分数/成员(score/member)对,如果指定添加的成员已经存在,则会更新分数,然后将成员更新到正确的位置,如下: 12172.16.12.3:6379&gt; zadd myzset 60 v1(integer) 1 首先是key,然后分数,最后是值. zscore返回有序集合key中, 指定成员的score值,如下:12172.16.12.3:6379&gt; zscore myzset v1&quot;60&quot; zrangezrange 命令可以根据下标返回元素, 要指定开始和结束的下标, 还可以加一个withscores 参数,可以将元素的score 12345678910111213141516171819202122172.16.12.3:6379&gt; zadd myzset 61 v2(integer) 1172.16.12.3:6379&gt; zadd myzset 62 v3(integer) 1172.16.12.3:6379&gt; zadd myzset 63 v4(integer) 1172.16.12.3:6379&gt; zrange myzset 0 31) &quot;v1&quot;2) &quot;v2&quot;3) &quot;v3&quot;4) &quot;v4&quot;172.16.12.3:6379&gt; zrange myzset 0 3 withscores1) &quot;v1&quot;2) &quot;60&quot;3) &quot;v2&quot;4) &quot;61&quot;5) &quot;v3&quot;6) &quot;62&quot;7) &quot;v4&quot;8) &quot;63&quot; zrevrangezrevrange和zrange的功能基本一样,只不过zrevrange返回数据的顺序是反着的,如下: 1234567891011121314172.16.12.3:6379&gt; zrevrange myzset 0 3 withscores1) &quot;v4&quot;2) &quot;63&quot;3) &quot;v3&quot;4) &quot;62&quot;5) &quot;v2&quot;6) &quot;61&quot;7) &quot;v1&quot;8) &quot;60&quot;172.16.12.3:6379&gt; zrevrange myzset 0 31) &quot;v4&quot;2) &quot;v3&quot;3) &quot;v2&quot;4) &quot;v1&quot; zcard可以返回zset中元素的个数,如下:12172.16.12.3:6379&gt; zcard myzset(integer) 4 zcountzcount可以返回有序集key中，score值在min和max之间(默认包括score值等于min或max)的成员的数量 12172.16.12.3:6379&gt; zcount myzset 60 63(integer) 4 如果我们想统计的不需要包含 开始或者结束区间的值,可以加一个(,如下:1234172.16.12.3:6379&gt; zcount myzset 60 (63(integer) 3172.16.12.3:6379&gt; zcount myzset (60 (63(integer) 2 zrangebyscorezrangebyscore,可以按照分数的范围,获取对应的元素,加上withscores可以连score一起返回,如下:1234567891011121314151617172.16.12.3:6379&gt; zrangebyscore myzset 60 631) &quot;v1&quot;2) &quot;v2&quot;3) &quot;v3&quot;4) &quot;v4&quot;172.16.12.3:6379&gt; zrangebyscore myzset 60 63 withscores1) &quot;v1&quot;2) &quot;60&quot;3) &quot;v2&quot;4) &quot;61&quot;5) &quot;v3&quot;6) &quot;62&quot;7) &quot;v4&quot;8) &quot;63&quot;172.16.12.3:6379&gt; zrangebyscore myzset (60 (631) &quot;v2&quot;2) &quot;v3&quot; zrankzrank,可以返回有序集合中指定成员的排名, 在zset中,成员的排名是按照score分数从小到大去排序的,排名是从0开始的,如下: 1234172.16.12.3:6379&gt; zrank myzset v1(integer) 0172.16.12.3:6379&gt; zrank myzset v3(integer) 2 zrevrankzrevrank和zrank是差不多的,只不过zrevrank是按照score是从大到小去排序的,如下:1234172.16.12.3:6379&gt; zrevrank myzset v3(integer) 1172.16.12.3:6379&gt; zrevrank myzset v1(integer) 3 zincrby语法是:1zincrby key increment member 为有序集合key的成员member的score值加上增量increment 如果key中不存在member,就在key中添加一个member,score是increment 如果key不存在,就创建一个只含有指定member成员的有序集合. 当key不是有序集类型时,返回一个错误 12172.16.12.3:6379&gt; zincrby myzset 10 v1&quot;70&quot; zinterstore语法如下:1ZINTERSTORE destination numkeys key [key ...] [WEIGHTS weight] [SUM|MIN|MAX] 该命令是用来计算给定的numkeys个有序集合的交集,并且把结果放到destination中 在给定要计算的key和其它参数之前,必须先给定key个数(numberkeys) 默认情况下,结果集destination中一个元素的分数是有序集合中该元素分数之和,前提是该元素在这些有序集合中都存在,因为交集要求其成员必须是给定的每个有序集合中的成员 WEIGHTS,是在执行的过程中给原score乘以weights后再求和 如果destination存在，就把它覆盖 返回值是结果有序集合destination中元素个数 具体使用如下:12345678910111213141516171819172.16.12.3:6379&gt; zadd zset1 1 one(integer) 1172.16.12.3:6379&gt; zadd zset1 2 two(integer) 1172.16.12.3:6379&gt; zadd zset2 1 one(integer) 1172.16.12.3:6379&gt; zadd zset2 2 two(integer) 1172.16.12.3:6379&gt; zadd zset2 3 three(integer) 1172.16.12.3:6379&gt; zinterstore outzset 2 zset1 zset2 weights 2 3(integer) 2172.16.12.3:6379&gt; zrange outzset 0 -1 withscores1) &quot;one&quot;2) &quot;5&quot;3) &quot;two&quot;4) &quot;10&quot; zremzrem命令,可以从集合中移除一个元素,如下:12345172.16.12.3:6379&gt; zrem outzset one(integer) 1172.16.12.3:6379&gt; zrange outzset 0 -1 withscores1) &quot;two&quot;2) &quot;10&quot; zlexcount命令用于计算有序集合中指定成员之间的成员数量,如下:123456789101112131415172.16.12.3:6379&gt; zrange myzset 0 -1 withscores1) &quot;v2&quot;2) &quot;61&quot;3) &quot;v3&quot;4) &quot;62&quot;5) &quot;v4&quot;6) &quot;63&quot;7) &quot;v1&quot;8) &quot;70&quot;172.16.12.3:6379&gt; zlexcount myzset - +(integer) 4172.16.12.3:6379&gt; zlexcount myzset [v1 [v2(integer) 1 可以用-和+表示得分最小值和最大值,如果使用成员名的话,一定要在成员名之前加上[. zrangebylexzrangebylex返回指定成员区间内的成员,按成员字典正序排序, 分数必须相同,如下：1234567172.16.12.3:6379&gt; zrangebylex myzset [v1 [v21) &quot;v2&quot;172.16.12.3:6379&gt; zrangebylex myzset - +1) &quot;v2&quot;2) &quot;v3&quot;3) &quot;v4&quot;4) &quot;v1&quot; 好了,就介绍这么多的命令,其他需要的话,请参考官方文档]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-5-列表与集合的一些常用命令]]></title>
    <url>%2F2019%2F04%2F27%2FRedis-5-%E5%88%97%E8%A1%A8%E4%B8%8E%E9%9B%86%E5%90%88%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[述前文中,介绍了String数据类型的一些常用的命令,那么下面来看一下一些针对列表与集合的常用命令 列表(list)列表是Redis中另外一种数据类型,下面我们来看看列表中一些基本的操作命令 lpush语法如下:1LPUSH key value [value ...] 将一个或多个值插入到列表key的表头,如果有多个value,则会按从左到右的顺序依次插入表头,具体使用如下:12345678172.16.12.3:6379&gt; lpush k1 v1 v2 v3(error) WRONGTYPE Operation against a key holding the wrong kind of value172.16.12.3:6379&gt; del k1(integer) 1172.16.12.3:6379&gt; lpush k1 v1 v2 v3(integer) 3172.16.12.3:6379&gt; lpush k1 v4 v5(integer) 5 来看一下上面的命令,我在第一次执行的时候,报了一个错,因为我们之前k1对应的值不是一个list,就会报错. 第二点,lpush命令的返回值是push操作后的list的长度,如上面,我第一次push了3个value进去, 长度就是3,然后又加了两个.那长度就是5 lrange返回列表key中的指定 区间内的元素,区间以偏移量start和stop指定,start和stop都以0开始,即0代表第一个元素,1代表第二个元素… 以此类推,也可以使用负数作为下标,以-1表示列表的最后一个元素,-2表示倒数第二个元素…. 以此类推,具体使用如下:1234567891011172.16.12.3:6379&gt; lrange k1 0 -21) &quot;v5&quot;2) &quot;v4&quot;3) &quot;v3&quot;4) &quot;v2&quot;172.16.12.3:6379&gt; lrange k1 0 51) &quot;v5&quot;2) &quot;v4&quot;3) &quot;v3&quot;4) &quot;v2&quot;5) &quot;v1&quot; rpushrpush和lpush的功能基本一致,只不过rpush中,value是按照从右到左的顺序依次插入的,使用如下:123456789101112172.16.12.3:6379&gt; get k2&quot;c&quot;172.16.12.3:6379&gt; del k2(integer) 1172.16.12.3:6379&gt; rpush k2 1 2 3 4 5(integer) 5172.16.12.3:6379&gt; lrange k2 0 -11) &quot;1&quot;2) &quot;2&quot;3) &quot;3&quot;4) &quot;4&quot;5) &quot;5&quot; rpoprpop命令,可以移除列表中最后一个元素,并且返回该元素的值,使用如下:12345678172.16.12.3:6379&gt; rpop k2&quot;5&quot;172.16.12.3:6379&gt; lrange k2 0 -11) &quot;1&quot;2) &quot;2&quot;3) &quot;3&quot;4) &quot;4&quot;172.16.12 lpoplpop和rpop类似,只不过移除的是列表中的第一个元素,使用如下:123456172.16.12.3:6379&gt; lpop k2&quot;1&quot;172.16.12.3:6379&gt; lrange k2 0 -11) &quot;2&quot;2) &quot;3&quot;3) &quot;4&quot; lindexlindex命令,可以返回指定下标对应的元素的值,可以用正数下标,0表示第一个元素,也可以用负数下标, -1表示最后一个元素,使用如下:1234172.16.12.3:6379&gt; lindex k2 0&quot;2&quot;172.16.12.3:6379&gt; lindex k2 -1&quot;4&quot; ltrimltrim命令,可以对一个列表进行修剪,需要指定一个下标开始值和结束值, 不在这个区间的元素都会被删除掉, 下标的取值和上面都一样的,使用如下: 123456789101112172.16.12.3:6379&gt; lrange k1 0 -11) &quot;v5&quot;2) &quot;v4&quot;3) &quot;v3&quot;4) &quot;v2&quot;5) &quot;v1&quot;172.16.12.3:6379&gt; ltrim k1 0 2OK172.16.12.3:6379&gt; lrange k1 0 -11) &quot;v5&quot;2) &quot;v4&quot;3) &quot;v3&quot; blpopBLPOP是阻塞式列表的弹出原语,blpop是lpop的阻塞版本 当给定列表内,没有任何元素可以供弹出的时候,连接将被blpop命令阻塞, 当 给定的是多个key时,按参数key的先后顺序依次检查各个列表,弹出一个非空列表的第一个元素 使用这个命令的时候,需要指定阻塞的时长,单位是秒,如果在规定时间内没有元素可供弹出,则阻塞结束,返回结果是key-value的组合,具体使用如下:12345678910111213141516172.16.12.3:6379&gt; lrange k1 0 -11) &quot;v5&quot;2) &quot;v4&quot;3) &quot;v3&quot;172.16.12.3:6379&gt; blpop k1 51) &quot;k1&quot;2) &quot;v5&quot;172.16.12.3:6379&gt; blpop k1 51) &quot;k1&quot;2) &quot;v4&quot;172.16.12.3:6379&gt; blpop k1 51) &quot;k1&quot;2) &quot;v3&quot;172.16.12.3:6379&gt; blpop k1 5(nil)(5.02s) 来看一下上面的命令,最开始集合是有三个元素的,然后每次给定阻塞时长是5秒,弹出第一个元素,然后一共执行三次,列表中也就没有元素了,这时候,执行第四次弹出的操作,就可以看到,这里阻塞了5秒中之后,发现,还是没有可以弹出的元素,然后就阻塞结束,返回nil了 集合(set)接下来看一些针对集合的操作命令 saddsadd命令,可以添加一个或多个指定的member元素到集合的key中, 如果值在这个集合中存在的话就忽略掉,如果集合key不存在就先新建集合,然后添加member元素到集合,具体使用如下: 1234172.16.12.3:6379&gt; sadd k1 v1 v2(integer) 2172.16.12.3:6379&gt; sadd k1 v2(integer) 0 第一个命令添加了v1,v2,然后下面又去添加v2,是直接被忽略掉了,已经存在的元素不会被加进去, 这里的返回值是新添加到集合里的元素的数量,不包括已经存在于集合中的元素 sremsrem命令可以在key集合中移除指定的元素,如果指定的元素不存在,则忽略,如果key集合不存在,则被视为一个空的集合,返回0,使用如下:1234172.16.12.3:6379&gt; srem k1 v1(integer) 1172.16.12.3:6379&gt; srem k1 v3(integer) 0 sismembersismember可以返回指定的值,是不是这个key里面的成员 ,使用如下:1234172.16.12.3:6379&gt; sismember k1 v3(integer) 0172.16.12.3:6379&gt; sismember k1 v2(integer) 1 v3是不在k1这个集合中的, 所以返回0 , v2存在,就返回1 scardscard 命令可以返回指定集合中的元素的数量,如下:123456172.16.12.3:6379&gt; scard k1(integer) 1172.16.12.3:6379&gt; sadd k1 v1(integer) 1172.16.12.3:6379&gt; scard k1(integer) 2 可以看到,最开始元素的数量是1,然后添加一个元素进去,再查一下就是2了. smembersmember命令,可以返回指定key集合中的所有元素,如下:123456789172.16.12.3:6379&gt; sadd k1 v3 v4 v5 v6(integer) 4172.16.12.3:6379&gt; smembers k11) &quot;v4&quot;2) &quot;v1&quot;3) &quot;v3&quot;4) &quot;v5&quot;5) &quot;v6&quot;6) &quot;v2&quot; srandmembersrandmember命令,传入key的名称,然后会随机返回key集合中的一个元素,从redis2.6开始,这个命令可以指定返回元素的个数(count) count参数详解: 如果是整数且小于元素的个数,就会返回count个随机元素 如果count是整数且大于集合中元素的个数时,则返回集合中的所有元素 当count是负数,则会返回一个包含count的绝对值的个数元素的数组 如果count的绝对值大于元素的个数,则返回的结果集里会出现一个元素出现多次的情况 如下:123456789101112131415161718192021222324172.16.12.3:6379&gt; srandmember k1&quot;v2&quot;172.16.12.3:6379&gt; srandmember k1 21) &quot;v5&quot;2) &quot;v4&quot;172.16.12.3:6379&gt; srandmember k1 91) &quot;v2&quot;2) &quot;v6&quot;3) &quot;v4&quot;4) &quot;v5&quot;5) &quot;v3&quot;6) &quot;v1&quot;172.16.12.3:6379&gt; srandmember k1 -11) &quot;v6&quot;172.16.12.3:6379&gt; srandmember k1 -91) &quot;v3&quot;2) &quot;v1&quot;3) &quot;v5&quot;4) &quot;v4&quot;5) &quot;v1&quot;6) &quot;v4&quot;7) &quot;v4&quot;8) &quot;v2&quot;9) &quot;v3&quot; spopspop命令和srandmember类似,不同的是 spop每次返回的元素都会被从这个集合中移除掉,如下:12345678172.16.12.3:6379&gt; spop k1&quot;v3&quot;172.16.12.3:6379&gt; SMEMBERS k11) &quot;v5&quot;2) &quot;v6&quot;3) &quot;v2&quot;4) &quot;v4&quot;5) &quot;v1&quot; sdiffsdiff用来取两个集合的差集,如下:123456789101112131415172.16.12.3:6379&gt; smembers k11) &quot;v5&quot;2) &quot;v6&quot;3) &quot;v2&quot;4) &quot;v4&quot;5) &quot;v1&quot;172.16.12.3:6379&gt; smembers k21) &quot;v7&quot;2) &quot;v2&quot;3) &quot;v3&quot;172.16.12.3:6379&gt; sdiff k1 k21) &quot;v5&quot;2) &quot;v6&quot;3) &quot;v1&quot;4) &quot;v4&quot; sdiffstoresdiffstore和sdiff命令基本一致,不同的是会把返回的结果保存在一个新的集合中,使用如下:123456789101112172.16.12.3:6379&gt; sdiff k1 k21) &quot;v5&quot;2) &quot;v6&quot;3) &quot;v1&quot;4) &quot;v4&quot;172.16.12.3:6379&gt; sdiffstore k3 k1 k2(integer) 4172.16.12.3:6379&gt; smembers k31) &quot;v5&quot;2) &quot;v6&quot;3) &quot;v1&quot;4) &quot;v4&quot; sintersinter命令,用来计算指定集合的交集,如下:123456789101112172.16.12.3:6379&gt; smembers k11) &quot;v5&quot;2) &quot;v6&quot;3) &quot;v2&quot;4) &quot;v4&quot;5) &quot;v1&quot;172.16.12.3:6379&gt; smembers k21) &quot;v7&quot;2) &quot;v2&quot;3) &quot;v3&quot;172.16.12.3:6379&gt; sinter k1 k21) &quot;v2&quot; sinterstoresinterstore命令,和上面的sinter类似,但是会把返回值保存到一个新的集合中,如下:1234172.16.12.3:6379&gt; sinterstore k4 k1 k2(integer) 1172.16.12.3:6379&gt; smembers k41) &quot;v2&quot; sunion用来计算集合的并集,如下:12345678172.16.12.3:6379&gt; sunion k1 k21) &quot;v5&quot;2) &quot;v6&quot;3) &quot;v2&quot;4) &quot;v4&quot;5) &quot;v3&quot;6) &quot;v7&quot;7) &quot;v1&quot; sunionstoresunionstore和sunion类似,但是会把返回结果保存在一个新的集合中,如下:12345678910172.16.12.3:6379&gt; sunionstore k5 k1 k2(integer) 7172.16.12.3:6379&gt; smembers k51) &quot;v5&quot;2) &quot;v6&quot;3) &quot;v2&quot;4) &quot;v4&quot;5) &quot;v3&quot;6) &quot;v7&quot;7) &quot;v1&quot; 关于list和set的一些常用命令就说这么多,主要就是一些添加,删除,交集并集差集的操作]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-4-针对String数据类型中Bit的相关命令]]></title>
    <url>%2F2019%2F04%2F27%2FRedis-4-%E9%92%88%E5%AF%B9String%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%B8%ADBit%E7%9A%84%E7%9B%B8%E5%85%B3%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[概述上篇文章我们对String数据类型中一些基本的命令进行了介绍,但是没有涉及到Bit相关的命令,本文我们就来看看几个和Bit相关的命令. redis中的字符串都是以二进制的方式进行存储的,举个例子,执行以下命令:12172.16.12.3:6379&gt; set k1 aOK a对应的ASCII码是97,转换为二进制数据是01100001,我们Bit相关命令都是对这个二进制数据进行操作 ASCII码对照表 常用命令getbitgetbit命令可以返回key对应的value在offset处的bit值,以上文提到的k1为例,a对应的二进制数据是01100001,所以当offset为0时,对应的bit值为0,offset为1时,对应的bit值为1,offset为2时,对应的bit值为1,offset为3时,对应的bit值为0,依此类推…如下: 12345678910111213141516172.16.12.3:6379&gt; getbit k1 0(integer) 0172.16.12.3:6379&gt; getbit k1 1(integer) 1172.16.12.3:6379&gt; getbit k1 2(integer) 1172.16.12.3:6379&gt; getbit k1 3(integer) 0172.16.12.3:6379&gt; getbit k1 4(integer) 0172.16.12.3:6379&gt; getbit k1 5(integer) 0172.16.12.3:6379&gt; getbit k1 6(integer) 0172.16.12.3:6379&gt; getbit k1 7(integer) 1 setbitsetbit可以用来修改二进制数据, 举个例子,如下对照表: 控制字符 ASCII值 二进制值 a 97 01100001 b 99 01100011 可以看到 a和c的二进制值,区别在于第六位(下标从0开始), 一个是0 一个是 1, 我们可以通过setbit命令,将上面添加的k1的第六位改为1,如下:123456172.16.12.3:6379&gt; get k1&quot;a&quot;172.16.12.3:6379&gt; setbit k1 6 1(integer) 0172.16.12.3:6379&gt; get k1&quot;c&quot; 这时,可以看到,k1的value从a变成了c, setbit命令的返回值,是该位上原本的bit值. bitcountbitcount命令,可以统计二进制数据中的,1的个数,比如我们k1现在的二进制值是01100011,里面有4个1,然后返回就是4,如下:12172.16.12.3:6379&gt; bitcount k1(integer) 4 关于这个命令,redis官网上有一个使用案例:统计用户上线次数,如下: 举个例子，如果今天是网站上线的第 100 天，而用户 peter 在今天阅览过网站，那么执行命令 SETBIT peter 100 1 ；如果明天 peter 也继续阅览网站，那么执行命令 SETBIT peter 101 1 ，以此类推。当要计算 peter 总共以来的上线次数时，就使用 BITCOUNT 命令：执行 BITCOUNT peter ，得出的结果就是 peter 上线的总天数。 用这种方式去统计,好处就是节省空间而且运行速度快,每天占用一个bit,年也就365个bit,10年也就10*365个bit,也就是456个字节,对于这么大的数据,bit的操作速度非常快. bitop该命令语法如下:1BITOP operation destkey key [key ...] 对一个或多个保存二进制位的字符串 key 进行位元操作,并将结果保存到 destkey 上,bittop 命令支持 AND(并), OR(或) , NOT(非) , XOR(异或) 这四种操作中的任意一种 除了 NOT 操作之外,其他操作都可以接受一个或多个 key 作为输入, 执行结果将始终保持到destkey里面 使用案例如下:123456789101112172.16.12.3:6379&gt; set k1 aOK172.16.12.3:6379&gt; set k2 cOK172.16.12.3:6379&gt; bitop and k3 k1 k2(integer) 1172.16.12.3:6379&gt; get k3&quot;a&quot;172.16.12.3:6379&gt; bitop xor k3 k1 k2(integer) 1172.16.12.3:6379&gt; get k3&quot;\x02&quot; 执行not只能有一个key作为输入,使用如下:12172.16.12.3:6379&gt; bitop not k3 k4(integer) 2 这里会对k4的二进制位串取反,将取反结果交给k3. 这个逻辑运算,我没搞懂到底是怎算的, 官网文档在这里 bitposbitops命令,用来获取二进制位串中的第一个1或者0的位置,使用如下 1234172.16.12.3:6379&gt; bitpos k1 1(integer) 1172.16.12.3:6379&gt; bitpos k1 0(integer) 0 他的语法其实是bitpos key bit [start] [end],就是说可以在后面设置一个范围,但是这个范围是字节的范围,而不是二进制位串的范围]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-3-针对String数据类型的一些操作]]></title>
    <url>%2F2019%2F04%2F27%2FRedis-3-%E9%92%88%E5%AF%B9String%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84%E4%B8%80%E4%BA%9B%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[概述上文中,我们看了一些所有数据类型都通用的一些命令,下面再来看一下一些针对String类型独有的命令 操作String类型数据的一些命令append追加字符串,如果key存在的话,就直接在value后面追加内容,如果key不存在,就会先创建一个value是空字符串的数据,然后再追加,具体使用方式如下:12345678910172.16.12.3:6379&gt; get key1(nil)172.16.12.3:6379&gt; append key1 hello(integer) 5172.16.12.3:6379&gt; get key1&quot;hello&quot;172.16.12.3:6379&gt; append key1 world(integer) 10172.16.12.3:6379&gt; get key1&quot;helloworld&quot; decrdecr命令可以对指定key的value执行减1的操作. 如果key的value不为数字,则会报错 如果key不存在,则key对应的初始值会被置为0, 然后再进行减1的操作 具体使用如下:12345678910111213141516172.16.12.3:6379&gt; set k2 20OK172.16.12.3:6379&gt; decr k2(integer) 19172.16.12.3:6379&gt; get k2&quot;19&quot;172.16.12.3:6379&gt; set k3 testOK172.16.12.3:6379&gt; decr k3(error) ERR value is not an integer or out of range172.16.12.3:6379&gt; decr k4(integer) -1172.16.12.3:6379&gt; get k4&quot;-1&quot; decrby和decr差不多,只不过decrby可以指定步长,如下命令,指定减5:123456172.16.12.3:6379&gt; get k2&quot;19&quot;172.16.12.3:6379&gt; decrby k2 5(integer) 14172.16.12.3:6379&gt; get k2&quot;14&quot; getget命令用来获取对应key的value,如果不存在,就返回nil,如下:1234172.16.12.3:6379&gt; get k2&quot;14&quot;172.16.12.3:6379&gt; get k5(nil) getrange跟java的substring类似,用来返回key所对应的value的子串, 需要传入开始的下标和结束的下标 从左往右数每个字符的下标依次是0,1,2,3…. 如果是从右往左数的话,下标依次是-1,-2,-3,-4….. 具体使用方式如下:12345678910111213172.16.12.3:6379&gt; set k5 helloworldOK172.16.12.3:6379&gt;172.16.12.3:6379&gt; get key5(nil)172.16.12.3:6379&gt; get 5(nil)172.16.12.3:6379&gt; get k5&quot;helloworld&quot;172.16.12.3:6379&gt; getrange k5 0 4&quot;hello&quot;172.16.12.3:6379&gt; getrange k5 -5 -1&quot;world&quot; getsetgetset命令,用来获取指定key所对应的value,并且给这个key设置新的value,具体如下:123456172.16.12.3:6379&gt; get k5&quot;helloworld&quot;172.16.12.3:6379&gt; getset k5 newvalue&quot;helloworld&quot;172.16.12.3:6379&gt; get k5&quot;newvalue&quot; incrincr命令可以对指定key的value进行加1的操作 如果指定key的value不是数字,就会报错 如果指定key不存在,会先将key的value设置为0,然后再加1,最终值也就是1123456789101112172.16.12.3:6379&gt; get k2&quot;14&quot;172.16.12.3:6379&gt; incr k2(integer) 15172.16.12.3:6379&gt; get k2&quot;15&quot;172.16.12.3:6379&gt; incr k5(error) ERR value is not an integer or out of range172.16.12.3:6379&gt; incr k6(integer) 1172.16.12.3:6379&gt; get k6&quot;1&quot; incrby和incr命令相似,只是可以指定步,使用方式如下: 123456172.16.12.3:6379&gt; get k6&quot;1&quot;172.16.12.3:6379&gt; incrby k6 10(integer) 11172.16.12.3:6379&gt; get k6&quot;11&quot; incrbyfloatincrbyfloat命令,用来增长浮点数,如下:123456172.16.12.3:6379&gt; get k6&quot;11&quot;172.16.12.3:6379&gt; incrbyfloat k6 0.5&quot;11.5&quot;172.16.12.3:6379&gt; incrbyfloat k6 0.11&quot;11.61&quot; mget与mset批量操作命令,用来批量设置值和批量获取值,如下:123456172.16.12.3:6379&gt; mset k1 v1 k2 v2 k3 v3OK172.16.12.3:6379&gt; mget k1 k2 k31) &quot;v1&quot;2) &quot;v2&quot;3) &quot;v3&quot; setexsetex命令可以给key同时设置value和过期时间,相当于先给key设置value然后再设置一个有效时间,使用方式如下:123456172.16.12.3:6379&gt; setex k1 30 v1OK172.16.12.3:6379&gt; ttl k1(integer) 25172.16.12.3:6379&gt; get k1&quot;v1&quot; psetexpsetex命令和setex类似,只不过设置的时间单位是毫秒,如下:1234172.16.12.3:6379&gt; psetex k1 100000 v1OK172.16.12.3:6379&gt; pttl k1(integer) 96942 setnxsetnx是 set if not exists, 我们在使用set命令的时候,对于已经存在的key,就会新值覆盖掉旧值 而setnx命令就是说,如果key是存在的,那就不进行任何操作,如果key不存在,那效果跟set命令是一样的,具体使用如下: 1234567891011121314172.16.12.3:6379&gt; get k2&quot;v2&quot;172.16.12.3:6379&gt; get k1(nil)172.16.12.3:6379&gt; get k2&quot;v2&quot;172.16.12.3:6379&gt; setnx k1 v1(integer) 1172.16.12.3:6379&gt; setnx k2 test(integer) 0172.16.12.3:6379&gt; get k1&quot;v1&quot;172.16.12.3:6379&gt; get k2&quot;v2&quot; msetnxmsetnx具备了setnx和mset的特性, 这里有需要注意的一点,如果有一个key存在,那所有的都不会执行,如下:12345678910172.16.12.3:6379&gt; get k6&quot;11.61&quot;172.16.12.3:6379&gt; get k7(nil)172.16.12.3:6379&gt; msetnx k6 v6 k7 v7(integer) 0172.16.12.3:6379&gt; get k6&quot;11.61&quot;172.16.12.3:6379&gt; get k7(nil) 因为k6是存在的,所以k7也没执行成功 setrangesetrange命令,用来覆盖一个已经存在的key的value,使用方式如下:12345678172.16.12.3:6379&gt; set k1 helloworldOK172.16.12.3:6379&gt; get k1&quot;helloworld&quot;172.16.12.3:6379&gt; setrange k1 5 redis(integer) 10172.16.12.3:6379&gt; get k1&quot;helloredis&quot; 如果已经存在的key的value长度小于offset,则不足的地方用0补齐,如下:1234172.16.12.3:6379&gt; setrange k1 20 --java(integer) 26172.16.12.3:6379&gt; get k1&quot;helloredis\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00--java&quot; strlenstrlen命令用来计算指定key对应的value的长度,如下:12172.16.12.3:6379&gt; strlen k1(integer) 26 关于string类型的常用命令,就先写这么多,如果需要用到其他的命令,可以去官方文档中查找]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-2-数据类型与常用命令]]></title>
    <url>%2F2019%2F04%2F27%2FRedis-2-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%B8%8E%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[reids中的数据类型redis中的数据都是以key-value的形式存储的,数据类型其实是说value的数据类型,一共有以下几种数据类型: String(字符串)String是redis中的最基本的数据类型,string类型是二进制安全的,意思是redis的string可以包含任何数据,比如jpg图片或者序列化的对象, string 类型的值最大能存储 512MB List(列表)List是一个简单的字符串列表,按照插入顺序进行排序,你可以添加一个元素到列表的头部(左边)或者尾部(右边) Hash(哈希)Hash类似于Java中的Map,是一个键值对集合,特别适合用于存储对象 Set(集合)Set是String类型的无序集合,而且Set里面的元素是不可重复的 zset(sorted set：有序集合)zset 和 Set 一样也是string类型元素的集合,且不允许重复的成员,不同的是每个元素都会关联一个double类型的分数,redis正是通过分数来为集合中的成员进行从小到大的排序,zset的成员是唯一的,但分数(score)却可以重复. 常用命令由于五大数据类型的数据结构本身有差异,因此对应的命令也会不同,但是有一些命令不管对于哪种数据类型都是存在的,下面就先来看下这些常用的命令 首先,按上篇文章中的连接方式,连接到redis set命令用来添加一条数据进去:12172.16.12.3:6379&gt; set testKey testValueOK OK就表示添加成功了 del命令通过del命令可以删除一个已存在的数据,比如我们要删除刚才添加的那条数据,命令如下:12172.16.12.3:6379&gt; del testKey(integer) 1 (integer) 1,就表示删除成功了 dump命令dump 命令可以序列化指定的数据,并返回序列化后的值 先添加一条数据进去,还是用set命令,然后用dump命令去序列化,如下:12172.16.12.3:6379&gt; dump testKey&quot;\x00\ttestValue\a\x00-|$\xb4Y\xc3|(&quot; exists命令exists命令,用来判断数据是否存在,具体使用方式如下:1234172.16.12.3:6379&gt; exists testKey(integer) 1172.16.12.3:6379&gt; exists testKey1(integer) 0 1表示存在,0表示不存在 ttl命令ttl命令可以查看数据的有效时间,使用方式如下:1234172.16.12.3:6379&gt; ttl testKey(integer) -1172.16.12.3:6379&gt; ttl testKey1(integer) -2 返回-1表示key存在并且没有设置过期时间(永久有效) 返回-2表示key不存在或者已过期 pttl命令和ttl命令基本一样,只不过返回的是毫秒数12172.16.12.3:6379&gt; pttl testKey(integer) 83823 expire命令expire命令可以给key设置有效时间,在有效期过后,key会被销毁1234172.16.12.3:6379&gt; expire testKey 30(integer) 1172.16.12.3:6379&gt; ttl testKey(integer) 25 30表示设置30秒的有效时间,返回1表示设置成功, 然后通过ttl去查询有效时间,返回25,表示有效期还剩25秒 pexpire命令和expire命令的功能一样,只不过设置时间的参数是毫秒12172.16.12.3:6379&gt; pexpire testKey 100000(integer) 1 persist命令persist命令,可以移除一个key的过期时间,这样这个key就永远不会过期1234172.16.12.3:6379&gt; expire testKey 100(integer) 1172.16.12.3:6379&gt; persist testKey(integer) 1 keys命令keys命令可以获取所有满足条件的key,比如下面的这个命令:123172.16.12.3:6379&gt; keys *1) &quot;testKey1&quot;2) &quot;testKey&quot; *表示所有,这里的*可以换成一个正则表达式,也就是返回所有的符合这个正则表达式的key.]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-1-远程连接]]></title>
    <url>%2F2019%2F04%2F27%2FRedis-1-%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[远程连接我们在linux上装好redis服务之后,希望在windows端或者是Java应用中远程连接redis,就需要做出一些配置 配置文件修改修改redis.conf文件,具体如下: bind 127.0.0.1,在配置文件中找到这个配置,注释掉 requirepass foobared,如果需要设置密码的话,找到这个配置,取消掉注释,然后将foobared改为你自己的密码 连接redis下载redis假设我们要在windows端连接redis的话,windows端需要有一个redis-cli.exe 首先,下载windows版的redis 解压下载完成之后,解压到一个文件夹,如下: 通过redis-cli.exe连接解压完成之后,打开cmd,切换到刚刚解压的redis的目录下面,用以下命令去连接远程redis:1redis-cli.exe -h 172.16.12.3 -p 6379 -a 123456 -h 远程redis的ip地址 -p 远程redis的端口号 -a redis的密码,如果没有设置的话,就不需要 如图所示,就表示连接成功.]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis-0-简介与环境搭建]]></title>
    <url>%2F2019%2F04%2F27%2FRedis-0-%E7%AE%80%E4%BB%8B%E4%B8%8E%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[简介REmote DIctionary Server(Redis) 是一个由Salvatore Sanfilippo写的key-value存储系统. Redis是一个开源的使用ANSI C语言编写,遵守BSD协议,支持网络,可基于内存亦可持久化的日志型Key-Value数据库,并提供多种语言的API. 它通常被称为数据结构服务器,因为值(value)可以是 字符串(String), 哈希(Hash), 列表(list), 集合(sets) 和 有序集合(sorted sets)等类型 官网 中文官网 下载地址 文章参考来源 优点 性能极高 – Redis能支持超过 100K+ 每秒的读写频率 丰富的数据类型 – Redis支持二进制案例的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作 原子 – Redis的所有操作都是原子性的,同时Redis还支持对几个操作合并后的原子性执行 丰富的特性 – Redis还支持 publish/subscribe, 通知, key 过期等等特性 环境搭建可以通过docker直接拉取镜像安装,或者是通过官网下载自己安装,通过docker的安装方法在docker分类下有详细说明,下面看下用普通方式安装方法 下载1wget http://download.redis.io/releases/redis-4.0.8.tar.gz 解压1tar -zxvf redis-4.0.8.tar.gz 编译123cd redis-4.0.8make 安装redis123cd srcmake install PREFIX=/usr/local/redis 移动配置文件到安装目录下12345cd ../mkdir /usr/local/redis/etccp redis.conf /usr/local/redis/etc 配置redis为后台启动123vim /usr/local/redis/etc/redis.conf//将daemonize no 改成daemonize yes 将redis加入到开机启动12345vi /etc/rc.local//在里面添加内容：/usr/local/redis/bin/redis-server/usr/local/redis/etc/redis.conf 启动redis1/usr/local/redis/bin/redis-server /usr/local/redis/etc/redis.conf 查看redis服务1ps -ef | grep -i redis 停止redis服务1234/usr/local/redis/bin/redis-cli shutdown//设置了密码的情况下/usr/local/redis/bin/redis-cli -a password shutdown 卸载123rm -rf /usr/local/redis //删除安装目录rm -rf /usr/bin/redis-* //删除所有redis相关命令脚本rm -rf /root/redis-4.0.8 //删除redis解压文件夹 推荐还是用docker去安装,快捷方便]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring-事务传播性]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-%E4%BA%8B%E5%8A%A1%E4%BC%A0%E6%92%AD%E6%80%A7%2F</url>
    <content type="text"><![CDATA[事务传播性事务的传播性一般是在事务嵌套的时候使用,比如方法A是事务管理的,方法B也是有事务管理的,然后方法A中调用了方法B,那么这两个事务是各自作为独立的事务提交呢还是内层的事务合并到外层一起提交,这就是事务传播性要确定的问题. 场景有如下两个方法12345@Transactionalpublic void A()&#123; // ... B();&#125; 1234@Transactionalpublic void B()&#123; // ...&#125; PROPAGATION_REQUIRED注解: @Transactional(propagation=Propagation.REQUIRED) PROPAGATION_REQUIRED是Spring默认的事务传播机制,就是如果有外层事务的话,当前事务加到外层事务里面,一起提交一起回滚,基本可以满足大多数业务场景. 举例以上面的场景为例,假设我们现在设置了事务的传播性@Transactional(propagation=Propagation.REQUIRED),然后A方法里面调用了B方法,所以B方法会加入A方法的事务,A和B两个方法同时提交,同时回滚. 这里要注意一点,在A方法里面调用B方法的时候加了try-catch,但是没有把异常抛出去,而B方法抛出了异常,那么整个事务也会回滚,这时候调用A方法的外层会收到Transaction rolled back because it has been marked as rollback-only的异常,而把B方法真正的异常吃掉了. 一般的话,在A,B两个方法中可以把异常抛出来,在调用A方法的时候再catch. PROPAGATION_REQUIRES_NEW注解: @Transactional(propagation=Propagation.REQUIRES_NEW) PROPAGATION_REQUIRES_NEW这个传播机制是每次开启一个新的事务,同时把外层的事务挂起,当前事务执行完毕后,再恢复上层事务的执行 举例以上面的场景为例,假设现在事务的传播方式设置的是@Transactional(propagation=Propagation.REQUIRES_NEW),首先调用A方法的时候会开启一个事务,然后A方法中调用B的时候会把A方法的事务挂起,然后再开启一个新的事务来执行B方法,执行完毕后再恢复A方法的事务. 然后呢有这样一个问题,如果B方法执行异常,回滚了,这时候A方法是否会回滚呢?如果A方法中调用B的时候,加了try-catch,如果异常没有在catch中throw出来,那么A方法就不会回滚,这时候B方法的提交或回滚是对A方法没有影响的. 如果A方法中没有加try-catch,那么A方法也会回滚. PROPAGATION_SUPPORTS注解: @Transactional(propagation=Propagation.SUPPORTS) PROPAGATION_SUPPORTS 该传播机制是如果外层有事务则加入到该事务中,如果不存在,也不会创建新事务,直接使用非事务方式执行 举例以上面的场景为例,假设传播方式设置为@Transactional(propagation=Propagation.SUPPORTS),如果A方法的调用放没有开启事务的话,那么A,B两个方法都不会开启事务,因为外层是没有事务,也就不会创建,都是用非事务的执行方式了. 如果A方法的传播性是PROPAGATION_REQUIRED,B方法的传播性是PROPAGATION_SUPPORTS的时候,这时候外层的A方法会开启一个事务(或者说A的调用方有事务就加入该事务),然后B方法执行的时候会加入到A的事务中,同时提交同时回滚. PROPAGATION_NOT_SUPPORTED注解:@Transactional(propagation=Propagation.NOT_SUPPORTED) PROPAGATION_NOT_SUPPORTED,该传播机制不支持事务,如果外层有事务的话,挂起外层事务,执行完毕后恢复. 举例如果A方法的传播性是PROPAGATION_REQUIRED,B方法的传播性是PROPAGATION_NOT_SUPPORTED,首先A方法会开启一个事务(A外层有的话就加入进去),然后调用B方法的时候会挂起A的事务,然后执行B方法,执行完成之后再恢复A的事务.无论B方法有没有抛出异常,B方法的事务都不会回滚,因为他不在事务的范围内, 那么A方法的事务呢?如果A方法调用B的时候加了try-catch, 如果在执行过程中catch住的异常没有抛出去,那么就不会回滚,如果抛出去了,就回滚. PROPAGATION_NEVER注解:@Transactional(propagation=Propagation.NEVER) PROPAGATION_NEVER该传播机制不支持事务,如果外层存在事务的话,直接抛出异常IllegalTransactionStateException(&quot;Existing transaction found for transaction marked with propagation &#39;never&#39;&quot;) PROPAGATION_MANDATORY注解:@Transactional(propagation=Propagation.MANDATORY) PROPAGATION_MANDATORY,该传播机制是只能在已存在事务的方法中调用,如果在没有事务的方法中调用的话,抛出异常IllegalTransactionStateException(&quot;No existing transaction found for transaction marked with propagation &#39;mandatory&#39;&quot;); PROPAGATION_NESTED注解:@Transactional(propagation=Propagation.NESTED) PROPAGATION_NESTED该传播机制特点是可以保存状态保存点,当事务回滚后,会回滚到某一个保存点上,从而避免所有嵌套事务都回滚. 举例假设上面A,B方法传播性都设置为:@Transactional(propagation=Propagation.NESTED),如果B方法抛出异常,两个方法还是都会回滚,因为B方法虽然回滚到了savePoint,而savePoint里面确实包含了A方法的操作,但是savePoint还是会把异常throw给A,这就导致了A的回滚. 总结 PROPAGATION_REQUIRED:有外层事务就加到外层事务,一起提交回滚. PROPAGATION_REQUIRES_NEW:新开一个事务,挂起外层事务,执行完毕后再恢复外层事务. PROPAGATION_SUPPORTS:外层有事务的话就加进去,没有的话就用非事务的方式执行. PROPAGATION_NOT_SUPPORTED:外层有事务的话,就挂起,然后以非事务的方式执行,执行完毕后恢复外层事务. PROPAGATION_NEVER:不支持事务,外层有事务的话就报错. PROPAGATION_MANDATORY: 外层没有事务的话,就报错. PROPAGATION_NESTED: 可以回滚到savePoint上.]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring-事务隔离性]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E6%80%A7%2F</url>
    <content type="text"><![CDATA[事务隔离性事务隔离性是指多个事务并发执行的时候互相之间不受彼此的干扰,是事务ACID中的I,根据隔离程度会对隔离性有分类. 数据库并发操作存在的几个问题在说事务隔离性之前,需要知道的几个名词,如下: 脏读脏读是指一个事务中访问到了另一个事务未提交的数据. 举个例子,假设现在有两个事务,A和B,他们同时去更新一个数据,比如要更新id=1的这条数据,事务A先查询到这条数据,然后把id更新成2,但是没有提交,这时候事务B在A没有提交的情况下执行搜索,结果搜出来id=2,这就是脏读 不可重复读不可重复读是指,一个事务内在未提交的前提下,多次搜索一个数据,搜出来的结果不一样. 原因就是在多次搜索期间这个数据被别的事务更新了. 幻读幻读是指同一个事务内多次查询(查询sql可能不一样), 返回的结果集不一样(比如多了或少了几条数据). 举个例子,事务A内查询数据,第一次查询的时候有100条数据,第二次用相同的条件去查询却查询出来101条数据,同等条件查询两次结果不一样,这就是幻读. 出现幻读的原因其实和不可重复读是一样的,也是另外一个事务新增或删除第一个事务结果集里面的数据. 幻读跟不可重复读的区别就是,幻读是数据变多或者变少了,不可重复读是数据的内容发生了改变. 事务隔离级别Spring有五大事务隔离级别,接下来详细的看下每个隔离级别 ISOLATION_DEFAULT(默认)用底层数据库的默认隔离级别,数据库设置的隔离级别是什么就是什么. 其他四个隔离级别,是和数据库的事务隔离级别一样的 ISOLATION_READ_UNCOMMITTED(未提交读)最低的隔离级别,事务未提交之前,就可以被其他事务读取. 会出现幻读 脏读 不可重复读 ISOLATION_READ_COMMITTED(提交读)一个事务提交之后才能被其他事务读取到,会造成幻读,不可重复读,是SQL Server数据库的默认隔离级别 ISOLATIONREPEATABLEREAD(可重复读)保证多次读取一个数据时,其值都和事务开始时候的内容是一致的,禁止读取到别的事务未提交的数据,这个隔离级别可能会造成幻读,是MySql数据库的默认隔离级别 ISOLATION_SERIALIZABLE(序列化)代价最高的最可靠的隔离级别,能防止脏读,幻读,不可重复读. 总结 隔离级别 脏读 不可重复读 幻读 ISOLATION_READ_UNCOMMITTED(未提交读) √ √ √ ISOLATION_READ_COMMITTED(提交读) √ √ ISOLATIONREPEATABLEREAD(可重复读) √ ISOLATION_SERIALIZABLE(序列化)]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-23-Spring Cloud Stream整合RabbitMQ(下)]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-Cloud-23-Spring-Cloud-Stream%E6%95%B4%E5%90%88RabbitMQ-%E4%B8%8B%2F</url>
    <content type="text"><![CDATA[述上文中我们简单的使用了一下Spring Cloud Stream,我们的消息是通过rabbitmq的web管理页面去发送的,如果我们想通过代码去发送消息要怎么实现呢? 下面就来看一些Spring Cloud Stream的使用细节 自定义消息通道上文中我们用到了Sink接口, Sink和Source这两个接口,分别定义了输入通道和输出通道, 而Processor通过继承Source和Sink,同时具有输入通道和输出通道.这里我们就模仿Sink和Source,来定义一个自己的消息通道. MySink在上文的项目基础上,新建一个接口,叫MySink,代码如下:12345678public interface MySink &#123; String INPUT = &quot;mychannel&quot;; @Input(INPUT) SubscribableChannel input();&#125; 这里定义了一个消息通道,名称是mychannel,@Input注解的参数表示了消息通道的名称 同时这里定义了一个方法返回一个SubscribableChannel对象,该对象用来维护消息通道订阅者. MySource定义一个MySource接口,代码如下:123456public interface MySource &#123; @Output(MySink.INPUT) MessageChannel output();&#125; @Output注解中描述了消息通道的名称,还是mychannel 这里也定义了一个方法,返回一个MessageChannel对象,该对象中有一个向消息通道发送消息的方法 消息接收类最后,定义一个消息接收类来接收消息,代码如下:12345678910@EnableBinding(value = &#123;MySink.class&#125;)@Slf4jpublic class MySinkReceiver &#123; @StreamListener(MySink.INPUT) public void receive(Object playload) &#123; log.info(&quot;MySinkReceiver收到消息:&#123;&#125;&quot;, playload); &#125;&#125; 这里就是绑定消息通道,然后监听我们自定义的消息通道 测试上面都操作完成之后, 写个单元测试来测试一下,代码如下:1234567891011121314151617181920@RunWith(SpringRunner.class)@SpringBootTest@EnableBinding(MySource.class)@WebAppConfigurationpublic class AppTests &#123; @Autowired private MySource mySource; @Test public void contextLoads() &#123; mySource.output().send(MessageBuilder.withPayload(&quot;hello&quot;).build()); &#125; @Test public void sendMsg() &#123; User user = new User(1L, &quot;张三&quot;, &quot;男&quot;); mySource.output().send(MessageBuilder.withPayload(user).build()); &#125;&#125; 第一个方法中是发送了一个文本的消息,第二个方法中是发送了一个对象 ,然后分别运行两个方法,控制台输出如下: 回执消息如果我们想在消息接收成功之后给一个回执,也是可以的,修改刚才的MySinkReceiver类,代码如下:123456@StreamListener(MySink.INPUT)@SendTo(Source.OUTPUT)public String receive(Object playload) &#123; log.info(&quot;MySinkReceiver收到消息:&#123;&#125;&quot;, playload); return &quot;MySinkReceiver回执消息:&quot; + playload;&#125; @SendTo(Source.OUTPUT)这个注解是定义回执发送的消息通道 方法的返回值就是回执消息,回执消息在系统默认的output通道中,所以,如果我们想接收回执消息的话,就必须要监听这个通道 创建一个ReceiptReceiver类,用来监听回执消息,代码如下:12345678910@EnableBinding(value = &#123;Source.class&#125;)@Slf4jpublic class ReceiptReceiver &#123; @StreamListener(Source.OUTPUT) public void receive2(String msg) &#123; log.info(&quot;receive2 收到回执消息:&#123;&#125;&quot;, msg); &#125;&#125; 最后在测试类的@EnableBinding中加上Source类就可以测试了,@EnableBinding({MySource.class,Source.class}),修改好之后随便执行一个方法,然后查看控制台输出,如下: 消费组我们的服务可能会有多个实例在运行,如果不做任何设置的话,发送一条消息可能被所有的实例接收到,但是有的时候我们只希望被一个实例接收,这个就可以通过消息分组来解决, 只要给项目配置消息组和主题即可,如下配置:1234567spring: cloud: stream: bindings: mychannel: group: g1 destination: dest1 这里我们设置该工程都属于g1消费组,输入通道的主题名是dest1,以上就是消费者的配置 消息生产者中可以做出以下配置:123456spring: cloud: stream: bindings: mychannel: destination: dest1 生产者中配置的消息主题也是dest1(如果发送和接收就在同一个应用中,则这里可以不配置) 这时,我们启动项目两个实例(端口不同),再发送消息,则会被两个实例中的一个接收到,另外一个是接收不到的,但是两个实例时哪一个接收,这就不能确定了 消息分区有的时候,我们可能需要相同特征的消息能够总是被发送到同一个消费者上去处理,如果我们只是单纯的使用消费组则无法实现功能,此时我们需要借助于消息分区,消息分区之后,具有相同特征的消息就可以总是被同一个消费者处理了,配置方式如下(这里的配置都是在消费组的配置基础上完成的): 消费者配置:123456789spring: cloud: stream: bindings: mychannel: consumer: partitioned: true instance-count: 2 instance-index: 0 partitioned 表示开启消息分区 instance-count: 表示当前消费者的总的实例个数 instance-index: 表示当前实例的索引, 我们启动多个实例的时候,需要启动时在命令行配置索引 然后是生产者的配置:12345678spring: cloud: stream: bindings: mychannel: producer: partitionKeyExpression: payload partitionCount: 2 partitionKeyExpression:配置分区键的表达式规则 partitionCount: 设置消息分区数量 此时我们再启动多个消费者实例,然后重复发送多条消息,这些消息都会被同一个消费者处理掉]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-22-Spring Cloud Stream整合RabbitMQ(上)]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-Cloud-22-Spring-Cloud-Stream%E6%95%B4%E5%90%88RabbitMQ-%E4%B8%8A%2F</url>
    <content type="text"><![CDATA[述Spring Cloud Stream是一个构建消息驱动的微服务框架.它构建在Spring Boot之上,用以创建工业级的应用程序,并且通过Spring Integration提供了和消息代理的连接. Spring Cloud Stream为一些供应商的消息中间件产品提供了个性化的自动化配置实现(目前仅支持RabbitMQ和Kafka),同时引入了发布订阅,消费组和分区的语义概念. 下面来通过一个简单的消息收发,看一下Spring Cloud Stream的基本用法. 整合RabbitMQ新建工程新建一个工程,名称是stream-rabbitmq 引入依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rabbit&lt;/artifactId&gt;&lt;/dependency&gt; spring-cloud-starter-stream-rabbit是Spring Cloud Stream对RabbitMQ的封装,这里包含了对RabbitMQ的默认配置,比如连接的ip默认是localhost,默认端口是5672,默认用户是guest,如果我们需要修改的话,还是和上文一样,在配置文件中配置就可以了 修改配置如果是用的默认配置的话,就不用修改,否则,就改成自己的配置, 直接把上篇文章中的配置复制过来就好,具体如下:12345678910spring: application: name: stream-rabbitmq rabbitmq: host: 172.16.12.3 port: 5672 username: admin password: adminserver: port: 2010 创建消息消费者创建一个消费者用来接收消息,代码如下:1234567891011121314/** * @author 周泽 * @date Create in 16:34 2019/4/8 * @Description 消息消费者 */@EnableBinding(Sink.class)@Slf4jpublic class SinkReceiver &#123; @StreamListener(Sink.INPUT) public void receive(Object playload) &#123; log.info(&quot;收到消息:&#123;&#125;&quot;, playload); &#125;&#125; @EnableBinding: 这个注解用来实现对消息通道的绑定,这个注解中还传了一个参数Sink.class,Sink是一个接口,该接口是Spring Cloud Stream中默认实现的对输入消息通道绑定的定义. 然后是receive()方法上面的@StreamListener(Sink.INPUT)注解,这个注解表示该方法是消息中间件上数据流的监听器,Sink.INPUT,表示这是input消息通道上的监听处理器. 测试上面的东西都搞好之后,启动项目,在日志中可以看到以下内容: 表示我们的工程通过admin用户创建了一个指向rabbitmq的连接,这时打开mq的web管理页面,查看队列,就可以看到他创建的这个队列了 然后点击这个队列,找到Publish message选项,推送一条消息. 推送完成之后,查看控制台输出 这里已经收到了消息,只是消息没有序列化而已]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-21-Spring Cloud Bus整合RabbitMQ]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-Cloud-21-Spring-Cloud-Bus%E6%95%B4%E5%90%88RabbitMQ%2F</url>
    <content type="text"><![CDATA[述Spring Cloud Bus也是微服务架构中的必备组件,Spring Cloud Bus可以将分布式系统的节点与轻量级消息代理链接,然后可以实现广播状态更改(例如配置更改)或广播其他管理指令 Spring Cloud Bus就像一个分布式执行器,用于扩展的Spring Boot的应用程序,也可以作为应用程序之间的通信通道,这里就涉及到了消息代理,Spring Cloud Bus支持RabbitMQ和Kafka,下面先来看一下整合RabbitMQ RabbitMQ的安装首先需要一个rabbitmq的服务,可以去手动安装,或者使用docker去安装mq 整合RabbitMQ新建工程新建一个工程,名称是bus-rabbitmq 引入依赖引入amqp的依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-bus-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 修改配置修改application.yml文件,配置如下:12345678910spring: application: name: bus-rabbitmq rabbitmq: host: 172.16.12.3 port: 5672 username: admin password: adminserver: port: 2009 主要就是配置了rabbitmq的ip,端口,还有用户名密码 消息生产者发送消息可以使用spring封装好的AmqpTemplate,具体生产者类如下:123456789101112131415161718192021/** * @author 周泽 * @date Create in 15:42 2019/4/8 * @Description 消息生产者 */@Component@Slf4jpublic class Sender &#123; @Autowired private AmqpTemplate amqpTemplate; public void send()&#123; String msg = &quot;test msg&quot; + System.currentTimeMillis(); log.info(&quot;发送消息:&#123;&#125;&quot;, msg); amqpTemplate.convertAndSend(&quot;test.queue&quot;, msg); &#125;&#125; 这里就是直接注入AmqpTemplate,然后通过convertAndSend方法,向名称为test.queue的队列中发送一个消息. 消息消费者代码如下:12345678910111213141516/** * @author 周泽 * @date Create in 15:46 2019/4/8 * @Description 消息消费者 */@Component@Slf4j@RabbitListener(queues = &quot;test.queue&quot;)public class Consumer &#123; @RabbitHandler public void process(String msg) &#123; log.info(&quot;消费者收到消息:&#123;&#125;&quot;, msg); &#125;&#125; 这里有两个注解 @RabbitListener(queues = &quot;test.queue&quot;):表示要监听的队列名 @RabbitHandler:表示该方法用来处理接收到的消息 配置消息队列上面我们用到了名称是test.queue的这个队列,需要创建一下,代码如下:1234567891011121314/** * @author 周泽 * @date Create in 15:48 2019/4/8 * @Description 消息队列的配置 */@Configurationpublic class RabbitConfig &#123; @Bean public Queue testQueue() &#123; return new Queue(&quot;test.queue&quot;); &#125;&#125; 测试上面的内容都完成后, 用单元测试来测试一下,代码如下:12345678910111213141516@RunWith(SpringRunner.class)@SpringBootTestpublic class AppTests &#123; @Test public void contextLoads() &#123; &#125; @Autowired private Sender sender; @Test public void sendTest()&#123; sender.send(); &#125;&#125; 运行以后,观察日志,输出: 可以看到,这里接收发送都没有问题,再看一下mq的管理页面 我们刚刚创建的队列也创建好了]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-20-分布式配置中心客户端配置细节]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-Cloud-20-%E5%88%86%E5%B8%83%E5%BC%8F%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%85%8D%E7%BD%AE%E7%BB%86%E8%8A%82%2F</url>
    <content type="text"><![CDATA[述上文中详细的介绍了分布式配置中心的服务端配置细节,本文来看一下客户端的一些配置细节 服务化配置中心前几篇文章中,配置中心的服务端和客户端都是直接启动的,并没有交给注册中心去管理,而且在config-client中配置服务端的地址的时候都是直接写死的地址,这样显然不够灵活 我们可以交给服务注册中管理应用,在配置的时候直接使用服务名就ok了.那下面就来看一下具体怎么修改吧 服务端修改config-server中需要做出如下修改: 首先添加eureka的依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt;&lt;/dependency&gt; 然后再启动类添加@EnableDiscoveryClient注解,表示这是一个eureka客户端,如下:12345678910@SpringBootApplication@EnableConfigServer@EnableDiscoveryClientpublic class App &#123; public static void main(String[] args) &#123; SpringApplication.run(App.class, args); &#125;&#125; 最后在application.yml中配置eureka的地址:1234eureka: client: service-url: defaultZone: http://peer1:1111/eureka/,http://peer2:1112/eureka/ 至此,服务端的配置就ok了,然后是客户端的配置 客户端修改首先还是添加eureka的依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt;&lt;/dependency&gt; 启动类添加@EnableDiscoveryClient注解:12345678910@SpringBootApplication@EnableConfigurationProperties@EnableDiscoveryClientpublic class App &#123; public static void main(String[] args) &#123; SpringApplication.run(App.class, args); &#125;&#125; 最后,修改配置文件bootstrap.yml,具体配置如下:123456789101112131415161718spring: application: name: application cloud: config: profile: dev label: master username: root password: 123456 discovery: enabled: true service-id: config-serverserver: port: 2008eureka: client: service-url: defaultZone: http://peer1:1111/eureka/,http://peer2:1112/eureka/ 相对于之前的配置就是添加了eureka的注册中心地址,还有两个分别是 spring.cloud.config.discovery.enabled: 表示开启通过服务名来访问config-server spring.cloud.config.discovery.service-id=config-server: 表示config-server的服务名 到这里客户端也修改完成了,然后启动测试一下 测试先启动注册中心, 我上面的配置是两个注册中心,所以启动两个,然后启动config-server和config-client.然后访问http://localhost:1111/.结果如下: 可以看到,两个服务都已经注册了. 再访问一下http://localhost:2008/test3,看看能不能正常使用 返回值也没问题,服务化配置中心就构建成功了 失败快速响应默认情况下,如果我们只启动了config-client,不启动config-server,然后获取数据,是不报错的,然后返回空(Dalston.SR4这个版本是这样).我们希望在config-server没有启动的情况下,客户端能快速知道服务端没启动, 方式很简单,在config-client的bootstrap.yml中,加入以下配置:1234spring: cloud: config: fail-fast: true 这时候,不启动config-server直接启动config-client的话,就会直接报错,如下: 重试机制如果由于网络抖动等原因导致config-client在启动的时候访问config-server没有成功,然后直接报错了,这就有点不合适了,遇到这种启动快的时候,我们希望是有一个重试的机制,能重试几次, 这里添加重试机制也是很简单 首先引入如下两个依赖:123456789&lt;!-- 重试机制 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.retry&lt;/groupId&gt; &lt;artifactId&gt;spring-retry&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;&lt;/dependency&gt; 引入依赖后就ok了,直接启动看下效果 可以看到,这里尝试访问了6次之后才报错的 相关配置和重试机制相关的配置有以下几个:123456789101112spring: cloud: config: retry: # 配置重试次数,默认是6 max-attempts: 6 # 间隔乘数,默认1.1 multiplier: 1.1 # 初始重试间隔时间 默认1000 initial-interval: 1000 # 最大间隔时间,默认2000 max-interval: 2000 动态刷新配置当我们更新了Git仓库中的配置的时候,如何让config-client能及时感知到呢, 首先需要引入actuator的依赖:1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 这个依赖中包含了/refresh端点的实现,我们将利用这个端点来刷新配置信息, 然后在application.yml中忽略掉权限拦截:123management: security: enabled: false 启动注册中心,再启动config-server和config-client,访问http://localhost:2008/test3,结果如下: 然后我们修改一个在git仓库中的配置,修改提交之后,用postman来请求一下http://localhost:2008/refresh,结果如下: 最后,再次访问http://localhost:2008/test3,结果如下: 这里的配置已经是获取到新的了]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-19-分布式配置中心服务端配置细节]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-Cloud-19-%E5%88%86%E5%B8%83%E5%BC%8F%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E6%9C%8D%E5%8A%A1%E7%AB%AF%E9%85%8D%E7%BD%AE%E7%BB%86%E8%8A%82%2F</url>
    <content type="text"><![CDATA[述上文中我们了解了分布式配置中心Spring Cloud Config的一个基本的使用,那么本文就来详细的了解一下Spring Cloud Config,它里面的一些配置细节以及工作流程等 工作流程 上图就是Config Server的一个大致的工作流程. 首先,需要一个远程的Git仓库,我们在写demo的时候可以直接使用github,在实际生产环境中是需要有一个Git服务器的,Git仓库主要就是用来保存我们的配置文件的. 除了远程的Git仓库之外,还需要一个本地的Git仓库,每次Config Server访问远程Git仓库时,都会在本地保存一份,当远程仓库挂掉或者无法访问的情况下,就是用本地仓库中的配置信息. 图中的service A B就是我们的微服务具体应用,这些应用启动的时候就会从Config Server中加载相应的配置信息 当微服务A/B尝试去从Config Server中加载配置信息的时候,Config Server会先通过git clone命令克隆一份配置文件保存到本地 配置文件是存储在Git仓库中,所以配置文件天然的具备版本管理功能,Git中的Hook功能可以实时监控配置文件的修改. Git URI中的占位符假设我们有两个微服务应用A和B,两个应用的配置文件分别放在git仓库中的两个文件夹中,比如https://github.com/zhouze-java/spring-cloud-demo-config/a和https://github.com/zhouze-java/spring-cloud-demo-config/b,两个文件夹中,但是我们的Config Server是只有一个的,那么当 A和B两个应用连接上Config Server的时候Config Server怎么知道去哪里取配置文件,那这个时候就可以使用占位符来处理这种情况了. 上文中,我们已经使用过{application},{profile},{label}这三个占位符了,这些占位符除了用来标识配置文件的规则,还可以用在Config Server中对Git仓库的URI配置,用在URI配置中时,这三个占位符分别代表的含义如下: {application}映射到客户端的 spring.application.name {profile}映射到客户端上的 spring.profiles.active {label}这是一个服务器端功能，标记”版本”的配置文件集 举个例子. 假设我不同环境的配置文件分别放在以下目录中:https://github.com/zhouze-java/spring-cloud-demo-config/application/devhttps://github.com/zhouze-java/spring-cloud-demo-config/application/testhttps://github.com/zhouze-java/spring-cloud-demo-config/application/prod 然后客户端的文件是这样配置的:12345678spring: application: name: application cloud: config: profile: dev label: master uri: http://localhost:2007/ 然后,在config server中按以下方式配置就可以了:1234567891011spring: application: name: config-server cloud: config: server: git: # uri表示配置中心所在仓库的位置 uri: https://github.com/zhouze-java/spring-cloud-demo-config.git # search-paths表示仓库下的子目录 search-paths: &#123;application&#125;/&#123;profile&#125; 这种存储方式不一定好用,这里只展示占位符的用法. 在默认情况下,Config Server从远程仓库 clone下来的文件保存在C:\Users\&lt;当前用户&gt;\AppData\Local\Temp目录下,我们可以通过如下配置来修改:123456spring: cloud: config: server: git: basedir: D:\\xxx\\ 健康监测机制默认情况下Spring Cloud Config会为配置中心服务端创建一个健康监测器,该检测器默认情况下是访问的仓库文件是{application}为app的配置文件,如果仓库中不存在这个文件,健康显示器就会显示仓库无法连接. 此时有以下两种解决方案: 向仓库中添加相应的配置文件. 重新指定检测的配置 重新指定检测的配置文件,需要在服务端做出如下配置.123456789101112spring: application: name: config-server cloud: config: server: health: repositories: check: name: application label: master profiles: dev 此时重启应用,访问http://localhost:2007/application/dev/master,如果能访问到,那就说明已经连接上了,而且访问http://localhost:2007/health,会返回`{&quot;status&quot;:&quot;UP&quot;}` 安全性在开发中,我们的配置中心肯定是不能被随便访问的, 我们可以给开发中心加一个密码,我们的项目是spring boot为基础的,所以可以直接整合Spring Security,具体步骤如下: 首先需要引入security的依赖:12345&lt;!-- serurity --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt; 然后再配置文件中配置用户名和密码:1234security: user: name: root password: 123456 最后 在配置中心的客户端也配置好用户名密码就好了,还是在bootstrap.yml中配置,配置如下:12345spring: cloud: config: username: root password: 123456 配置完成之后就ok了. 加解密在微服务架构中,由于独立的服务多,加上前期测试工作量大,一些原本由运维人员维护的敏感信息会被我们直接写在微服务中,以提高开发效率,但是这种明文存储方式显然是非常危险的,所以我们要对这些信息进行加密,而Spring Cloud Config则提供了对称加解密/非对称加解密的功能来帮助我们完成这一需求 接下来就看一下具体如何实现 JCE下载默认情况下JRE中自带了JCE,但默认是一个有限长度的版本,我们这里需要一个不限长度的版本,可以从Oracle官网中下载,内容如下然后把这两个jar放到%JAVA_HOME%\jre\lib\security这个目录下,覆盖原来的文件即可. 版本修改Spring Cloud的Dalston.SR3和Dalston.SR2版本在这个问题上是有BUG的,如果用这两个版本在这里测试会没有效果,应该避开使用这两个版本,我这里使用的是Dalston.SR4版本 对称加解密这个比较简单,直接配置密钥就可以了,在我们的config-server工程中配置,这个密钥的配置是需要放到bootstrap.yml里面的12encrypt: key: testKey 配置完成之后,启动项目,然后访问http://localhost:2007/encrypt/status,如果看到以下结果,就表示配置成功了 然后用postman去测试一下,需要先把config-server中security的验证先去掉,然后用postman访问/encrypt和/decrypt接口去加解密,比如要给”dev”这个字符串加密,方式如下: 解密方式如下: 当我们拿到加密的字符串后,就可以在配置文件中使用了,比如之前的配置文件application-dev.yml,改成以下内容:1sang: &apos;&#123;cipher&#125;9b7d72b9506c758b86d7e94f39606f3cd4bea122e11a6a45428506833ce260fe&apos; 配置文件中,值如果是以{cipher}开头,表示这个值是一个加密的字符串,配置中心获取到这个值之后,会先进行解密,然后再返回给客户端使用. 修改完配置文件之后,启动config-server和config-client,然后访问我们之前的接口http://localhost:2008/test3, 返回如下:可以看到,我们在客户端获取到的配置是解密后的字符串 非对称加密上面我们用的是对称加密的方式来保证配置文件的安全性,如果使用非对称的加解密的方式,那安全性就更高了,下面看一下具体怎么使用. 首先需要生成密钥对,用jdk中自带的keytools工具,方式如下: 先打开cmd,然后运行以下命令1keytool -genkeypair -alias config-server -keyalg RSA -keystore config-server.keystore 然后输入口令,其他地方回车,最后确定,如下: 执行成功之后,会在执行目录下面生成一个config-server.keystore文件,然后复制到config-server的src\main\resources目录下,然后还是在bootstrap.yml中做如下配置:123456encrypt: key-store: location: config-server.keystore alias: config-server password: 123456 secret: 123456 ok,到这里非对称加密就配置好了,然后去测试一下,和非对称的加密的测试方式是一样的 加密:解密: 最后修改配置文件: 然后可以提交配置文件,启动项目去测试一下]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-18-分布式配置中心Spring Cloud Config]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-Cloud-18-%E5%88%86%E5%B8%83%E5%BC%8F%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83Spring-Cloud-Config%2F</url>
    <content type="text"><![CDATA[述随着分布式项目越来越大,就需要将项目的配置文件抽出来单独管理,就需要用到Spring Cloud Config,Spring Cloud Config为分布式系统中的外部配置提供服务器和客户端支持 我们可以使用Config Server在所有环境中管理应用程序的外部属性,Config Server本质上也是一个微服务应用,用来连接配置仓库,将获取到的配置信息提供给客户端使用(客户端就是我们的每个微服务应用),我们在客户端上指定配置中心的位置,客户端在启动的时候就会自动去从配置中心获取和加载配置信息 Spring Cloud Config可以与任意语言运行的应用程序一起使用,服务器存储后端的默认实现使用git,因此它轻松支持配置信息的版本管理,当然我们也可以使用Git客户端工具来管理配置信息.本文我们就先来看下Spring Cloud Config的一个基本使用. 构建配置中心新建工程创建一个新的工程作为配置中心,名称是config-server 添加依赖项目添加到主项目管理下面,然后添加如下依赖:12345&lt;!-- 配置中心 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt;&lt;/dependency&gt; 入口类修改入口类添加注解@EnableConfigServer,表示开启配置中心服务端功能:123456789@SpringBootApplication@EnableConfigServerpublic class App &#123; public static void main(String[] args) &#123; SpringApplication.run(App.class, args); &#125;&#125; 新建git仓库在Git服务端,或者github上面新建一个仓库,我这里名称是spring-cloud-demo-config 配置文件修改修改项目的配置文件application.yml,如下:12345678910111213141516spring: application: name: config-server cloud: config: server: git: # uri表示配置中心所在仓库的位置 uri: https://github.com/zhouze-java/spring-cloud-demo-config.git # search-paths表示仓库下的子目录 search-paths: config-repo # github的用户名和密码 username: xxxxx password: xxxxxserver: port: 2007 各个配置的意思在注释中都有说明 构建配置仓库接下来,需要在github上设置好配置中心,先在本地找一个空的文件夹,在这个文件夹中创建一个config-repo的文件夹,然后创建四个配置文件: 四个配置文件的内容分别如下:application.yml:1sang: default config application-dev.yml:1sang: dev config application-prod.yml:1sang: prod config application-test.yml:1sang: test config 创建完成之后,上传到git仓库.具体命令这里就不贴了. 访问上传到git上面之后,启动配置中心就可以访问了, 路径是/{application}/{profile}/{label} application: 配置文件名称,我们刚刚创建的文件名就是application profile: 环境 label: 分支,我们默认的都是放在master上面的 然后启动项目,访问试一下,地址是http://localhost:2007/application/prod/master,结果如下: 返回值:1234567891011121314151617181920212223&#123; &quot;name&quot;:&quot;application&quot;, &quot;profiles&quot;:[ &quot;prod&quot; ], &quot;label&quot;:&quot;master&quot;, &quot;version&quot;:null, &quot;state&quot;:null, &quot;propertySources&quot;:[ &#123; &quot;name&quot;:&quot;https://github.com/zhouze-java/spring-cloud-demo-config.git/config-repo/application-prod.yml&quot;, &quot;source&quot;:&#123; &quot;sang&quot;:&quot;prod config&quot; &#125; &#125;, &#123; &quot;name&quot;:&quot;https://github.com/zhouze-java/spring-cloud-demo-config.git/config-repo/application.yml&quot;, &quot;source&quot;:&#123; &quot;sang&quot;:&quot;default config&quot; &#125; &#125; ]&#125; 当成功访问的时候,在我们项目的控制台会输出以下信息:122019-04-01 15:27:46.075 INFO 16292 --- [nio-2007-exec-1] o.s.c.c.s.e.NativeEnvironmentRepository : Adding property source: file:/C:/Users/ADMINI~1/AppData/Local/Temp/config-repo-2221561102471049379/config-repo/application-prod.yml2019-04-01 15:27:46.075 INFO 16292 --- [nio-2007-exec-1] o.s.c.c.s.e.NativeEnvironmentRepository : Adding property source: file:/C:/Users/ADMINI~1/AppData/Local/Temp/config-repo-2221561102471049379/config-repo/application.yml 他会在我们本地保存一份,这样可以确保说在Git仓库挂了的时候,不影响项目的正常运行,此时如果断网,再次访问http://localhost:2007/application/prod/master,还是可以拿到一样的数据,就是从本地获取的. 客户端配置服务端搭建好之后,再来看一下客户端是如何使用的 新建工程新建一个工程,名称是config-client. 引入依赖项目加到主项目下面,然后引入如下依赖:1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt;&lt;/dependency&gt; 配置修改新建配置文件bootstrap.yml,以下配置必须是放到bootstrap.yml中才能生效,内容如下:12345678910spring: application: name: application cloud: config: profile: dev label: master uri: http://localhost:2007/server: port: 2008 这里的name对应了配置文件中的application部分,profile对应了profile部分,label对应了label部分,uri则表示配置中心的地址. 测试配置完成之后创建一个测试的Controller12345678910111213141516171819202122232425262728@RestController@RefreshScopepublic class TestController &#123; @Value(&quot;$&#123;sang&#125;&quot;) private String sang; @Autowired private Environment env; @Autowired private Constants constants; @RequestMapping(&quot;test1&quot;) public String test1()&#123; return sang; &#125; @RequestMapping(&quot;test2&quot;) public String test2()&#123; return env.getProperty(&quot;sang&quot;, &quot;未定义&quot;); &#125; @RequestMapping(&quot;test3&quot;) public String test3()&#123; return constants.getSang(); &#125;&#125; 这里一共有三种方式去获取配置文件中的值,第一种直接注入,第二种就是用Environment,第三种是用@ConfigurationProperties来获取的, Constants类的内容如下:12345678@ConfigurationProperties@Configuration@Datapublic class Constants &#123; private String sang;&#125; 然后再启动类再加上@EnableConfigurationProperties注解就好了. 最后,访问http://localhost:2008/test3,结果如下: 这里还有一个要注意的,就是@RefreshScope这个注解, 我们的配置文件放在git中,如果修改掉以后,客户端是感知不到的,加上这个注解,另外引入actuator的依赖,就可以获取到配置文件中最新的值,不加的话获取到的可能还是原来的值.]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-17-Zuul异常处理源码分析]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-Cloud-17-Zuul%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[述本文来看一下Zuul的异常处理流程,以及如何自定义异常信息 Zuul生命周期看一下官方给出的Zuul请求的生命周期图: 从图上可以看出,正常情况下,请求都是按照pre→route→post的顺序来执行,最后由post返回response. 在看一下pre下面的custom filter,就是说在pre阶段,如果有用户自定义的过滤器则执行自定义的过滤器. pre routing post的任意一个阶段如果抛异常了,则执行error过滤器,然后再执行post给出响应. 源码分析来看下源码com.netflix.zuul.http.ZuulServlet这个类中的service方法,是整个调用过程的核心: 可以看到,内层的3个catch只捕获ZuulException这个异常,其他的异常是交给外层的catch去捕获. pre和route执行出错之后都会先执行error再执行post,而post执行出错之后就只执行error而不会再执行post. ZuulFilter最终会在com.netflix.zuul.FilterProcessor的processZuulFilter方法中被调用,在该方法中会判断runFilter是否执行成功,如果执行失败,则将异常信息提取出来,然后抛出异常,抛出的异常如果是ZuulException的实例,则抛出一个ZuulException类型的异常,如果不是ZuulException的实例,则抛出一个状态码为500的ZuulException类型的异常,所以无论如何,我们最终看到的都是ZuulException类型的异常,下面我贴出processZuulFilter方法的代码,如下: 再看下runfilter这个方法: 在Zuul中,所有的错误问题的最终都是被SendErrorFilter类来处理,该类在早期的版本是一个post类型的filter,post类型的filter有一个缺陷就是不能处理post中抛出的异常,需要我们手动去完善,而我目前使用的这个版本(Dalston.SR3)已经修复了这个问题,SendErrorFilter现在是一个error类型的filter,而且只要RequestContext中有异常就会进入到SendErrorFilter中,错误信息也都从exception对象中提取出来,核心代码如下: 自定义异常信息创建一个类,继承DefaultErrorAttributes类,然后重写getErrorAttributes 方法,具体代码如下:1234567891011public class MyErrorAttribute extends DefaultErrorAttributes &#123; @Override public Map&lt;String, Object&gt; getErrorAttributes(RequestAttributes requestAttributes, boolean includeStackTrace) &#123; Map&lt;String, Object&gt; map = super.getErrorAttributes(requestAttributes, includeStackTrace); map.put(&quot;status&quot;, 222); map.put(&quot;error&quot;, &quot;error&quot;); map.put(&quot;exception&quot;, &quot;exception&quot;); map.put(&quot;message&quot;, &quot;message&quot;); return map; &#125;&#125;]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-16-Zuul网关配置详解]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-Cloud-16-Zuul%E7%BD%91%E5%85%B3%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[述上文中初步使用了Zuul的路由和请求过滤,接下来就详细的看一下Zuul的路由配置 路由配置上文中,我们配置的一个路由规则是这样的:12345zuul: routes: api-a: path: /api-a/** serviceId: feign-consumer 要指定一个path和一个serviceId,这里可以简化一下,如下:123zuul: routes: feign-consumer: /api-a/** zuul.routes后面是服务名,值是路径,上面这两种配置的方式都是可以的,只是下面这样写更简洁 默认配置如果我们不去配置映射规则的话,Zuul就会设置成默认的规则,默认配置是这样的:12zuul.routes.feign-consumer.path=/feign-consumer/**zuul.routes.feign-consumer.serviceId=feign-consumer 写成yml格式的话就是:123zuul: routes: feign-consumer: /feign-consumer/** 默认的路径就是/服务名/** 通配符详解我们上面设置路径,最后都是两个*作为通配符,Zuul中的路由匹配规则使用了Ant风格定义,一共有三种不同的通配符: ?: 可以匹配单个字符,比如路径是/api/?,就可以匹配到/api/a,/api/b等 *: 匹配任意数量的字符,比如路径是/api/*,就能匹配到/api/aa,/api/bb等,但是只能匹配一级,像/api/aa/bb这样的路径就匹配不到了 **: 匹配任意数量的字符,比如路径是/api/**,就能匹配到/api/aa,/api/aa/b等 指定的服务不创建路由规则默认情况下,Zuul会为Eureka上所有注册的服务都创建映射关系, 这里就会出现一个问题, 在我们现在这个demo中,eureka-client作为服务提供者,他只是给项目中的消费者提供服务,但是默认的Zuul也会给他创建映射规则,这时候我们可以采用以下配置来跳过该服务,不创建路由规则:12zuul: ignored-services: eureka-client 指定的接口不创建路由规则上面的配置可以忽略掉某一个服务,不给这个服务创建映射规则,这个配置也可以进一步细化,比如说不想给/hello接口创建路由规则,那么就可以按下面这样去配置:12zuul: ignored-patterns: /**/hello/** 配置完成以后,可以去启动项目测试一下,访问hello接口试一下:报404,说明配置生效了 接口地址重合比如我们有两个服务一个叫feign-consumer和feign-consumer-hello,配置都路由规则假如在application.properties是这样的12345zuul.routes.feign-consumer.path=/feign-consumer/**zuul.routes.feign-consumer.serviceId=feign-consumerzuul.routes.feign-consumer-hello.path=/feign-consumer/hello/**zuul.routes.feign-consumer-hello.serviceId=feign-consumer-hello 这时,访问feign-consumer-hello的路径就会被这两条规则都匹配了,Zuul中的路径匹配方式是一种线性匹配方式,即按照路由匹配规则的存储顺序依次匹配,所以我们得确保feign-consumer-hello的匹配规则先于feign-consumer 被定义,但是在properties文件中是不能保证先后顺序的,所以就必须使用yml的配置方式,在yml中的配置如下:12345678zuul: routes: feign-consumer-hello: path: /feign-consumer/hello/** serviceId: feign-consumer-hello feign-consumer: path: /feign-consumer/** serviceId: feign-consumer 这样就可以确保先加载的是feign-consumer-hello的匹配规则. 路由前缀为所有路由规则增加前缀,配置如下:12zuul: prefix: /myapi 比如配置之前访问路径是http://localhost:2006/api-a/hello1?login=1,配置了前缀之后,访问路径就变成了http://localhost:2006/myapi/api-a/hello1?login=1. 重启项目测试一下,访问原路径就会404 再访问加上前缀的路径: 网关项目中增加业务逻辑正常来说,API网关只是作为系统的统一入口,但是有些时候我们要在网关上做一点业务逻辑,比如在api-gateway项目中新建如下Controller:12345678@RestControllerpublic class TestController &#123; @GetMapping(&quot;local&quot;) public String local()&#123; return &quot;Zuul Local&quot;; &#125;&#125; 这时,我们希望用户访问/local接口的时候,就跳到上面这个controller中,那么就需要配置Zuul的本地跳转,如下:12345zuul: routes: local: path: /local/** url: forward:/local 配置好之后,访问http://localhost:2006/myapi/local,结果如下:这里还有一个要注意的, 我们上篇文章中写过一个过滤器,但是走这个接口的时候是没有经过过滤器的 设置请求Host头信息我们在使用nginx的时候,会涉及到一个请求头信息的配置,防止页面重定向后跳转到上游服务器上去,zuul中也存在相同的问题,比如说,我的feign-consumer中提供了一个/hello4的接口,当访问/hello4的时候,页面重定向到/hello,在默认情况下,重定向的地址是具体的服务实例的地址,而不是API网关的跳转地址,这样就会暴露真实的服务地址,所以需要在zuul中做如下配置,表示API网关在进行请求路由转发之前为请求设置Host头信息.12zuul: add-host-header: true 默认情况下,敏感的头信息无法经过API网关进行传递,我们可以通过如下配置使之可以传递:1234zuul: routes: feign-consumer: sensitiveHeaders: 关闭重试机制在Zuul中,Ribbon和Hystrix的配置还是和之前一样的, 如果想关闭Hystrix的重试机制可以通过下面的配置: 关闭全局重试机制:12zuul: retryable: false 关闭某一个服务的重试机制:1234zuul: routes: feign-consumer: retryable: false]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-15-Zuul网关使用及请求过滤]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-Cloud-15-Zuul%E7%BD%91%E5%85%B3%E4%BD%BF%E7%94%A8%E5%8F%8A%E8%AF%B7%E6%B1%82%E8%BF%87%E6%BB%A4%2F</url>
    <content type="text"><![CDATA[述微服务,就是把一个大的项目拆分成多个独立的模块,然后通过服务治理让这些独立的模块配合工作,随着业务的扩展,整个系统越来越大,随之带来的也会有很多问题 比如说,我的项目中有很多个独立服务都要对外提供服务,那么对于开发和运维人员来说,这些接口应该怎么管理?在微服务中,一个独立的系统被拆分成很多个模块,为了确保安全,我们是不是需要在每一个服务中都加上相同的鉴权代码来确保系统不被非法访问,这样的话就太繁琐了,而且不便于维护为了解决这些问题,就需要用到API网关, 就是所有的外部访问都需要先经过网关,由网关去实现请求路由,负载均衡,权限验证等, Spring Cloud中 Spring Cloud Zuul实现了API网关功能. 下面就来看一下具体如何使用 环境搭建新建项目新建一个子项目,名称是api-gateway 引入依赖引入网关和eureka的依赖即可, spring-cloud-starter-zuul这个包里面就包含了Ribbon,Hystrix,actuator等 123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zuul&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt;&lt;/dependency&gt; 开启Zuul的功能在入口类中添加@EnableZuulProxy注解,开启Zuul的API网关服务功能.123456789@SpringBootApplication@EnableZuulProxypublic class App &#123; public static void main(String[] args) &#123; SpringApplication.run(App.class, args); &#125;&#125; 配置路由规则在application.yml中配置路由规则和项目的基本信息等,具体如下:1234567891011121314151617spring: application: name: api-gatewayserver: port: 2006# 路由规则设置zuul: routes: api-a: path: /api-a/** serviceId: feign-consumer# 服务注册中心设置eureka: client: serviceUrl: defaultZone: http://peer1:1111/eureka/,http://peer2:1112/eureka/ 来看一下这个里的路由规则配置,path配置的值是/api-a/**,意思就是符合/api-a/**的请求,都会被转发到下面设置的serviceId上去,这里也就是会转发到feign-consumer上去,至于feign-consumer的服务地址交给服务注册中心去分析, 我们不需要关注. 以我们上面的配置为例,假如我现在请求http://localhost:2006/api-a/hello1,就相当于是请求http://localhost:2101/hello1,我这里的feign-consumer地址是http://localhost:2101/ 在路由规则中配置的api-a,是路由的名字是自定义的,但是一组path和serviceId映射关系的路由名要相同. 测试上面配置什么的都ok以后,依次启动服务注册中心,提供者,消费者和网关这些项目, 然后去访问http://localhost:2006/api-a/hello1,结果如下: 请求确实被转发到消费者那边了, 至此一个网关的简单搭建就ok了 请求过滤下面再来看一下,如何通过网关实现一个简单的权限验证,这里就要用到Zuul的请求过滤的功能,类似于一个拦截器,先把请求拦截下来,然后做出相应处理,决定是否放行,下面就来看下具体如何使用 定义过滤器首先,需要自定义一个过滤器,然后继承ZuulFilter,代码如下:12345678910111213141516171819202122232425262728293031public class PermisFilter extends ZuulFilter &#123; @Override public String filterType() &#123; return &quot;pre&quot;; &#125; @Override public int filterOrder() &#123; return 0; &#125; @Override public boolean shouldFilter() &#123; return true; &#125; @Override public Object run() &#123; RequestContext ctx = RequestContext.getCurrentContext(); HttpServletRequest request = ctx.getRequest(); String login = request.getParameter(&quot;login&quot;); if (login == null) &#123; ctx.setSendZuulResponse(false); ctx.setResponseStatusCode(HttpStatus.UNAUTHORIZED.value()); ctx.addZuulResponseHeader((&quot;content-type&quot;), &quot;text/html;charset=utf-8&quot;); ctx.setResponseBody(&quot;非法访问&quot;); &#125; return null; &#125;&#125; 各个方法详细说明 filterType(): 返回值为过滤器的类型,过滤器的类型决定了过滤器在哪个生命周期执行,上面代码中返回但是pre,表示是在路由之前执行过滤,其他可选值还有post,error,route和static, 也可以自定义. filterOrder(): 返回过滤器的执行顺序,当过滤器有很多的时候,这个方法就有用了 shouldFilter(): 这个方法用来判断这个过滤器是否执行,true就是执行,在实际的使用中我们可以根据当前请求地址来决定要不要执行这个过滤器,这里为了测试,直接返回true了. run():过滤器的具体逻辑,在上面的例子中,意思就是只要请求携带了login参数,就放行,否则就拦截下来,要拦截的话,首先需要设置ctx.setSendZuulResponse(false);,表示这个请求就不进行路由了,然后再设置http状态码和具体返回的body, run方法的返回值在目前的版本(Dalston.SR3)中没有任何意义,随便返回都行. 在实际开发过程中,在run方法中可能是先获取用户对象,然后做权限判断等,根据具体的业务具体实现 配置过滤器在配置类中通过@Bean注入过滤器,代码如下:123456789@Configurationpublic class Config &#123; @Bean public PermisFilter permisFilter()&#123; return new PermisFilter(); &#125; &#125; 测试上面的内容都ok以后,就可以去启动项目进行测试了, 依次启动注册中心,服务提供者,消费者,还有网关. 启动好了之后,访问http://localhost:2006/api-a/hello1,结果如下: 然后再加上login参数,再次访问 总结API网关作为系统的统一入口,将微服务中的内部细节都屏蔽掉了,而且能够自动的维护服务实例,实现负载均衡的路由转发,同时,它提供的过滤器为所有的微服务提供统一的权限校验机制,使得服务自身只需要关注业务逻辑即可.]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-14-Feign配置详解]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-Cloud-14-Feign%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[述之前有说过,Fegin是整合了Ribbon和Hystrix的,下面就看一下在Feign中是如何配置Ribbon和Hystrix的 Ribbon配置在application.yml中的配置如下:1234567891011ribbon: # 连接超时时间 ConnectTimeout: 600 # 读取超时时间 ReadTimeout: 6000 # 对所有操作请求都进行重试 OkToRetryOnAllOperations: true # 切换实例的重试次数 MaxAutoRetriesNextServer: 2 # 对当前实例的重试次数 MaxAutoRetries: 1 这样配置的话,如果访问超时了,就会对当前的实例再次请求访问一次,如果还是失败,就换实例,切换实例的次数一共可以是两次,如果两次之后还是没有拿到访问结果,就会报超时的错误 测试这里测试可以把服务提供者的接口中加一个线程休眠,读取超时我们上面配置的是6秒,然后在服务提供者中线程休眠7s,就可以看到消费者端报Read timed out异常 不同的服务不同的配置在上面配置中是全局配置的,就是对所有的请求都生效,如果我们想针对不同的服务做出不同的配置,那么就可以在配置属性前面,加上服务的名字,比如服务名称是eureka-client,就可以做出如下配置:123456789101112eureka-client: ribbon: # 连接超时时间 ConnectTimeout: 600 # 读取超时时间 ReadTimeout: 6000 # 对所有操作请求都进行重试 OkToRetryOnAllOperations: true # 切换实例的重试次数 MaxAutoRetriesNextServer: 2 # 对当前实例的重试次数 MaxAutoRetries: 1 这样的话,这些配置就只针对eureka-client的所有服务了 Hystrix配置跟Ribbon的配置类似, 基础配置如下:123456789101112131415hystrix: command: default: execution: isolation: thread: # 设置熔断超时时间 timeoutInMilliseconds: 10000 # 开启熔断功能 timeout: enabled: truefeign: hystrix: # 关闭hystrix功能(不和上面的配置一起用) enabled: false 这样的配置也是全局的配置,如果要针对某一个接口配置,比如/hello接口,可以把上面配置中的default替换成接口名称,如下:1234567891011hystrix: command: hello: execution: isolation: thread: # 设置熔断超时时间 timeoutInMilliseconds: 10000 # 开启熔断功能 timeout: enabled: true 如果接口名称重复的话,同名的接口会共用这条配置 服务降级配置下面再来看一下用Feign如何实现服务降级 首先需要在配置中启用Hystrix,如下:123feign: hystrix: enabled: true 然后新建一个类,HelloServiceFallback,实现HelloService ,具体如下:12345678910111213141516171819202122@Componentpublic class HelloServiceFallback implements HelloService &#123; @Override public String hello() &#123; return &quot;hello error&quot;; &#125; @Override public String hello(String name) &#123; return &quot;error:&quot; + name; &#125; @Override public User hello(String name, String job, Long id) &#123; return new User(1L, &quot;error&quot;, &quot;error&quot;); &#125; @Override public String hello(User user) &#123; return &quot;error user&quot;; &#125;&#125; 这里方法的实现都是相应方法的服务降级的逻辑 然后在HelloService中通过@FeignClient注解来指定服务降级的处理类,如下:123456789101112131415@FeignClient(value = &quot;eureka-client&quot;, fallback = HelloServiceFallback.class)public interface HelloService &#123; @GetMapping(&quot;/hello&quot;) String hello(); @GetMapping(&quot;/hello1&quot;) String hello(@RequestParam(&quot;name&quot;) String name); @GetMapping(&quot;/hello2&quot;) User hello(@RequestHeader(&quot;name&quot;) String name, @RequestHeader(&quot;job&quot;) String job, @RequestHeader(&quot;id&quot;) Long id); @PostMapping(&quot;/hello3&quot;) String hello(@RequestBody User user);&#125; 上面的配置都ok以后,启动服务注册中心,再启动消费者, 不启动服务提供者,然后去访问接口,结果如下: 其他配置Spring Cloud Feign支持对请求和响应进行GZIP压缩,以提高通信效率,配置方式如下: 123456789101112feign: compression: request: # 配置请求GZIP压缩 enabled: true # 配置压缩支持的MIME TYPE mime-types: text/xml,application/xml,application/json # 配置压缩数据大小的下限 min-request-size: 2048 response: # 配置响应GZIP压缩 enabled: true Feign为每个Feign都提供了Feign.Logger实例,可以在配置中开启日志,具体步骤如下: application.yml中配置日志输出,设置日志输出级别:1234# 格式为logging.level.Feign客户端路径logging: level: com.euerka.consumer.service.HelloService: debug 然后在配置类中添加以下内容:123456789@Configurationpublic class CloudConfig &#123; @Bean Logger.Level feignLoggerLevel()&#123; return Logger.Level.FULL; &#125; &#125; 配置完成之后,控制台就会输出请求的详细日志.]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-13-Feign的继承特性]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-Cloud-13-Feign%E7%9A%84%E7%BB%A7%E6%89%BF%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[述上文中,初步的使用了Feign,在写HelloService接口的时候,可以发现,代码可以直接从服务提供者中复制过来. 这些可以复制的代码Spring Cloud Feign对它进行了进一步的抽象,这里就用到了Feign的继承特性,那么下面就来看下如何使用Feign的继承特性,进一步简化代码 环境搭建新建工程新建一个子工程,名字是hello-service-api,跟之前一样,添加到父工程下面. 创建公共接口新建一个HelloService接口,内容如下:12345678910111213@RequestMapping(&quot;helloService&quot;)public interface HelloSerivce &#123; @GetMapping(&quot;/hello1&quot;) String hello(@RequestParam(&quot;name&quot;) String name); @GetMapping(&quot;/hello2&quot;) User hello(@RequestHeader(&quot;name&quot;) String name, @RequestHeader(&quot;job&quot;) String job, @RequestHeader(&quot;id&quot;) Long id) throws UnsupportedEncodingException; @PostMapping(&quot;/hello3&quot;) String hello(@RequestBody User user);&#125; 这个就是我们上篇文章中的HelloService的内容,只不过加了一个请求前缀. 服务提供者修改首先需要在服务提供者工程中,引入hello-service-api的依赖,如下:123456&lt;!-- hello service --&gt;&lt;dependency&gt; &lt;groupId&gt;com.hello.service&lt;/groupId&gt; &lt;artifactId&gt;hello-service-api&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; 然后新建一个Controller,实现刚才创建的helloService接口,具体如下:12345678910111213141516171819@RestControllerpublic class HelloController implements HelloSerivce &#123; @Override public String hello(@RequestParam String name) &#123; return &quot;name:&quot; + name; &#125; @Override public User hello(@RequestHeader String name, @RequestHeader String job, @RequestHeader Long id) throws UnsupportedEncodingException &#123; return new User(id, URLDecoder.decode(name,&quot;UTF-8&quot;), URLDecoder.decode(job,&quot;UTF-8&quot;)); &#125; @Override public String hello(@RequestBody User user) &#123; return user.getName(); &#125;&#125; 实现HelloSerivce中的这几个方法, 然后方法的具体实现还是和之前的是一样的,不同的地方是,这里不需要再方法上加@RequestMapping注解,这些注解在HelloSerivce接口中都有了 方法中的参数@RequestHeader和@RequestBody注解还是要添加,@RequestParam注解可以不添加 服务消费者修改首先需要把hello-service-api的依赖引进来:12345&lt;dependency&gt; &lt;groupId&gt;com.hello.service&lt;/groupId&gt; &lt;artifactId&gt;hello-service-api&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; 消费者中新建一个HelloService2,然后继承上面创建的HelloSerivce,代码如下:123@FeignClient(&quot;eureka-client&quot;) public interface HelloService2 extends HelloSerivce &#123;&#125; 这个接口是不需要添加任何方法,方法都在父接口中,这里只需要在类上面添加@FeignClient注解,来绑定服务即可. 最后在Controller中提供调用的接口:1234567891011121314151617181920212223@RestController@RequestMapping(&quot;/hello&quot;)public class HelloController &#123; @Autowired private HelloService2 helloService; @GetMapping(&quot;/hello1&quot;) public String hello1()&#123; return helloService.hello(&quot;张三&quot;); &#125; @GetMapping(&quot;/hello2&quot;) public User hello2() throws UnsupportedEncodingException &#123; return helloService.hello(URLEncoder.encode(&quot;张三&quot;, &quot;UTF-8&quot;), URLEncoder.encode(&quot;测试&quot;, &quot;UTF-8&quot;), 1L); &#125; @GetMapping(&quot;/hello3&quot;) public String hello3()&#123; User user = new User(1L, &quot;张三&quot;, &quot;开发&quot;); return helloService.hello(user); &#125;&#125; 测试依次启动,服务注册中心,服务提供者,消费者,然后请求上面这个controller中的几个接口. 看一下返回值: 总结总的实现过程呢就是先创建一个提供者和消费者公用的接口, 然后提供者的Controller实现公共接口, 然后消费者创建接口继承这个公共接口. 这种方式用起来方便,但是耦合度太高,此时如果服务提供者修改了一个接口的定义,服务消费者可能也得跟着变化,进而带来很多未知的工作量,因此小伙伴们在使用继承特性的时候,要慎重考虑.]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-12-Feign入门]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-Cloud-12-Feign%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[述前面,我们使用了Ribbon,RestTemplate,Hystrix这些组件,都是spring cloud的一些非常基础的组件,在使用过程中,这些东西都是同时出现的,而且配置也差不多,每次开发都有很多相同的代码,因此Spring Cloud基于Netflix Feign整合了Ribbon和Hystrix,让开发更简化,同时,还提供了一种声明式的Web服务客户端定义方式. 之前我们,为了简化RestTemplate操作,我们把请求都封装在了service中,但同时也发现,service中的方法都是模板式的,写起来很枯燥,Spring Cloud Feign对此进行了进一步的封装,简化了我们的封装操作,接下来就来看一下具体是怎么使用的. Spring Cloud Feign入门环境搭建新建工程还是在原来项目的基础上再创建一个子项目,名称是consumer-feign. 添加依赖把项目加到主项目下,然后引入以下几个依赖:1234567891011121314dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-feign&lt;/artifactId&gt;&lt;/dependency&gt; 启动类添加注解启动类添加@EnableFeignClients注解,表示开启Spring Cloud Feign的功能, 如下:123456789101112@EnableFeignClients@EnableDiscoveryClient@SpringBootApplicationpublic class App &#123; public static void main(String[] args) &#123; new SpringApplicationBuilder(App.class) .web(true) .run(args); &#125;&#125; 配置修改修改application.yml,配置服务注册中心,端口号和应用名称. 内容如下:123456789spring: application: name: feign-consumerserver: port: 2101eureka: client: serviceUrl: defaultZone: http://peer1:1111/eureka/,http://peer2:1112/eureka/ 声明服务写一个HelloService接口,通过@FeignClient注解来指定服务名绑定服务,然后通过Spring MVC提供的注解来绑定服务提供者提供的接口. 来看一下HelloService的代码:1234567@FeignClient(&quot;eureka-client&quot;)public interface HelloService &#123; @GetMapping(&quot;/hello&quot;) String hello();&#125; 这里要看一下@FeignClient(&quot;eureka-client&quot;),这个注解的意思就是,绑定了一个名称是eureka-client的服务提供者,这个eureka-client大小写无所谓, 然后下面有一个hello方法, 就是说绑定了名字是eureka-client的服务提供者的hello接口. 这里绑定了hello接口,也就是说,在服务提供者中,需要有一个hello接口, 所以在服务提供者中新增以下接口.1234@GetMapping(&quot;/hello&quot;)public String hello()&#123; return &quot;hello&quot;;&#125; Controller调用在消费者中,调用刚才HelloService中的接口:12345678910111213@RestController@Slf4jpublic class FeignController &#123; @Autowired private HelloService helloService; @GetMapping(&quot;/hello&quot;) public String hello()&#123; return helloService.hello(); &#125; &#125; 测试上面的操作都ok以后,依次启动服务注册中心,服务提供者,和刚刚创建的consumer-feign, 然后访问http://localhost:2101/hello, 可以看到返回了hello Feign具备了Ribbon和Hystrix的功能,使用起来更加简单方便. 参数传递上面的这个例子中,只是调用了个接口,没有任何参数的传递,在实际开发过程中,会有各种各样的参数传递,那接下来就来看一下,参数传递要怎么实现: 服务提供者修改在服务提供者中添加三个接口,如下:1234567891011121314 @GetMapping(&quot;/hello1&quot;)public String hello1(@RequestParam String name) &#123; return &quot;name:&quot; + name;&#125;@GetMapping(&quot;/hello2&quot;)public User hello2(@RequestHeader String name, @RequestHeader String job, @RequestHeader Long id) throws UnsupportedEncodingException &#123; return new User(id, URLDecoder.decode(name,&quot;UTF-8&quot;), URLDecoder.decode(job,&quot;UTF-8&quot;));&#125;@PostMapping(&quot;/hello3&quot;)public String hello3(@RequestBody User user) &#123; return user.getName();&#125; hello1,就是接收一个String类型的参数,然后再返回去. hello2,是接收请求头中的参数,在请求头中传递中文参数,会乱码,所以就要传过来的时候编码,接收以后再解码. hello3,是接收了一个body,一个user对象. 消费者修改再来看一下我们的consumer-feign这个消费者工程需要怎么修改, 具体如下:123456789101112131415@FeignClient(&quot;eureka-client&quot;)public interface HelloService &#123; @GetMapping(&quot;/hello&quot;) String hello(); @GetMapping(&quot;/hello1&quot;) String hello(@RequestParam(&quot;name&quot;) String name); @GetMapping(&quot;/hello2&quot;) User hello(@RequestHeader(&quot;name&quot;) String name, @RequestHeader(&quot;job&quot;) String job, @RequestHeader(&quot;id&quot;) Long id); @PostMapping(&quot;/hello3&quot;) String hello(@RequestBody User user);&#125; 这里要注意一下, 在服务提供者的接口中的@RequestParam和@RequestHeader是没有指定value的,Spring MVC中,默认会使用参数名作为注解的value, 但是在Feign中,必须指定value,否则会报以下错误:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name &apos;feignController&apos;: Unsatisfied dependency expressed through field &apos;helloService&apos;; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name &apos;com.eureka.consumer.service.HelloService&apos;: FactoryBean threw exception on object creation; nested exception is java.lang.IllegalStateException: RequestHeader.value() was empty on parameter 0 at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:588) ~[spring-beans-4.3.9.RELEASE.jar:4.3.9.RELEASE] at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:88) ~[spring-beans-4.3.9.RELEASE.jar:4.3.9.RELEASE] at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessPropertyValues(AutowiredAnnotationBeanPostProcessor.java:366) ~[spring-beans-4.3.9.RELEASE.jar:4.3.9.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1264) ~[spring-beans-4.3.9.RELEASE.jar:4.3.9.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:553) ~[spring-beans-4.3.9.RELEASE.jar:4.3.9.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:483) ~[spring-beans-4.3.9.RELEASE.jar:4.3.9.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:306) ~[spring-beans-4.3.9.RELEASE.jar:4.3.9.RELEASE] at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230) ~[spring-beans-4.3.9.RELEASE.jar:4.3.9.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:302) ~[spring-beans-4.3.9.RELEASE.jar:4.3.9.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:197) ~[spring-beans-4.3.9.RELEASE.jar:4.3.9.RELEASE] at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:761) ~[spring-beans-4.3.9.RELEASE.jar:4.3.9.RELEASE] at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:867) ~[spring-context-4.3.9.RELEASE.jar:4.3.9.RELEASE] at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:543) ~[spring-context-4.3.9.RELEASE.jar:4.3.9.RELEASE] at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.refresh(EmbeddedWebApplicationContext.java:122) ~[spring-boot-1.5.4.RELEASE.jar:1.5.4.RELEASE] at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:693) ~[spring-boot-1.5.4.RELEASE.jar:1.5.4.RELEASE] at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:360) ~[spring-boot-1.5.4.RELEASE.jar:1.5.4.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:303) ~[spring-boot-1.5.4.RELEASE.jar:1.5.4.RELEASE] at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:134) [spring-boot-1.5.4.RELEASE.jar:1.5.4.RELEASE] at com.eureka.consumer.App.main(App.java:16) [classes/:na]Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name &apos;com.eureka.consumer.service.HelloService&apos;: FactoryBean threw exception on object creation; nested exception is java.lang.IllegalStateException: RequestHeader.value() was empty on parameter 0 at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:175) ~[spring-beans-4.3.9.RELEASE.jar:4.3.9.RELEASE] at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.getObjectFromFactoryBean(FactoryBeanRegistrySupport.java:103) ~[spring-beans-4.3.9.RELEASE.jar:4.3.9.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.getObjectForBeanInstance(AbstractBeanFactory.java:1634) ~[spring-beans-4.3.9.RELEASE.jar:4.3.9.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:254) ~[spring-beans-4.3.9.RELEASE.jar:4.3.9.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-4.3.9.RELEASE.jar:4.3.9.RELEASE] at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:208) ~[spring-beans-4.3.9.RELEASE.jar:4.3.9.RELEASE] at org.springframework.beans.factory.support.DefaultListableBeanFactory.addCandidateEntry(DefaultListableBeanFactory.java:1316) ~[spring-beans-4.3.9.RELEASE.jar:4.3.9.RELEASE] at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:1282) ~[spring-beans-4.3.9.RELEASE.jar:4.3.9.RELEASE] at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1101) ~[spring-beans-4.3.9.RELEASE.jar:4.3.9.RELEASE] at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1066) ~[spring-beans-4.3.9.RELEASE.jar:4.3.9.RELEASE] at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:585) ~[spring-beans-4.3.9.RELEASE.jar:4.3.9.RELEASE] ... 18 common frames omittedCaused by: java.lang.IllegalStateException: RequestHeader.value() was empty on parameter 0 at feign.Util.checkState(Util.java:128) ~[feign-core-9.5.0.jar:na] at org.springframework.cloud.netflix.feign.annotation.RequestHeaderParameterProcessor.processArgument(RequestHeaderParameterProcessor.java:62) ~[spring-cloud-netflix-core-1.3.1.RELEASE.jar:1.3.1.RELEASE] at org.springframework.cloud.netflix.feign.support.SpringMvcContract.processAnnotationsOnParameter(SpringMvcContract.java:238) ~[spring-cloud-netflix-core-1.3.1.RELEASE.jar:1.3.1.RELEASE] at feign.Contract$BaseContract.parseAndValidateMetadata(Contract.java:107) ~[feign-core-9.5.0.jar:na] at org.springframework.cloud.netflix.feign.support.SpringMvcContract.parseAndValidateMetadata(SpringMvcContract.java:133) ~[spring-cloud-netflix-core-1.3.1.RELEASE.jar:1.3.1.RELEASE] at feign.Contract$BaseContract.parseAndValidatateMetadata(Contract.java:64) ~[feign-core-9.5.0.jar:na] at feign.ReflectiveFeign$ParseHandlersByName.apply(ReflectiveFeign.java:146) ~[feign-core-9.5.0.jar:na] at feign.ReflectiveFeign.newInstance(ReflectiveFeign.java:53) ~[feign-core-9.5.0.jar:na] at feign.Feign$Builder.target(Feign.java:218) ~[feign-core-9.5.0.jar:na] at org.springframework.cloud.netflix.feign.HystrixTargeter.target(HystrixTargeter.java:39) ~[spring-cloud-netflix-core-1.3.1.RELEASE.jar:1.3.1.RELEASE] at org.springframework.cloud.netflix.feign.FeignClientFactoryBean.loadBalance(FeignClientFactoryBean.java:145) ~[spring-cloud-netflix-core-1.3.1.RELEASE.jar:1.3.1.RELEASE] at org.springframework.cloud.netflix.feign.FeignClientFactoryBean.getObject(FeignClientFactoryBean.java:166) ~[spring-cloud-netflix-core-1.3.1.RELEASE.jar:1.3.1.RELEASE] at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:168) ~[spring-beans-4.3.9.RELEASE.jar:4.3.9.RELEASE] ... 28 common frames omitted Controller调用在controller中调用这几个接口,如下:12345678910111213141516171819202122232425262728@RestController@Slf4jpublic class FeignController &#123; @Autowired private HelloService helloService; @GetMapping(&quot;/hello&quot;) public String hello()&#123; return helloService.hello(); &#125; @GetMapping(&quot;/hello1&quot;) public String hello1()&#123; return helloService.hello(&quot;张三&quot;); &#125; @GetMapping(&quot;/hello2&quot;) public User hello2() throws UnsupportedEncodingException &#123; return helloService.hello(URLEncoder.encode(&quot;张三&quot;, &quot;UTF-8&quot;), URLEncoder.encode(&quot;测试&quot;, &quot;UTF-8&quot;), 1L); &#125; @GetMapping(&quot;/hello3&quot;) public String hello3()&#123; User user = new User(1L, &quot;张三&quot;, &quot;开发&quot;); return helloService.hello(user); &#125;&#125; 测试启动服务,分别访问hello1,hello2,hello3. 然后看下结果:]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-11-Hystrix仪表盘与Turbine集群监控]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-Cloud-11-Hystrix%E4%BB%AA%E8%A1%A8%E7%9B%98%E4%B8%8ETurbine%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[述Hystrix仪表盘,主要用来监控Hystrix的实时运行状态,通过仪表盘可以看到Hystrix的各项指标信息,从而快速发现系统中存在的问题, 记下来就来看一下具体如何使用 本文主要通过两个方面来使用Hystrix仪表盘,一个是单体应用的监控,另一个是整合Turbine对集群进行监控. 单体应用监控环境搭建新建工程单独创建一个新的工程来做Hystrix Dashboard. 我这里还是在之前项目中创建了一个子项目,名字是hystrix-dashboard. 创建好之后,修改pom.xml, parent改成主项目等等,还和之前的操作一样. 引入依赖修改完成之后,引入以下依赖1234567891011121314&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix-dashboard&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 添加注解最后在入口类App.java中添加@EnableHystrixDashboard注解, 表示开启仪表盘功能,如下:123456789@SpringBootApplication@EnableHystrixDashboardpublic class App &#123; public static void main(String[] args) &#123; SpringApplication.run(App.class, args); &#125;&#125; 配置修改application.yml中,修改一个应用的名字,和端口号,如下:12345spring: application: name: hystrix-dashboardserver: port: 2001 运行效果上面的配置都修改好之后,启动项目,运行http://localhost:2001/hystrix,界面如下: 现在仪表盘的服务已经OK了,但是要监控服务的话,还需要被监控的服务提供一个/hystrix.stream接口,比如说我们要监控消费者工程, 那就需要让消费者工程提供一个/hystrix.stream接口 被监控的服务修改修改一下消费者工程,很简单,只需要引入两个包就好,如下:12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 这两个包在我们之前的案例中应该已经引过了, 没有的话再加进去 最后需要在消费者的启动类中添加@EnableCircuitBreaker注解, 之前也应该是有的. 这些都改好之后,启动服务注册中心,服务提供者,消费者. 然后在消费者启动的时候可以看到如下日志:12019-03-27 11:12:10.013 INFO 3448 --- [ main] o.s.b.a.e.mvc.EndpointHandlerMapping : Mapped &quot;&#123;[/hystrix.stream/**]&#125;&quot; onto public org.springframework.web.servlet.ModelAndView org.springframework.cloud.netflix.endpoint.ServletWrappingEndpoint.handle(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse) throws java.lang.Exception 这个日志表示当前工程已经有了/hystrix.stream接口了, 可以直接访问了, 但是这里需要注意:要访问/hystrix.stream接口的话,需要先访问这个工程的一个其他任意接口,直接访问/hystrix.stream的话,会打印出一连串的ping:ping:… 所以这里先随便访问一个消费者工程的接口,然后再访问/hystrix.stream接口,地址是http://localhost:9000/hystrix.stream , 然后看一下页面的返回: 返回的是一堆的json,直接看很乱,所以要在仪表盘中看这串json 在仪表盘中,把监控的地址输上.如下: 然后进去.就可以看到以下的监控画面: 参数详解 Turbine集群监控在实际应用中,我们需要监控的一般是一个集群,这时候,就需要用到Turbine集群监控了,Turbine有一个重要功能就是汇聚监控信息,并将汇聚到的监控信息提供给Hystrix Dashborad来集中显示和监控,接下来就来看一下如何使用. 环境搭建新建工程和之前是一样的,还是在原来主项目的基础上,新建一个子项目,名称是turbine 引入依赖修改pom文件, 还是先把当前项目加到主项目下,然后引入如下依赖:123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-turbine&lt;/artifactId&gt;&lt;/dependency&gt; 启动类修改依赖添加完毕之后,在启动类上面添加@EnableTurbine注解,开启仪表盘功能12345678910@SpringBootApplication@EnableDiscoveryClient@EnableTurbinepublic class App &#123; public static void main(String[] args) &#123; SpringApplication.run(App.class, args); &#125;&#125; 配置修改修改application.yml文件,添加eureka和turbine的相关配置,如下:123456789101112131415spring: application: name: turbineserver: port: 2002management: port: 2003eureka: client: service-url: defaultZone: http://peer1:1111/eureka/,http://peer2:1112/eureka/turbine: app-config: eureka-consumer cluster-name-expression: &quot;default&quot; combine-host-port: true 来看一下这个配置: turbine.app-config: 配置要监控的应用名称 turbine.cluster-name-expression: “default”,表示集群的名字是default turbine.combine-host-port: 表示同一主机上的服务通过host和port的组合来进行区分,默认情况下是使用host来区分,这样会使本地调试有问题 注意一下turbine.cluster-name-expression: &quot;default&quot; 这个,可能在启动的时候会报错,错误信息如下:1Property or field &apos;default&apos; cannot be found on object of type &apos;com.netflix.appinfo.InstanceInfo&apos; 解决方案就是,把这个配置改成下面这样:12turbine: cluster-name-expression: new String(&apos;default&apos;) 查看监控图监控服务创建成功后,再把所有的服务都启动起来,服务注册中心,服务提供者,消费者,这里消费者启动两个实例,来模拟集群,最后再启动hystrix-dashboard和turbine,然后在监控之前我们先访问一下两个消费者端的任意一个接口,然后打开http://localhost:2001/hystrix/,输入监控的地址是http://localhost:2002/turbine.stream,然后访问,结果如下: 这里可以看到,集群下的主机报告显示的就和之前的不一样了.]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-10-Hystrix请求合并]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-Cloud-10-Hystrix%E8%AF%B7%E6%B1%82%E5%90%88%E5%B9%B6%2F</url>
    <content type="text"><![CDATA[述在微服务的架构中,各个模块是通过互相请求来进行通信的,在高并发的情况下,通信次数的增加会导致总的通信时间增加,同时线程池的资源也是有限的,高并发的环境下,会有大量的线程处于等待状态,进而导致了响应延迟,要解决这些问题就可以使用Hystrix的请求合并. 具体原理是怎样的呢? Hystrix中的请求合并,就是利用一个合并处理器,把对同一个服务连续发起的几个请求,合并成一个请求去发送(连续请求的时间窗默认是10ms). 那么接下来就来看一下,如何实现请求合并? 第一种方式服务提供者接口修改服务提供者的controller中,添加两个方法,如下:12345678910111213141516171819202122232425262728@GetMapping(&quot;/dc/&#123;id&#125;&quot;)public String dc(@PathVariable Long id) throws InterruptedException &#123; String services = &quot;Services: &quot; + discoveryClient.getServices(); log.info(services); log.info(&quot;-------------/dc/&#123;&#125;&quot;, id); if (id.equals(1L))&#123; return &quot;id是1的返回值&quot;; &#125; else if (id.equals(2L)) &#123; return &quot;id是2的返回值&quot;; &#125; return services;&#125;@GetMapping(&quot;/test&quot;)public List&lt;String&gt; test(String ids) throws InterruptedException &#123; log.info(&quot;-------------/test,参数:&#123;&#125;&quot;, ids); List&lt;String&gt; list = new ArrayList&lt;&gt;(3); list.add(&quot;Java&quot;); list.add(&quot;Spring Cloud&quot;); list.add(&quot;dubbo&quot;); return list;&#125; 一个是根据id获取数据的接口,一个是批处理的接口, 批处理的接口中的参数ids,格式是用逗号隔开的id字符串,比如”1,2,3,4”这样的, 实际情况中,我们要根据传过来的id,找到对应的数据,组成集合,然后返回去,这里为了方便,就直接返回固定的数据了. 服务消费者修改在消费者的HelloService中,添加以下两个方法:123456789101112131415161718/** * 单个请求测试 * @param id id */public String getByid(Long id)&#123; return restTemplate.getForObject(&quot;http://EUREKA-CLIENT/dc/&#123;1&#125;&quot;, String.class, id);&#125;/** * 批处理获取集合 * @param ids ids */public List&lt;String&gt; getList(List&lt;Long&gt; ids)&#123; log.info(&quot;getList------&#123;&#125;,Thread.currentThread().getName():&#123;&#125;&quot;, ids, Thread.currentThread().getName()); // RestTemplate获取的返回值是个集合的话,需要用数组接收,然后再转成集合 String[] strings = restTemplate.getForObject(&quot;http://EUREKA-CLIENT/test?ids=&#123;1&#125;&quot;, String[].class, StringUtils.join(ids, &quot;,&quot;)); return Arrays.asList(strings);&#125; getByid()用来调用单个id的接口,getList用来调用批处理的接口, 在getList中,打印了该方法执行时,所处线程的名称,为了方便观察 在服务提供者中,批处理接口返回的信息是一个list集合,用RestTemplate请求的话,需要用一个数组去接收,然后再转一下. 修改完HelloService后,新建一个类,名称是StringBatchCommand,内容如下:12345678910111213141516171819202122/** * @author 周泽 * @date Create in 14:49 2019/3/12 * @Description 批处理命令 */public class StringBatchCommand extends HystrixCommand&lt;List&lt;String&gt;&gt; &#123; private List&lt;Long&gt; ids; private HelloService helloService; public StringBatchCommand(List&lt;Long&gt; ids, HelloService helloService) &#123; super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(&quot;CollapsingGroup&quot;)).andCommandKey(HystrixCommandKey.Factory.asKey(&quot;CollapsingKey&quot;))); this.ids = ids; this.helloService = helloService; &#125; @Override protected List&lt;String&gt; run() throws Exception &#123; return helloService.getList(ids); &#125;&#125; 这个类和之前的StringCommand类是差不多的,都是继承自HystrixCommand类,用来处理合并之后的请求,在run方法中调用HelloService的getList方法. 完了之后呢,还需要一个合并请求的类,名称是StringCollapseCommand,内容如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * @author 周泽 * @date Create in 14:58 2019/3/12 * @Description 请求合并实现类 */@Slf4jpublic class StringCollapseCommand extends HystrixCollapser&lt;List&lt;String&gt;, String, Long&gt; &#123; private Long id; private HelloService helloService; public StringCollapseCommand(HelloService helloService, Long id)&#123; // 设置了请求时间窗为1000ms，即请求时间间隔在100ms之内的请求会被合并为一个请求。 super(Setter.withCollapserKey(HystrixCollapserKey.Factory.asKey(&quot;stringCollapseCommand&quot;)).andCollapserPropertiesDefaults(HystrixCollapserProperties.Setter().withTimerDelayInMilliseconds(1000))); this.helloService = helloService; this.id = id; &#125; @Override public Long getRequestArgument() &#123; return id; &#125; /** * 合并请求的方法 */ @Override protected HystrixCommand&lt;List&lt;String&gt;&gt; createCommand(Collection&lt;CollapsedRequest&lt;String, Long&gt;&gt; collection) &#123; List&lt;Long&gt; ids = new ArrayList&lt;&gt;(collection.size()); ids.addAll(collection.stream().map(CollapsedRequest::getArgument).collect(Collectors.toList())); StringBatchCommand stringBatchCommand = new StringBatchCommand(ids, helloService); return stringBatchCommand; &#125; /** * 为每个请求设置请求结果 */ @Override protected void mapResponseToRequests(List&lt;String&gt; strings, Collection&lt;CollapsedRequest&lt;String, Long&gt;&gt; collection) &#123; log.info(&quot;mapResponseToRequests....&quot;); int count = 0; for (CollapsedRequest&lt;String, Long&gt; collapsedRequest : collection) &#123; String str = strings.get(count++); collapsedRequest.setResponse(str); &#125; &#125;&#125; 构造方法: 中设置了请求的时间窗是1000ms,意思就是在1000ms之内的请求都会被合并成一个 createCommand()方法: 用来合并请求,在这里获取到单个请求的各个id,然后放到一个集合中去,然后创建出一个StringBatchCommand对象,用该对象去发起一个批量请求. mapResponseToRequests()方法: 主要用来为每个请求设置请求结果,该方法的第一个参数就是批处理的请求结果.第二个参数collapsedRequests代表了每一个被合并的请求. 然后通过遍历请求结果,为collapsedRequests 来设置请求结果. 总结一下就是,构造设置请求时间窗,然后合并请求,用批处理的对象去调用服务,然后遍历返回值为每个请求设置返回值.以上都搞完之后,就可以去测试了. 测试在消费者的controller中添加以下接口,用来测试:1234567891011121314151617181920212223242526272829@GetMapping(&quot;/test3&quot;)public void test3() throws ExecutionException, InterruptedException &#123; HystrixRequestContext context = HystrixRequestContext.initializeContext(); StringCollapseCommand sc1 = new StringCollapseCommand(helloService, 1L); StringCollapseCommand sc2 = new StringCollapseCommand(helloService, 2L); StringCollapseCommand sc3 = new StringCollapseCommand(helloService, 3L); StringCollapseCommand sc4 = new StringCollapseCommand(helloService, 4L); Future&lt;String&gt; q1 = sc1.queue(); Future&lt;String&gt; q2 = sc2.queue(); Future&lt;String&gt; q3 = sc3.queue(); String str1 = q1.get(); String str2 = q2.get(); String str3 = q3.get(); Thread.sleep(3000); Future&lt;String&gt; q4 = sc4.queue(); String str4 = q4.get(); log.info(&quot;str1:&#123;&#125;&quot;, str1); log.info(&quot;str2:&#123;&#125;&quot;, str2); log.info(&quot;str3:&#123;&#125;&quot;, str3); log.info(&quot;str4:&#123;&#125;&quot;, str4); context.close();&#125; 来看一下这个接口:第一步,需要实例化HystrixRequestContext.第二步,创建StringCollapseCommand类的示例,来发起请求,我们创建了四个请求,然后先是发送了前三个,然后线程休眠了3秒钟, 而我们请求合并的时间窗是1秒,所以在发送第四个请求的时候,是不会和前三个请求合并的,是单独的一个线程去处理的. 启动服务,访问http://localhost:9000/test3,然后看下控制台打印.123456789101112131415161718192019-03-20 11:42:41.014 INFO 17304 --- [llapsingGroup-1] c.eureka.consumer.service.HelloService : getList------[1, 2, 3],Thread.currentThread().getName():hystrix-CollapsingGroup-12019-03-20 11:42:41.055 INFO 17304 --- [llapsingGroup-1] s.c.a.AnnotationConfigApplicationContext : Refreshing org.springframework.context.annotation.AnnotationConfigApplicationContext@2a7d0649: startup date [Wed Mar 20 11:42:41 CST 2019]; parent: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@55120f992019-03-20 11:42:41.128 INFO 17304 --- [llapsingGroup-1] f.a.AutowiredAnnotationBeanPostProcessor : JSR-330 &apos;javax.inject.Inject&apos; annotation found and supported for autowiring2019-03-20 11:42:41.422 INFO 17304 --- [llapsingGroup-1] c.netflix.config.ChainedDynamicProperty : Flipping property: EUREKA-CLIENT.ribbon.ActiveConnectionsLimit to use NEXT property: niws.loadbalancer.availabilityFilteringRule.activeConnectionsLimit = 21474836472019-03-20 11:42:41.473 INFO 17304 --- [llapsingGroup-1] c.n.u.concurrent.ShutdownEnabledTimer : Shutdown hook installed for: NFLoadBalancer-PingTimer-EUREKA-CLIENT2019-03-20 11:42:41.513 INFO 17304 --- [llapsingGroup-1] c.netflix.loadbalancer.BaseLoadBalancer : Client: EUREKA-CLIENT instantiated a LoadBalancer: DynamicServerListLoadBalancer:&#123;NFLoadBalancer:name=EUREKA-CLIENT,current list of Servers=[],Load balancer stats=Zone stats: &#123;&#125;,Server stats: []&#125;ServerList:null2019-03-20 11:42:41.523 INFO 17304 --- [llapsingGroup-1] c.n.l.DynamicServerListLoadBalancer : Using serverListUpdater PollingServerListUpdater2019-03-20 11:42:41.562 INFO 17304 --- [llapsingGroup-1] c.netflix.config.ChainedDynamicProperty : Flipping property: EUREKA-CLIENT.ribbon.ActiveConnectionsLimit to use NEXT property: niws.loadbalancer.availabilityFilteringRule.activeConnectionsLimit = 21474836472019-03-20 11:42:41.566 INFO 17304 --- [llapsingGroup-1] c.n.l.DynamicServerListLoadBalancer : DynamicServerListLoadBalancer for client EUREKA-CLIENT initialized: DynamicServerListLoadBalancer:&#123;NFLoadBalancer:name=EUREKA-CLIENT,current list of Servers=[localhost:8081],Load balancer stats=Zone stats: &#123;defaultzone=[Zone:defaultzone; Instance count:1; Active connections count: 0; Circuit breaker tripped count: 0; Active connections per server: 0.0;]&#125;,Server stats: [[Server:localhost:8081; Zone:defaultZone; Total Requests:0; Successive connection failure:0; Total blackout seconds:0; Last connection made:Thu Jan 01 08:00:00 CST 1970; First connection made: Thu Jan 01 08:00:00 CST 1970; Active Connections:0; total failure count in last (1000) msecs:0; average resp time:0.0; 90 percentile resp time:0.0; 95 percentile resp time:0.0; min resp time:0.0; max resp time:0.0; stddev resp time:0.0]]&#125;ServerList:org.springframework.cloud.netflix.ribbon.eureka.DomainExtractingServerList@6df180492019-03-20 11:42:41.844 INFO 17304 --- [llapsingGroup-1] c.e.c.hystrix.StringCollapseCommand : mapResponseToRequests....2019-03-20 11:42:42.528 INFO 17304 --- [erListUpdater-0] c.netflix.config.ChainedDynamicProperty : Flipping property: EUREKA-CLIENT.ribbon.ActiveConnectionsLimit to use NEXT property: niws.loadbalancer.availabilityFilteringRule.activeConnectionsLimit = 21474836472019-03-20 11:42:44.920 INFO 17304 --- [llapsingGroup-2] c.eureka.consumer.service.HelloService : getList------[4],Thread.currentThread().getName():hystrix-CollapsingGroup-22019-03-20 11:42:44.928 INFO 17304 --- [llapsingGroup-2] c.e.c.hystrix.StringCollapseCommand : mapResponseToRequests....2019-03-20 11:42:44.928 INFO 17304 --- [nio-9000-exec-2] c.e.consumer.controller.DcController : str1:Java2019-03-20 11:42:44.928 INFO 17304 --- [nio-9000-exec-2] c.e.consumer.controller.DcController : str2:Spring Cloud2019-03-20 11:42:44.929 INFO 17304 --- [nio-9000-exec-2] c.e.consumer.controller.DcController : str3:dubbo2019-03-20 11:42:44.929 INFO 17304 --- [nio-9000-exec-2] c.e.consumer.controller.DcController : str4:Java 重点是这几条:123456782019-03-20 11:42:41.014 INFO 17304 --- [llapsingGroup-1] c.eureka.consumer.service.HelloService : getList------[1, 2, 3],Thread.currentThread().getName():hystrix-CollapsingGroup-12019-03-20 11:42:41.844 INFO 17304 --- [llapsingGroup-1] c.e.c.hystrix.StringCollapseCommand : mapResponseToRequests....2019-03-20 11:42:44.920 INFO 17304 --- [llapsingGroup-2] c.eureka.consumer.service.HelloService : getList------[4],Thread.currentThread().getName():hystrix-CollapsingGroup-22019-03-20 11:42:44.928 INFO 17304 --- [llapsingGroup-2] c.e.c.hystrix.StringCollapseCommand : mapResponseToRequests.... 上面是第一次请求,id是1,2,3,也就是说,前三个请求被合并成了一个请求,下面的是请求id是4,最后一个请求是单独去发送的. 注解方式实现请求合并上面这种方式呢,虽然是实现了请求合并,但是写着有点蛋疼,太多了,我们也可以通过注解来实现请求合并,具体实现方式如下: 修改HelloService添加以下两个方法:1234567891011@HystrixCollapser(batchMethod = &quot;test2&quot;, collapserProperties = &#123;@HystrixProperty(name = &quot;timerDelayInMilliseconds&quot; ,value = &quot;1000&quot;)&#125;)public Future&lt;String&gt; test(Long id)&#123; return null;&#125;@HystrixCommandpublic List&lt;String&gt; test2(List&lt;Long&gt; ids)&#123; log.info(&quot;test2------&#123;&#125;,Thread.currentThread().getName():&#123;&#125;&quot;, ids, Thread.currentThread().getName()); String[] strings = restTemplate.getForObject(&quot;http://EUREKA-CLIENT/test?ids=&#123;1&#125;&quot;, String[].class, StringUtils.join(ids, &quot;,&quot;)); return Arrays.asList(strings);&#125; test方法上添加注解@HystrixCollapser,用来实现请求合并,用batchMethod属性来指定把请求合并之后的处理方法,collapserProperties属性用来指定其他属性,比如上面设置了时间窗是1000ms. test2方法就是用来等请求合并完了去发送请求,接收返回值的. 测试在controller中添加如下方法用来测试:12345678910111213141516171819202122232425@GetMapping(&quot;/test4&quot;)public void test4() throws ExecutionException, InterruptedException &#123; HystrixRequestContext context = HystrixRequestContext.initializeContext(); Future&lt;String&gt; q1 = helloService.test(1L); Future&lt;String&gt; q2 = helloService.test(2L); Future&lt;String&gt; q3 = helloService.test(3L); String str1 = q1.get(); String str2 = q2.get(); String str3 = q3.get(); Thread.sleep(3000); Future&lt;String&gt; q4 = helloService.test(4L); String str4 = q4.get(); log.info(&quot;str1:&#123;&#125;&quot;, str1); log.info(&quot;str2:&#123;&#125;&quot;, str2); log.info(&quot;str3:&#123;&#125;&quot;, str3); log.info(&quot;str4:&#123;&#125;&quot;, str4); context.close();&#125; ok, 添加完之后启动项目,访问http://localhost:9000/test4,测试. 控制台输出:1232019-03-21 16:06:49.312 INFO 8548 --- [-HelloService-1] c.eureka.consumer.service.HelloService : test2------[1, 2, 3],Thread.currentThread().getName():hystrix-HelloService-12019-03-21 16:06:53.239 INFO 8548 --- [-HelloService-2] c.eureka.consumer.service.HelloService : test2------[4],Thread.currentThread().getName():hystrix-HelloService-2 效果和上面是一样的, 但是这样写就很舒服. 总结请求合并呢,就是将多个请求合并成一个请求去一次性处理,可以有效节省网络带宽和线程池资源,但是同时也会引发其他的问题,比如说一个请求用5ms就搞定了,但是我们请求时间窗设置的10ms,那么就还得等10ms看看还有没有别的请求,那么这个请求总共耗时就是15ms,不过,如果我们要发起的命令本身就是一个高延迟的命令,那么这个时候就可以使用请求合并了,因为这个时候时间窗的时间消耗就显得微不足道了,另外高并发也是请求合并的一个非常重要的场景.]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-9-Hystrix请求缓存]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-Cloud-9-Hystrix%E8%AF%B7%E6%B1%82%E7%BC%93%E5%AD%98%2F</url>
    <content type="text"><![CDATA[述在高并发的情况下,处理好缓存,就能有效的降低服务器的压力,Spring Cloud的Hystrix也提供了请求缓存的能力,接下来,我们来看一下如何开启. 环境准备还是基于之前的服务注册中心,提供者和消费者, 先把服务注册中心开启, 服务提供者和消费者需要修改. 通过方法重载开启缓存当我们在使用自定义Hystrix请求命令时,我们只需要在实现HystrixCommand或者HystrixObservableCommand时,通过重载getCacheKey()方法来开启请求缓存. 修改消费者修改一下之前的StringCommand类,如下:123456789101112131415161718192021222324252627282930313233343536public class StringCommand extends HystrixCommand&lt;String&gt; &#123; private RestTemplate restTemplate; private Long id; @Override protected String getFallback() &#123; Throwable executionException = getExecutionException(); log.info(&quot;Exception:&#123;&#125;&quot;, executionException.getMessage()); return new String(&quot;StringCommand.getFallback&quot;); &#125; public StringCommand(Setter setter, RestTemplate restTemplate, Long id) &#123; super(setter); this.restTemplate = restTemplate; this.id = id; &#125; public StringCommand(Setter setter, RestTemplate restTemplate) &#123; super(setter); this.restTemplate = restTemplate; &#125; @Override protected String run() throws Exception &#123; return restTemplate.getForObject(&quot;http://EUREKA-CLIENT/dc/&quot; + id, String.class); &#125; @Override protected String getCacheKey() &#123; return String.valueOf(id); &#125;&#125; 修改的部分就是,加了一个构造函数,添加了id, 然后在向提供者发请求时,携带了id参数,然后重写了getCacheKey()方法. 系统在运行时会根据getCacheKey()方法的返回值来判断这个请求和之前执行过的请求是否一样,如果一样的话,就会直接使用缓存数据,而不去请求服务提供者,所以很明显,getCacheKey()方法是在run()方法之前执行的. 然后在controller中添加一个方法,如下:1234567891011121314151617181920@GetMapping(&quot;/testCache&quot;)public String testCache()&#123; HystrixCommandKey commandKey = HystrixCommandKey.Factory.asKey(&quot;commandKey&quot;); HystrixRequestContext.initializeContext(); StringCommand stringCommand1 = new StringCommand(HystrixCommand.Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(&quot;&quot;)).andCommandKey(commandKey), restTemplate, 1L); String str1 = stringCommand1.execute(); StringCommand stringCommand2 = new StringCommand(HystrixCommand.Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(&quot;&quot;)).andCommandKey(commandKey), restTemplate, 1L); String str2 = stringCommand2.execute(); StringCommand stringCommand3 = new StringCommand(HystrixCommand.Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(&quot;&quot;)).andCommandKey(commandKey), restTemplate, 1L); String str3 = stringCommand3.execute(); log.info(&quot;str1:&#123;&#125;&quot;, str1); log.info(&quot;str2:&#123;&#125;&quot;, str2); log.info(&quot;str3:&#123;&#125;&quot;, str3); return null;&#125; 这里就是发送了三个请求,id都是1 修改服务提供者修改eureka-client工程,中的/dc接口, 修改如下:123456789101112131415@GetMapping(&quot;/dc/&#123;id&#125;&quot;)public String dc(@PathVariable Long id) throws InterruptedException &#123; String services = &quot;Services: &quot; + discoveryClient.getServices(); log.info(services); log.info(&quot;-------------/dc/&#123;&#125;&quot;, id); if (id.equals(1L))&#123; return &quot;id是1的返回值&quot;; &#125; else if (id.equals(2L)) &#123; return &quot;id是2的返回值&quot;; &#125; return services;&#125; 测试修改好之后,启动服务提供者和消费者.然后访问http://localhost:9000/testCache 然后来看一下消费者端的控制台输出:1232019-02-25 15:33:25.284 INFO 30380 --- [nio-9000-exec-2] c.e.consumer.controller.DcController : str1:id是1的返回值2019-02-25 15:33:25.284 INFO 30380 --- [nio-9000-exec-2] c.e.consumer.controller.DcController : str2:id是1的返回值2019-02-25 15:33:25.284 INFO 30380 --- [nio-9000-exec-2] c.e.consumer.controller.DcController : str3:id是1的返回值 是打印了三条内容 再来看一下服务提供者的控制台输出:12019-02-25 15:33:25.248 INFO 21400 --- [nio-8081-exec-5] c.eureka.client.controller.DcController : -------------/dc/1 只输出了一次,说明client端只收到了一次请求. 其他两次请求都是走的缓存. 清除缓存当我们把服务提供者的信息修改掉之后,那么缓存中的数据也应该要被清除,否则用户在读取数据的时候可能取到的是缓存中的旧数据,那么如何清除缓存,来看一下: 修改controller的/testCache接口,如下:1234567891011121314151617181920212223@GetMapping(&quot;/testCache&quot;)public String testCache()&#123; HystrixCommandKey commandKey = HystrixCommandKey.Factory.asKey(&quot;commandKey&quot;); HystrixRequestContext.initializeContext(); StringCommand stringCommand1 = new StringCommand(HystrixCommand.Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(&quot;&quot;)).andCommandKey(commandKey), restTemplate, 1L); String str1 = stringCommand1.execute(); // 清除缓存 HystrixRequestCache.getInstance(commandKey, HystrixConcurrencyStrategyDefault.getInstance()).clear(String.valueOf(1L)); StringCommand stringCommand2 = new StringCommand(HystrixCommand.Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(&quot;&quot;)).andCommandKey(commandKey), restTemplate, 1L); String str2 = stringCommand2.execute(); StringCommand stringCommand3 = new StringCommand(HystrixCommand.Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(&quot;&quot;)).andCommandKey(commandKey), restTemplate, 1L); String str3 = stringCommand3.execute(); log.info(&quot;str1:&#123;&#125;&quot;, str1); log.info(&quot;str2:&#123;&#125;&quot;, str2); log.info(&quot;str3:&#123;&#125;&quot;, str3); return null;&#125; 其实就是第一次请求完成后加了一行HystrixRequestCache.getInstance(commandKey, HystrixConcurrencyStrategyDefault.getInstance()).clear(String.valueOf(1L)); 来清除缓存. 再次访问http://localhost:9000/testCache,然后看下控制台输出. 消费者端的输出还是:1232019-02-25 16:04:42.370 INFO 21660 --- [nio-9000-exec-1] c.e.consumer.controller.DcController : str1:id是1的返回值2019-02-25 16:04:42.370 INFO 21660 --- [nio-9000-exec-1] c.e.consumer.controller.DcController : str2:id是1的返回值2019-02-25 16:04:42.370 INFO 21660 --- [nio-9000-exec-1] c.e.consumer.controller.DcController : str3:id是1的返回值 而提供者端的输出是:122019-02-25 16:04:42.324 INFO 21400 --- [nio-8081-exec-9] c.eureka.client.controller.DcController : -------------/dc/12019-02-25 16:04:42.358 INFO 21400 --- [nio-8081-exec-2] c.eureka.client.controller.DcController : -------------/dc/1 可以看到,提供者是接收到了两次请求的, 因为我们在第一次请求结束后清理了缓存. 通过注解开启缓存通过注解也可以开启缓存,相关注解有以下三个: @CacheResult @CacheKey @CacheRemove 分别来看一下这几个注解 @CacheResult@CacheResult可以用在方法上面,作用就是给该方法开启缓存,默认情况下所有的参数都会作为缓存的key 修改HelloService的方法,如下:1234567891011@CacheResult@HystrixCommand(fallbackMethod = &quot;helloFallback&quot;)public String helloService(Long id, String name)&#123; return restTemplate.getForObject(&quot;http://EUREKA-CLIENT/dc/&#123;1&#125;&quot;, String.class, id);&#125;@HystrixCommandpublic String helloFallback(Long id, String name)&#123; return &quot;error&quot;;&#125; 我们在helloService这个方法上添加了@CacheResult注解,此时这个方法就会开启缓存,所有的参数都会作为缓存的key,如果再次调用传入的参数和之前的参数都一致的话,就会直接使用缓存,否则才会发起请求: controller中,添加以下方法:123456789101112131415@GetMapping(&quot;/testCache2&quot;)public String testCache2()&#123; HystrixRequestContext.initializeContext(); // 第一次请求 helloService.helloService(1L, &quot;zhangsan&quot;); // 第二次请求,参数和第一次一致 helloService.helloService(1L, &quot;zhangsan&quot;); // 第三次请求,修改参数 helloService.helloService(2L, &quot;lisi&quot;); return null;&#125; 这里的第一次请求会给提供者发送请求,第二次的话,因为参数和第一次请求是一致的,所以这里会走缓存, 第三次请求因为参数变了,所以会给提供者发送请求获取数据 自定义缓存的key我们也可以自定义缓存的key,不用默认的规则,只需要在@CacheResult中添加cacheKeyMethod属性来指定返回缓存key的方法,注意,key必须是String类型的,看个例子:123456789@CacheResult(cacheKeyMethod = &quot;getCacheKey&quot;)@HystrixCommand(fallbackMethod = &quot;helloFallback&quot;)public String helloService(Long id, String name)&#123; return restTemplate.getForObject(&quot;http://EUREKA-CLIENT/dc/&#123;1&#125;&quot;, String.class, id);&#125;public String getCacheKey(Long id, String name)&#123; return String.valueOf(id);&#125; 此时,就会用id作为key,默认的规则就失效了. @CacheKey我们也可以通过使用@CacheKey注解来指定缓存的key,如下:12345@CacheResult@HystrixCommand(fallbackMethod = &quot;helloFallback&quot;)public String helloService(@CacheKey Long id, String name)&#123; return restTemplate.getForObject(&quot;http://EUREKA-CLIENT/dc/&#123;1&#125;&quot;, String.class, id);&#125; 这里我们给id这个参数加了@CacheKey注解,意思就是会使用id这个参数来作为Key,跟name这个参数就没有任何关系,此时,只要id相同,就可以认为是同一个请求. 如果同时指定了@CacheResult中的cacheKeyMethod来指定key,又使用了@CacheKey注解,这个时候@CacheKey是没有效果的 @CacheRemove这个就是让缓存失效的注解,具体用法如下:123456789101112131415@CacheResult(cacheKeyMethod = &quot;getCacheKey&quot;)@HystrixCommand(fallbackMethod = &quot;helloFallback&quot;)public String helloService(Long id, String name)&#123; return restTemplate.getForObject(&quot;http://EUREKA-CLIENT/dc/&#123;1&#125;&quot;, String.class, id);&#125;public String getCacheKey(Long id, String name)&#123; return String.valueOf(id);&#125;@CacheRemove(commandKey = &quot;helloService&quot;)@HystrixCommandpublic String cacheRemove(@CacheKey Long id, String name)&#123; return null;&#125; 这里必须指定commandKey,commandKey的值就是缓存的位置,配置了commandKey属性的值,Hystrix才能找到请求命令缓存的位置.在controller中测试如下:123456789101112131415161718@GetMapping(&quot;/testCache2&quot;)public String testCache2()&#123; HystrixRequestContext.initializeContext(); // 第一次请求 helloService.helloService(1L, &quot;zhangsan&quot;); // 清除缓存 helloService.cacheRemove(1L,&quot;&quot;); // 第二次请求,缓存被清除,重新发起 helloService.helloService(1L, &quot;zhangsan&quot;); // 第三次请求,参数一致,使用缓存 helloService.helloService(1L, &quot;lisi&quot;); return null;&#125;]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-8-Hystrix的服务降级与异常处理]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-Cloud-8-Hystrix%E7%9A%84%E6%9C%8D%E5%8A%A1%E9%99%8D%E7%BA%A7%E4%B8%8E%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[之前我们用过了fallbackMethod这个东西,本文将详细说一下这个东西,也就是服务降级 服务降级前文中,fallbackMethod所描述的函数,就是一个备胎,用来实现服务的降级处理,在@HystrixCommand注解中,可以通过fallbackMethod来指定请求失败后调用的方法,在自定义的Hystrix请求命令时, 可以通过重写getFallback()方法来处理服务降级之后的逻辑 使用@HystrixCommand注解指定fallbackMethod方法时,指定的方法要和注解处在同一个类中 指定的这个fallbackMethod方法在执行的过程中,也可能会发生异常,同理我们也可以给这个方法使用@HystrixCommand注解,指定fallbackMethod,比如这样:123456789101112@HystrixCommand(fallbackMethod = &quot;helloFallback&quot;)public String helloService()&#123; return restTemplate.getForObject(&quot;http://EUREKA-CLIENT/dc&quot;, String.class);&#125;@HystrixCommand(fallbackMethod = &quot;fallback2&quot;)public String helloFallback()&#123; return &quot;error&quot;;&#125;public String fallback2()&#123; return &quot;error2&quot;;&#125; 实际使用中,只给必要的方法添加断路器就好了. 异常处理在调用服务提供者的时候,也可能会出现异常,默认情况下,方法抛了异常就会自动进行服务降级,交给服务降级中的方法去处理,下面看下两种Hystrix请求方式下如何捕捉异常 自定义的Hystrix请求方式在自定义的Hystrix请求方式下,可以在getFallback中捕捉抛出的异常,修改我们之前的StringCommand类,代码如下:1234567891011121314151617181920212223242526@Slf4jpublic class StringCommand extends HystrixCommand&lt;String&gt; &#123; private RestTemplate restTemplate; @Override protected String getFallback() &#123; Throwable executionException = getExecutionException(); log.info(&quot;Exception:&#123;&#125;&quot;, executionException.getMessage()); return new String(&quot;StringCommand.getFallback&quot;); &#125; public StringCommand(Setter setter, RestTemplate restTemplate) &#123; super(setter); this.restTemplate = restTemplate; &#125; @Override protected String run() throws Exception &#123; // 这里就会抛异常,然后我们通过getFallback()方法 捕捉这里抛出的异常 int i = 1 / 0; return restTemplate.getForObject(&quot;http://EUREKA-CLIENT/dc&quot;, String.class); &#125;&#125; 这里就是在,run()方法中int i = 1 / 0;会抛一个异常,然后我们在getFallback()方法中通过Throwable executionException = getExecutionException();来捕获异常. 之后还是通过http://localhost:9000/testStringCommand来测试. 页面返回: 控制台输出:12019-02-21 16:54:45.735 INFO 8324 --- [ hystrix--1] c.eureka.consumer.hystrix.StringCommand : Exception:/ by zero 这里就已经进行了服务降级 使用注解的方式如果是用了注解,只需要在服务降级的方法中添加一个,Throwable类型的参数即可,具体代码如下:12345678910@HystrixCommand(fallbackMethod = &quot;helloFallback&quot;)public String helloService()&#123; int i = 1 / 0; return restTemplate.getForObject(&quot;http://EUREKA-CLIENT/dc&quot;, String.class);&#125;public String helloFallback(Throwable throwable)&#123; log.info(&quot;Exception:&#123;&#125;&quot;, throwable.getMessage()); return &quot;error&quot;;&#125; 将异常抛给用户如果有一个异常,我们并不希望进入到服务降级的方法中,而是想让他直接抛给用户,么可以在@HystrixCommand注解中添加异常忽略,代码如下:12345678910@HystrixCommand(fallbackMethod = &quot;helloFallback&quot;,ignoreExceptions = ArithmeticException.class)public String helloService()&#123; int i = 1 / 0; return restTemplate.getForObject(&quot;http://EUREKA-CLIENT/dc&quot;, String.class);&#125;public String helloFallback(Throwable throwable)&#123; log.info(&quot;Exception:&#123;&#125;&quot;, throwable.getMessage()); return &quot;error&quot;;&#125; 然后再次调用,执行结果如下:这里可以看到,并没有服务降级,而是直接把错误抛给了用户. 这里的实现原理很简单，因为有一个名叫HystrixBadRequestException的异常不会进入到服务降级方法中去，当我们定义了ignoreExceptions为ArithmeticException.class之后，当抛出ArithmeticException异常时，Hystrix会将异常信息包装在HystrixBadRequestException里边然后再抛出，此时就不会触发服务降级方法了。]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-7-自定义Hystrix请求命令]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-Cloud-7-%E8%87%AA%E5%AE%9A%E4%B9%89Hystrix%E8%AF%B7%E6%B1%82%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[上文中,介绍了Hystrix断路器的简单使用,是通过注解的方式去实现的, 我们也可以通过继承类的方式来实现. 自定义HystrixCommand除了使用@HystrixCommand注解,也可以自定义类,继承HystrixCommand类. 如下:12345678910111213141516171819public class StringCommand extends HystrixCommand&lt;String&gt; &#123; private RestTemplate restTemplate; @Override protected String getFallback() &#123; return new String(&quot;StringCommand.getFallback&quot;); &#125; public StringCommand(Setter setter, RestTemplate restTemplate) &#123; super(setter); this.restTemplate = restTemplate; &#125; @Override protected String run() throws Exception &#123; return restTemplate.getForObject(&quot;http://EUREKA-CLIENT/dc&quot;, String.class); &#125;&#125; 创建StringCommand类,注入RestTemplate,然后重写getFallback(),run()这两个方法,还有创建一个构造方法. getFallback(): 服务调用失败时回调. run():执行请求时调用 构造方法:第一个参数主要用来保存一些分组信息 同步调用和异步调用StringCommand搞好后,就可以在controller中调用了,代码如下:12345678910111213@GetMapping(&quot;/testStringCommand&quot;)public String testStringCommand() throws ExecutionException, InterruptedException &#123; StringCommand stringCommand = new StringCommand(HystrixCommand.Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(&quot;&quot;)), restTemplate); // 同步调用// String str = stringCommand.execute();// log.info(&quot;同步调用:&#123;&#125;&quot;, str); // 异步调用 Future&lt;String&gt; queue = stringCommand.queue(); String str1 = queue.get(); return str1;&#125; 这里可以看到,有两种方式执行请求,第一个是调用stringCommand对象的execute()方法,发起一个同步请求, 第二种是调用queue()方法发起一个异步请求. 同步请求可以直接返回请求结果,而异步请求中,我们需要通过get()方法来获取请求结果. 测试开启注册中心,启动两个服务提供者应用,还是一个端口8081,一个8082,然后启动当前的消费者工程, 最后关掉8081端口的应用.此时访问http://localhost:9000/testStringCommand,就会间隔的看到以下界面: 第一个界面中的信息,就是我们在StringCommand类中定义的getFallback()方法返回的.就是服务调用失败的时候调用的 通过注解实现异步请求之前我们使用注解配置Hystrix,是这样的:1234@HystrixCommand(fallbackMethod = &quot;helloFallback&quot;)public String helloService()&#123; return restTemplate.getForObject(&quot;http://EUREKA-CLIENT/dc&quot;, String.class);&#125; 这是同步请求的方式,那么如何通过注解实现异步请求呢? 首先,我们需要在配置类(Config.java)中注入HystrixCommandAspect类,具体代码如下:1234@Beanpublic HystrixCommandAspect hystrixCommandAspect()&#123; return new HystrixCommandAspect();&#125; 然后通过AsyncResult来执行调用,还是使用@HystrixCommand,方法如下(在HelloService.java):123456789@HystrixCommandpublic Future&lt;String&gt; async()&#123; return new AsyncResult&lt;String&gt;() &#123; @Override public String invoke() &#123; return restTemplate.getForObject(&quot;http://EUREKA-CLIENT/dc&quot;, String.class); &#125; &#125;;&#125; 然后再controller中调用1234567@GetMapping(&quot;/asyncGetResult&quot;)public String asyncGetResult() throws ExecutionException, InterruptedException, TimeoutException &#123; Future&lt;String&gt; stringFuture = helloService.async(); // get()方法可以设置超时时长 return stringFuture.get(2, TimeUnit.SECONDS);&#125; 这里项目启动,然后第一次调用会报java.util.concurrent.TimeoutException: null,这个错.暂时没找到原因.]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-6-Hystrix断路器]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-Cloud-6-Hystrix%E6%96%AD%E8%B7%AF%E5%99%A8%2F</url>
    <content type="text"><![CDATA[概念在微服务架构中,我们把系统拆分成多个服务单元,各个单元都运行在不同的进程中,他们之前如果要交互的话,就是通过远程调用的方式,这样就有可以能会因为网络原因,或者自身服务出现故障或延迟,而导致调用方的服务也出现延迟,若此时调用方的请求不断增加,最后就会因为等待出现故障的依赖方影响形成任务积压,最终导致服务瘫痪. 在微服务架构中,存在着那么多的服务单元,若一个单元出现故障,就很容易因依赖关系而引发故障的蔓延,最终导致整个系统的瘫痪这样的架构相较传统架构更加不稳定.为了解决这样的问题,产生了断路器等一系列的服务保护机制. 在分布式系统中,断路器的作用就是,当某个服务单元发生故障之后,通过断路器的故障监控,向调用方返回一个错误响应,而不是长时间的等待,这样就不会使得线程因调用故障服务被长时间占用不释放,避免了故障在分布式系统中的蔓延. 实战实验服务注册中心启动eureka-server工程, 可以是单节点启动,也可以是高可用的方式启动 . 服务提供者启动两个服务提供者,也就是eureka-client工程,端口分别是8081,8082 服务消费者启动消费者工程consumer-ribbon,端口9000 在加入断路器之前,我们先来看一下没有断路器的效果, 把端口是8081的eureka-client先关掉, 然后浏览器访问 http://localhost:9000/consumer, 可以看到报500,错误信息是1I/O error on GET request for &quot;http://localhost:8081/dc&quot;: Connection refused: connect; nested exception is java.net.ConnectException: Connection refused: connect 就是连接失败 接下来,开始引入断路器 改造消费者工程在consumer-ribbon工程中,引入以下依赖:1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-ribbon&lt;/artifactId&gt;&lt;/dependency&gt; 在启动类中添加@EnableCircuitBreaker注解,启动断路器 然后改造服务消费方式,新建一个-HelloService类,在service中调用client的接口,具体代码如下:123456789101112131415@Servicepublic class HelloService &#123; @Autowired RestTemplate restTemplate; @HystrixCommand(fallbackMethod = &quot;helloFallback&quot;) public String helloService()&#123; return restTemplate.getForObject(&quot;http://EUREKA-CLIENT/dc&quot;, String.class); &#125; public String helloFallback()&#123; return &quot;error&quot;; &#125;&#125; 在helloService()方法上添加了@HystrixCommand注解, 是用来指定回调方法的,也就是helloFallback()方法. 然后,修改controller类,代码如下:123456789101112@RestController@Slf4jpublic class DcController &#123; @Autowired private HelloService helloService; @GetMapping(&quot;/consumer&quot;) public String consumer()&#123; return helloService.helloService(); &#125;&#125; 模拟服务关闭然后来验证一下,通过断路器实现的服务回调逻辑, 重新启动之前关闭的8081的client,此时两个提供者都已经启动了,然后访问两次http://localhost:9000/consumer,轮询了两个client,并返回了文字信息. 这时候再关闭8081的client,然后再访问http://localhost:9000/consumer,这时当轮询到8081的实例时, 返回的不再是之前的错误信息,而是error,说明Hystrix的服务回调生效了. 模拟服务阻塞接下来,模仿一下服务阻塞的情况,对eureka-client工程的/dc接口做一些修改,如下123456789101112@GetMapping(&quot;/dc&quot;)public String dc() throws InterruptedException &#123; String services = &quot;Services: &quot; + discoveryClient.getServices(); log.info(services); // 线程休息几秒 int sleepTime = new Random().nextInt(3000); log.info(&quot;sleepTime:&#123;&#125;&quot;, sleepTime); Thread.sleep(sleepTime); return services;&#125; 就是让线程休眠,时间是一个3000以内的随机数,由于Hystrix默认超时时间为2000毫秒，所以这里采用了0至3000的随机数以让处理过程有一定概率发生超时来触发断路器.为了更精准地观察断路器的触发,在消费者调用函数中做一些时间记录,具体如下:123456789101112@GetMapping(&quot;/consumer&quot;)public String consumer()&#123; long startTime = System.currentTimeMillis(); String result = helloService.helloService(); long endTime = System.currentTimeMillis(); log.info(&quot;Spend time:&#123;&#125;&quot;, endTime - startTime); return result;&#125; 重新启动client和consumer,连续访问http://localhost:9000/consumer几次,我们可以观察到,当consumer的控制台中输出的Spend time大于2000的时候,就会返回error,即服务消费者因调用的服务超时从而触发熔断请求,并调用回调逻辑返回结果]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-5-RestTemplate详解]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-Cloud-5-RestTemplate%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[RestTemplate上文中,我们使用RestTemplate实现了最简单的服务访问,并且通过配置@LoadBalanced实现了负载均衡. 接下来,我们来看一下,RestTemplate针对几种不同请求类型和参数类型的服务调用实现 GET 请求在RestTemplate中,对于GET请求,有两种方式: 第一种:getForEntity()该函数返回的是ResponseEntity,主要存储了HTTP的几个重要元素,比如HTTP请求状态码,父类HttpEntity中还包含了请求头HttpHeaders等信息, 有以下三种重载实现12345public &lt;T&gt; ResponseEntity&lt;T&gt; getForEntity(String url, Class&lt;T&gt; responseType, Object... uriVariables) throws RestClientException ;public &lt;T&gt; ResponseEntity&lt;T&gt; getForEntity(String url, Class&lt;T&gt; responseType, Map&lt;String, ?&gt; uriVariables) throws RestClientException ;public &lt;T&gt; ResponseEntity&lt;T&gt; getForEntity(URI url, Class&lt;T&gt; responseType) throws RestClientException ; 先看一下第一种,举个例子,我们要访问eureka-client的/dc接口,代码如下1restTemplate.getForEntity(&quot;http://EUREKA-CLIENT/dc?name=&#123;1&#125;&quot;, String.class, &quot;Mike&quot;); 第一个参数呢,就是url,需要传递参数可以使用占位符,第三个参数会替换url中的占位符, 第二个参数是接收的返回值的类型,比如我们希望返回值是string类型,那就是String.class,如果希望返回值是个User对象,那么就是User.class 第二种请求的方式,就是将参数放到了map对象中传过去.具体使用方式如下:123Map&lt;String,String&gt; map = new HashMap&lt;&gt;();map.put(&quot;name&quot;,&quot;Mike&quot;);restTemplate.getForEntity(&quot;http://EUREKA-CLIENT/dc?name=&#123;name&#125;&quot;, String.class, map); 第三种方法是使用URI对象来替代之前的url和参数,这里不做详细说明. 第二种:getForObject()对getForEntity的进一步封装, 对HTTP的请求响应体body内容进行对象转换,实现请求直接返回包装好的对象内容,比如1String result = restTemplate.getForObject(url, String.class); 当返回值是个user对象时,可以这样:1User user = restTemplate.getForObject(url, User.class); 如果我们只关心返回的body而不关系其他内容时,该函数就非常好用,可以少一个从Response中获取body的步骤,它与getForEntity函数类似,也提供了三种不同的重载实现,如下: 12345public &lt;T&gt; T getForObject(String url, Class&lt;T&gt; responseType, Object... uriVariables) throws RestClientException;public &lt;T&gt; T getForObject(String url, Class&lt;T&gt; responseType, Map&lt;String, ?&gt; uriVariables) throws RestClientException;public &lt;T&gt; T getForObject(URI url, Class&lt;T&gt; responseType) throws RestClientException; 跟getForEntity是一样的 POST 请求对于post请求,有三种方法可以使用 第一种:postForEntity()和getForEntity类似,调用后返回ResponseEntity\&lt;T>对象,有如下三种不同的重载方法:12345public &lt;T&gt; ResponseEntity&lt;T&gt; postForEntity(String url, Object request, Class&lt;T&gt; responseType, Object... uriVariables) throws RestClientException;public &lt;T&gt; ResponseEntity&lt;T&gt; postForEntity(String url, Object request, Class&lt;T&gt; responseType, Map&lt;String, ?&gt; uriVariables) throws RestClientException ;public &lt;T&gt; ResponseEntity&lt;T&gt; postForEntity(URI url, Object request, Class&lt;T&gt; responseType) throws RestClientException ; 这些函数的用法大部分与getForEntity一致,需要注意的是第二个参数Object request 该参数可以是一个普通对象,也可以是一个HttpEntity对象.如果是一个普通对象,而非HttpEntity对象的时候,RestTemplate会将请求对象转换为一个HttpEntity对象来处理,其中Object就是request的类型,request内容会被视作完整的body来处理,而如果request是一个HttpEntity对象,那么就会被当作一个完成的HTTP请求对象来处理,这个request中不仅包含了body的内容,也包含了header的内容. 第二种:postForObject()用法和getForObject()是一致的,只关心返回的body而不关系其他内容时,用这个就好12345public &lt;T&gt; T postForObject(String url, Object request, Class&lt;T&gt; responseType, Object... uriVariables) throws RestClientException;public &lt;T&gt; T postForObject(String url, Object request, Class&lt;T&gt; responseType, Map&lt;String, ?&gt; uriVariables) throws RestClientException;public &lt;T&gt; T postForObject(URI url, Object request, Class&lt;T&gt; responseType) throws RestClientException; 第三种:postForLocation()是用post请求提交资源,并返回新资源的uri. postForLocation的参数和前面两种方法的参数基本一致,只不过该方法的返回值为Uri,这个只需要服务提供者返回一个Uri即可,该Uri表示新资源的位置.123public URI postForLocation(String url, Object request, Object... uriVariables) throws RestClientException;public URI postForLocation(String url, Object request, Map&lt;String, ?&gt; uriVariables) throws RestClientException; PUT 请求put()方法PUT请求,可以通过put方法进行调用,如下:123User user = new User();Long id = 110L;restTemplate.put(&quot;http://EUREKA-CLIENT/dc&quot;, user, id); put()方法也有三种重载方法:12345public void put(String url, Object request, Object... uriVariables) throws RestClientException;public void put(String url, Object request, Map&lt;String, ?&gt; uriVariables) throws RestClientException;public void put(URI url, Object request) throws RestClientException; 因为是void类型的函数,所以也就不需要指定responseType参数,除此之外的其他传入参数定义与用法与postForObject基本一致. DELETE 请求DELETE请求可以通过delete()方法来进行调用 delete()方法例:1restTemplate.put(&quot;http://EUREKA-CLIENT/dc&quot;); delete()方法的三种重载如下:12345public void delete(String url, Object... uriVariables) throws RestClientException;public void delete(String url, Map&lt;String, ?&gt; uriVariables) throws RestClientException;public void delete(URI url) throws RestClientException;]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-4-服务发现与消费 Ribbon负载均衡]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-Cloud-4-%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0%E4%B8%8E%E6%B6%88%E8%B4%B9-Ribbon%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%2F</url>
    <content type="text"><![CDATA[之前的例子中,已经搭建起来了服务注册中心,包括单节点模式和高可用模式,同时呢服务提供者也有了,就是eureka-client,然后接下来尝试构建一个消费者. 服务消费者主要完成两个目标,发现服务以及消费服务. 其中,发现服务的任务是由eureka的客户端完成的,而服务消费的任务是由ribbon完成的. Ribbon是一个基于HTTP和TCP的客户端负载均衡器. 准备工作为了实验Ribbon的客户端负载均衡功能,所以我们以jar的方式启动两个服务提供者(eureka-client)12java -jar eureka-client.jar --server.port=8081java -jar eureka-client.jar --server.port=8082 启动完成后,从eureka信息面板中可以看到,已经有两个服务了 创建消费者工程新创建一个工程,取名为consumer-ribbon maven依赖maven引入依赖:123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-ribbon&lt;/artifactId&gt;&lt;/dependency&gt; application.yml配置然后配置application.yml123456789spring: application: name: eureka-consumerserver: port: 9000eureka: client: service-url: defaultZone: http://peer1:1111/eureka/,http://peer2:1112/eureka/ 服务注册中心的地址,要和我们eureka-client的地址是一样的,不然是发现不了该服务的. RestTemplate配置创建一个配置类Config,加入@Configuration注解然后创建RestTemplate的Spring实例, 具体如下:12345678910@Configurationpublic class Config &#123; @Bean @LoadBalanced public RestTemplate restTemplate()&#123; return new RestTemplate(); &#125;&#125; @LoadBalanced注解,是用来开启客户端负载均衡 Controller创建创建一个Controller,通过RestTemplate来对eureka-client提供的接口进行调用,具体代码如下:123456789101112@RestController@Slf4jpublic class DcController &#123; @Autowired private RestTemplate restTemplate; @GetMapping(&quot;/consumer&quot;) public String consumer()&#123; return restTemplate.getForObject(&quot;http://EUREKA-CLIENT/dc&quot;, String.class); &#125;&#125; 可以看到,这里的地址是服务名EUREKA-CLIENT,而不是一个具体的地址. 启动类最后,启动类加入注解@EnableDiscoveryClient,然后启动应用 在看一下eureka信息面板可以看到,除了之前的服务提供者(eureka-client)还多了一个EUREKA-CONSUMER的消费者应用. 测试访问 http://localhost:9000/consumer ,这时候可以打开两个eureka-client的控制台,可以看到,接口被调用的日志是在两个进程交替打印的,可以用来判断消费者对eureka-client的调用是否是负载均衡的]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-3-高可用注册中心]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-Cloud-3-%E9%AB%98%E5%8F%AF%E7%94%A8%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%2F</url>
    <content type="text"><![CDATA[在微服务架构这样的分布式环境中,我们需要充分考虑发生故障的情况,所以在生产环境中必须对各个组件进行高可用部署,对于微服务如此,对于服务注册中心也一样. Eureka Server的设计一开始就考虑了高可用问题,在eureka的服务治理设计中,所有节点既是服务提供方,也是服务消费方,注册中心也不例外.我们之前有设置过,让注册中心不注册自己,而高可用,实际上就是将自己作为服务,向服务注册中心注册自己,这样就可以形成一组互相注册的服务注册中心,以实现服务清单的互相同步,达到高可用的效果 搭建高可用服务注册中心我们在之前创建的那个eureka-server的工程基础上去做 增加配置文件首先,需要增加两个配置文件,先创建application-peer1.yml,作为peer1的服务中心的配置,将serviceUrl指向peer2,如下12345678server: port: 1111eureka: instance: hostname: peer1 client: service-url: defaultZone: http://peer2:1112/eureka/ 再创建application-peer2.yml,作为peer2的服务中心的配置,将serviceUrl指向peer1,如下:12345678server: port: 1112eureka: instance: hostname: peer2 client: service-url: defaultZone: http://peer1:1111/eureka/ 修改host文件为了peer1和peer2能够被正确访问到,我们需要在host文件中加入以下内容:12127.0.0.1 peer1 127.0.0.1 peer2 运行项目先打包,执行mvn package,打包完成之后,以jar包的方式来执行这个项目,分别启动peer1和peer2.12java -jar eureka-server-0.0.1-SNAPSHOT.jar --spring.profiles.active=peer1 java -jar eureka-server-0.0.1-SNAPSHOT.jar --spring.profiles.active=peer2 执行完成以后,我们的注册中心就启动了两个了,如图我们可以看到,在peer1的节点的DS replicas我们已经可以看到peer2节点了,在peer2的DS replicas中我们也可以看到peer1节点了. 直到这里,多节点的服务注册中心就搭建好了. 服务提供方的配置服务提供方(就是我们之前的eureka-client)还需要做一些简单的配置,才能将服务注册到Eureka Server集群中. 在eureka-client项目中,修改application.yml1234eureka: client: service-url: defaultZone: http://peer1:1111/eureka/,http://peer2:1112/eureka/ 就是在serviceUrl中添加了两个注册中心的地址,用逗号隔开,修改好之后,启动项目 通过访问localhost:1111 和 localhost:1112可以发现,eureka-client在两个服务中心都被注册了至此,一个高可用的服务注册中心就完成了. 然后,我们来思考这样一个问题,如果在client端的service-url,只配置了peer1的注册中心地址的话, 那peer2还会被注册嘛? 答案是会的,因为eureka可以通过在各个节点进行复制来共享服务清单, 但是如果peer1挂掉的情况下,就不会注册到peer2中,也就达不到高可用的目的.]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-2-Eureka Server & Client]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-Cloud-2-Eureka-Server-Client%2F</url>
    <content type="text"><![CDATA[搭建服务注册中心可以通过Spring Initializr来创建一个Spring boot的工程,命名为eureka-server,然后引入依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka-server&lt;/artifactId&gt;&lt;/dependency&gt; 然后在启动类中加入@EnableEurekaServer注解就ok了. 在默认的设置下,该服务注册中心也会将自己作为客户端来尝试注册它自己,所以我们需要禁用它的客户端注册行为.在application.yml中做如下配置:123456789101112131415spring: application: name: eureka-serverserver: port: 1111eureka: instance: hostname: localhost client:# eureka.client.register-with-eureka 由于该应用为注册中心,所以设置为false,代表不向注册中心注册自己. register-with-eureka: false# 注册中心的职责就是维护服务实例,并不需要去检索服务,所以也设置为false fetch-registry: false service-url: defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ 完成了上面的配置后,启动应用,访问http://localhost:1111/.就可以看到Eureka信息面板.如下: 如图,Instances currently registered with Eureka栏是空的,说明该注册中心还没有注册任何服务 注册服务提供者注册中心搭建好之后,来创建一个client端,向注册中心发布自己还是先创建一个项目,命名为eureka-client,修改pom引入依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt;&lt;/dependency&gt; 启动类添加注解@EnableEurekaClient,然后修改配置文件application.yml,如下:123456789spring: application: name: eureka-clientserver: port: 2001eureka: client: service-url: defaultZone: http://localhost:1111/eureka/ 这里再创建一个controller,用来测试,以及后面使用 12345678910111213@RestController@Slf4jpublic class DcController &#123; @Autowired private DiscoveryClient discoveryClient; @GetMapping(&quot;/dc&quot;) public String dc()&#123; String services = &quot;Services: &quot; + discoveryClient.getServices(); log.info(services); return services; &#125;&#125; 然后,启动项目,再次到eureka面板中看下,这个client已经注册进来了 自我保护模式完成之后,面板中可能会出现一行红字,内容是:1EMERGENCY! EUREKA MAY BE INCORRECTLY CLAIMING INSTANCES ARE UP WHEN THEY&apos;RE NOT. RENEWALS ARE LESSER THAN THRESHOLD AND HENCE THE INSTANCES ARE NOT BEING EXPIRED JUST TO BE SAFE. 这个是在server中出现的一个报警,为什么会出现这个报警呢? 因为Eureka Server端和client端采用的是心跳机制,server会不停的去检查client是否上线,在一定时间server会统计client的存活率,当存活率低于一定值的时候就会出现这个报警.意思就是这个client上线率太低了,可能我现在都不知道你是上线还是下线,但是虽然我不知道但是我会当做是你上线,其实是一种自我保护. 自我保护机制的工作机制是如果在15分钟内超过85%的客户端节点都没有正常的心跳,那么Eureka就认为客户端与注册中心出现了网络故障,Eureka Server自动进入自我保护机制,此时会出现以下几种情况: Eureka Server不再从注册列表中移除因为长时间没收到心跳而应该过期的服务 Eureka Server仍然能够接受新服务的注册和查询请求,但是不会被同步到其它节点上,保证当前节点依然可用 当网络稳定时,当前Eureka Server新的注册信息会被同步到其它节点中 如何关闭?在开发环境可以将这个设置关闭，避免当你在调用微服务的时候显示上线 但是实际是下线 在server端的配置文件中,加入以下配置:123eureka: server: enable-self-preservation: false 在生产环境中建议打开此配置]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-1-服务治理与Eureka]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-Cloud-1-%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86%E4%B8%8EEureka%2F</url>
    <content type="text"><![CDATA[服务治理服务治理可以说是微服务架构中最为核心和基础的模块,它主要用来实现各个微服务实例的自动化注册与发现. 那么为什么需要服务治理呢? 举个例子,某个系统在最初开始构建微服务系统的时候可能服务并不多,我们可以通过做一些静态配置来完成服务的调用.比如,有两个服务 A 和 B, 其中服务 A 需要调用服务 B来完成一个业务操作时,为了实现服务 B 的高可用,不论采用服务端负载均衡还是客户端负载均衡,都需要手工维护服务 B 的具体实例清单.但是随着业务的发展,系统功能越来越复杂,相应的微服务应用也不断增加,我们的静态配置就会变得越来越难以维护.并且面对不断发展的业务,我们的集群规模,服务的位置, 服务的命名等都有可能发生变化,如果还是通过手工维护的方式,那么极易发生错误或是命名冲突等问题.同时,对于这类静态内容的维护也必将消耗大量的人力. 通过服务注册和服务发现来完成对微服务应用实例的自动化管理 服务注册在服务治理框架中,通常都会构建一个注册中心,每个服务单元向注册中心登记自己提供的服务,将主机和端口号,版本号,通信协议等一些附加信息告知注册中心,注册中心按服务名分类,组织服务清单. 比如,我们有两个提供服务A的进程分别运行于192.168.0.100:8000和192.168.0.101:8000位置上,另外还有三个提供服务B的进程分别运行于192.168.0.100:9000,192.168.0.101:9000,192.168.0.102:9000位置上.当这些进程均启动,并向注册中心注册自己的服务之后,注册中心就会维护类似下面的一个服务清单. 服务名 位置 服务A 192.168.0.100:8000,192.168.0.101:8000 服务B 192.168.0.100:9000,192.168.0.101:9000,192.168.0.102:9000 另外,服务注册中心还需要以心跳的方式去监测清单中的服务是否可用,若不可用,需要从服务清单中剔除,达到排除故障服务的效果. 服务发现由于在服务治理框架下运作,服务间的调用不再通过指定具体的实例地址来实现,而是通过向服务名发起请求调用实现.所以,服务调用方在调用服务提供方接口的时候,并不知道具体的服务实例位置.因此,调用方需要向服务注册中心咨询服务,并获取所有服务的实例清单,以实现对具体服务实例的访问. 比如,现有服务C希望调用服务A,服务C就需要向注册中心发起咨询服务请求,服务注册中心就会将服务A的位置清单返回给服务C,比如按上个例子中服务A的情况,C便获得了服务A的两个可用位置192.168.0.100:8000和192.168.0.101:8000.当服务C要发起调用的时候,便从该清单中以某种轮询策略取出一个位置来进行服务调用,这就是后续我们将会介绍的客户端负载均衡.这里我们只是列举了一种简单的服务治理逻辑,以方便理解服务治理框架的基本运行思路. 实际的框架为了性能等因素, 不会采用每次都向服务注册中心获取服务的方式.不同的应用场景在缓存和服务剔除等机制上也会有一些不同的实现策略. Netflix EurekaEureka是Netflix开发的服务发现框架,Spring Cloud将它集成在自己的子项目spring-cloud-netflix中,实现SpringCloud的服务发现功能. 为什么要使用Eureka ? 因为在一个完整的系统架构中,任何单点的服务都不能保证不会中断,因此我们需要服务发现机制,在某个节点中断后,其它的节点能够继续提供服务,从而保证整个系统是高可用的. 服务发现有两种模式:一种是客户端发现模式,一种是服务端发现模式.Erueka采用的是客户端发现模式. Eureka Server会提供服务注册服务,各个服务节点启动后,会在Eureka Server中进行注册,这样Eureka Server中就有了所有服务节点的信息,并且Eureka有监控页面,可以在页面中直观的看到所有注册的服务的情况. 同时Eureka有心跳机制,当某个节点服务在规定时间内没有发送心跳信号时,Eureka会从服务注册表中把这个服务节点移除. Eureka还提供了客户端缓存的机制,即使所有的Eureka Server都挂掉,客户端仍可以利用缓存中的信息调用服务节点的服务. Eureka一般配合Ribbon进行使用,Ribbon提供了客户端负载均衡的功能,Ribbon利用从Eureka中读取到的服务信息,在调用服务节点提供的服务时,会合理的进行负载. Eureka通过心跳检测、健康检查、客户端缓存等机制,保证了系统具有高可用和灵活性.]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud-0-什么是微服务架构]]></title>
    <url>%2F2019%2F04%2F10%2FSpring-Cloud-0-%E4%BB%80%E4%B9%88%E6%98%AF%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[什么是微服务架构简单说,微服务是系统架构上的一种设计风格,它的主旨是将一个原本独立的系统拆分成多个小型服务,这些小型服务在各自的进程中运行,服务与服务之间通过HTTP的RESTful API进行通信. 比如说一个商城,可以将购买流程拆分成 登陆 选择商品 下单 商品发货 订单完成 等多个服务.被拆分的每个服务都围绕系统中的某一项或者耦合度较高的业务来构建的,并且每个服务都维护着自身的数据存储,业务开发,测试以及独立的部署机制.有了轻量级的通信协议做基础所以每个服务可以用不同的语言开发. 微服务和分布式的区别分布式是将一个大的系统划分为多个业务模块,分别部署到不同的机器上,各个业务模块之间通过接口进行数据交互.区别分布式的方式是根据不同机器不同业务.微服务与分布式的细微差别是,微服务的应用不一定是分散在多个服务器上,他也可以是同一个服务器. 与单体架构的区别在传统的系统架构中,一个系统大致分为三部分:数据库,服务端,前端.为了应对不同的业务,我们将业务分为不同的业务模块,同时随着移动端设备的进步,前端展示形式已经不局限于web形式这对于后端向前端接口的支持,会需要更多的接口模块,在加上越来越多的业务需求使得单体应用越来越臃肿. 这个时候单体应用的问题就凸显出来: 由于单体系统部署在一个进程,往往我们修改了一个很小的功能 然后部署上线这个时候就会影响其他业务模块. 由于单体这些模块的使用场景,并发量,消耗的资源类型各不相同但是对资源的利用又互相影响,使得我们对各个业务模块的容量很难给出较为准确的评估,使得维护成本变大且越来越难以控制. Spring Cloud简介Spring Cloud是一个基于Spring Boot实现的微服务架构开发工具. 它为微服务架构中涉及的配置管理,服务治理,断路器,智能路由,微代理,控制总线,全局锁,决策竞选,分布式会话和集群状态管理等操作提供了一种简单的开发方式.]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
      <tags>
        <tag>Spring Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-110-production mode下启动时的bootstrap check]]></title>
    <url>%2F2019%2F04%2F10%2FElasticsearch-110-production-mode%E4%B8%8B%E5%90%AF%E5%8A%A8%E6%97%B6%E7%9A%84bootstrap-check%2F</url>
    <content type="text"><![CDATA[bootstrap check经常会碰到一些es的用户,遇到一些奇怪的问题,主要是因为他们没有配置一些重要的设置.在es以前的老版本中,对这些设置错误的配置,会在日志里记录一些warning告警.但是有时候用户会忽略这些日志中的告警信息.为了确保说这些设置的错误配置告警信息可以引起用户的注意,es的新版本中引入了bootstrap check,也就是启动时检查. 这些启动时检查操作,会检查许多es和系统的设置,将这些配置的值跟es期望的安全值去进行比较.如果es在development mode下,那么失败的检查仅仅在日志中打印warning.如果es运行在生产模式下,任何启动时检查的失败都会导致es拒绝启动. development mode 和 production mode默认情况下,es绑定到localhost hostname,来进行http和内部通信.这对于下载es并简单试用一下,包括日常的开发,都是非常方便的,但是对于生产环境是不行的 如果要组建一个es集群,es实例必须能够通过内部通信协议互相连通,所必须绑定通信到一个外部的接口上. 因此如果一个es实例没有绑定通信到外部接口(默认情况下),那么就认为es是处于开发模式下.反之,如果绑定通信到外部接口,那么就是处于生产模式下. 我们可以通过http.host和transport.host,单独配置http的传输.这就可以配置一个es实例通过http可达,但是却不触发生产模式. 因为有时用户需要将通信绑定到外部解耦来测试client的调用.对于这种场景,es提供了single-node恢复模式(将discovery.type设置为single-node),配置过后,一个节点会选举自己作为master,而且不会跟其他任何节点组成集群. 如果在生产模式下运行一个single node实例,就可以规避掉启动时检查(不要将通信绑定到外部接口,或者将通信绑定到外部接口,但是设置discovery type为single-node).在这种场景下,可以设置es.enforce.bootstrap.checks为true(通过jvm参数来设置),来强制bootstrap check的执行. heap size check如果jvm启动的时候设置的初始堆大小和最大堆大小不同,可能会导致es运行期间的暂停,因为jvm堆在系统运行期间可能会改变大小.为了避免这种jvm resize导致的es进程暂停,建议启动jvm时,将初始堆大小和最大堆大小设置的相等. 除此之外,如果bootstrap.memory_lock被启用了,jvm会在启动期间锁定jvm的初始大小. 如果要通过heap size check,就必须合理设置heap size.默认情况下,es的jvm堆的最小和最大大小都是2g.如果在生产环境中使用,应该配置合理的heap size确保es有足够的堆内存可以使用. 在jvm.options中设置的Xms和Xmx会用来分配jvm堆内存大小. 这些设置的值依赖于服务器上可用的总内存大小.下面是一些最佳实践的建议: 将heap的最小和最大大小设置为一样大 es有更多的heap大小,就有更多的内存用来进行缓存,但是过大的jvm heap可能会导致长时间的gc停顿 不要设置最大heap size超过物理内存的50%,给核心的file system cache留下足够的内存 不要将Xmx设置超过32GB,否则jvm无法启用compressed oops,将对象指针进行压缩,确认启动日志里compressed oops显示开启 更好的选择是,heap size设置的小于zero-based compressed 也就是26GB,但是有时也可以是30GB.通过-XX:+UnlockDiagnosticVMOptions和-XX:+PrintCompressedOopsMode开启,确认有heap address: 0x000000011be00000, size: 27648 MB, zero based Compressed Oops,而不是heap address: 0x0000000118400000, size: 28672 MB, Compressed Oops with base: 0x00000001183ff000 在jvm.options文件中,可以通过-Xms2g和-Xmx2g来配置heap size 也可以通过ES_JAVA_OPTS环境变量来设置heap size file descriptor checkfile descriptor是unix操作系统的一种数据结构,用来track打开的文件.在unix操作系统中,所有东西都是file.比如,file可以是物理文件,虚拟文件,或者网络socket.es需要大量的file descriptor,比如说每个shard都由多个segment和其他文件组成,还有跟其他节点之间的网络通信连接. 因为es要使用大量的file descriptor,所以如果file descriptor耗尽的话,会是一场灾难,甚至可能会导致数据丢失.尽量给es的file descriptor提升到65536,甚至更高. 可以在/etc/security/limits.conf中,设置nofile为65536 可以用以下请求检查每个node上的file descriptor数量1GET _nodes/stats/process?filter_path=**.max_file_descriptors lucene会使用大量的文件,同时es也会使用大量的socket在节点间和client间进行通信,这些都是需要大量的file descriptor的.但是通常来说,现在的linux操作系统,都是给每个进程默认的1024个file descriptor的,这对于一个es进程来说是远远不够的. 我们需要将es进程的file descriptor增加到非常非常大,比如说65535个.一般需要根据我们的操作系统的文档来查看如何设置file descriptor 然后可以直接对es集群查看GET,来确认file descriptor的数量:123456789101112131415161718192021222324252627&#123; &quot;cluster_name&quot;: &quot;elasticsearch&quot;, &quot;nodes&quot;: &#123; &quot;nLd81iLsRcqmah-cuHAbaQ&quot;: &#123; &quot;timestamp&quot;: 1471516160318, &quot;name&quot;: &quot;Marsha Rosenberg&quot;, &quot;transport_address&quot;: &quot;127.0.0.1:9300&quot;, &quot;host&quot;: &quot;127.0.0.1&quot;, &quot;ip&quot;: [ &quot;127.0.0.1:9300&quot;, &quot;NONE&quot; ], &quot;process&quot;: &#123; &quot;timestamp&quot;: 1471516160318, &quot;open_file_descriptors&quot;: 155, &quot;max_file_descriptors&quot;: 10240, &quot;cpu&quot;: &#123; &quot;percent&quot;: 0, &quot;total_in_millis&quot;: 25084 &#125;, &quot;mem&quot;: &#123; &quot;total_virtual_in_bytes&quot;: 5221900288 &#125; &#125; &#125; &#125;&#125; memory lock check如果jvm进行一个major gc的话,那么就会涉及到heap中的每一个内存页,此时如果任何一个内存页被swap到了磁盘上,那么此时就会被swap回内存中.这就会导致很多的磁盘读写开销,而这些磁盘读写开销如果节省下来,可以让es服务更多的请求.有很多方法可以配置系统禁止swap.其中一种方法就是让jvm去lock heap内存在物理内存中,设置bootstrap.memory_lock即可. 然后通过以下命令可以检查mlockall是否开启了:1GET _nodes?filter_path=**.mlockall 如果发现mlockall是false,那么意味着mlockall请求失败了.会看到一行日志,unable to lock jvm memory. 最大可能的原因,就是在linux系统中,启动es进程的用户没有权限去lock memory,需要通过以下方式进行授权:1ulimit -l unlimited 另外一个原因可能是临时目录使用noexec option来mount了.可以通过指定一个新的临时目录来解决1export ES_JAVA_OPTS=&quot;$ES_JAVA_OPTS -Djava.io.tmpdir=/path/to/temp/dir&quot; 当然也可以通过在jvm.options文件中来设置这个参数 maximum number of thread checkes会将每个请求拆分成多个stage,然后将stage分配到不同的线程池中去执行.在es中有多个线程池来执行不同的任务.所以es会创建许多的线程.最大线程数量的检查会确保说,es实例有权限去创建足够的线程.如果要通过这个检查,必须允许es进程能够创建超过2048个线程. /etc/security/limits.conf,在这个文件中,用nproc来设置 maximum size virtual memory checkes使用mmap来将索引映射到es的address space中,这可以让jvm heap外但是内存中的索引数据,可以有非常高速的读写速度.因此es需要拥有unlimited address space.最大虚拟内存大小的检查,会要求es进程有unlimited address space. /etc/security/limits.conf,设置as为unlimited maximum map count check要高效使用mmap的话,es同样要求创建许多memory-mapped area.因此要求linux内核允许进程拥有至少262144个memory-mapped area,需要通过sysctl设置vm.max_map_count至少超过262144. client jvm checkjvm有两种模式,client jvm和server jvm.不同的jvm会用不同的编译器来从java源码生成可执行机器代码.client jvm被优化了来减少startup time和内存占用,server jvm被优化了来最大化性能.两种jvm之间的性能区别是很明显的.client jvm check会确保es没有运行在client jvm下.必须使用server jvm模式来启动es,而server jvm是默认的. use serial collector check针对不同的工作负载,jvm提供了不同的垃圾回收器.串行化垃圾回收期对于单cpu机器或者小内存,是很有效的.但是对于es来说,用串行化垃圾回收器,会成为一场性能上的灾难.因此这个check会确保es没有被配置使用串行化垃圾回收器.es默认的就是cms垃圾回收器. system call filter checkes会根据不同的操作系统来安装system call filter,用来阻止执行作为defense机制的fork相关system call,进而避免任意代码执行的攻击.这个check会检查是否允许system call filter,然后安装这些system call filter.避免bootstrap.system_call_filter设置为false. OnError and OnOutOfMemoryError checkjvm参数,OnError和OnOutOfMemoryError允许在jvm遇到了fatal error或者是OutOfMemoryErro的时候,执行我们预定义的命令.然而,默认情况下,es system call filter是启用的,这些filter是阻止forking操作的.因此,用OnError和OnOutOfMemroyError和system call filter是不兼容的.这个check会检查,如果启用了system call filter,还设置了这两个jvm option,那么就不能启动.所以不要在jvm option中设置这两个参数. early-access checkjdk提供了early-access快照,为即将到来的版本.这些版本不适合用作生产环境.这个check会检查有没有使用jdk的early-access快照版本.我们应该用jdk稳定版本,而不是试用版本. G1 GC checkjdk 8的jvm早期版本中的g1 gc,有已知的问题可能导致索引破损.在JDK 8u40之前的版本都有这个问题.这个check会检查是否使用了那种早期的JDk版本.]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-109-生产集群部署时对操作系统的一些设置]]></title>
    <url>%2F2019%2F04%2F10%2FElasticsearch-109-%E7%94%9F%E4%BA%A7%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%97%B6%E5%AF%B9%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[概述理想情况下,es应该单独在一个服务器上运行,能够使用服务器上的所有资源.为了达到上述目标,我们需要配置操作系统,来允许用户运行es并且获取比默认情况下更多的资源. 在生产环境中下面的一些设置必须配置一下: 禁止swapping 确保拥有足够的虚拟内存 确保拥有足够的线程数量 默认情况下,es会假设你是在开发模式下运行的.如果上面的任何配置没有正确的设置,那么会输出一些warning到日志文件中,但是我们还是可以启动es进程的. 但是如果我们配置了网络设置,比如network.host,es会认为我们是运行在生产环境中的,然后就会将上述warning升级为exception.这些exception会阻止我们的es节点启动.这是一个重要的安全保障措施来确保我们不会因为错误的配置了es server,而导致数据丢失. 配置系统设置在/etc/security/limits.conf中,可以配置系统设置 也可以用ulimit临时配置系统设置,在linux操作系统中,ulimit可以用来临时的改变资源限制.通常需要用root权限来设置ulimit.举例,如果要设置file descriptor为65536,可以用如下的命令来设置:1ulimit -n 65536 但是在linux操作系统中,实际上永久性的资源限制可以通过编辑/etc/security/limits.conf文件来设置.比如要设置file descriptor,可以再limits.conf中加入下面的行:1elasticsearch - nofile 65536 在下一次elasticsearch用户开启一个新的会话时就会生效 设置jvm option一般建议通过jvm.options配置文件来设置es的jvm option.默认的地址是config/jvm.options,每行是一个jvm argument 此外,如也可以通过ES_JAVA_OPTS环境变量来设置jvm option,比如下面的命令:1export ES_JAVA_OPTS=&quot;$ES_JAVA_OPTS -Djava.io.tmpdir=/path/to/temp/dir&quot; 禁止swapping大多数操作系统都会使用尽量多的内存来进行file system cache,并且尽量将不经常使用的java应用的内存swap到磁盘中去.这会导致jvm heap的部分内存,甚至是用来执行代码的内存页被swap到磁盘中去. swapping对于性能来说是非常差劲的,为了es节点的稳定性考虑,应该尽量避免这种swapping.因为swapping会导致gc过程从毫秒级变成分钟级,在gc的时候需要将内存从磁盘中swapping到内存里,特别耗时,这会导致es节点响应请求变得很慢,甚至导致es node跟cluster失联.在一个弹性的分布式系统中,让操作系统kill掉某一个节点,是很高效的. 有三种方法可以disable swapping.推荐的option是彻底禁用swap,如果做不到的化,也得尽量最小化swappiness的影响,比如通过lock memory的方法. 禁用所有的swapping file通常来说,es进程会在一个节点上单独运行,那么es进程的内存使用是由jvm option控制的. 可以使用下面的命令临时性禁止swap1swapoff -a 要永久性的禁止swap,需要修改/etc/fstab文件,然后将所有包含swap的行都注释掉 配置swappiness另外一个方法就是通过sysctl,将vm.swappiness设置为1,这可以尽量减少linux内核swap的倾向,在正常情况下,就不会进行swap,但是在紧急情况下,还是会进行swap操作.1sysctl -w vm.swappiness=1 启用bootstrap.memory_lock最后一个选项,就是用mlockall,将es jvm进程的address space锁定在内存中,阻止es内存被swap out到磁盘上去. 在config/elasticsearch.yml中,可以配置:1bootstrap.memory_lock: true 然后通过以下命令可以检查mlockall是否开启了:1GET _nodes?filter_path=**.mlockall 如果发现mlockall是false,那么意味着mlockall请求失败了.会看到一行日志,unable to lock jvm memory. 最大可能的原因,就是在linux系统中,启动es进程的用户没有权限去lock memory,需要通过以下方式进行授权:1ulimit -l unlimited 另外一个原因可能是临时目录使用noexec option来mount了.可以通过指定一个新的临时目录来解决1export ES_JAVA_OPTS=&quot;$ES_JAVA_OPTS -Djava.io.tmpdir=/path/to/temp/dir&quot; 当然也可以通过在jvm.options文件中来设置java.io.tmpdir 虚拟内存es使用hybrid mmapfs / niofs目录来存储index数据,操作系统的默认mmap count限制是很低的,可能会导致内存耗尽的异常. 需要提升mmap count的限制:1sysctl -w vm.max_map_count=262144 如果要永久性设置这个值,要修改/etc/sysctl.conf,将vm.max_map_count的值修改一下,重启过后,用sysctl vm.max_map_count来验证一下数值是否修改成功 es同时会用NioFS和MMapFS来处理不同的文件,我们需要设置最大的map刷另,这样我们才能有足够的虚拟内存来给mmapped文件使用,可以用sysctl来设置:1sysctl -w vm.max_map_count=262144 还可以再/etc/sysctl.conf中,对vm.max_map_count来设置. 设置线程的数量es用了很多线程池来应对不同类型的操作,在需要的时候创建新的线程是很重要的.要确保es用户能创建的最大线程数量至少在2048以上. 可以通过ulimit -u 2048来临时设置,也可以在/etc/security/limits.conf中设置nproc为2048来永久性设置.]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-108-Jvm和服务器内存分配的最佳实践以及原理分析]]></title>
    <url>%2F2019%2F04%2F10%2FElasticsearch-108-Jvm%E5%92%8C%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%E4%BB%A5%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[概述前文中,说了一些针对es集群可能会出现的一些问题的配置,本文来说一下如何合理的对es进行内存的分配,为什么? Jvm Heap分配es默认会给jvm heap分配2个G的大小,对于几乎所有的生产环境来说,这个内存都太小了.如果用这个默认的heap size,那么生产环境的集群肯定表现不会太好. 调节方式有以下两个方式来调节es中的jvm heap size. 第一种方式就是设置环境变量,ES_HEAP_SIZE.当es进程启动的时候,会读取这个环境变量的值,然后设置为jvm的heap size.举例来说,可以这样来设置:export ES_HEAP_SIZE=10g. 第二种方式是在启动es进程的时候,传递一个jvm的option,比如:ES_JAVA_OPTS=&quot;-Xms10g -Xmx10g&quot; ./bin/elasticsearch,但是要注意-Xms和-Xmx最小和最大堆内存一定设置的一样,避免运行过程中的jvm heap resize,那会是一个非常耗时的过程. 在老版本的es中,比如es 2.x里面,一般推荐用ES_HEAP_SIZE环境变量的方式来设置jvm heap size. 在新版本的es中,比如es 5.x里面,一般推荐在jvm.options文件里面去设置jvm相关的参数. 机器内存分配一个常见的问题就是将es进程的jvm heap size设置的过于大了.比如我们有一台64G的机器,可能我们甚至想要给es jvm size设置64G内存.但是这是错误的. 虽然heap对于es来说是非常重要的,jvm heap被es用来存放很多内存中的数据结构来提供更快的操作性能.但是还有另外一个内存的用户,那就是lucene. lucene的设计就是要使用底层的os filesystem cache来缓存数据结构.lucene的segment是保存在单独的文件中的.因为这些segment是不可变的,所以这些文件实际上也从来不会改变.这样的话,就可以更好的缓存这些文件,底层的os cache会将hot segment驻留在内存中以供更快的访问. 这些segment包括了倒排索引(为了全文检索)以及正排索引(为了聚合操作).lucene的性能是严重依赖于底层的os cache的,但是如果我们给了过多的内存到es的jvm heap,那么就没有足够的内存留给lucene.这会极大的影响性能. 这里想告诉大家的是,就是说,es的性能很大的一块,其实是由有多少内存留给操作系统的os cache,供lucene去缓存索引文件,来决定的.所以说lucene的os cache有多少是非常重要的. 分配建议一般建议的是,将50%的内存分配给es jvm heap,然后留50%的内存给os cache. 留给os cache的内存是不会不使用的,lucene会将剩下的内存全部用光,用来cache segment file. 如果我们没有对任何分词的text field进行聚合操作,那么我们就不需要使用fielddata,我们甚至可以考虑给os cache更多的内存,因为fielddata是要用jvm heap.如果我们给jvm heap更少的内存,那么实际上es的性能反而会更好,因为更多的内存留给了lucene用os cache提升索引读写性能,同时es的jvm heap的gc耗时会更少. 不要给jvm分配超过32G内存还有另外一个原因不要将过多的内存分配给es的jvm heap. 如果heap小于32G的话,jvm会用一种技术来压缩对象的指针,object pointer.在java中,所有的对象都会被分配到heap中,然后被一个pointer给引用.object pointer会指向heap中的对象,引用的是二进制格式的地址. 对于32位的系统来说,jvm最大的heap size就是4G,为什么呢?二进制的值只有0和1,在32位的操作系统中,0和1在32位的组合是2^32次方的字节,除以1024就是多少k,再除以1024就是多少mb,再除以1024就是多少gb,最后算下来就是4G. 对于64位的系统来说,heap size可以更大,但是64位的object pointer会耗费更多的空间,因为object pointer更大了.比浪费更多内存空间更恶劣的是,过大的object pointer会在cpu,main memory和LLC、L1等多级缓存间移动数据的时候,吃掉更多的带宽. 所以jvm用了一种技术,叫做compressed oops来解决object pointer耗费过大空间的问题.这个技术的核心思想是,不要让object pointer引用内存中的二进制地址,而是让object pointer引用object offset.这就意味着32位的pointer可以引用400万个对象,而不是400万字节.这也意味着,使用32位的pointer,最大的heap大小可以到32G.此时只要heap size在32G以内,jvm就会自动启用32位的object pointer,因为32位的对象指针,足够引用32G的内存了,就可以用32位的pointer替代64位的pointer.但是32位的pointer比64位的pointer可以耗费更少的内存耗费. 如果你给jvm heap分配的内存小于32G,此时jvm会自动使用32位的object pointer,同时是让pointer指向对象的offset,32位的object pointer就足以引用32G的内存,同时32位的pointer占用的内存空间很少,对cpu和memory之间移动数据的带宽开销也很少.这个过程就叫做compressed oops. 但是一旦我们越过了32G这个界限,就是给jvm heap分配了超过32G的内存,比较坑了.就没有办法用32位的pointer和引用object offset的模式了,因为32位的pointer最多引用32G的内存,超过了32G,就没法用32位pointer.不用32位pointer,就只能用64位pointer,才能引用超过32G的内存空间.此时pointer就会退回到传统的object pointer引用对象的二进制地址的模式,此时object pinter的大小会急剧增长,更多的cpu到内存的带宽会被占据,更多的内存被耗费.实际上,不用compressed oops时,你如果给jvm heap分配了一个40~50G的内存的可用空间,实际上被object pointer可能都要占据十几G的内存空间,可用的空间量,可能跟使用了compressed oops时的32GB内存的可用空间,20多个G,几乎是一样的. 因此,即使我们有很多内存,但是还是要分配给heap在32GB以内,否则的话浪费更多的内存,降低cpu性能,而且会让jvm回收更大的heap. 综上所述,如果你给jvm heap分配超过32G的内存,实际上是没有什么意义的,因为用64位的pointer,1/3的内存都给object pointer给占据了,这段内存空间就浪费掉了.还不如分配32G以内,启用compressed oops,可用空间跟你分配50个G的内存,是一样的. 所以也正是因为32G的限制,一般来说,都是建议说,如果你的es要处理的数据量上亿的话,几亿,或者十亿以内的规模的话,建议,就是用64G的内存的机器比较合适,有个5台,差不多也够了.给jvm heap分配32G,留下32G给os cache. 在32G以内的话具体应该设置heap为多大？这个是根据具体情况而定的,不是固定死的,根据不同的jvm和平台而变.一般而言,将jvm heap size设置为31G比较安全一些.主要是要确保说,你设置的这个jvm heap大小,可以让es启用compressed oops这种优化机制. 此外,可以给jvm option加入-XX:+PrintFlagsFinal,然后可以打印出来UseCompressedOops是否为true.这就可以让我们找到最佳的内存设置.因为可以不断调节内存大小,然后观察是否启用compressed oops. 举例来说,如果在mac os上启动一个java 1.7,同时将heap size设置为32600mb,那么compressed oops是会开启的；但是如果设置为32766m,compressed oops就不会开启.相反的是,使用jdk 1.8的话,分配32766m,compressed oops是会开启的,设置为32767m,就不会开启.所以说,这个东西不是固定的.根据不同的操作系统以及jvm版本而定. 在es启动日志中,我们可以查看compressed oops是否开启,比如下面的字样:1[2015-12-16 13:53:33,417][INFO ][env] [Illyana Rasputin] heap size [989.8mb], compressed ordinary object pointers [true]. 对于超大内存机器的分配假设我们有一个服务器,内存达到了512G或者128G等,该怎么办? 首先es官方是建议避免用这种服务器来部署es集群的,但是如果我们只有这种机器可以用的话,我们要考虑以下几点: 我们是否在做大量的全文检索?考虑一下分配4~32G的内存给es进程,同时给lucene留下其余所有的内存用来做os filesystem cache.所有的剩余的内存都会用来cache segment file,而且可以提供非常高性能的搜索,几乎所有的数据都是可以在内存中缓存的,es集群的性能会非常高 是否在做大量的排序或者聚合操作?聚合操作是不是针对数字,日期或者未分词的string?如果是的话,那么还是给es 4~32G的内存即可,其他的留给es filesystem cache,可以将聚合好用的正排索引,doc values放在os cache中 如果在针对分词的string做大量的排序或聚合操作?如果是的话,那么就需要使用fielddata,这就得给jvm heap分配更大的内存空间.此时不建议运行一个节点在机器上,而是运行多个节点在一台机器上,那么如果我们的服务器有128G的内存,可以运行两个es节点,然后每个节点分配32G的内存,剩下64G留给os cache.如果在一台机器上运行多个es node,建议设置:cluster.routing.allocation.same_shard.host: true.这会避免在同一台物理机上分配一个primary shard和它的replica shard. swapping如果频繁的将es进程的内存swap到磁盘上,绝对会是一个服务器的性能杀手.想象一下,内存中的操作都是要求快速完成的,如果需要将内存页的数据从磁盘swap回main memory的话,性能会有多差.如果内存被swap到了磁盘,那么可能100微秒的操作会瞬间变成10毫秒,那么如果是大量的这种内存操作呢?这会导致性能急剧下降. 因此通常建议彻底关闭机器上的swap,swapoff -a,如果要永久性关闭,需要在/etc/fstab中配置 如果没法完全关闭swap,那么可以尝试调低swappiness至,这个值是控制os会如何将内存swap到磁盘的.这会在正常情况下阻止swap,但是在紧急情况下,还是会swap.一般用sysctl来设置,vm.swappiness = 1.如果swappiness也不能设置,那么就需要启用mlockall,这就可以让我们的jvm lock住自己的内存不被swap到磁盘上去,在elasticsearch.yml中可以设置:bootstrap.mlockall: true.]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-107-不要随意调节JVM和Thread Pool的原因]]></title>
    <url>%2F2019%2F04%2F10%2FElasticsearch-107-%E4%B8%8D%E8%A6%81%E9%9A%8F%E6%84%8F%E8%B0%83%E8%8A%82JVM%E5%92%8CThread-Pool%E7%9A%84%E5%8E%9F%E5%9B%A0%2F</url>
    <content type="text"><![CDATA[概述es中有很多配置我们可以去调节,但是在99.99%的情况下,对于es来说,大部分的参数保留为默认的就可以了,因为这些参数经常被滥用和错误的调节,继而导致严重的稳定性问题以及性能的急剧下降. Jvm GCes默认用的垃圾回收器是CMS,jvm使用垃圾回收器来释放掉不用的内存,千万不要去调节默认的垃圾回收行为. CMS回收器是并发式的回收器,能够跟应用程序工作线程并发工作,最大程度减少垃圾回收时的服务停顿时间.但是CMS还是会有两个停顿阶段,同时在回收特别大的heap时也会有一些问题.尽管有一些缺点,但是CMS对于要求低延时请求响应的软件来说,还是最佳的垃圾回收器,因此官方的推荐就是使用CMS垃圾回收器. 有一种最新的垃圾回收器叫做G1.G1回收器可以比CMS提供更少的回收停顿时间,而且能够这对大heap有更好的回收表现.它会将heap划分为多个region,然后自动预测哪个region会有最多可以回收的空间.通过回收那些region,就可以最小化停顿时长,而且可以针对大heap进行回收.听起来还挺不错的,但是不幸的是,G1还是比较年轻的一种垃圾回收器,而且经常会发现一些新的bug,这些bug可能会导致jvm挂掉.lucene的测试套件就检查出来了G1的一些bug.因此es官方不推荐现在使用G1垃圾回收器,也许在不久的未来,等G1更加稳定的时候,可以使用G1. Threadpool每个人都很喜欢去调优线程池,而且大部分人都特别喜欢增加线程池的线程数量,无论是大量的写入,还是大量的搜索,或者是感觉服务器的cpu idle空闲率太高,都会增加更多的线程.在es中,默认的threadpool设置是非常合理的,对于所有的threadpool来说,除了搜索的线程池,都是线程数量设置的跟cpu core一样多的.如果我们有8个cpu core,那么就可以并行运行8个线程.那么对于大部分的线程池来说,分配8个线程就是最合理的数量. 不过搜索会有一个更加大的threadpool,一般被配置为：cpu core * 3 / 2 + 1. 也许我们会觉得有些线程可能会因为磁盘IO等操作block住,所以我们需要更多的线程.但是在es中这并不是一个问题,大多数的磁盘IO操作都是由lucene的线程管理的,而不是由es管理的,因此es的线程不需要关心这个问题.此外,threadpool还会通过在彼此之间传递任务来协作执行,我们不需要担心某一个网络线程会因为等待一次磁盘写操作,而导致自己被block住,无法处理网络请求.网络线程可以将那个磁盘写操作交给其他线程池去执行,然后自己接着回来处理网络请求. 其实我们的进程的计算能力是有限的,分配更多的线程只会强迫cpu在多个线程上下文之间频繁来回切换.一个cpu core在同一时间只能运行一条线程,所以如果cpu要切换到另外一个线程去执行,需要将当前的state保存起来,然后加载其他的线程进来执行.如果线程上下文切换发生在一个cpu core内,那么还好一些,但是如果在多个cpu core之间发生线程上下文切换,那么还需要走一个cpu core内部的通信.这种线程上下文切换会消耗掉很多的cpu资源,对于现在的cpu来说,每次线程上下文切换,都会导致30微秒的时间开销,所以宁愿将这些时间花费在任务的处理上. 很多人会将threadpool大小设置为一些很愚蠢的数值,在一个8核的机器上,可能运行了超过60,100,甚至1000个线程.这么多的线程会导致cpu资源利用率很低.所以下次如果我们要调节线程池的话,记住,千万别这么干.如果一定要调节线程数量,也得记住要根据你的cpu core数量来调节,比如设置为cpu core的两倍,如果设置的再多,那么就是一种浪费了.]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-106-针对集群重启时shard恢复耗时过长问题的参数设置]]></title>
    <url>%2F2019%2F04%2F10%2FElasticsearch-106-%E9%92%88%E5%AF%B9%E9%9B%86%E7%BE%A4%E9%87%8D%E5%90%AF%E6%97%B6shard%E6%81%A2%E5%A4%8D%E8%80%97%E6%97%B6%E8%BF%87%E9%95%BF%E9%97%AE%E9%A2%98%E7%9A%84%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[问题在集群重启的时候,有一些配置会影响shard的恢复,首先,我们需要理解默认配置下,shard恢复过程会发生什么事情. 假设集群中有10个node,每个node上面有一个shard, 可能是primary shard也可能是replica shard,就比如说我们有一个index,有5个primary shard,每个primary shard对应有一个replica shard 如果我们将整个集群关闭了进行一些维护性的操作,比如给机器安装新的磁盘之类的事情.当我们重启集群的时候,肯定节点是一个接一个的启动的,可能会出现5个节点先启动了,然后剩下5个节点还没启动. 也许是因为剩下的5个节点还没来得及启动,或者是因为一些原因耽搁了,总之不管是什么原因,就是现在只有5个节点是在线的.这5个节点会通过gossip协议互相通信,选举出一个master,然后组成一个集群.他们会发现数据没有被均匀的分布,因为有5个节点没有启动,那么那5个节点上的shard就是不可用的,集群中就少了一半的shard.此时在线的5个node就会将部分replica shard提升为primary shard,同时为每个primary shard复制足够的replica shard. 最后,可能剩下的5个节点启动好了,加入了集群,但是这些节点发现本来是他们持有的shard已经被重新复制并且放在之前的5个node上了,此时他们就会删除自己本地的数据,然后集群又会开始进行shard的rebalance操作,将最早启动的5个node上的shard均匀分布到后来启动的5个node上去. 在这个过程中,这些shard重新复制,移动,删除,再次移动的过程,会大量的耗费网络和磁盘资源.对于数据量庞大的集群来说,可能导致每次集群重启时,都有TB级别的数据无端移动,可能导致集群启动会耗费很长时间.但是如果所有的节点都可以等待整个集群中的所有节点都完全上线之后,所有的数据都有了以后,再决定是否要复制和移动shard,情况就会好很多. 图解就上面的例子来说,假设10个node,每个node上面一个shard,如下图: 这时,我们的集群维护重启,然后上面的5个node先启动了,如图: 下面的5个node启动起来之前,集群发现primary shard不是都在,就会认为某些primary shard宕机了,此时,那些primary shard对应的replica shard就会提升为primary shard 接下来,集群又会发现,没有任何一个replica shard是存在的,因为我们定义的是每个primary shard都要有一个replica shard, 此时又会进行复制操作,结果如下图: 当剩下的5个node启动起来了,如下: 这时候,集群会发现shard已经过剩了,有primary shard也有replica shard,这时候,后面启动起来的这些node会把自己的shard删除,如下: 删除之后呢,集群又发现shard分配不均匀,因为上面的5个node上每个都有两个shard,下面的5个node一个都没有,此时就会触发rebalance操作, rebalance之后,就又会变成每个node上面一个shard,和集群重启之前的状态一样,如下: 所引发的问题在上面的重启过程中一共引发了如下几个问题: 平白无故多复制了5个shard出来 后上线的5个node将自己本地的数据删除 最后又有5个shard做了网络传输和移动 解决方案问题我们现在知道了,那么就可以通过一些设置来解决这个问题,首先需要设置一个参数gateway.recover_after_nodes,这个参数意思是让es有足够的node上线之后,再开始shard recovery的过程,所以这个参数的值是跟具体的集群相关的,根据集群中的节点数量去设置. 此外,还应该设置一个集群中至少要有多少个node和等待那些node的时间,通过gateway.expected_nodes和gateway.recover_after_time这两个参数去设置 假设我们的配置如下:123gateway.recover_after_nodes: 8 gateway.expected_nodes: 10gateway.recover_after_time: 5m 就是说在es集群中,等待至少8个节点都在线,然后最多等待5分钟,或者说10个节点都在线的时候,再开始shard recovery的过程,这样可以避免少数node启动的时候就立即开始shard recovery,消耗大量网络和磁盘资源]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-105-针对集群脑裂问题定制的重要参数]]></title>
    <url>%2F2019%2F04%2F09%2FElasticsearch-105-%E9%92%88%E5%AF%B9%E9%9B%86%E7%BE%A4%E8%84%91%E8%A3%82%E9%97%AE%E9%A2%98%E5%AE%9A%E5%88%B6%E7%9A%84%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[概念首先,来认识一下什么是集群脑裂,假设我们现在有一个集群,里面有三个节点,一个是master两个data,如下图: 假设现在出现了网络故障,导致一个集群被划分成了两片,两片中的节点无法相互通信,如下图: 如图就出现了两个netword partition, 图中在左边这个network partition里面的data node因为连接不上master node了,可能就会发起一次选举,将自己选举成为master node,如下图: 这样,集群就可能出现了两个master,因为master是集群中的非常重要的角色,主宰了集群状态的维护,以及shard的分配,因此,如果有两个master的话,可能会导致数据被破坏, 这就是集群的脑裂问题 discovery.zen.minimum_master_nodes参数discovery.zen.minimum_master_nodes,这个参数对于集群的可靠性来说是非常重要的,同时这个设置也可以预防脑裂问题,也就是一个集群中出现两个master 这个参数的作用,就是告诉ES,直到有足够的master候选节点的时候,才可以选举出一个master,否则就不选举出master. quorum这个参数必须被设置为集群中master候选节点的quorum的数量,至于quorum的算法,就是:master候选节点数量 / 2 + 1; 举例说明假设我们现在有10个节点,都能维护数据,也可以是master候选节点,那么quorum就是 10 / 2 + 1 = 6. 假设现在有三个master节点,还有100个数据节点,那么quorum就是 3 / 2 + 1 = 2. 上面这两个例子都没有问题, 那么再来看一个例子 quorum注意事项假设现在有两个节点,都是master候选节点,那么quorum就是 2 / 2 + 1 = 2, 此时就会有问题了,因为如果有一个node挂掉后,剩下的这一个master候选节点,是不满足quorum数量的,也就无法选举出新的master节点了,此时整个集群就挂掉了,所以我们只能将参数设置为1, 但是这样的话,就无法阻止脑裂问题的发生了. 来详细看一下,假如两个节点quorum设置为2的情况下,如图 此时,假设这个master节点宕机了,如图: 也就是说,集群中只剩下一个存活的节点了,而我们的quorum设置的是2,那么就不会发起master选举,这个集群也就挂掉了 那么再来看一下两个节点,quorum设置为1的情况. 如图: 假设出现了network partition,如上图中,这两个节点不能相互通信了,此时我们quorum设置为1,就导致data node所在的network partition还有一个master候选节点,他自己还是可以发起选举,然后变成master 如上图,集群中又出现了两个master,导致了脑裂问题 综上所述,一个生产环境的es集群,至少要有3个节点,同时将discovery.zen.minimum_master_nodes这个参数设置为quorum,也就是2 执行原理下面来思考一个问题,在三个es节点的集群中,discovery.zen.minimum_master_nodes = 2的情况下,是如何避免脑裂现象产生的? 假设集群环境如下图: 出现网络分区无非有以下两种情况: 第一种情况,如下图: master节点被分在了一个network partition,另外两个节点在另一个network partition, 对于上面的现在这个master而言,已经没有足够数量的候选节点连接他了(包括自己在内,必须有两个候选节点) 所以此时,这个master node的master身份会解除掉,尝试重新发起master选举,但是因为master候选节点不够,所以无法发起选举,他就是个data node了 再来看一下下面这个网络分区,两个data node都无法连接master了,而且此时这个网络分区中的master候选节点数量达到了要求,2个,就可以发起选举, 此时,下面这个网络分区中的某一个data node被选举成了master node, 如下: 第二种情况,如下图:一个master node 和一个data node在一个网络分区内,剩下的一个data node在一个网络分区内. 先看一下左边的这个网络分区, 因为有足够的master候选节点,两个,所以不会对master有任何影响,右边这个网络分区内,这个data node虽然无法连接master,会尝试发起master选举,但是没有足够的master候选节点,所以无法选举成功, 这样就确保说这个不会有两个master的出现 设置方法说了这么多,那这个参数怎么设置呢 在elasticsearch.yml中配置discovery.zen.minimum_master_nodes: 2,每一个节点都需要配置. 在我们的es集群中节点是可以动态的增加和下线的,所以quorum可能会随时改变,所以这个参数也可以通过api去随时修改,特别是在节点上线下线的时候,都要做出相应的更改,一旦修改过后,这个配置就会持久化保存下来 api请求如下:123456PUT /_cluster/settings&#123; &quot;persistent&quot; : &#123; &quot;discovery.zen.minimum_master_nodes&quot; : 2 &#125;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-104-生产集群必须配置的一些重要参数]]></title>
    <url>%2F2019%2F01%2F25%2FElasticsearch-104-%E7%94%9F%E4%BA%A7%E9%9B%86%E7%BE%A4%E5%BF%85%E9%A1%BB%E9%85%8D%E7%BD%AE%E7%9A%84%E4%B8%80%E4%BA%9B%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[es的默认参数es的默认参数是非常好的,适合绝大多数的情况,尤其是一些性能相关的配置.因此刚开始部署一个生产环境下的es集群时,几乎所有的配置参数都可以用默认的设置.有很多的生产环境场景下,都是因为es集群管理人员自己去调整es的某些配置,结果导致集群出现了严重的故障. 比如mysql或者oracle这种关系型数据库,也许是需要非常重的调优,但是es是真的不用.如果我们现在面临着一些es的性能问题,通常建议的解决方案是更好的进行数据结构的布局,或者增加更多的节点和机器资源.在es的性能调优中,真的很少有那种magic knobs,就是某个参数一调节,直接性能提升上百倍.即使有这种参数,es官方也早就将其设置为默认的最佳值了. 但是在生产环境中,还是有极少数跟公司和业务相关的配置是需要我们修改的.这些设置都是具体的公司和业务相关联的,是没法预先给予最好的默认配置的. 集群名称和节点名称的配置默认情况下,es会启动一个名称为elasticsearch的集群.通常建议一定要将自己的集群名称重新进行命名,主要是避免公司网络环境中,也许某个开发人员的开发机会无意中加入你的集群. 此外,每个node启动的时候,es也会分配一个随机的名称.这个也不适合在生产环境中,因为这会导致我们没法记住每台机器.而且每次重启节点都会随机分配,就导致node名称每次重启都会变化.因此通常我们在生产环境中是需要给每个node都分配一个名称的. 这两个名称的配置在上文中已经配置过了 文件路径配置主要是数据文件的路径,日志路径和插件路径,这里的配置是很重要的默认情况下,es会将plugin,log,还有data,config,这些文件都放在es 的安装目录中,这有一个问题,就是在进行es升级的时候,可能会导致这些目录被覆盖掉.导致我们丢失之前安装好的plugin,已有的log,还有已有的数据,以及配置好的配置文件. 所以一般建议在生产环境中,必须将这些重要的文件路径,都重新设置一下,放在es安装目录之外的路径 path.data用于设置数据文件的目录 path.logs用于设置日志文件的目录 path.plugins用于设置插件存放的目录 path.data可以指定多个目录,用逗号分隔即可(multiple data path).如果多个目录在不同的磁盘上,那么这就是一个最简单的RAID 0的方式,将数据在本地进行条带化存储了,可以提升整体的磁盘读写性能.es会自动将数据在多个磁盘的多个目录中条带化存储数据. 在RAID 0的存储级别下,每个磁盘上会存储一部分数据,但是如果一个磁盘故障了,那么可能导致这台机器上的部分数据就丢失了.如果我们的es是有replica的,那么在其他机器上还是会有一份副本的.如果data file指定了多个目录,为了尽量减少数据丢失的风险,es会将某个shard的数据都分配到一个磁盘上去.这就意味着每个shard都仅仅会放在一个磁盘上.es不会将一个shard的数据条带化存储到多个磁盘上去,因为如果一个磁盘丢失了,就会导致整个shard数据丢失. 虽然multiple data path是一个很有用的功能,但是es毕竟不是一个专门的RAID软件.如果我们要对RAID存储策略进行更多的配置,提高存储的健壮性以及灵活性,还是要用专门的RAID软件来进行机器的磁盘数据存储,而不是用multiple data path策略. 综上所述,multiple data path功能在实际的生产环境中,其实是较少使用的. 然后我们来实际配置一下,先创建以下几个目录1234mkdir -p /var/log/elasticsearchmkdir -p /var/data/elasticsearchmkdir -p /var/plugin/elasticsearchmkdir -p /etc/elasticsearch 一般情况下,建议就存放在这几个目录中 目录创建好了之后,修改elasticsearch.yml里面的配置,将以下的配置copy进去就ok123path.logs: /var/log/elasticsearchpath.data: /var/data/elasticsearchpath.plugins: /var/plugin/elasticsearch 日志配置es使用log4j2来记录日志,log4j2可以通过log4j2.properties文件来进行配置.比如下面的这份配置文件:123456789101112131415161718# 配置了appender类型是RollingFileappender.rolling.type = RollingFile appender.rolling.name = rolling# 配置了日志路径是/var/log/elasticsearch/production.logappender.rolling.fileName = $&#123;sys:es.logs.base_path&#125;$&#123;sys:file.separator&#125;$&#123;sys:es.logs.cluster_name&#125;.log appender.rolling.layout.type = PatternLayoutappender.rolling.layout.pattern = [%d&#123;ISO8601&#125;][%-5p][%-25c] %.10000m%nappender.rolling.filePattern = $&#123;sys:es.logs.base_path&#125;$&#123;sys:file.separator&#125;$&#123;sys:es.logs.cluster_name&#125;-%d&#123;yyyy-MM-dd&#125;.log appender.rolling.policies.type = Policies# 配置了用基于时间的roll策略appender.rolling.policies.time.type = TimeBasedTriggeringPolicy # 这个设置了每天一份日志文件appender.rolling.policies.time.interval = 1 # 这个设置了根据自然天来划分文件,而不是24小时appender.rolling.policies.time.modulate = true 我们可以在config/log4j2.properties中加入以下的配置1234567891011121314# 配置了默认的DefaultRolloverStrategyappender.rolling.strategy.type = DefaultRolloverStrategy # 配置了Delete action,在rollover之后,就会删除文件appender.rolling.strategy.action.type = Delete # 配置了es log的基础路径appender.rolling.strategy.action.basepath = $&#123;sys:es.logs.base_path&#125; # 配置了rollover发生的条件,是基于IfLastModifiedappender.rolling.strategy.action.condition.type = IfLastModified # 配置了保留的天数,这里是7天appender.rolling.strategy.action.condition.age = 7D # 配置了删除匹配7天前的文件appender.rolling.strategy.action.PathConditions.type = IfFileName # 配置了一个删除文件的格式,这样就只是删除过期日志文件,但是不要删除慢查询日志appender.rolling.strategy.action.PathConditions.glob = $&#123;sys:es.logs.cluster_name&#125;-* 移动配置文件到自定义的目录日志文件以及配置文件修改好之后,把config目录下的所有文件都复制到之前创建好的 /etc/elasticsearch目录里面123cp elasticsearch.yml /etc/elasticsearch/cp jvm.options /etc/elasticsearch/cp log4j2.properties /etc/elasticsearch/]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-103-集群部署及zen discovery集群发现机制]]></title>
    <url>%2F2019%2F01%2F25%2FElasticsearch-103-%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E5%8F%8Azen-discovery%E9%9B%86%E7%BE%A4%E5%8F%91%E7%8E%B0%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[下载解压在elasticsearch的github上下载elasticsearch的v5.5.0版本部署, github传送门 下载的是tar.gz版本的,下载好之后,上传到服务器,解压,我这里上传到了/usr/elasticsearch目录下面1cd /usr/elasticsearch 解压1tar -zxvf elasticsearch-5.5.0.tar.gz 在三台虚拟机上重复上面的步骤 目录结构es安装包的目录结构大致如下: bin：存放es的一些可执行脚本,比如用于启动进程的elasticsearch命令,以及用于安装插件的elasticsearch-plugin插件 conf：用于存放es的配置文件,比如elasticsearch.yml data：用于存放es的数据文件,就是每个索引的shard的数据文件 logs：用于存放es的日志文件 plugins：用于存放es的插件 script：用于存放一些脚本文件 master node和data nodees是一种peer to peer.也就是p2p点对点的分布式系统架构,而不是采用的那种master-slave主从架构的分布式系统,集群中的每个node是直接跟其他节点进行通信的,几乎所有的API操作,比如index,delete,search,等等,都不是说client跟master通信,而是client跟任何一个node进行通信,那个node再将请求转发给对应的node来进行执行.这块的原理,路由,之前有说过了 es的node分为两种,master node 和data node,正常情况下只有一个master node,master node的责任就是负责维护整个集群的状态信息,也就是一些集群元数据信息,同时在有新的node加入集群或者说有node从集群中下线的时候重新分配shard,或者是创建或删除了一个索引.包括每次cluster state如果有改变的化,那么master都会负责将集群状态同步给所有的node. master node负责接收所有的cluster state相关的变化信息,然后将这个改变后的最新的cluster state推送给集群中所有的data node,除了master之外的node,也就是data node就是负责数据的存储和读写的,写入索引,搜索数据 如果要让多个node组成一个es集群,首先第一个要设置的参数,就是cluster.name,多个node的cluster.name如果一样,才满足组成一个集群的基本条件. (elasticsearch.yml中设置) 这个cluster.name的默认值是elasticsearch,在生产环境中,一定要修改这个值,否则可能会导致未知的node无端加入集群,造成集群运行异常. master eligible node配置node的时候,是可以配置多个 master eligible node的,但是只是说,从这些master eligible node中选举一个node出来作为master node,其他的master eligible node只是在master node有故障的时候,可以接替他的资格,但是还是去作为data node去使用的 . 一般建议是设置三个 master eligible node即可,elasticsearch.yml中设置node.master: true,node.data: false, node.data设置为false的话这个节点就只能做master的候选 而不会去做索引的读写等操作,剩下的node做为data node,设置为 node.master: false,node.data:true. 如果你的节点数量小于10个,小集群,那所有的node就不要做上面的这些额外的配置了,默认情况下既是master eligible node,同时也是data node 集群发现机制我们在每台机器上都部署了一个es,每台机器都启动一个es进程,那么怎样才能让多台机器上的es相互发现对方,从而组成一个集群呢? 默认情况下,es进程会绑定在自己的回环地址上,也就是127.0.0.1,然后扫描本机上的9300~9305端口号,尝试跟那些端口上启动的其他es进程进行通信,然后组成一个集群.这对于在本机上搭建es集群的开发环境是很方便的. 就比如说如果我们在windows上启动两个es的话 他会自己组成一个集群 但是在生产环境下这样肯定是没有用的,一般不会在一个机器上部署多个节点,所以需要将每台es进程绑定在一个非回环的ip地址上,才能跟其他节点进行通信,同时需要使用集群发现机制来跟其他节点上的es node进行通信. 集群环境中,需要让节点绑定到一个非回环的ip地址上,一般会配置：network.host: 192.168.1.10 一旦我们配置了network.host,那么es就会认为我们从开发模式迁移到生产模式,同时会启用一系列的bootstrap check. 在生产环境中的多台机器上部署es集群,就涉及到了es的discovery机制,也就是集群中各个节点互相发现然后组成一个集群的机制,同时discovery机制也负责es集群的master选举 zen discovery集群发现机制es中默认的discovery机制,就是zen discovery机制 zen discovery机制提供了unicast discovery集群发现机制,集群发现时的节点间通信是依赖的transport module,也就是es底层的网络通信模块和协议. unicastes默认配置为使用unicast集群发现机制,以让经过特殊配置的节点可以组成一个集群,而不是随便哪个节点都可以组成一个集群.但是默认配置下,unicast是本机,也就是localhost,因此只能在一台机器上启动多个node来组成一个集群.虽然es还是会提供multicast plugin作为一个发现机制,但是已经不建议在生产环境中使用了.虽然我们可能想要multicast的简单性,就是所有的node可以再接收到一条multicast ping之后就立即自动加入集群.但是multicast机制有很多的问题,而且很脆弱,比如网络有轻微的调整,就可能导致节点无法发现对方.因此现在建议在生产环境中用unicast机制,提供一个es种子node作为中转路由节点就可以了. unicast discovery集群发现机制是要求配置一个主机列表,用来作为gossip(流言式)通信协议的路由器.这些机器如果通过hostname来指定,那么在ping的时候会被解析为ip地址.unicast discovery机制最重要的两个配置如下所示:1hosts：用逗号分割的主机列表 1hosts.resolve_timeout：hostname被DNS解析为ip地址的timeout等待时长 简单来说,如果要让多个节点发现对方并且组成一个集群,那么就得有一个中间的公共节点,然后不同的节点就发送请求到这些公共节点,接着通过这些公共节点交换各自的信息,进而让所有的node感知到其他的node存在,并且进行通信,最后组成一个集群.这就是基于gossip流言式通信协议的unicast集群发现机制. 当一个node与unicast node list中的一个成员通信之后,就会接收到一份完整的集群状态,这里会列出集群中所有的node.接着那个node再通过cluster state跟master通信,并且加入集群中.这就意味着,我们的unicast list node是不需要列出集群中的所有节点的.只要提供少数几个node,比如3个,让新的node可以连接上即可.如果我们给集群中分配了几个节点作为专门的master节点,那么只要列出我们那三个专门的master节点即可.用如下的配置即可:1discovery.zen.ping.unicast.hosts: [&quot;host1&quot;, &quot;host2:port&quot;] 然后这边在我们的集群里面配置 一下就是1discovery.zen.ping.unicast.hosts: [&quot;elasticsearch01&quot;, &quot;elasticsearch02&quot;,&quot;elasticsearch03&quot;] 我们是部署了三台机器,然后要把三个机器上的es的elasticsearch.yml都修改掉 master选举在ping发现过程中,为集群选举出一个master也是很重要的,es集群会自动完成这个操作.这里建议设置discovery.zen.ping_timeout参数(默认是3s),如果因为网络慢或者拥塞,导致master选举超时,那么可以增加这个参数,确保集群启动的稳定性. 在完成一个集群的master选举之后,每次一个新的node加入集群,都会发送一个join request到master node,可以设置discovery.zen.join_timeout保证node稳定加入集群,增加join的timeout等待时长,如果一次join不上,默认会重试20次. 如果master node被停止了,或者自己宕机了,那么集群中的node会再次进行一次ping过程,并且选举出一个新的master.如果discovery.zen.master_election.ignore_non_master_pings设置为了true,那么会强制区分master候选节点,如果node的node.master设置为了false,还来发送ping请求参与master选举,那么这些node会被忽略掉,因为他们没有资格参与. discovery.zen.minimum_master_nodes参数用于设置对于一个新选举的master,要求必须有多少个master候选node去连接那个新选举的master.而且还用于设置一个集群中必须拥有的master候选node.如果这些要求没有被满足,那么master node就会被停止,然后会重新选举一个新的master.这个参数必须设置为我们的master候选node的quorum数量. 一般避免说只有两个master候选node,因为2的quorum还是2.如果在那个情况下,任何一个master候选节点宕机了,集群就无法正常运作了. 修改elasticsearch.yml这些参数了解完之后,修改elasticsearch.yml的配置 首先,修改每台虚拟机中的es的集群名称和节点名称1cluster.name: cluster-elasticsearch-prod 1node.name: node-elasticsearch01 三台机器的集群名称是相同的,然后节点名称不同 然后配置discovery zen的配置,3台机器中都是一样的123discovery.zen.ping.unicast.hosts: [&quot;elasticsearch01&quot;,&quot;elasticsearch02&quot;,&quot;elasticsearch03&quot;]discovery.zen.ping_timeout: 30s discovery.zen.join_timeout: 60s 修改network.host,绑定到非回环的地址上去, 三台机器各自设置各自的ip地址即可1network.host: 本机ip地址 这些改完之后就初步配置好了 各个节点,首先通过network.host绑定到了非回环的ip地址,从而可以跟其他节点通信 通过discovery.zen.ping.unicast.hosts配置了一批unicast中间路由的node 所有node都可以发送ping消息到路由node,再从路由node获取cluster state回来 接着所有node会选举出一个master 所有node都会跟master进行通信,然后加入master的集群 要求cluster.name必须一样,才能组成一个集群 node.name就标识出了每个node我们自己设置的一个名称 集群故障探查es有两种集群故障探查机制,第一种是通过master进行的,master会ping集群中所有的其他node,确保它们是否是存活着的.第二种,每个node都会去ping master node来确保master node是存活的,否则就会发起一个选举过程. 有下面三个参数用来配置集群故障的探查过程123ping_interval：每隔多长时间会ping一次node,默认是1sping_timeout：每次ping的timeout等待时长是多长时间,默认是30sping_retries：如果一个node被ping多少次都失败了,就会认为node故障,默认是3次 一般情况不需要去修改,用默认的就ok了 集群状态更新master node是集群中唯一一个可以对cluster state进行更新的node. master node每次会处理一个集群状态的更新事件,应用这次状态更新,然后将更新后的状态发布到集群中所有的node上去. 每个node都会接收publish message,然后ack这个message,但是不会应用这个更新.如果master没有在discovery.zen.commit_timeout指定的时间内(默认是30s),从至少discovery.zen.minimum_master_nodes个节点获取ack响应,那么这次cluster state change事件就会被reject,不会应用. 但是一旦在指定时间内,指定数量的node都返回了ack消息,那么cluster state就会被commit,然后一个message会被发送给所有的node.所有的node接收到那个commit message之后,接着才会将之前接收到的集群状态应用到自己本地的状态副本中去.接着master会等待所有节点再次响应是否更新自己本地副本状态成功,在一个等待超时时长内,如果接收到了响应,那么就会继续处理内存queue中保存的下一个更新状态.discovery.zen.publish_timeout默认是30s,这个超时等待时长是从plublish cluster state开始计算的. 不因为master宕机阻塞集群操作如果要让集群正常运转,那么必须有一个master,还有discovery.zen.minimum_master_nodes指定数量的master候选node,都在运行.discovery.zen.no_master_block可以控制当master宕机时,什么样的操作应该被拒绝.有下面两个选项: all:一旦master当即,那么所有的操作都会被拒绝 write:这是默认的选项,所有的写操作都会被拒绝,但是读操作是被允许的]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-102-虚拟机搭建集群环境配置]]></title>
    <url>%2F2019%2F01%2F25%2FElasticsearch-102-%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%90%AD%E5%BB%BA%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[配置要求搭建一个4个节点的es集群,需要用到4台虚拟机,每个虚拟机的配置是2核4G 创建虚拟机可以用virtual box 或者 VM,然后下载CentOS 7 64位的镜像,然后分配4G内存,其他配置按默认的就ok 配置网络虚拟机创建好之后,配置网络,配置一个静态的ip,具体配置方法可以看这里 传送门 配置好之后,试试主机和虚拟之间能不能相互ping通就好了 关闭防火墙1systemctl stop firewalld.service 1systemctl disable firewalld.service host配置配置本机的hostname 和 ip的映射比如虚拟机的ip是192.168.1.110 名称是elasticsearch01就在host中配置1192.168.1.110 elasticsearch01 创建的是4个虚拟机,每个虚拟机的host中都要把4个的ip 名称都写进去比如1234192.168.1.110 elasticsearch01192.168.1.111 elasticsearch02192.168.1.112 elasticsearch03192.168.1.113 elasticsearch04 安装java安装1.8的版本,然后配置好环境变量,用java -version测试是否安装成功. 安装教程传送门 配置4台机器免密码通信首先,需要在三台机器上配置对本机的ssh免密码登录 生成本机的公钥,过程中不断敲回车即可,ssh-keygen命令默认会将公钥放在/root/.ssh目录下1ssh-keygen -t rsa 将公钥复制为authorized_keys文件,此时使用ssh连接本机就不需要输入密码了12cd /root/.sshcp id_rsa.pub authorized_keys 接着配置四台机器互相之间的ssh免密码登录 将本机的公钥拷贝到指定机器的authorized_keys文件中1ssh-copy-id -i hostname 都复制完了之后可以用下面的命令来测试是不是免密码登录1ssh hostname]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-101-生产集群部署服务器配置等建议]]></title>
    <url>%2F2019%2F01%2F24%2FElasticsearch-101-%E7%94%9F%E4%BA%A7%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E7%AD%89%E5%BB%BA%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[述我们之前一直是在windows环境下去启动一个单节点的es进程,如果是在生产环境中,肯定是要部署在linux集群上面了 那么部署es的服务器对配置有什么要求呢 内存es是很吃内存的,es吃的主要不是你的jvm的内存,一般来说es用jvm heap(堆内存)还是用的比较少的,主要吃的是你的机器的内存 es底层基于lucene,lucene是基于磁盘文件来读写和保存你的索引数据的,比如正排索引 倒排索引这些,lucene的特点就是会基于os filesystem cache,会尽量将频繁访问的磁盘文件的数据在操作系统的内存中进行缓存,然后尽量提升磁盘文件读写的性能 es的性能80%取决于说,你的机器上,除了分配给jvm heap内存以外,还剩下多少内存,剩下的内存会留给es的磁盘索引文件做缓存,如果os cache能够缓存更多的磁盘文件的数据,索引文件的数据,索引读写的性能都会高很多,特别是检索 但是如果你的大量的索引文件在os cache中放不下,还是停留在磁盘上,那么搜索 聚合的时候大量的都是读写磁盘文件,性能就会很低了 如果说在es生产环境中,哪种资源最容易耗尽,那么就是内存了.排序和聚合都会耗费掉很多jvm 堆内存,所以给es进程分配足够的jvm heap内存是很重要的.除了给jvm heap分配内存,还需要给予足够的内存给os filesystem cache.因为lucene用的数据结构都是给予磁盘的格式,es是通过os cache来进行高性能的磁盘文件读写的. 一般而言,除非说你的数据量很小,比如就是一些OA系统,各种信息管理系统,要做一个内部的检索引擎,有几万,几十万,几百万的数据量,对机器的资源配置要求还是蛮低的.一般而言,如果你的数据量过亿,几亿,几十亿.那么其实建议你的每台机器都给64G的内存的量. 如果一个机器有64G的内存,那么是比较理想的状态,但是32GB和16GB的内存也是ok的.具体的内存数量还是根据数据量来决定.但是如果一个机器的内存数量小于8G,那么就不太适合生产环境了,因为我们可能就需要很多小内存的机器来搭建集群.而大于64G的机器也不是很有必要. CPU大多数的es集群对于cpu的要求都会比较低一些,因此一台机器有多少个cpu core其实对生产环境的es集群部署相对来说没有那么的重要了,至少没有内存来的重要.当然,肯定是要用多核处理器的,一般来说2个cpu core~8个cpu core都是可以的. 此外,如果说要选择是较少的cpu core但是cpu性能很高,还是较多的cpu core但是cpu性能较为一般,那么肯定是选择性能较为一般但是更多的cpu core.因为更多的cpu core可以提供更强的并发处理能力,远比单个cpu性能高带来的效果更加明显. 磁盘对于es的生产环境来说,磁盘是非常重要的,尤其是对那些大量写入的es集群,比如互联网公司将每天的实时日志数据以高并发的速度写入es集群.在服务器上,磁盘是最慢的那个资源,所以对于大量写入的es集群来说,会很容易因为磁盘的读写性能造成整个集群的性能瓶颈. 如果我们能够使用SSD固态硬盘,而不是机械硬盘,那么当然是最好的,SSD的性能比机械硬盘可以高很多倍,可以让es的读写性能都高很多倍.所以,如果公司出的起钱大量使用固态硬盘,那么当然是最好的. 如果我们用SSD硬盘的话,那么需要检查我们的I/O scheduler,需要正确的配置IO scheduler.当我们将数据写入磁盘时,IO scheduler会决定什么时候数据才会真正的写入磁盘,而不是停留在os cache内存缓冲中.大多数机器上,默认的IO scheduler是cfq,也就是completely fair queuing. 这个scheduler会给每个进程都分配一些时间分片(time slice), 然后会优化每个进程的数据如何写入磁盘中,优化的思路主要 是根据磁盘的物理布局来决定如何将数据写入磁盘,进而提升写入磁盘的性能.这是针对机械硬盘做出的优化,因为机械硬盘是一种旋转存储介质,是通过机械旋转磁盘+磁头进行磁盘读写的机制. 但是scheduler的这种默认的执行机制,对于SSD来说是不太高效的,因为SSD跟机械硬盘是不一样的,SSD不涉及到机械磁盘旋转和磁头读取这种传统的读写机制.对于SSD来说,应该用deadline/noop scheduler.deadline scheduler会基于写操作被pending了多长时间来进行写磁盘优化,而noop scheduler就是一个简单的FIFO队列先进先出的机制. 调整io scheduler可以带来很大的性能提升,甚至可以达到数百倍. 如果我们没有办法使用SSD,只能使用机械硬盘,那么至少得尽量正确读写速度最快的磁盘,比如高性能的服务器磁盘. 此外,使用RAID 0也是一种提升磁盘读写速度的高效的方式,无论是对于机械硬盘,还是SSD,都一样.RAID 0也被称之为条带式存储机制(striping),,在RAID各种级别中性能是最高的. RAID 0的基本原理,是把连续的数据分散存储到多个磁盘上进行读写,也就是对数据进行条带式存储.这样系统的磁盘读写请求就可以被分散到多个磁盘上并行执行.但是没有必要使用镜像或者RAID的其他模式,因为我们不需要通过RAID来实现数据高可用存储,es的replica副本机制本身已经实现了数据高可用存储. 最后,我们要避免跟网络相关的存储模式,network-attached storage,NAS,比如基于网络的分布式存储模式.虽然很多供应商都说他们的NAS解决方案性能非常高,而且比本地存储的可靠性更高.但是实际上用起来会有很多性能和可靠性上的风险,一般因为网络传输会造成较高的延时,同时还有单点故障的风险. 网络对于es这种分布式系统来说,快速而且可靠的网络是非常的重要的.因为高速网络通信可以让es的节点间通信达到低延时的效果,高带宽可以让shard的移动和恢复,以及分配等操作更加的快速.现代的数据中心的网络对于大多数的集群来说,性能都足够高了.比如千兆网卡,这都是可以的. 但是要避免一个集群横跨多个数据中心,比如异地多机房部署一个集群,因为那样的话跨机房,跨地域的传输会导致网络通信和数据传输性能较差.es集群是一种p2p模式的分布式系统架构,不是master-slave主从分布式系统.在es集群中,所有的node都是相等的,任意两个node间的互相通信都是很频繁和正常的.因此如果部署在异地多机房,那么可能会导致node间频繁跨地域进行通信,通信延时会非常高,甚至造成集群运行频繁不正常. 就跟NAS存储模式一样,很多供应商都说跨地域的多数据中心是非常可靠的,而且低延时的.一般来说,可能的确是这样,但是一旦发生了网络故障,那么集群就完了.通常来说,跨地域多机房部署一个es集群带来的效益,远远低于维护这样的集群所带来的额外成本. 自建集群 vs 云部署现在一般很容易就可以拿到高性能的机器来部署集群,很多高性能的机器可以有上百G的内存资源,还有几十个cpu core.但是同时我们也可以再云供应商上,比如阿里云,租用大量的小资源的虚拟机.那么对于自己购买昂贵高性能服务器自建集群,以及租用云机器来部署,该选择哪种方案呢? 比如说,你自己购买5台 8核64G的物理机,搭建es集群 或者是上阿里云买了16台2核4G的虚拟机来搭建es集群 一般来说,对于es集群而言,是建议拥有少数机器,但是每个机器的资源都非常多,尽量避免拥有大量的少资源的虚拟机.因为对于运维和管理来说,管理5个物理机组成的es集群,远远比管理100个虚拟机组成的es集群要简单的多. 同时即使是自建集群,也要尽量避免那种超大资源量的超级服务器,因为那样可能造成资源无法完全利用,然后在一个物理机上部署多个es节点,这会导致我们的集群管理更加的复杂. JVM对于最新的es版本,一般多建议用最新的jvm版本,除非es明确说明要用哪个jdk版本.es和lucene都是一种满足特殊需求的软件,lucene的单元测试和集成测试中,经常会发现jvm自身的一些bug.这些bug涵盖的范围很广,因此尽量用最新的jvm版本,bug会少一些. 就目前es 5.x版本而言,建议用jdk 8,而不是jdk 7,同时jdk 6已经不再被支持了. 如果我们用java编写es应用程序,而且在使用transport client或者node client,要确保运行我们的应用程序的jvm版本跟es服务器运行的jvm版本是一样的.在es中,有些java的本地序列化机制都被使用了,比如ip地址,异常信息,等等.而jvm可能在不同的minor版本之间修改序列化格式,所以如果client和server的jvm版本不一致,可能有序列化的问题. 同时官方推荐,绝对不要随便调整jvm的设置.虽然jvm有几百个配置选项,而且我们可以手动调优jvm的几乎方方面面.同时遇到一个性能场景的时候,每个人都会第一时间想到去调优jvm,但是es官方还是推荐我们不要随便调节jvm参数.因为es是一个非常复杂的分布式软件系统,而且es的默认jvm配置都是基于真实业务场景中长期的实践得到的.随便调节jvm配置反而有可能导致集群性能变得更加差,以及出现一些未知的问题.反而是很多情况下,将自定义的jvm配置全部删除,性能是保持的最好的. 容量规划在规划你的es集群的时候,一般要规划你需要多少台服务器,每台服务器要有多少资源,能够支撑你预计的多大的数据量.但是这个东西其实不是一概而论的,要视具体的读写场景,包括你执行多么复杂的操作,读写QPS来决定的.不过一般而言,对于很多的中小型公司,都是建议es集群承载的数据量在10亿规模以内.用最合理的技术做最合理的事情. 比如做网站或者app的搜索引擎,一般数据量会相对来说大一些,但是通常而言,一个网站或者app的内容都是有限的,不会无限膨胀,通常数据量从百万级到亿级不等,因此用于搜索的数据都放在es中是合理的. 然后一些软件系统或者特殊项目的搜索引擎,根据项目情况不同,数据量也是从百万量级到几十亿,甚至几百亿,或者每日增量几亿,都有可能,那么此时就要根据具体的业务场景来决定了.如果数据量特别大,日增量都几亿规模,那么其实建议不要将每天全量的数据都写入es中,es也不适合这种无限规模膨胀的场景.es是很耗费内存的,无限膨胀的数据量,会导致我们无法提供足够的资源来支撑这么大的数据量.可以考虑是不是就将部分热数据,比如最近几天的数据,放到es中做高频高性能搜索,然后将大量的很少访问的冷数据放大数据系统做离线批量处理,比如hadoop系统里面. 同时如果数据量在10亿以内的规模,那么一般而言,如果提供5台以上的机器,每台机器的配置到8核64G的配置,一般而言都能hold住.当然,这个也看具体的使用场景,如果你读写特别频繁,或者查询特别复杂,那么可能还需要更多的机器资源.如果你要承载更大的数据量,那么就相应的提供更多的机器和资源. 要提升你的es的性能,最重要的,还是说规划合理的数据量,物理内存资源大小,os cache]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-100-Java API基于地理位置的搜索]]></title>
    <url>%2F2019%2F01%2F23%2FElasticsearch-100-Java-API%E5%9F%BA%E4%BA%8E%E5%9C%B0%E7%90%86%E4%BD%8D%E7%BD%AE%E7%9A%84%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[案例背景比如我们有很多的4s店,然后呢给了用户一个app,在某个地方的时候,可以根据当前的地理位置搜索一下,自己附近的4s店 数据准备添加一个地理位置的field,手动设置mapping映射,然后添加一条测试数据123456789101112POST /car_shop/_mapping/shops&#123; &quot;properties&quot;: &#123; &quot;pin&quot;: &#123; &quot;properties&quot;: &#123; &quot;location&quot;: &#123; &quot;type&quot;: &quot;geo_point&quot; &#125; &#125; &#125; &#125;&#125; 12345678910PUT /car_shop/shops/1&#123; &quot;name&quot;: &quot;上海至全宝马4S店&quot;, &quot;pin&quot; : &#123; &quot;location&quot; : &#123; &quot;lat&quot; : 40.12, &quot;lon&quot; : -71.34 &#125; &#125;&#125; 依赖引入maven中引入基于地理位置查询的依赖1234567891011121314151617&lt;dependency&gt; &lt;groupId&gt;org.locationtech.spatial4j&lt;/groupId&gt; &lt;artifactId&gt;spatial4j&lt;/artifactId&gt; &lt;version&gt;0.6&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.vividsolutions&lt;/groupId&gt; &lt;artifactId&gt;jts&lt;/artifactId&gt; &lt;version&gt;1.13&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;xerces&lt;/groupId&gt; &lt;artifactId&gt;xercesImpl&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 矩形搜索搜索两个坐标点,组成的一个矩形区域1234567891011121314@Testpublic void getBoundingBoxQuery()&#123; QueryBuilder queryBuilder = QueryBuilders.geoBoundingBoxQuery(&quot;pin.location&quot;).setCorners(40.73, -74.1, 40.01, -71.12); SearchResponse response = client.prepareSearch(&quot;car_shop&quot;) .setTypes(&quot;shops&quot;) .setQuery(queryBuilder) .get(); for (SearchHit hit : response.getHits().getHits()) &#123; log.info(&quot;hit:&#123;&#125;&quot;, hit.getSourceAsString()); &#125;&#125; console:1234567892019-01-23 16:39:39.306 [main] INFO com.demo.elasticsearch.CarShopTests - hit:&#123; &quot;name&quot;: &quot;上海至全宝马4S店&quot;, &quot;pin&quot; : &#123; &quot;location&quot; : &#123; &quot;lat&quot; : 40.12, &quot;lon&quot; : -71.34 &#125; &#125;&#125; 多个坐标点区域查询多个坐标点组成一个多边形区域123456789101112131415161718192021@Testpublic void geoPolygonQuery()&#123; // 多个坐标点 List&lt;GeoPoint&gt; points = new ArrayList&lt;&gt;(3); points.add(new GeoPoint(40.73, -74.1)); points.add(new GeoPoint(40.01, -71.12)); points.add(new GeoPoint(50.56, -90.58)); // 查询条件 QueryBuilder queryBuilder = QueryBuilders.geoPolygonQuery(&quot;pin.location&quot;, points); SearchResponse response = client.prepareSearch(&quot;car_shop&quot;) .setTypes(&quot;shops&quot;) .setQuery(queryBuilder) .get(); for (SearchHit hit : response.getHits().getHits()) &#123; log.info(&quot;hit:&#123;&#125;&quot;, hit.getSourceAsString()); &#125;&#125; console:1234567892019-01-23 16:49:05.885 [main] INFO com.demo.elasticsearch.CarShopTests - hit:&#123; &quot;name&quot;: &quot;上海至全宝马4S店&quot;, &quot;pin&quot; : &#123; &quot;location&quot; : &#123; &quot;lat&quot; : 40.12, &quot;lon&quot; : -71.34 &#125; &#125;&#125; 根据当前坐标的范围查询以某一坐标为基准,查询距离该坐标2km以内的数据1234567891011121314151617@Testpublic void geoDistanceQuery()&#123; // 查询条件 QueryBuilder queryBuilder = QueryBuilders.geoDistanceQuery(&quot;pin.location&quot;) .point(40, -70) .distance(200, DistanceUnit.KILOMETERS); SearchResponse response = client.prepareSearch(&quot;car_shop&quot;) .setTypes(&quot;shops&quot;) .setQuery(queryBuilder) .get(); for (SearchHit hit : response.getHits().getHits()) &#123; log.info(&quot;hit:&#123;&#125;&quot;, hit.getSourceAsString()); &#125;&#125; console:1234567892019-01-23 16:55:23.224 [main] INFO com.demo.elasticsearch.CarShopTests - hit:&#123; &quot;name&quot;: &quot;上海至全宝马4S店&quot;, &quot;pin&quot; : &#123; &quot;location&quot; : &#123; &quot;lat&quot; : 40.12, &quot;lon&quot; : -71.34 &#125; &#125;&#125; 最后以上的java Api的代码,都放在了github,传送门]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-99-Java API 常用的几种查询与组合查询]]></title>
    <url>%2F2019%2F01%2F23%2FElasticsearch-99-Java%20API%20%E5%B8%B8%E7%94%A8%E7%9A%84%E5%87%A0%E7%A7%8D%E6%9F%A5%E8%AF%A2%E4%B8%8E%E7%BB%84%E5%90%88%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[常用的几种查询准备数据添加一条数据进去,供测试1234567PUT /car_shop/cars/5&#123; &quot;brand&quot;: &quot;华晨宝马&quot;, &quot;name&quot;: &quot;宝马318&quot;, &quot;price&quot;: 270000, &quot;produce_date&quot;: &quot;2017-01-20&quot;&#125; 全文检索需求: 全文查询 “宝马”, 一次从brand中搜索,一次从brand和name中搜索 1234567891011@Testpublic void matchQuery()&#123; SearchResponse response = client.prepareSearch(&quot;car_shop&quot;) .setTypes(&quot;cars&quot;) .setQuery(QueryBuilders.matchQuery(&quot;brand&quot;, &quot;宝马&quot;)) .get(); for (SearchHit hit : response.getHits().getHits()) &#123; log.info(&quot;hit:&#123;&#125;&quot;, hit.getSourceAsString()); &#125;&#125; console:123456782019-01-23 15:30:18.072 [main] INFO com.demo.elasticsearch.CarShopTests - hit:&#123; &quot;brand&quot;: &quot;华晨宝马&quot;, &quot;name&quot;: &quot;宝马318&quot;, &quot;price&quot;: 270000, &quot;produce_date&quot;: &quot;2017-01-20&quot;&#125;2019-01-23 15:30:18.073 [main] INFO com.demo.elasticsearch.CarShopTests - hit:&#123;&quot;brand&quot;:&quot;宝马&quot;,&quot;name&quot;:&quot;宝马320&quot;,&quot;price&quot;:310000,&quot;produce_date&quot;:&quot;2018-01-01&quot;&#125; 第二次,从多个field中查询数据1234567891011@Testpublic void multiMatchQuery()&#123; SearchResponse response = client.prepareSearch(&quot;car_shop&quot;) .setTypes(&quot;cars&quot;) .setQuery(QueryBuilders.multiMatchQuery(&quot;宝马&quot;, &quot;brand&quot;, &quot;name&quot;)) .get(); for (SearchHit hit : response.getHits().getHits()) &#123; log.info(&quot;hit:&#123;&#125;&quot;, hit.getSourceAsString()); &#125;&#125; console:123456782019-01-23 15:32:40.852 [main] INFO com.demo.elasticsearch.CarShopTests - hit:&#123; &quot;brand&quot;: &quot;华晨宝马&quot;, &quot;name&quot;: &quot;宝马318&quot;, &quot;price&quot;: 270000, &quot;produce_date&quot;: &quot;2017-01-20&quot;&#125;2019-01-23 15:32:40.853 [main] INFO com.demo.elasticsearch.CarShopTests - hit:&#123;&quot;brand&quot;:&quot;宝马&quot;,&quot;name&quot;:&quot;宝马320&quot;,&quot;price&quot;:310000,&quot;produce_date&quot;:&quot;2018-01-01&quot;&#125; 精准查询精确搜索,就是termQuery需求: 查询名称是宝马320的数据 1234567891011@Testpublic void termQuery()&#123; SearchResponse response = client.prepareSearch(&quot;car_shop&quot;) .setTypes(&quot;cars&quot;) .setQuery(QueryBuilders.termQuery(&quot;name.raw&quot;, &quot;宝马320&quot;)) .get(); for (SearchHit hit : response.getHits().getHits()) &#123; log.info(&quot;hit:&#123;&#125;&quot;, hit.getSourceAsString()); &#125;&#125; console:12019-01-23 15:38:13.984 [main] INFO com.demo.elasticsearch.CarShopTests - hit:&#123;&quot;brand&quot;:&quot;宝马&quot;,&quot;name&quot;:&quot;宝马320&quot;,&quot;price&quot;:310000,&quot;produce_date&quot;:&quot;2018-01-01&quot;&#125; 前缀搜索需求:搜索name是宝开头的数据1234567891011@Testpublic void prefixQuery()&#123; SearchResponse response = client.prepareSearch(&quot;car_shop&quot;) .setTypes(&quot;cars&quot;) .setQuery(QueryBuilders.prefixQuery(&quot;name&quot;, &quot;宝&quot;)) .get(); for (SearchHit hit : response.getHits().getHits()) &#123; log.info(&quot;hit:&#123;&#125;&quot;, hit.getSourceAsString()); &#125;&#125; console:123456782019-01-23 15:47:10.956 [main] INFO com.demo.elasticsearch.CarShopTests - hit:&#123; &quot;brand&quot;: &quot;华晨宝马&quot;, &quot;name&quot;: &quot;宝马318&quot;, &quot;price&quot;: 270000, &quot;produce_date&quot;: &quot;2017-01-20&quot;&#125;2019-01-23 15:47:10.956 [main] INFO com.demo.elasticsearch.CarShopTests - hit:&#123;&quot;brand&quot;:&quot;宝马&quot;,&quot;name&quot;:&quot;宝马320&quot;,&quot;price&quot;:310000,&quot;produce_date&quot;:&quot;2018-01-01&quot;&#125; 组合查询需求: 查询brand必须包含宝马 name必须不是宝马318 produce_date可以是2017-01-01~2017-01-31之内 过滤出价格是28000-35000的数据 1234567891011121314151617181920@Testpublic void boolQuery()&#123; // 组装查询条件 QueryBuilder queryBuilder = QueryBuilders.boolQuery() .must(QueryBuilders.matchQuery(&quot;brand&quot;, &quot;宝马&quot;)) .mustNot(QueryBuilders.termQuery(&quot;name.raw&quot;, &quot;宝马318&quot;)) .should(QueryBuilders.rangeQuery(&quot;produce_date&quot;).gte(&quot;2017-01-01&quot;).lte(&quot;2017-01-31&quot;)) .filter(QueryBuilders.rangeQuery(&quot;price&quot;).gte(280000).lte(350000)); // 然后调用搜索接口 SearchResponse response = client.prepareSearch(&quot;car_shop&quot;) .setTypes(&quot;cars&quot;) .setQuery(queryBuilder) .get(); // 输出 for (SearchHit hit : response.getHits().getHits()) &#123; log.info(&quot;hit:&#123;&#125;&quot;, hit.getSourceAsString()); &#125;&#125; console:12019-01-23 16:01:17.074 [main] INFO com.demo.elasticsearch.CarShopTests - hit:&#123;&quot;brand&quot;:&quot;宝马&quot;,&quot;name&quot;:&quot;宝马320&quot;,&quot;price&quot;:310000,&quot;produce_date&quot;:&quot;2018-01-01&quot;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-98-Java API 常用的几种API及搜索模板调用]]></title>
    <url>%2F2019%2F01%2F23%2FElasticsearch-98-Java-API-%E5%B8%B8%E7%94%A8%E7%9A%84%E5%87%A0%E7%A7%8DAPI%E5%8F%8A%E6%90%9C%E7%B4%A2%E6%A8%A1%E6%9D%BF%E8%B0%83%E7%94%A8%2F</url>
    <content type="text"><![CDATA[案例背景以汽车零售为案例背景,简单来说,会涉及到三个数据,汽车信息,汽车销售记录,汽车4S店信息 准备工作创建索引123456789101112131415161718192021222324252627PUT /car_shop&#123; &quot;mappings&quot;: &#123; &quot;cars&quot;: &#123; &quot;properties&quot;: &#123; &quot;brand&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;fields&quot;: &#123; &quot;raw&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;fields&quot;: &#123; &quot;raw&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 常用java Api使用upsert Api需求: 首先更新宝马320的价格为310000,如果这个汽车不存在的话就新增,如果存在的话,就做更新 注入client12@Autowiredprivate TransportClient client; upsert操作123456789101112131415161718192021222324@Testpublic void upsert() throws IOException, ExecutionException, InterruptedException &#123; // index操作 IndexRequest indexRequest = new IndexRequest(&quot;car_shop&quot;, &quot;cars&quot;, &quot;1&quot;) .source(jsonBuilder() .startObject() .field(&quot;brand&quot;, &quot;宝马&quot;) .field(&quot;name&quot;,&quot;宝马320&quot;) .field(&quot;price&quot;,320000) .field(&quot;produce_date&quot;, &quot;2018-01-01&quot;) .endObject() ); // update操作 UpdateRequest updateRequest = new UpdateRequest(&quot;car_shop&quot;, &quot;cars&quot;, &quot;1&quot;) .doc(jsonBuilder() .startObject() .field(&quot;price&quot;, 310000) .endObject() ).upsert(indexRequest); // 客户端执行 client.update(updateRequest).get();&#125; 第一次执行,因为我们刚创建的索引,并没有这条数据,所以第一次执行完成后,这条数据被添加了进去,price还是320000第二次执行,数据已经存在了,就是走下面的updateRequest,把价格更新为310000 mget Api场景: 一般来说,我们都可以在一些汽车网站上,或者混合销售多个品牌的汽车4s店的内部,都可以在系统里调出来多个汽车的信息,放在网页上,进行对比 mget,批量查询,一次性将多个document的数据查询出来 先再添加一条数据供测试1234567PUT /car_shop/cars/2&#123; &quot;brand&quot;: &quot;奔驰&quot;, &quot;name&quot;: &quot;奔驰C200&quot;, &quot;price&quot;: 350000, &quot;produce_date&quot;: &quot;2017-01-05&quot;&#125; 创建完成,用java的mget api,查询出来 12345678910111213141516@Testpublic void mgetTest()&#123; MultiGetResponse multiGetResponse = client.prepareMultiGet() .add(&quot;car_shop&quot;, &quot;cars&quot;, &quot;1&quot;) .add(&quot;car_shop&quot;, &quot;cars&quot;, &quot;2&quot;) .get(); // 输出 for (MultiGetItemResponse itemResponse : multiGetResponse) &#123; GetResponse responses = itemResponse.getResponse(); if (responses.isExists())&#123; log.info(&quot;response:&#123;&#125;&quot;, responses.getSourceAsString()); &#125; &#125;&#125; 控制台输出:12345672019-01-23 11:07:00.302 [main] INFO com.demo.elasticsearch.CarShopTests - response:&#123;&quot;brand&quot;:&quot;宝马&quot;,&quot;name&quot;:&quot;宝马320&quot;,&quot;price&quot;:310000,&quot;produce_date&quot;:&quot;2018-01-01&quot;&#125;2019-01-23 11:07:00.303 [main] INFO com.demo.elasticsearch.CarShopTests - response:&#123; &quot;brand&quot;: &quot;奔驰&quot;, &quot;name&quot;: &quot;奔驰C200&quot;, &quot;price&quot;: 350000, &quot;produce_date&quot;: &quot;2017-01-05&quot;&#125; 两条数据就都查询出来了 bulk Api场景: 有一个汽车销售公司,有很多家4s店,这些4S店的数据,都会在一段时间内陆续传递过来汽车的销售数据,现在希望能够在内存中缓存比如1000条销售数据,然后一次性批量上传到es中去 首先添加两条数据供后面使用12345678910111213141516171819PUT /car_shop/sales/1&#123; &quot;brand&quot;: &quot;宝马&quot;, &quot;name&quot;: &quot;宝马320&quot;, &quot;price&quot;: 320000, &quot;produce_date&quot;: &quot;2017-01-01&quot;, &quot;sale_price&quot;: 300000, &quot;sale_date&quot;: &quot;2017-01-21&quot;&#125;PUT /car_shop/sales/2&#123; &quot;brand&quot;: &quot;宝马&quot;, &quot;name&quot;: &quot;宝马320&quot;, &quot;price&quot;: 320000, &quot;produce_date&quot;: &quot;2017-01-01&quot;, &quot;sale_price&quot;: 300000, &quot;sale_date&quot;: &quot;2017-01-21&quot;&#125; 需求: 更新id是1的document中sale_price, 删除id是2的数据, 添加一条新的数据 12345678910111213141516171819202122232425262728293031323334353637@Testpublic void bulkTest() throws IOException &#123; BulkRequestBuilder bulkRequestBuilder = client.prepareBulk(); // index操作,添加一条销售记录进去 IndexRequestBuilder indexRequestBuilder = client.prepareIndex(&quot;car_shop&quot;, &quot;sales&quot;, &quot;3&quot;) .setSource(jsonBuilder() .startObject() .field(&quot;brand&quot;, &quot;奔驰&quot;) .field(&quot;name&quot;, &quot;奔驰C200&quot;) .field(&quot;price&quot;, 350000) .field(&quot;produce_date&quot;, &quot;2017-01-05&quot;) .field(&quot;sale_price&quot;, 340000) .field(&quot;sale_date&quot;, &quot;2017-02-03&quot;) .endObject() ); // update操作 更新一条id是1的数据 UpdateRequestBuilder updateRequestBuilder = client.prepareUpdate(&quot;car_shop&quot;, &quot;sales&quot;, &quot;1&quot;) .setDoc(jsonBuilder() .startObject() .field(&quot;sale_price&quot;, 290000) .endObject() ); // 删除操作 删除id是2的数据 DeleteRequestBuilder deleteRequestBuilder = client.prepareDelete(&quot;car_shop&quot;, &quot;sales&quot;, &quot;2&quot;); // 请求都添加到bulk中 bulkRequestBuilder.add(indexRequestBuilder) .add(updateRequestBuilder) .add(deleteRequestBuilder); // 发送请求 BulkResponse responses = bulkRequestBuilder.get();&#125; 执行完成后再来看一下sales中的数据1GET /car_shop/sales/_search 返回值 :12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;car_shop&quot;, &quot;_type&quot;: &quot;sales&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;brand&quot;: &quot;宝马&quot;, &quot;name&quot;: &quot;宝马320&quot;, &quot;price&quot;: 320000, &quot;produce_date&quot;: &quot;2017-01-01&quot;, &quot;sale_price&quot;: 290000, &quot;sale_date&quot;: &quot;2017-01-21&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;car_shop&quot;, &quot;_type&quot;: &quot;sales&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;brand&quot;: &quot;奔驰&quot;, &quot;name&quot;: &quot;奔驰C200&quot;, &quot;price&quot;: 350000, &quot;produce_date&quot;: &quot;2017-01-05&quot;, &quot;sale_price&quot;: 340000, &quot;sale_date&quot;: &quot;2017-02-03&quot; &#125; &#125; ] &#125;&#125; 已经完成了, 再看一下代码里面, 就是用的工厂模式,先搞了一个bulk的builder, 然后将每个请求写出来,添加到工厂中去,最后发送请求,就ok了 scroll Api对大数据的批量查询 先插入一条数据供测试用123456789PUT /car_shop/sales/4&#123; &quot;brand&quot;: &quot;宝马&quot;, &quot;name&quot;: &quot;宝马320&quot;, &quot;price&quot;: 320000, &quot;produce_date&quot;: &quot;2017-01-01&quot;, &quot;sale_price&quot;: 280000, &quot;sale_date&quot;: &quot;2017-01-25&quot;&#125; 现在,宝马的销售记录是有两条数据的,然后就查询宝马的数据,分两次查询,每次一条,使用scroll Api 12345678910111213141516171819202122232425public void scrollTests()&#123; // scroll查询 时间为60s,查询是宝马的数据每次查询一条 SearchResponse response = client.prepareSearch(&quot;car_shop&quot;) .setTypes(&quot;sales&quot;) .setScroll(new Scroll(new TimeValue(60000))) .setQuery(QueryBuilders.termQuery(&quot;brand.keyword&quot;, &quot;宝马&quot;)) .setSize(1) .get(); // 继续往下查询 while (response.getHits().getHits().length &gt; 0)&#123; for (SearchHit hit : response.getHits().getHits()) &#123; // 拿到每条数据去处理 log.info(&quot;hit:&#123;&#125;&quot;, hit.getSourceAsString()); &#125; // 继续下一次查询 response = client.prepareSearchScroll(response.getScrollId()) .setScroll(new TimeValue(60000)) .execute() .actionGet(); &#125;&#125; 控制台输出:123456789102019-01-23 14:38:35.598 [main] INFO com.demo.elasticsearch.CarShopTests - hit:&#123; &quot;brand&quot;: &quot;宝马&quot;, &quot;name&quot;: &quot;宝马320&quot;, &quot;price&quot;: 320000, &quot;produce_date&quot;: &quot;2017-01-01&quot;, &quot;sale_price&quot;: 280000, &quot;sale_date&quot;: &quot;2017-01-25&quot;&#125;2019-01-23 14:38:35.620 [main] INFO com.demo.elasticsearch.CarShopTests - hit:&#123;&quot;brand&quot;:&quot;宝马&quot;,&quot;name&quot;:&quot;宝马320&quot;,&quot;price&quot;:320000,&quot;produce_date&quot;:&quot;2017-01-01&quot;,&quot;sale_price&quot;:290000,&quot;sale_date&quot;:&quot;2017-01-21&quot;&#125; 两条数据都查询了出来 调用搜索模板首先,需要创建一个搜索模板,在es的/config/scripts目录下新建page_query_by_brand.mustache模板文件,内容如下: 123456789&#123; &quot;from&quot;: &#123;&#123;from&#125;&#125;, &quot;size&quot;: &#123;&#123;size&#125;&#125;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;brand.keyword&quot;: &quot;&#123;&#123;brand&#125;&#125;&quot; &#125; &#125;&#125; 然后,调用模板查询12345678910111213141516171819public void searchTemplate()&#123; // 请求参数 Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(3); map.put(&quot;from&quot;, 0); map.put(&quot;size&quot;, 1); map.put(&quot;brand&quot;, &quot;宝马&quot;); SearchResponse response = new SearchTemplateRequestBuilder(client) .setScript(&quot;page_query_by_brand&quot;) .setScriptType(ScriptType.FILE) .setScriptParams(map) .setRequest(new SearchRequest(&quot;car_shop&quot;).types(&quot;sales&quot;)) .get() .getResponse(); for (SearchHit hit : response.getHits().getHits()) &#123; log.info(&quot;hit:&#123;&#125;&quot;, hit.getSourceAsString()); &#125;&#125; 控制台输出:123456782019-01-23 15:07:57.875 [main] INFO com.demo.elasticsearch.CarShopTests - hit:&#123; &quot;brand&quot;: &quot;宝马&quot;, &quot;name&quot;: &quot;宝马320&quot;, &quot;price&quot;: 320000, &quot;produce_date&quot;: &quot;2017-01-01&quot;, &quot;sale_price&quot;: 280000, &quot;sale_date&quot;: &quot;2017-01-25&quot;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-97-基于地理位置的搜索和聚合]]></title>
    <url>%2F2019%2F01%2F22%2FElasticsearch-97-%E5%9F%BA%E4%BA%8E%E5%9C%B0%E7%90%86%E4%BD%8D%E7%BD%AE%E7%9A%84%E6%90%9C%E7%B4%A2%E5%92%8C%E8%81%9A%E5%90%88%2F</url>
    <content type="text"><![CDATA[案例背景一个酒店o2o的app,根据用户指定的位置,找到周围的符合条件的酒店. geo_point地理位置数据类型geo_point,就是一个地理位置坐标点,记录了经度和纬度,通过经纬度,就可以定位地球上的位置 创建索引123456789101112PUT /my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;location&quot;:&#123; &quot;type&quot;: &quot;geo_point&quot; &#125; &#125; &#125; &#125;&#125; field的类型设置为geo_point 写入geo_point的三种方法第一种12345678PUT /my_index/my_type/1&#123; &quot;text&quot;: &quot;Geo-point as an object&quot;, &quot;location&quot;: &#123; &quot;lat&quot;: 41.12, &quot;lon&quot;: -71.34 &#125;&#125; latitude(lat): 纬度 longitude(lon): 经度 第二种12345PUT my_index/my_type/2&#123; &quot;text&quot;: &quot;Geo-point as a string&quot;, &quot;location&quot;: &quot;41.12,-71.34&quot; &#125; 第三种12345PUT my_index/my_type/4&#123; &quot;text&quot;: &quot;Geo-point as an array&quot;, &quot;location&quot;: [ -71.34, 41.12 ] &#125; 后面两种不推荐使用 搜索geo_bounding_box查询geo_bounding_box查询: 查询某个矩形的地理位置范围内的坐标点 看一下搜索请求:1234567891011121314151617GET /my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;geo_bounding_box&quot;:&#123; &quot;location&quot;:&#123; &quot;top_left&quot;:&#123; &quot;lat&quot;:42, &quot;lon&quot;:-72 &#125;, &quot;bottom_right&quot;:&#123; &quot;lat&quot;:40, &quot;lon&quot;:-74 &#125; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&#123; &quot;took&quot;: 17, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;text&quot;: &quot;Geo-point as a string&quot;, &quot;location&quot;: &quot;41.12,-71.34&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;text&quot;: &quot;Geo-point as an array&quot;, &quot;location&quot;: [ -71.34, 41.12 ] &#125; &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;text&quot;: &quot;Geo-point as an object&quot;, &quot;location&quot;: &#123; &quot;lat&quot;: 41.12, &quot;lon&quot;: -71.34 &#125; &#125; &#125; ] &#125;&#125; 搜索的时候指定了一个top_left,和一个bottom_right如上图,以top_left,和一个bottom_right 成一个矩形,然后这个矩形之内的坐标对应的数据都可以被搜索出来 geo_polygon查询geo_polygon查询:以多个坐标组成一个多边形来查询1234567891011121314151617181920212223242526272829303132GET /my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match_all&quot;: &#123;&#125; &#125; ], &quot;filter&quot;: &#123; &quot;geo_polygon&quot;: &#123; &quot;location&quot;: &#123; &quot;points&quot;: [ &#123; &quot;lat&quot;: 40.73, &quot;lon&quot;: -74.1 &#125;, &#123; &quot;lat&quot; : 40.01, &quot;lon&quot; : -71.12 &#125;, &#123; &quot;lat&quot; : 50.56, &quot;lon&quot; : -90.58 &#125; ] &#125; &#125; &#125; &#125; &#125;&#125; 跟上面的矩形搜索是类似的,只不过这里是以多个坐标组成的多边形去搜索 距离查询比如说要查询距离自己200km以内的酒店, 我们自己的坐标是知道的 app都可以获取的到 搜索123456789101112131415161718192021GET /my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match_all&quot;: &#123;&#125; &#125; ], &quot;filter&quot;: &#123; &quot;geo_distance&quot;: &#123; &quot;distance&quot;: &quot;200km&quot;, &quot;location&quot;: &#123; &quot;lat&quot;: 40.73, &quot;lon&quot;: -74.1 &#125; &#125; &#125; &#125; &#125;&#125; location里面的经纬度就是我们自己的坐标,然后distance,设置查询距离 聚合分析需求:统计距离我当前位置的几个范围内的酒店数量, 比如我0-100m有几个酒店,100-300m有几个,300以上的有几个 请求:12345678910111213141516171819202122232425262728GET /my_index/my_type/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;agg_by_distance_range&quot;: &#123; &quot;geo_distance&quot;: &#123; &quot;field&quot;: &quot;location&quot;, &quot;origin&quot;: &#123; &quot;lat&quot;: 52.376, &quot;lon&quot;: 4.894 &#125;, &quot;unit&quot;: &quot;mi&quot;, &quot;ranges&quot;: [ &#123; &quot;to&quot;: 100 &#125;, &#123; &quot;from&quot;: 100, &quot;to&quot;: 300 &#125;, &#123; &quot;from&quot;: 300 &#125; ] &#125; &#125; &#125;&#125; 返回值:12345678910111213141516171819202122232425262728293031323334353637&#123; &quot;took&quot;: 13, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;agg_by_distance_range&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;*-100.0&quot;, &quot;from&quot;: 0, &quot;to&quot;: 100, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key&quot;: &quot;100.0-300.0&quot;, &quot;from&quot;: 100, &quot;to&quot;: 300, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key&quot;: &quot;300.0-*&quot;, &quot;from&quot;: 300, &quot;doc_count&quot;: 3 &#125; ] &#125; &#125;&#125; 请求中的几个参数 unit:单位,可以是mi 也可以是km origin: 自己当前的坐标 ranges: 要统计的几个区间]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-96-使用动态映射模板定制自己的映射策略]]></title>
    <url>%2F2019%2F01%2F22%2FElasticsearch-96-%E4%BD%BF%E7%94%A8%E5%8A%A8%E6%80%81%E6%98%A0%E5%B0%84%E6%A8%A1%E6%9D%BF%E5%AE%9A%E5%88%B6%E8%87%AA%E5%B7%B1%E7%9A%84%E6%98%A0%E5%B0%84%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[述比如说,我们本来没有某个type,或者说没有某个field,但是希望在插入数据的时候,es自动为我们做一个识别,动态映射出这个type的mapping,包括每个field的数据类型,一般用的动态映射就是dynamic mapping 这里有个问题,如果说,我们其实对dynamic mapping有一些独特的需求,比如说,es默认情况下,如果识别到一个数字,比如filed:10,这样的数据,es默认会搞成long类型的,再比如说field:&quot;10&quot;默认就是text类型,还会带一个内置的keyword,我们没法改变 但是我们希望在动态映射的时候,根据我们的需求去映射,而不是按照默认的规则去玩儿 dynamic mapping template我们可以预先定义一个模板,然后插入数据的时候,相关的field,如果能够根据我们预先定义的规则,匹配某个我们预定义的模板,那么就会根据我们的模板来进行mapping,决定这个field的数据类型 默认的动态映射规则首先看一下es默认的映射规则是咋样的1DELETE my_index 删除之后,直接插入一条数据来看一下12345PUT /my_index/my_type/1&#123; &quot;test_string&quot;: &quot;hello world&quot;, &quot;test_number&quot;: 10&#125; 查询一下my_index的mapping1GET /my_index/_mapping/my_type 返回值:12345678910111213141516171819202122&#123; &quot;my_index&quot;: &#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;test_number&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;test_string&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 这个就是es默认的动态映射规则. 那我们现在想要的效果是:test_number这个field,如果是个数字,我们默认就是integer类型的.test_string这个field,如果是字符串,我们希望默认就是text,内置的field名字叫做raw,而不是keyword,类型还是keyword,保留500个字符 动态映射模板,有两种方式,第一种,是根据新加入的field的默认的数据类型来进行匹配,匹配上某个预定义的模板第二种是根据新加入的field的名字去匹配预定义的名字或者一个预定义的通配符,然后匹配上某个预定义的模板 根据类型匹配映射模板删除索引1DELETE /my_index 定义模板12345678910111213141516171819202122232425262728293031PUT /my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;dynamic_templates&quot;:[ &#123; &quot;integers&quot;:&#123; // 模板名称(自定义) &quot;match_mapping_type&quot;:&quot;long&quot;, // 匹配的数据类型 &quot;mapping&quot;:&#123; &quot;type&quot;:&quot;integer&quot; // 要转换成的数据类型 &#125; &#125; &#125;, &#123; &quot;strings&quot;:&#123; &quot;match_mapping_type&quot;:&quot;string&quot;, &quot;mapping&quot;:&#123; &quot;type&quot;:&quot;text&quot;, &quot;fields&quot;:&#123; &quot;raw&quot;:&#123; // 内置field的名称 &quot;type&quot;:&quot;keyword&quot;, &quot;ignore_above&quot;:500 &#125; &#125; &#125; &#125; &#125; ] &#125; &#125;&#125; dynamic_templates中加入两个模板,设置好要匹配的数据类型和转换后的数据类型就可以了 添加数据12345PUT /my_index/my_type/1&#123; &quot;test_long&quot;: 1, &quot;test_string&quot;: &quot;hello world&quot;&#125; 查询mapping1GET /my_index/_mapping/my_type 返回值:12345678910111213141516171819202122232425262728293031323334353637383940414243444546&#123; &quot;my_index&quot;: &#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;dynamic_templates&quot;: [ &#123; &quot;integers&quot;: &#123; &quot;match_mapping_type&quot;: &quot;long&quot;, &quot;mapping&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125; &#125; &#125;, &#123; &quot;strings&quot;: &#123; &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;mapping&quot;: &#123; &quot;fields&quot;: &#123; &quot;raw&quot;: &#123; &quot;ignore_above&quot;: 500, &quot;type&quot;: &quot;keyword&quot; &#125; &#125;, &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; ], &quot;properties&quot;: &#123; &quot;test_long&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;test_string&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;raw&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 500 &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 上面是我们定义的模板,下面的properties里面就是数据类型等 根据字段名去匹配映射模板删除索引1DELETE /my_index 定义模板12345678910111213141516171819PUT /my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;:&#123; &quot;dynamic_templates&quot;:[ &#123; &quot;string_as_integer&quot;:&#123; // 模板名称 &quot;match_mapping_type&quot;:&quot;string&quot;, // 匹配的类型 &quot;match&quot;:&quot;long_*&quot;, // 匹配的名称 &quot;unmatch&quot;:&quot;*_text&quot;, // 不匹配的名称 &quot;mapping&quot;:&#123; &quot;type&quot;:&quot;integer&quot; // 转换后的类型 &#125; &#125; &#125; ] &#125; &#125;&#125; 插入数据12345PUT /my_index/my_type/1&#123; &quot;long_field&quot;: &quot;10&quot;, &quot;long_filed_text&quot;: &quot;10&quot;&#125; 数据中两个数据都是字符串类型,但是long_field是可以匹配到定以好的模板的,所以会转换为integer类型的数据 查询mapping1GET /my_index/_mapping/my_type 返回值:12345678910111213141516171819202122232425262728293031323334&#123; &quot;my_index&quot;: &#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;dynamic_templates&quot;: [ &#123; &quot;string_as_integer&quot;: &#123; &quot;match&quot;: &quot;long_*&quot;, &quot;unmatch&quot;: &quot;*_text&quot;, &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;mapping&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125; &#125; &#125; ], &quot;properties&quot;: &#123; &quot;long_field&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;long_filed_text&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 使用场景假设一个系统中,每天都会产生一堆数据,这些数据每天的数据都放在一个新的type中,此时就可以预先定义一个模板,搞一个脚本,每天都预先生成一个新的type,里面将你的各个field都匹配到一个你预定义的模板中去,就ok了]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-95-基于completion suggest实现搜索提示]]></title>
    <url>%2F2019%2F01%2F21%2FElasticsearch-95-%E5%9F%BA%E4%BA%8Ecompletion-suggest%E5%AE%9E%E7%8E%B0%E6%90%9C%E7%B4%A2%E6%8F%90%E7%A4%BA%2F</url>
    <content type="text"><![CDATA[completion suggest搜索联想,智能提示,自动完成(auto completion)比如说,google搜索 Elastic,会自动提示elasticsearch, 等等 案例创建索引首先,需要创建索引的时候,设置completion1234567891011121314151617181920212223PUT /news_website&#123; &quot;mappings&quot;: &#123; &quot;news&quot; : &#123; &quot;properties&quot; : &#123; &quot;title&quot; : &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;fields&quot;: &#123; &quot;suggest&quot; : &#123; &quot;type&quot; : &quot;completion&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; &#125; &#125; &#125;, &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; &#125; &#125; &#125; &#125;&#125; title里面,内置了一个field, suggest type设置为completion completion:es实现的时候是非常高性能的,不是用倒排索引,也不是用正排索引,就是纯用于进行前缀搜索的一种特殊的数据结构,而且会全部放在内存中,所以auto completion进行的前缀搜索提示,性能是很高的 添加测试数据123456789101112131415PUT /news_website/news/1&#123; &quot;title&quot;: &quot;大话西游电影&quot;, &quot;content&quot;: &quot;大话西游的电影时隔20年即将在2017年4月重映&quot;&#125;PUT /news_website/news/2&#123; &quot;title&quot;: &quot;大话西游小说&quot;, &quot;content&quot;: &quot;某知名网络小说作家已经完成了大话西游同名小说的出版&quot;&#125;PUT /news_website/news/3&#123; &quot;title&quot;: &quot;大话西游手游&quot;, &quot;content&quot;: &quot;网易游戏近日出品了大话西游经典IP的手游，正在火爆内测中&quot;&#125; 搜索请求1234567891011GET /news_website/news/_search&#123; &quot;suggest&quot;:&#123; &quot;my-suggest&quot;:&#123; &quot;prefix&quot;:&quot;大话西游&quot;, &quot;completion&quot;:&#123; &quot;field&quot;:&quot;title.suggest&quot; &#125; &#125; &#125;&#125; 返回值:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&#123; &quot;took&quot;: 13, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 0, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;suggest&quot;: &#123; &quot;my-suggest&quot;: [ &#123; &quot;text&quot;: &quot;大话西游&quot;, &quot;offset&quot;: 0, &quot;length&quot;: 4, &quot;options&quot;: [ &#123; &quot;text&quot;: &quot;大话西游小说&quot;, &quot;_index&quot;: &quot;news_website&quot;, &quot;_type&quot;: &quot;news&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;大话西游小说&quot;, &quot;content&quot;: &quot;某知名网络小说作家已经完成了大话西游同名小说的出版&quot; &#125; &#125;, &#123; &quot;text&quot;: &quot;大话西游手游&quot;, &quot;_index&quot;: &quot;news_website&quot;, &quot;_type&quot;: &quot;news&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;大话西游手游&quot;, &quot;content&quot;: &quot;网易游戏近日出品了大话西游经典IP的手游，正在火爆内测中&quot; &#125; &#125;, &#123; &quot;text&quot;: &quot;大话西游电影&quot;, &quot;_index&quot;: &quot;news_website&quot;, &quot;_type&quot;: &quot;news&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;大话西游电影&quot;, &quot;content&quot;: &quot;大话西游的电影时隔20年即将在2017年4月重映&quot; &#125; &#125; ] &#125; ] &#125;&#125; 这样所有大话西游前缀的数据都出来了, 然后我们比如要搜大话西游电影,那么请求就是12345678GET /news_website/news/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;大话西游电影&quot; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445&#123; &quot;took&quot;: 4, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 1.3495269, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;news_website&quot;, &quot;_type&quot;: &quot;news&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1.3495269, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;大话西游电影&quot;, &quot;content&quot;: &quot;大话西游的电影时隔20年即将在2017年4月重映&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;news_website&quot;, &quot;_type&quot;: &quot;news&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1.217097, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;大话西游手游&quot;, &quot;content&quot;: &quot;网易游戏近日出品了大话西游经典IP的手游，正在火爆内测中&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;news_website&quot;, &quot;_type&quot;: &quot;news&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1.1299736, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;大话西游小说&quot;, &quot;content&quot;: &quot;某知名网络小说作家已经完成了大话西游同名小说的出版&quot; &#125; &#125; ] &#125;&#125; 大话西游电影就被排在了最前面]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-94-search template搜索模板化]]></title>
    <url>%2F2019%2F01%2F21%2FElasticsearch-94-search-template%E6%90%9C%E7%B4%A2%E6%A8%A1%E6%9D%BF%E5%8C%96%2F</url>
    <content type="text"><![CDATA[简介search template 搜索模板,可以将一些常用的搜索模板化,每次要执行这个搜索的时候,就直接调用模板,然后传入参数就好了 入门先写一个简单的模板1234567891011121314GET /blog_website/blogs/_search/template&#123; &quot;inline&quot;:&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;&#123;&#123;field&#125;&#125;&quot; : &quot;&#123;&#123;value&#125;&#125;&quot; &#125; &#125; &#125;, &quot;params&quot;: &#123; &quot;field&quot;:&quot;title&quot;, &quot;value&quot;:&quot;博客&quot; &#125;&#125; inline 里面是模板,然后params里面传去参数里面是参数名 上面这个请求es会翻译成这样的12345678GET /blog_website/blogs/_search&#123; &quot;query&quot;: &#123; &quot;match&quot; : &#123; &quot;title&quot; : &quot;博客&quot; &#125; &#125;&#125; toJson123456789GET /blog_website/blogs/_search/template&#123; &quot;inline&quot;:&quot;&#123;\&quot;query\&quot;:&#123;\&quot;match\&quot;:&#123;&#123;#toJson&#125;&#125;matchCondition&#123;&#123;/toJson&#125;&#125;&#125;&#125;&quot;, &quot;params&quot;: &#123; &quot;matchCondition&quot;:&#123; &quot;title&quot;:&quot;博客&quot; &#125; &#125;&#125; 把对象类型的参数放进去 经过es转换后,会转为12345678GET /blog_website/blogs/_search&#123; &quot;query&quot;: &#123; &quot;match&quot; : &#123; &quot;title&quot; : &quot;博客&quot; &#125; &#125;&#125; join12345678910111213GET /blog_website/blogs/_search/template&#123; &quot;inline&quot; : &#123; &quot;query&quot; :&#123; &quot;match&quot;:&#123; &quot;title&quot;: &quot;&#123;&#123;#join delimiter=&apos; &apos;&#125;&#125;titles&#123;&#123;/join delimiter=&apos; &apos;&#125;&#125;&quot; &#125; &#125; &#125;, &quot;params&quot;: &#123; &quot;titles&quot;:[&quot;博客&quot;,&quot;网站&quot;] &#125;&#125; 将下面参数传入的数组用delimiter设置的值拼接起来 es转换后:12345678GET /blog_website/blogs/_search&#123; &quot;query&quot;: &#123; &quot;match&quot; : &#123; &quot;title&quot; : &quot;博客 网站&quot; &#125; &#125;&#125; default value加一个数据用来测试123456POST /blog_website/blogs/1/_update&#123; &quot;doc&quot;: &#123; &quot;views&quot;: 5 &#125;&#125; 模板:1234567891011121314151617GET blog_website/blogs/_search/template&#123; &quot;inline&quot;:&#123; &quot;query&quot;:&#123; &quot;range&quot;:&#123; &quot;views&quot;:&#123; &quot;gte&quot;:&quot;&#123;&#123;start&#125;&#125;&quot;, &quot;lte&quot;:&quot;&#123;&#123;end&#125;&#125;&#123;&#123;^end&#125;&#125;20&#123;&#123;/end&#125;&#125;&quot; &#125; &#125; &#125; &#125;, &quot;params&quot;: &#123; &quot;start&quot;:1, &quot;end&quot;:10 &#125;&#125; 就是,有传值的话就按传的值去搜索,不传值的话,就是按照设置的默认值来搜索,比如上面模板中的end,如果不传值的话就是默认的20 es转换后:1234567891011GET /blog_website/blogs/_search&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;views&quot;: &#123; &quot;gte&quot;: 1, &quot;lte&quot;: 20 &#125; &#125; &#125;&#125; conditional这里要去es的config/scripts目录下,预先把模板保存下来,这里文件名是conditonal,后缀名是.mustache 文件内容:1234567891011121314151617181920212223242526&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;line&quot;: &quot;&#123;&#123;text&#125;&#125;&quot; &#125; &#125;, &quot;filter&quot;: &#123; &#123;&#123;#line_no&#125;&#125; &quot;range&quot;: &#123; &quot;line_no&quot;: &#123; &#123;&#123;#start&#125;&#125; &quot;gte&quot;: &quot;&#123;&#123;start&#125;&#125;&quot; &#123;&#123;#end&#125;&#125;,&#123;&#123;/end&#125;&#125; &#123;&#123;/start&#125;&#125; &#123;&#123;#end&#125;&#125; &quot;lte&quot;: &quot;&#123;&#123;end&#125;&#125;&quot; &#123;&#123;/end&#125;&#125; &#125; &#125; &#123;&#123;/line_no&#125;&#125; &#125; &#125; &#125;&#125; 添加一条数据做测试:12345PUT my_index/my_type/1&#123; &quot;line&quot;: &quot;我的博客&quot;, &quot;line_no&quot;: 5&#125; 模板保存好了之后,去调用这个模板,传参,查询12345678910GET /my_index/my_type/_search/template&#123; &quot;file&quot;: &quot;conditional&quot;, &quot;params&quot;: &#123; &quot;text&quot;: &quot;博客&quot;, &quot;line_no&quot;: true, &quot;start&quot;: 1, &quot;end&quot;: 10 &#125;&#125; 返回值:12345678910111213141516171819202122232425&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.5753642, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.5753642, &quot;_source&quot;: &#123; &quot;line&quot;: &quot;我的博客&quot;, &quot;line_no&quot;: 5 &#125; &#125; ] &#125;&#125; 模板里有一个参数是list_no,然后底下调用的时候传了true,就会走filter里面的range,就跟mybaits里面的判空,然后加一个搜索条件是类似的 模板保存以上所有的模板,都可以保存到config/scripts目录下,模板文件必须是.mustache结尾的,然后调用的时候就是上面那样调用就好了 使用场景:可能在项目开发过程中,有很多人都会执行一些类似的操作,这时候就可以封装成模板,然后每个人使用的时候传入参数就可以了]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-93-深入剖析搜索结果的高亮显示]]></title>
    <url>%2F2019%2F01%2F18%2FElasticsearch-93-%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90%E6%90%9C%E7%B4%A2%E7%BB%93%E6%9E%9C%E7%9A%84%E9%AB%98%E4%BA%AE%E6%98%BE%E7%A4%BA%2F</url>
    <content type="text"><![CDATA[搜索结果高亮显示先来看一个最基本的高亮案例 首先创建一个索引1234567891011121314151617PUT /blog_website&#123; &quot;mappings&quot;: &#123; &quot;blogs&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; &#125;, &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; &#125; &#125; &#125; &#125;&#125; 加条数据进去12345PUT /blog_website/blogs/1&#123; &quot;title&quot;: &quot;我的第一篇博客&quot;, &quot;content&quot;: &quot;大家好，这是我写的第一篇博客，特别喜欢这个博客网站！！！&quot;&#125; 搜索12345678910111213GET /blog_website/blogs/_search &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;博客&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;title&quot;: &#123;&#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930&#123; &quot;took&quot;: 29, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.26742277, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;blog_website&quot;, &quot;_type&quot;: &quot;blogs&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.26742277, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;我的第一篇博客&quot;, &quot;content&quot;: &quot;大家好，这是我写的第一篇博客，特别喜欢这个博客网站！！！&quot; &#125;, &quot;highlight&quot;: &#123; &quot;title&quot;: [ &quot;我的第一篇&lt;em&gt;博客&lt;/em&gt;&quot; ] &#125; &#125; ] &#125;&#125; &lt;em&gt;&lt;/em&gt; 标签,会在网页中将内容变成红色,所以指定的field中,如果包含了那个搜索词的话,就会在那个field的文本中,对搜索词进行红色的高亮显示 多个filed高亮也是一样的12345678910111213141516171819202122232425GET /blog_website/blogs/_search &#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;博客&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;博客&quot; &#125; &#125; ] &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;title&quot;: &#123;&#125;, &quot;content&quot;: &#123;&#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233&#123; &quot;took&quot;: 8, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.6390219, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;blog_website&quot;, &quot;_type&quot;: &quot;blogs&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.6390219, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;我的第一篇博客&quot;, &quot;content&quot;: &quot;大家好，这是我写的第一篇博客，特别喜欢这个博客网站！！！&quot; &#125;, &quot;highlight&quot;: &#123; &quot;title&quot;: [ &quot;我的第一篇&lt;em&gt;博客&lt;/em&gt;&quot; ], &quot;content&quot;: [ &quot;大家好，这是我写的第一篇&lt;em&gt;博客&lt;/em&gt;，特别喜欢这个&lt;em&gt;博客&lt;/em&gt;网站！！！&quot; ] &#125; &#125; ] &#125;&#125; highlight中的field,必须跟query中的field是一一对应的 三种highlight介绍plain highlight默认的高亮就是用的这种,底层是lucene highlight posting highlight设置索引的 index_options = offset 后,高亮就是用的posting highlight 优点: 性能比plain highlight要高,因为不需要重新对高亮文本进行分词 对磁盘的消耗更少 将文本分割为句子,并且对句子进行高亮,效果更好 删除之前的索引,然后重新建123456789101112131415161718PUT /blog_website&#123; &quot;mappings&quot;: &#123; &quot;blogs&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; &#125;, &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;index_options&quot;: &quot;offsets&quot; &#125; &#125; &#125; &#125;&#125; content设置了 “index_options”: “offsets” ,然后还是把之前那条数据添加进去,对content搜索高亮12345678910111213GET /blog_website/blogs/_search &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;博客&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;content&quot;: &#123;&#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930&#123; &quot;took&quot;: 33, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.37159908, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;blog_website&quot;, &quot;_type&quot;: &quot;blogs&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.37159908, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;我的第一篇博客&quot;, &quot;content&quot;: &quot;大家好，这是我写的第一篇博客，特别喜欢这个博客网站！！！&quot; &#125;, &quot;highlight&quot;: &#123; &quot;content&quot;: [ &quot;大家好，这是我写的第一篇&lt;em&gt;博客&lt;/em&gt;，特别喜欢这个&lt;em&gt;博客&lt;/em&gt;网站！！！&quot; ] &#125; &#125; ] &#125;&#125; 其实效果跟plain是一样的 fast vector highlightindex-time时候term vector设置在mapping中,就会用fast vector highlight 在field值比较大(大于1MB)的情况下性能更高 还是将之前的删掉,重新建123456789101112131415161718PUT /blog_website&#123; &quot;mappings&quot;: &#123; &quot;blogs&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; &#125;, &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;term_vector&quot; : &quot;with_positions_offsets&quot; &#125; &#125; &#125; &#125;&#125; 在content里面设置了term_vector,就可以使用了 强制使用某种highlight比如,对于开启了term_vector的filed,强制使用plain highlight123456789101112131415GET /blog_website/blogs/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;博客&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;content&quot;:&#123; &quot;type&quot;:&quot;plain&quot; &#125; &#125; &#125;&#125; 自定义高亮html标签默认是&lt;em&gt;&lt;/em&gt;,可以通过设置pre_tags和post_tags来自定义html标签 1234567891011121314151617GET /blog_website/blogs/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;博客&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;pre_tags&quot;: [&quot;&lt;tag&gt;&quot;], &quot;post_tags&quot;: [&quot;&lt;/tag&gt;&quot;], &quot;fields&quot;: &#123; &quot;content&quot;:&#123; &quot;type&quot;:&quot;plain&quot; &#125; &#125; &#125;&#125; 高亮片段fragment的设置fragment_size先来添加一个比较长的数据12345PUT /blog_website/blogs/2&#123; &quot;title&quot;: &quot;我的第二篇博客&quot;, &quot;content&quot;: &quot;大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！&quot;&#125; 然后查询的时候设置 fragment_size123456789101112131415161718GET /blog_website/blogs/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;博客&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;pre_tags&quot;: [&quot;&lt;tag&gt;&quot;], &quot;post_tags&quot;: [&quot;&lt;/tag&gt;&quot;], &quot;fields&quot;: &#123; &quot;content&quot;:&#123; &quot;type&quot;:&quot;plain&quot;, &quot;fragment_size&quot;: 20 &#125; &#125; &#125;&#125; 返回值:12345678910111213141516171819202122232425262728293031323334&#123; &quot;took&quot;: 6, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.56305844, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;blog_website&quot;, &quot;_type&quot;: &quot;blogs&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.56305844, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;我的第二篇博客&quot;, &quot;content&quot;: &quot;大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！&quot; &#125;, &quot;highlight&quot;: &#123; &quot;content&quot;: [ &quot;大家好，这是我写的第二篇&lt;tag&gt;博客&lt;/tag&gt;，特别喜欢&quot;, &quot;这个&lt;tag&gt;博客&lt;/tag&gt;网站！！！大家好，这是我写&quot;, &quot;的第二篇&lt;tag&gt;博客&lt;/tag&gt;，特别喜欢这个&lt;tag&gt;博客&lt;/tag&gt;网站！！！大家好&quot;, &quot;，这是我写的第二篇&lt;tag&gt;博客&lt;/tag&gt;，特别喜欢这个&lt;tag&gt;博客&lt;/tag&gt;&quot;, &quot;网站！！！大家好，这是我写的第二篇&lt;tag&gt;博客&lt;/tag&gt;&quot; ] &#125; &#125; ] &#125;&#125; 设置了fragment_size是20,然后在highlight中,就会把content按长度是20的切割成多个 使用场景:有时候field长度太长,但是你的页面不可能全显示出来,可能只需要显示一段内容就好,然后就可以通过设置fragment_size(默认是100)来将field的值进行分割 number_of_fragmentsnumber_of_fragments: 用来指定显示多少个片段 比如上面,一共拆分出5个片段来,可能我们只需要三个,那就设置number_of_fragments为3,就可以了12345678910111213141516171819GET /blog_website/blogs/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;博客&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;pre_tags&quot;: [&quot;&lt;tag&gt;&quot;], &quot;post_tags&quot;: [&quot;&lt;/tag&gt;&quot;], &quot;fields&quot;: &#123; &quot;content&quot;:&#123; &quot;type&quot;:&quot;plain&quot;, &quot;fragment_size&quot;: 20, &quot;number_of_fragments&quot;: 3 &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526272829303132&#123; &quot;took&quot;: 4, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.56305844, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;blog_website&quot;, &quot;_type&quot;: &quot;blogs&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.56305844, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;我的第二篇博客&quot;, &quot;content&quot;: &quot;大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！大家好，这是我写的第二篇博客，特别喜欢这个博客网站！！！&quot; &#125;, &quot;highlight&quot;: &#123; &quot;content&quot;: [ &quot;大家好，这是我写的第二篇&lt;tag&gt;博客&lt;/tag&gt;，特别喜欢&quot;, &quot;这个&lt;tag&gt;博客&lt;/tag&gt;网站！！！大家好，这是我写&quot;, &quot;的第二篇&lt;tag&gt;博客&lt;/tag&gt;，特别喜欢这个&lt;tag&gt;博客&lt;/tag&gt;网站！！！大家好&quot; ] &#125; &#125; ] &#125;&#125; 只显示了前3个片段]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-92-查询term vectors词条向量信息]]></title>
    <url>%2F2019%2F01%2F18%2FElasticsearch-92-%E6%9F%A5%E8%AF%A2term-vectors%E8%AF%8D%E6%9D%A1%E5%90%91%E9%87%8F%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[term vectors介绍可以理解为,关于 词 的一些统计信息. 可以查询到的信息比如有 词条的信息,比如position位置,start_offset开始的偏移值, end_offset结束的偏移值,词条的payLoads(主要用于自定义字段的权重) 词条统计,doc_freq, ttf term_freq 该词出现的次数 频率 字段统计,包含sum_doc_freq:该字段中词的数量(去掉重复的数目) sum_ttf:文档中词的数量(包含重复的数目)、doc_count:涉及的文档数等等 默认这些统计信息都是基于分片的,可以设置dfs为true,返回全部的分片的信息,但是会有一定的性能问题,不推荐使用,还可以通过参数对返回的字段进行过滤,只返回感兴趣的部分 可以通过两种方式查询到term vector的信息 index-time,创建索引的时候,在mapping里面配置一下,就直接生成这些term和field的统计信息了 query-time,不需要提前创建,直接查询的时候使用就好了,是现场计算返回的 index-time生成创建索引12345678910111213141516171819202122232425262728293031323334353637PUT /my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;text&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;term_vector&quot;: &quot;with_positions_offsets_payloads&quot;, &quot;store&quot; : true, &quot;analyzer&quot; : &quot;fulltext_analyzer&quot; &#125;, &quot;fullname&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot; : &quot;fulltext_analyzer&quot; &#125; &#125; &#125; &#125;, &quot;settings&quot; : &#123; &quot;index&quot; : &#123; &quot;number_of_shards&quot; : 1, &quot;number_of_replicas&quot; : 0 &#125;, &quot;analysis&quot;: &#123; &quot;analyzer&quot;: &#123; &quot;fulltext_analyzer&quot;: &#123; &quot;type&quot;: &quot;custom&quot;, &quot;tokenizer&quot;: &quot;whitespace&quot;, &quot;filter&quot;: [ &quot;lowercase&quot;, &quot;type_as_payload&quot; ] &#125; &#125; &#125; &#125;&#125; 最下面是创建了一个分词器,然后settings里面是设置了shard的数量,上面mappings里面设置了两个field,再看text里面,设置了term_vector, fullname是没有设置的 添加两条数据进去1234567891011PUT /my_index/my_type/1&#123; &quot;fullname&quot; : &quot;Leo Li&quot;, &quot;text&quot; : &quot;hello test test test &quot;&#125;PUT /my_index/my_type/2&#123; &quot;fullname&quot; : &quot;Leo Li&quot;, &quot;text&quot; : &quot;other hello test ...&quot;&#125; 查询term vectors的数据123456789GET /my_index/my_type/1/_termvectors&#123; &quot;fields&quot;: [&quot;text&quot;], &quot;offsets&quot;: true, &quot;payloads&quot;: true, &quot;positions&quot;: true, &quot;term_statistics&quot;: true, &quot;field_statistics&quot;: true&#125; 用_termvectors查询,就是查询id是1doc中. text这个field里面的词,下面offsets,payloads,这些,都是用来控制这些数据在返回值显示不显示 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;took&quot;: 9, &quot;term_vectors&quot;: &#123; &quot;text&quot;: &#123; &quot;field_statistics&quot;: &#123; &quot;sum_doc_freq&quot;: 6, // 所有的document的trem的 doc_freq加起来 &quot;doc_count&quot;: 2, // 有多少document包含这个field &quot;sum_ttf&quot;: 8 // 所有document的trem 的ttf加起来 &#125;, &quot;terms&quot;: &#123; // 查询的这个field的值的所有term &quot;hello&quot;: &#123; // term值 &quot;doc_freq&quot;: 2, // 有多少document包含这个term &quot;ttf&quot;: 2, // 这个term在所有document中出现的频率 &quot;term_freq&quot;: 1, // 这个term在当前document中出现了几次 &quot;tokens&quot;: [ // 一个trem 可能在这个doc中出现了好几次,每个都是一个token &#123; &quot;position&quot;: 0, // 位置 &quot;start_offset&quot;: 0, // 开始下标 &quot;end_offset&quot;: 5, // 结束下标 &quot;payload&quot;: &quot;d29yZA==&quot; &#125; ] &#125;, &quot;test&quot;: &#123; &quot;doc_freq&quot;: 2, &quot;ttf&quot;: 4, &quot;term_freq&quot;: 3, &quot;tokens&quot;: [ &#123; &quot;position&quot;: 1, &quot;start_offset&quot;: 6, &quot;end_offset&quot;: 10, &quot;payload&quot;: &quot;d29yZA==&quot; &#125;, &#123; &quot;position&quot;: 2, &quot;start_offset&quot;: 11, &quot;end_offset&quot;: 15, &quot;payload&quot;: &quot;d29yZA==&quot; &#125;, &#123; &quot;position&quot;: 3, &quot;start_offset&quot;: 16, &quot;end_offset&quot;: 20, &quot;payload&quot;: &quot;d29yZA==&quot; &#125; ] &#125; &#125; &#125; &#125;&#125; query-time查询 term vector上面我们创建索引的时候是没有对fullname去设置 term vector的,所以查询fullname的term vector就是query-time生成的 语法还和之前一样12345678GET /my_index/my_type/1/_termvectors&#123; &quot;fields&quot; : [&quot;fullname&quot;], &quot;offsets&quot; : true, &quot;positions&quot; : true, &quot;term_statistics&quot; : true, &quot;field_statistics&quot; : true&#125; 返回值也是和上面是相同的, 一般来说,如果条件允许,就用query-time的term vector就可以了 手动指定doc的term vector请求:12345678910111213GET /my_index/my_type/_termvectors&#123; &quot;doc&quot; : &#123; &quot;fullname&quot; : &quot;Leo Li&quot;, &quot;text&quot; : &quot;hello test test test&quot; &#125;, &quot;fields&quot; : [&quot;text&quot;], &quot;offsets&quot; : true, &quot;payloads&quot; : true, &quot;positions&quot; : true, &quot;term_statistics&quot; : true, &quot;field_statistics&quot; : true&#125; 这里是手动指定了一个doc, 实际上不是去查这个doc,而是指定你想要去安插的词条,比如上面这个请求,是查询的text这个field, 那么就是将doc里的text进行分词,然后对每个term,都去计算它现有的所有doc中的一些统计信息 这个还是挺有用的,可以手动指定要探查的term的数据情况 手动指定分词器 来生成term vector12345678910111213141516GET /my_index/my_type/_termvectors&#123; &quot;doc&quot; : &#123; &quot;fullname&quot; : &quot;Leo Li&quot;, &quot;text&quot; : &quot;hello test test test&quot; &#125;, &quot;fields&quot; : [&quot;text&quot;], &quot;offsets&quot; : true, &quot;payloads&quot; : true, &quot;positions&quot; : true, &quot;term_statistics&quot; : true, &quot;field_statistics&quot; : true, &quot;per_field_analyzer&quot; : &#123; &quot;text&quot;: &quot;standard&quot; &#125;&#125; 前面的还是一样,就是请求最后,加了一个指定的分词器 terms filter123456789101112131415161718GET /my_index/my_type/_termvectors&#123; &quot;doc&quot; : &#123; &quot;fullname&quot; : &quot;Leo Li&quot;, &quot;text&quot; : &quot;hello test test test&quot; &#125;, &quot;fields&quot; : [&quot;text&quot;], &quot;offsets&quot; : true, &quot;payloads&quot; : true, &quot;positions&quot; : true, &quot;term_statistics&quot; : true, &quot;field_statistics&quot; : true, &quot;filter&quot; : &#123; &quot;max_num_terms&quot; : 3, &quot;min_term_freq&quot; : 1, &quot;min_doc_freq&quot; : 1 &#125;&#125; 请求后加了一个filter参数,常用的有 max_num_terms 最大的词条数目 min_term_freq 最小的词频，比如忽略那些在字段中出现次数小于一定值的词条。 max_term_freq 最大的词频 min_doc_freq 最小的文档频率，比如忽略那些在文档中出现次数小于一定的值的词条 max_doc_freq 最大的文档频率 min_word_length 忽略的词的最小长度 max_word_length 忽略的词的最大长度 就是说,根据term统计信息,过滤出想要看到term vector统计结果比如说,可以过滤掉一些出现频率过低的term multi term vector请求体中,指定index和type,id12345678910111213141516171819GET _mtermvectors&#123; &quot;docs&quot;: [ &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;term_statistics&quot;: true &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;fields&quot;: [ &quot;text&quot; ] &#125; ]&#125; 请求体中,指定type和id1234567891011121314151617GET /my_index/_mtermvectors&#123; &quot;docs&quot;: [ &#123; &quot;_type&quot;: &quot;test&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;fields&quot;: [ &quot;text&quot; ], &quot;term_statistics&quot;: true &#125;, &#123; &quot;_type&quot;: &quot;test&quot;, &quot;_id&quot;: &quot;1&quot; &#125; ]&#125; 请求体中指定id123456789101112131415GET /my_index/my_type/_mtermvectors&#123; &quot;docs&quot;: [ &#123; &quot;_id&quot;: &quot;2&quot;, &quot;fields&quot;: [ &quot;text&quot; ], &quot;term_statistics&quot;: true &#125;, &#123; &quot;_id&quot;: &quot;1&quot; &#125; ]&#125; 123456789101112131415161718192021GET /_mtermvectors&#123; &quot;docs&quot;: [ &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;doc&quot; : &#123; &quot;fullname&quot; : &quot;Leo Li&quot;, &quot;text&quot; : &quot;hello test test test&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;doc&quot; : &#123; &quot;fullname&quot; : &quot;Leo Li&quot;, &quot;text&quot; : &quot;other hello test ...&quot; &#125; &#125; ]&#125; 跟multi-type搜索模式是类似的]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-91-祖孙三层数据关系建模及搜索]]></title>
    <url>%2F2019%2F01%2F18%2FElasticsearch-91-%E7%A5%96%E5%AD%99%E4%B8%89%E5%B1%82%E6%95%B0%E6%8D%AE%E5%85%B3%E7%B3%BB%E5%BB%BA%E6%A8%A1%E5%8F%8A%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[数据建模祖孙三层的数据建模12345678910111213141516PUT /company&#123; &quot;mappings&quot;: &#123; &quot;country&quot;: &#123;&#125;, &quot;rd_center&quot;: &#123; &quot;_parent&quot;: &#123; &quot;type&quot;: &quot;country&quot; &#125; &#125;, &quot;employee&quot;: &#123; &quot;_parent&quot;: &#123; &quot;type&quot;: &quot;rd_center&quot; &#125; &#125; &#125;&#125; 看下请求,company下面有三个type,依次是country,rd_center,employee, 和之前一样,也是一层一层的指定parent就好了. 索引建好后,先在country里面添加几条数据12345POST /company/country/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;1&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;中国&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;2&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;美国&quot; &#125; 然后再添加研发中心的数据,和之前添加的方式是一样的,指定parent id就好了1234567POST /company/rd_center/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;1&quot;, &quot;parent&quot;: &quot;1&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;北京研发总部&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;2&quot;, &quot;parent&quot;: &quot;1&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;上海研发中心&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;3&quot;, &quot;parent&quot;: &quot;2&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;硅谷人工智能实验室&quot; &#125; 最后,添加员工的数据123456PUT /company/employee/1?parent=1&amp;routing=1&#123; &quot;name&quot;: &quot;张三&quot;, &quot;dob&quot;: &quot;1970-10-24&quot;, &quot;hobby&quot;: &quot;爬山&quot;&#125; 这里就和上面有些不一样了, country添加的时候,是用的自己的id去路由的,rd_center添加的时候,指定了parent id,所以他也是用的country 的id去路由的, 但是employee在添加的时候如果也是仅仅指定一个parent,那就表示用的是rd_center的id去路由,这就可能导致,这三种数据不在一个shard上. 所以孙子辈儿的doc添加的时候除了指定parent还需要加一个routing,值是爷爷辈儿的数据的id 搜索案例需求: 搜索有爬山爱好的员工所在的国家 123456789101112131415161718GET /company/country/_search&#123; &quot;query&quot;: &#123; &quot;has_child&quot;: &#123; &quot;type&quot;: &quot;rd_center&quot;, &quot;query&quot;: &#123; &quot;has_child&quot;: &#123; &quot;type&quot;: &quot;employee&quot;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;hobby&quot;: &quot;爬山&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324&#123; &quot;took&quot;: 24, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;company&quot;, &quot;_type&quot;: &quot;country&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;中国&quot; &#125; &#125; ] &#125;&#125; 需要一级一级用has_child去找]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-90-父子关系类型的数据建模与搜索聚合]]></title>
    <url>%2F2019%2F01%2F17%2FElasticsearch-90-%E7%88%B6%E5%AD%90%E5%85%B3%E7%B3%BB%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%90%9C%E7%B4%A2%E8%81%9A%E5%90%88%2F</url>
    <content type="text"><![CDATA[parent child建模之前用的nested object的建模方式,有个不好的地方,就是采取类似冗余数据的方式,将多个数据都放在了一起,维护成本比较高 parent child建模方式,采用的是类似数据库三范式的建模,多个实体都分割开来,每个实体之间都通过一些关联的方式,进行了父子关系的关联,各种数据都不需要放在一起,父doc和子doc在更新的时候,都不会影响对方 要点父子关系元数据映射,是用来保证查询时候的高性能,但是有一个限制,就是父子数据必须存在于同一个shard中 数据存在同一个shard中,而且还有映射其关联关系的元数据,那么搜索父子关系数据的时候,不用跨分片,一个分片本地自己搞定,性能自然就高 建模案例背景:以研发中心员工管理为案例,一个公司下有多个研发中心,一个研发中心下有多个员工 首先需要手动创建索引,设置mapping1234567891011PUT /company&#123; &quot;mappings&quot;: &#123; &quot;rd_center&quot;:&#123;&#125;, &quot;employee&quot;:&#123; &quot;_parent&quot;: &#123; &quot;type&quot;: &quot;rd_center&quot; &#125; &#125; &#125;&#125; 就是创建了一个company的索引,然后创建了两个type,一个是研发中心rd_center,一个是员工employee,员工里面设置了一个_parent,指向了rd_center,这样就建立了研发中心和员工的父子关系 父子关系建模的核心,就是多个type之间有父子关系的话,通过设置_parent指定父type 索引建好之后,添加几个研发中心的数据1234567POST /company/rd_center/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;1&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;北京研发总部&quot;, &quot;city&quot;: &quot;北京&quot;, &quot;country&quot;: &quot;中国&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;2&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;上海研发中心&quot;, &quot;city&quot;: &quot;上海&quot;, &quot;country&quot;: &quot;中国&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;3&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;硅谷人工智能实验室&quot;, &quot;city&quot;: &quot;硅谷&quot;, &quot;country&quot;: &quot;美国&quot; &#125; 然后,再添加子type的数据123456PUT /company/employee/1?parent=1 &#123; &quot;name&quot;: &quot;张三&quot;, &quot;birthday&quot;: &quot;1970-10-24&quot;, &quot;hobby&quot;: &quot;爬山&quot;&#125; 这里在添加的时候跟了一个parent的参数,参数值是父doc的id 传了这个值以后就不会根据id是1的这个employee doc去路由了,而是根据id是1的这个父doc的路由规则去路由 parent-child关系,就确保了说,父doc和子doc都是保存在一个shard上的.内部原理还是doc routing,employee和rd_center的数据,都会用parent id作为routing,这样就会到一个shard _bulk批量添加几条数据进去1234567POST /company/employee/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 2, &quot;parent&quot;: &quot;1&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;李四&quot;, &quot;birthday&quot;: &quot;1982-05-16&quot;, &quot;hobby&quot;: &quot;游泳&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 3, &quot;parent&quot;: &quot;2&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;王二&quot;, &quot;birthday&quot;: &quot;1979-04-01&quot;, &quot;hobby&quot;: &quot;爬山&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 4, &quot;parent&quot;: &quot;3&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;赵五&quot;, &quot;birthday&quot;: &quot;1987-05-11&quot;, &quot;hobby&quot;: &quot;骑马&quot; &#125; 搜索 聚合案例建立好父子关系的数据模型之后,就要基于这个模型进行各种搜索和聚合了 搜索一需求: 搜索1980年以后出生的员工的研发中心 123456789101112131415GET /company/rd_center/_search&#123; &quot;query&quot;: &#123; &quot;has_child&quot;: &#123; &quot;type&quot;: &quot;employee&quot;, &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;birthday&quot;: &#123; &quot;gte&quot;: &quot;1980-01-01&quot; &#125; &#125; &#125; &#125; &#125;&#125; 返回值:12345678910111213141516171819202122232425262728293031323334353637&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;company&quot;, &quot;_type&quot;: &quot;rd_center&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;北京研发总部&quot;, &quot;city&quot;: &quot;北京&quot;, &quot;country&quot;: &quot;中国&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;company&quot;, &quot;_type&quot;: &quot;rd_center&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;硅谷人工智能实验室&quot;, &quot;city&quot;: &quot;硅谷&quot;, &quot;country&quot;: &quot;美国&quot; &#125; &#125; ] &#125;&#125; 就是搜索的父级类型,然后用has_child,下面设置child的type名称,然后查询就ok了 搜索二需求: 搜索有姓名叫张三的员工的研发中心12345678910111213GET /company/rd_center/_search&#123; &quot;query&quot;: &#123; &quot;has_child&quot;: &#123; &quot;type&quot;: &quot;employee&quot;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;张三&quot; &#125; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;company&quot;, &quot;_type&quot;: &quot;rd_center&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;北京研发总部&quot;, &quot;city&quot;: &quot;北京&quot;, &quot;country&quot;: &quot;中国&quot; &#125; &#125; ] &#125;&#125; 搜索三需求: 搜索至少两个员工以上的研发中心123456789101112GET /company/rd_center/_search&#123; &quot;query&quot;: &#123; &quot;has_child&quot;: &#123; &quot;type&quot;: &quot;employee&quot;, &quot;min_children&quot;: 2, &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526&#123; &quot;took&quot;: 4, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;company&quot;, &quot;_type&quot;: &quot;rd_center&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;北京研发总部&quot;, &quot;city&quot;: &quot;北京&quot;, &quot;country&quot;: &quot;中国&quot; &#125; &#125; ] &#125;&#125; 搜索四需求: 搜索在中国的研发中心的员工12345678910111213GET /company/employee/_search&#123; &quot;query&quot;: &#123; &quot;has_parent&quot;: &#123; &quot;parent_type&quot;: &quot;rd_center&quot;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;country.keyword&quot;: &quot;中国&quot; &#125; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&#123; &quot;took&quot;: 6, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;company&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1, &quot;_routing&quot;: &quot;2&quot;, &quot;_parent&quot;: &quot;2&quot;, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;王二&quot;, &quot;birthday&quot;: &quot;1979-04-01&quot;, &quot;hobby&quot;: &quot;爬山&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;company&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_routing&quot;: &quot;1&quot;, &quot;_parent&quot;: &quot;1&quot;, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;张三&quot;, &quot;birthday&quot;: &quot;1970-10-24&quot;, &quot;hobby&quot;: &quot;爬山&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;company&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_routing&quot;: &quot;1&quot;, &quot;_parent&quot;: &quot;1&quot;, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;李四&quot;, &quot;birthday&quot;: &quot;1982-05-16&quot;, &quot;hobby&quot;: &quot;游泳&quot; &#125; &#125; ] &#125;&#125; 聚合需求: 对每个国家的员工的兴趣爱好进行统计. 先对国家划分bucket,然后再进行兴趣爱好划分bucket 12345678910111213141516171819202122232425GET /company/rd_center/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_country&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;country.keyword&quot; &#125;, &quot;aggs&quot;: &#123; &quot;group_by_child_employee&quot;: &#123; &quot;children&quot;: &#123; &quot;type&quot;: &quot;employee&quot; &#125;, &quot;aggs&quot;: &#123; &quot;group_by_hobby&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;hobby.keyword&quot; &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&#123; &quot;took&quot;: 12, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_country&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;中国&quot;, &quot;doc_count&quot;: 2, &quot;group_by_child_employee&quot;: &#123; &quot;doc_count&quot;: 3, &quot;group_by_hobby&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;爬山&quot;, &quot;doc_count&quot;: 2 &#125;, &#123; &quot;key&quot;: &quot;游泳&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125; &#125; &#125;, &#123; &quot;key&quot;: &quot;美国&quot;, &quot;doc_count&quot;: 1, &quot;group_by_child_employee&quot;: &#123; &quot;doc_count&quot;: 1, &quot;group_by_hobby&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;骑马&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125; &#125; &#125; ] &#125; &#125;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-89-针对nested object的聚合分析案例]]></title>
    <url>%2F2019%2F01%2F17%2FElasticsearch-89-%E9%92%88%E5%AF%B9nested-object%E7%9A%84%E8%81%9A%E5%90%88%E5%88%86%E6%9E%90%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[对于nested object里面的数据,如何做聚合分析呢? 以上文的数据作为背景 来看两个案例 案例一需求: 按照评论日期进行划分bucket,拿到每个月评论的stars的平均值请求123456789101112131415161718192021222324252627GET /website/blogs/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;comments_path&quot;: &#123; &quot;nested&quot;: &#123; &quot;path&quot;: &quot;comments&quot; &#125;, &quot;aggs&quot;: &#123; &quot;group_by_comments_date&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;comments.date&quot;, &quot;interval&quot;: &quot;month&quot;, &quot;format&quot;: &quot;yyyy-MM&quot; &#125;, &quot;aggs&quot;: &#123; &quot;avg_stars&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;comments.stars&quot; &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;took&quot;: 7, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;comments_path&quot;: &#123; &quot;doc_count&quot;: 2, &quot;group_by_comments_date&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key_as_string&quot;: &quot;2016-09&quot;, &quot;key&quot;: 1472688000000, &quot;doc_count&quot;: 1, &quot;avg_stars&quot;: &#123; &quot;value&quot;: 4 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-10&quot;, &quot;key&quot;: 1475280000000, &quot;doc_count&quot;: 1, &quot;avg_stars&quot;: &#123; &quot;value&quot;: 5 &#125; &#125; ] &#125; &#125; &#125;&#125; 看一下请求,第一个aggs,设置nested的path,然后第二个aggs是根据日期按月划分bucket的,第三个aggs是用来计算平均数的 案例二需求: 根据评论的年龄来划分bucket,然后按博客的tags进行分组 这里,评论的年龄是comments里面的数据,但是tags是外面的数据,那么如何用nested object外面的数据来进行聚合呢,先看一下请求 12345678910111213141516171819202122232425262728293031GET /website/blogs/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;comments_path&quot;: &#123; &quot;nested&quot;: &#123; &quot;path&quot;: &quot;comments&quot; &#125;, &quot;aggs&quot;: &#123; &quot;group_by_age&quot;: &#123; &quot;histogram&quot;: &#123; &quot;field&quot;: &quot;comments.age&quot;, &quot;interval&quot;: 10 &#125;, &quot;aggs&quot;: &#123; &quot;reverse_path&quot;:&#123; &quot;reverse_nested&quot;: &#123;&#125;, &quot;aggs&quot;: &#123; &quot;group_by_tags&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;tags.keyword&quot; &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;comments_path&quot;: &#123; &quot;doc_count&quot;: 2, &quot;group_by_comments_age&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: 20, &quot;doc_count&quot;: 1, &quot;reverse_path&quot;: &#123; &quot;doc_count&quot;: 1, &quot;group_by_tags&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;投资&quot;, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: &quot;理财&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125; &#125; &#125;, &#123; &quot;key&quot;: 30, &quot;doc_count&quot;: 1, &quot;reverse_path&quot;: &#123; &quot;doc_count&quot;: 1, &quot;group_by_tags&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;投资&quot;, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: &quot;理财&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125; &#125; &#125; ] &#125; &#125; &#125;&#125; 首先,第一个aggs还是先设置path,然后下钻第二个aggs根据年龄去分段划分bucket,继续下钻,第三个aggs的时候用了一个reverse_nested:{},加上这个之后,就可以使用nested object外面的数据了,最后再一次下钻,按照tags分组,因为tags是分词的,所以用了tags.keyword]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-88-nested object数据结构]]></title>
    <url>%2F2019%2F01%2F17%2FElasticsearch-88-nested-object%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[普通object类型用冗余数据的方式来建模,其实用的就是object类型,来看一个数据12345678910111213141516171819202122PUT /website/blogs/6&#123; &quot;title&quot;: &quot;花无缺发表的一篇帖子&quot;, &quot;content&quot;: &quot;我是花无缺，大家要不要考虑一下投资房产和买股票的事情啊。。。&quot;, &quot;tags&quot;: [ &quot;投资&quot;, &quot;理财&quot; ], &quot;comments&quot;: [ &#123; &quot;name&quot;: &quot;小鱼儿&quot;, &quot;comment&quot;: &quot;什么股票啊？推荐一下呗&quot;, &quot;age&quot;: 28, &quot;stars&quot;: 4, &quot;date&quot;: &quot;2016-09-01&quot; &#125;, &#123; &quot;name&quot;: &quot;黄药师&quot;, &quot;comment&quot;: &quot;我喜欢投资房产，风，险大收益也大&quot;, &quot;age&quot;: 31, &quot;stars&quot;: 5, &quot;date&quot;: &quot;2016-10-22&quot; &#125; ]&#125; 就是一个帖子下的所有评论,这种类型的数据,就是object类型的,然后来做一个搜索 搜索案例需求: 搜索被年龄是28岁的黄药师评论过的博客 12345678910111213141516171819GET /website/blogs/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;comments.name&quot;: &quot;黄药师&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;comments.age&quot;: &quot;28&quot; &#125; &#125; ] &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445&#123; &quot;took&quot;: 21, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1.8022683, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blogs&quot;, &quot;_id&quot;: &quot;6&quot;, &quot;_score&quot;: 1.8022683, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;花无缺发表的一篇帖子&quot;, &quot;content&quot;: &quot;我是花无缺，大家要不要考虑一下投资房产和买股票的事情啊。。。&quot;, &quot;tags&quot;: [ &quot;投资&quot;, &quot;理财&quot; ], &quot;comments&quot;: [ &#123; &quot;name&quot;: &quot;小鱼儿&quot;, &quot;comment&quot;: &quot;什么股票啊？推荐一下呗&quot;, &quot;age&quot;: 28, &quot;stars&quot;: 4, &quot;date&quot;: &quot;2016-09-01&quot; &#125;, &#123; &quot;name&quot;: &quot;黄药师&quot;, &quot;comment&quot;: &quot;我喜欢投资房产，风，险大收益也大&quot;, &quot;age&quot;: 31, &quot;stars&quot;: 5, &quot;date&quot;: &quot;2016-10-22&quot; &#125; ] &#125; &#125; ] &#125;&#125; 注意,我们在添加数据的时候,黄药师的年龄是31的,但是搜索的时候还是搜索出来了,这个结果就不准确了,在之前有写过object类型的底层存储是怎样的结构, 会将一个数组中的数据,进行扁平化, 我们上面这条数据在底层的存储可能就是这样的 12345678910&#123; &quot;title&quot;: [ &quot;花无缺&quot;, &quot;发表&quot;, &quot;一篇&quot;, &quot;帖子&quot; ], &quot;content&quot;: [ &quot;我&quot;, &quot;是&quot;, &quot;花无缺&quot;, &quot;大家&quot;, &quot;要不要&quot;, &quot;考虑&quot;, &quot;一下&quot;, &quot;投资&quot;, &quot;房产&quot;, &quot;买&quot;, &quot;股票&quot;, &quot;事情&quot; ], &quot;tags&quot;: [ &quot;投资&quot;, &quot;理财&quot; ], &quot;comments.name&quot;: [ &quot;小鱼儿&quot;, &quot;黄药师&quot; ], &quot;comments.comment&quot;: [ &quot;什么&quot;, &quot;股票&quot;, &quot;推荐&quot;, &quot;我&quot;, &quot;喜欢&quot;, &quot;投资&quot;, &quot;房产&quot;, &quot;风险&quot;, &quot;收益&quot;, &quot;大&quot; ], &quot;comments.age&quot;: [ 28, 31 ], &quot;comments.stars&quot;: [ 4, 5 ], &quot;comments.date&quot;: [ 2016-09-01, 2016-10-22 ]&#125; 所以,我们的查询就会查询到这个document,name=黄药师,age=28,正好符合 那么怎么来解决这种问题呢? nested object类型通过使用nested object就可以解决上面出现的问题 首先,需要修改mapping,把之前的index删掉,重新建1DELETE /website 然后创建索引,手动设置mapping12345678910111213141516171819PUT /website&#123; &quot;mappings&quot;: &#123; &quot;blogs&quot;: &#123; &quot;properties&quot;: &#123; &quot;comments&quot;:&#123; &quot;type&quot;: &quot;nested&quot;, &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;comment&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;age&quot;: &#123; &quot;type&quot;: &quot;short&quot; &#125;, &quot;stars&quot;: &#123; &quot;type&quot;: &quot;short&quot; &#125;, &quot;date&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 就是把comments的类型设置为nested,然后里面再设置各个属性,创建完成之后把之前那个数据再加进去. 搜索案例再次按之前的需求搜索,看一下请求1234567891011121314151617181920212223242526272829303132333435GET website/blogs/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;花无缺&quot; &#125; &#125;, &#123; &quot;nested&quot;: &#123; &quot;path&quot;: &quot;comments&quot;, &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;comments.name&quot;: &quot;黄药师&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;comments.age&quot;: &quot;28&quot; &#125; &#125; ] &#125; &#125; &#125; &#125; ] &#125; &#125;&#125; 首先,就是一个bool组合查询,查的是外层的数据,然后里面套了一个nested, 然后里面又是一个bool组合查询 返回值:1234567891011121314&#123; &quot;took&quot;: 4, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 0, &quot;max_score&quot;: null, &quot;hits&quot;: [] &#125;&#125; 这样的话就是查询不到的了. nested object底层数据结构12345678910111213141516171819&#123; &quot;comments.name&quot;: [ &quot;小鱼儿&quot; ], &quot;comments.comment&quot;: [ &quot;什么&quot;, &quot;股票&quot;, &quot;推荐&quot; ], &quot;comments.age&quot;: [ 28 ], &quot;comments.stars&quot;: [ 4 ], &quot;comments.date&quot;: [ 2014-09-01 ]&#125;&#123; &quot;comments.name&quot;: [ &quot;黄药师&quot; ], &quot;comments.comment&quot;: [ &quot;我&quot;, &quot;喜欢&quot;, &quot;投资&quot;, &quot;房产&quot;, &quot;风险&quot;, &quot;收益&quot;, &quot;大&quot; ], &quot;comments.age&quot;: [ 31 ], &quot;comments.stars&quot;: [ 5 ], &quot;comments.date&quot;: [ 2014-10-22 ]&#125;&#123; &quot;title&quot;: [ &quot;花无缺&quot;, &quot;发表&quot;, &quot;一篇&quot;, &quot;帖子&quot; ], &quot;body&quot;: [ &quot;我&quot;, &quot;是&quot;, &quot;花无缺&quot;, &quot;大家&quot;, &quot;要不要&quot;, &quot;考虑&quot;, &quot;一下&quot;, &quot;投资&quot;, &quot;房产&quot;, &quot;买&quot;, &quot;股票&quot;, &quot;事情&quot; ], &quot;tags&quot;: [ &quot;投资&quot;, &quot;理财&quot; ]&#125; nested object的底层数据结构是这样的,所以搜 name=黄药师而且age=28 的就搜索不到了 补充查询请求的时候,nested 里面可以加一个参数,是score_mode,作用是就是,如果搜索命中了多个nested document,如何将多个nested document的分数合并为一个分数.默认是avg,也可以设置max min none]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-87-基于共享锁和排它锁实现悲观锁并发控制]]></title>
    <url>%2F2019%2F01%2F16%2FElasticsearch-87-%E5%9F%BA%E4%BA%8E%E5%85%B1%E4%BA%AB%E9%94%81%E5%92%8C%E6%8E%92%E5%AE%83%E9%94%81%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[共享锁和排它锁共享锁: 这份数据是共享的,然后多个线程过来,都可以获取同一个数据的共享锁,然后对这个数据执行读操作.排它锁: 是排他的操作,只能一个线程获取排他锁,然后执行增删改操作 读写锁分离 如果是要读取数据的话,那么任意多少个线程都可以进来然后读取数据,每个线程都可以上一个共享锁,但是这个时候如果有线程过来修改数据,会尝试上排他锁排它锁会跟共享锁互斥,也就是说,如果有人已经上了共享锁了,那么排它锁就不能上,就得等. 就是说如果有人在读数据,就不允许别人来修改数据,反之也是一样的. 如果有人在修改数据,就是加了排他锁,这时候其他线程过来也要修改数据,也会尝试加排它锁,此时就会失败,锁冲突,就必须等待重试,同时只能有一个线程修改数据,如果有线程过来读取数据,就是尝试加共享锁,此时也会失败,因为共享锁和排它锁是互斥的 如果有线程再修改数据,就不许别的线程再来修改,也不许别的线程读取数据 案例多个线程上共享锁先试一下共享锁,多个线程同时读取数据. 上共享锁也需要一个groovy脚本,和之前一样,脚本内容是1if (ctx._source.lock_type == &apos;exclusive&apos;) &#123; assert false &#125;; ctx._source.lock_count++ 就是如果锁已经存在了,判断这个锁的类型是排它锁的话,直接报错,是共享锁的话,就把锁的数量加1 假设,先有一个线程过来上了一个共享锁1234567891011POST /fs/lock/1/_update&#123; &quot;upsert&quot;: &#123; &quot;lock_type&quot;:&quot;shared&quot;, &quot;lock_count&quot;: 1 &#125;, &quot;script&quot;: &#123; &quot;lang&quot;: &quot;groovy&quot;, &quot;file&quot;: &quot;judge-lock-2&quot; &#125;&#125; 返回值:1234567891011&#123; &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;lock&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 6, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;lock_type&quot;: &quot;shared&quot;, &quot;lock_count&quot;: 1 &#125;&#125; 添加的数据,一个是锁的类型,一个是锁的数量,已经成功上了一个共享锁了 这时候又有一个线程过来上共享锁1234567891011POST /fs/lock/1/_update&#123; &quot;upsert&quot;: &#123; &quot;lock_type&quot;:&quot;shared&quot;, &quot;lock_count&quot;: 1 &#125;, &quot;script&quot;: &#123; &quot;lang&quot;: &quot;groovy&quot;, &quot;file&quot;: &quot;judge-lock-2&quot; &#125;&#125; 返回值:123456789101112&#123; &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;lock&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 7, &quot;result&quot;: &quot;updated&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;&#125; 执行还是成功的,然后查询一下这个锁1GET /fs/lock/1 返回值:1234567891011&#123; &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;lock&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 7, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;lock_type&quot;: &quot;shared&quot;, &quot;lock_count&quot;: 2 &#125;&#125; lock_count是2,就是加了两个共享锁. 就是有线程上了共享锁以后,其他线程还要再上,直接上就可以了,只是lock_count + 1 共享锁没有释放的情况下 上排他锁上排他锁的请求1234PUT /fs/lock/1/_create&#123; &quot;lock_type&quot;:&quot;exclusive&quot;&#125; 这里用的是_create请求,就是强制创建,要求lock必须不能存在,但是之前已经有了共享锁了,/fs/lock/1是存在的 所以这里会创建失败,报错12345678910111213141516171819&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;version_conflict_engine_exception&quot;, &quot;reason&quot;: &quot;[lock][1]: version conflict, document already exists (current version [7])&quot;, &quot;index_uuid&quot;: &quot;RWijfql1Qlqa_OPoor0Ubw&quot;, &quot;shard&quot;: &quot;3&quot;, &quot;index&quot;: &quot;fs&quot; &#125; ], &quot;type&quot;: &quot;version_conflict_engine_exception&quot;, &quot;reason&quot;: &quot;[lock][1]: version conflict, document already exists (current version [7])&quot;, &quot;index_uuid&quot;: &quot;RWijfql1Qlqa_OPoor0Ubw&quot;, &quot;shard&quot;: &quot;3&quot;, &quot;index&quot;: &quot;fs&quot; &#125;, &quot;status&quot;: 409&#125; 释放共享锁这里也是需要一个释放共享锁的groovy脚本1if(--ctx._source.lock_count == 0)&#123;ctx.op = &apos;delete&apos;&#125; 就是执行脚本的时候,先把lock_count-1 然后判断是不是0,是的话直接删除掉.1234567POST /fs/lock/1/_update&#123; &quot;script&quot;: &#123; &quot;lang&quot;: &quot;groovy&quot;, &quot;file&quot;: &quot;unlock-shared&quot; &#125;&#125; 之前上了两个锁,执行两次,lock_count就是0了,然后就被删除了,所有的共享锁就都被释放了 再上排它锁1234PUT /fs/lock/1/_create&#123; &quot;lock_type&quot;:&quot;exclusive&quot;&#125; 返回值:12345678910111213&#123; &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;lock&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 10, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true&#125; 创建成功了,这时候如果再有其他线程过来申请排它锁1234PUT /fs/lock/1/_create&#123; &quot;lock_type&quot;:&quot;exclusive&quot;&#125; 返回值:12345678910111213141516171819&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;version_conflict_engine_exception&quot;, &quot;reason&quot;: &quot;[lock][1]: version conflict, document already exists (current version [10])&quot;, &quot;index_uuid&quot;: &quot;RWijfql1Qlqa_OPoor0Ubw&quot;, &quot;shard&quot;: &quot;3&quot;, &quot;index&quot;: &quot;fs&quot; &#125; ], &quot;type&quot;: &quot;version_conflict_engine_exception&quot;, &quot;reason&quot;: &quot;[lock][1]: version conflict, document already exists (current version [10])&quot;, &quot;index_uuid&quot;: &quot;RWijfql1Qlqa_OPoor0Ubw&quot;, &quot;shard&quot;: &quot;3&quot;, &quot;index&quot;: &quot;fs&quot; &#125;, &quot;status&quot;: 409&#125; 还是报错的,上锁失败,如果再上一个共享锁呢1234567891011POST /fs/lock/1/_update&#123; &quot;upsert&quot;: &#123; &quot;lock_type&quot;:&quot;shared&quot;, &quot;lock_count&quot;: 1 &#125;, &quot;script&quot;: &#123; &quot;lang&quot;: &quot;groovy&quot;, &quot;file&quot;: &quot;judge-lock-2&quot; &#125;&#125; 返回值:123456789101112131415161718192021222324&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;remote_transport_exception&quot;, &quot;reason&quot;: &quot;[f57uV91][127.0.0.1:9300][indices:data/write/update[s]]&quot; &#125; ], &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;failed to execute script&quot;, &quot;caused_by&quot;: &#123; &quot;type&quot;: &quot;script_exception&quot;, &quot;reason&quot;: &quot;error evaluating judge-lock-2&quot;, &quot;caused_by&quot;: &#123; &quot;type&quot;: &quot;power_assertion_error&quot;, &quot;reason&quot;: &quot;assert false\n&quot; &#125;, &quot;script_stack&quot;: [], &quot;script&quot;: &quot;&quot;, &quot;lang&quot;: &quot;groovy&quot; &#125; &#125;, &quot;status&quot;: 400&#125; 也是报错,加不了的 释放排他锁直接把这个删除掉就好了1DELETE /fs/lock/1]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-86-基于document锁实现悲观锁并发控制]]></title>
    <url>%2F2019%2F01%2F16%2FElasticsearch-86-%E5%9F%BA%E4%BA%8Edocument%E9%94%81%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[document锁上文中的全局锁,一次性就锁上了整个index,对这个index的所有增删改操作都会被block住,如果上锁不频繁,还可以,如果上锁释放锁的操作很频繁,显然不适用 document锁,顾名思义,每次就锁需要操作的几个doc,就可以了,被锁的doc,其他线程就不能对这些doc执行增删改操作了,但是只是对一部分doc上了锁,其他线程对于其他的doc还是可以操作的 具体实现.document是用脚本进行上锁的 es的config/script目录下,写一个名为judge-lock的groovy脚本,内容是:1if ( ctx._source.process_id != process_id ) &#123; assert false &#125;; ctx.op = &apos;noop&apos;; 加锁:1234567891011POST /fs/lock/1/_update&#123; &quot;upsert&quot;: &#123; &quot;process_id&quot;: 123 &#125;, &quot;script&quot;: &#123; &quot;lang&quot;: &quot;groovy&quot;, &quot;file&quot;: &quot;judge-lock&quot;, &quot;params&quot;: &#123; &quot;process_id&quot;: 123 &#125; &#125;&#125; fs/lock: 就是说fs下的lock type,专门用于进行上锁 fs/lock/id: 比如上面的1,id就是要上锁的那个doc的id,代表了某个doc对应的lock, 其实也是一个doc _update: 请求里面还有个upsert,执行的是upsert操作 script: 要执行的脚本 script.param: 传入的参数 script.param.process_id: 是要执行增删改操作的进程的唯一id,比如说在java系统中,启动的时候,可以给每个线程都用uuid生成一个唯一id,进程也可以分配一个uuid,process_id+thread_id就代表了某个进程下的某个线程 process_id很重要,会在lock中设置对对应的doc加锁的进程的id,这样其他进程过来的时候,才知道这条数据已经被别人锁了 请求进来的话,走groovy脚本,参数就是process_id,看一下这个脚本,意思就是process_id和当前数据中的process_id不相同的话就assert false,抛出异常,如果是相同的,就ctx.op = ‘noop’,不做任何修改 如果一个document之前没有被锁,拿上面的请求举例,比如说/fs/lock/1之前不存在,也就是id是1的doc没有被别人上锁, 因为用的是upsert,那么执行index操作,创建一个/fs/lock/1,用params中的数据作为这个lock的数据,process_id被设置为123,脚本不执行,完成之后,就标识id是1的这个doc被process_id=123的进程锁上了 然后现在又有一个process_id是234的进程过来尝试加锁,请求如下1234567891011POST /fs/lock/1/_update&#123; &quot;upsert&quot;: &#123; &quot;process_id&quot;: 234 &#125;, &quot;script&quot;: &#123; &quot;lang&quot;: &quot;groovy&quot;, &quot;file&quot;: &quot;judge-lock&quot;, &quot;params&quot;: &#123; &quot;process_id&quot;: 234 &#125; &#125;&#125; 这时候,这个doc已经被上一个进程锁上了,/fs/lock/1 这条数据也已经存在了, 那么这个upsert就是执行script脚本了,比对process_id,发现两个id并不相同,也就是说这个doc已经被别的进程锁上了,就直接报错了,需要一直重试,直到上锁成功 那如果说,process_id=123的这个线程,又要做一些其他的操作, 也还是过来先上锁,在执行脚本的时候,发现两个process_id是相同的,此时就会返回ctx.op= ‘noop’,什么都不做,所以这个请求是不会被 block的 最后做完全部的操作后,就要释放这个进程上的所有锁 案例还是之前的文件系统,两个线程要同时去修改id是1的doc的文件名 第一个线程过来,尝试上锁..1234567891011POST /fs/lock/1/_update&#123; &quot;upsert&quot;: &#123;&quot;process_id&quot;: 123&#125;, &quot;script&quot;: &#123; &quot;lang&quot;: &quot;groovy&quot;, &quot;file&quot;: &quot;judge-lock&quot;, &quot;params&quot;: &#123; &quot;process_id&quot;: 123 &#125; &#125;&#125; 返回值:123456789101112&#123; &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;lock&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;&#125; process_id是123的进程上锁成功,看一下1GET /fs/lock/1 返回值:12345678910&#123; &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;lock&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;process_id&quot;: 123 &#125;&#125; 这个时候第二个线程过来尝试加锁1234567891011POST /fs/lock/1/_update&#123; &quot;upsert&quot;: &#123;&quot;process_id&quot;: 234&#125;, &quot;script&quot;: &#123; &quot;lang&quot;: &quot;groovy&quot;, &quot;file&quot;: &quot;judge-lock&quot;, &quot;params&quot;: &#123; &quot;process_id&quot;: 234 &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;remote_transport_exception&quot;, &quot;reason&quot;: &quot;[f57uV91][127.0.0.1:9300][indices:data/write/update[s]]&quot; &#125; ], &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;failed to execute script&quot;, &quot;caused_by&quot;: &#123; &quot;type&quot;: &quot;script_exception&quot;, &quot;reason&quot;: &quot;error evaluating judge-lock&quot;, &quot;caused_by&quot;: &#123; &quot;type&quot;: &quot;power_assertion_error&quot;, &quot;reason&quot;: &quot;assert false\n&quot; &#125;, &quot;script_stack&quot;: [], &quot;script&quot;: &quot;&quot;, &quot;lang&quot;: &quot;groovy&quot; &#125; &#125;, &quot;status&quot;: 400&#125; 直接就报错了,是加不了锁的 如果说还是第一个线程来获取锁的话会怎样呢1234567891011POST /fs/lock/1/_update&#123; &quot;upsert&quot;: &#123;&quot;process_id&quot;: 123&#125;, &quot;script&quot;: &#123; &quot;lang&quot;: &quot;groovy&quot;, &quot;file&quot;: &quot;judge-lock&quot;, &quot;params&quot;: &#123; &quot;process_id&quot;: 123 &#125; &#125;&#125; 返回值:123456789101112&#123; &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;lock&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;noop&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 0, &quot;successful&quot;: 0, &quot;failed&quot;: 0 &#125;&#125; 返回值就是noop,什么都不做,这时候,这个进程就可以对id是1的这个doc进行各种操作,比如修改文件名123456POST /fs/file/1/_update&#123; &quot;doc&quot;: &#123; &quot;name&quot;: &quot;README1.txt&quot; &#125;&#125; 返回值:123456789101112&#123; &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;file&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 6, &quot;result&quot;: &quot;updated&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;&#125; 更新成功了,最后一步释放掉锁1POST /fs/_refresh 手动refresh一下,数据写入os cache并被打开供搜索的过程,叫做refresh 查询这个线程id,持有的所有锁,用scroll查询,或者普通的搜索都可以12345678910GET /fs/lock/_search?scroll=1m&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;process_id&quot;: &#123; &quot;value&quot;: &quot;123&quot; &#125; &#125; &#125;&#125; 返回值:12345678910111213141516171819202122232425&#123; &quot;_scroll_id&quot;: &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAAyjFmY1N3VWOTF4U19HUlRRUzJIbzgxcmcAAAAAAAAMpBZmNTd1VjkxeFNfR1JUUVMySG84MXJnAAAAAAAADKUWZjU3dVY5MXhTX0dSVFFTMkhvODFyZwAAAAAAAAymFmY1N3VWOTF4U19HUlRRUzJIbzgxcmcAAAAAAAAMohZmNTd1VjkxeFNfR1JUUVMySG84MXJn&quot;, &quot;took&quot;: 14, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;lock&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;process_id&quot;: 123 &#125; &#125; ] &#125;&#125; 搜索到所有的要释放的锁之后, 删除掉,这里用一个_bulk批量操作12PUT /fs/lock/_bulk&#123;&quot;delete&quot;:&#123;&quot;_id&quot; : 1 &#125;&#125; 返回值:12345678910111213141516171819202122&#123; &quot;took&quot;: 26, &quot;errors&quot;: false, &quot;items&quot;: [ &#123; &quot;delete&quot;: &#123; &quot;found&quot;: true, &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;lock&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 2, &quot;result&quot;: &quot;deleted&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;status&quot;: 200 &#125; &#125; ]&#125; 删除成功,锁释放掉了,这时候,process_id=234的线程尝试上锁1234567891011POST /fs/lock/1/_update&#123; &quot;upsert&quot;: &#123;&quot;process_id&quot;: 234&#125;, &quot;script&quot;: &#123; &quot;lang&quot;: &quot;groovy&quot;, &quot;file&quot;: &quot;judge-lock&quot;, &quot;params&quot;: &#123; &quot;process_id&quot;: 234 &#125; &#125;&#125; 返回值:123456789101112&#123; &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;lock&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;&#125; 上锁成功,和前面的一样,做完各种操作释放掉就ok了]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-85-基于全局锁实现悲观锁并发控制]]></title>
    <url>%2F2019%2F01%2F15%2FElasticsearch-85-%E5%9F%BA%E4%BA%8E%E5%85%A8%E5%B1%80%E9%94%81%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[并发控制的几种方案结合之前的文件系统的案例, 假设有多个线程过来,要并发的给/workspace/projects/helloworld下的README.txt修改文件名,这个时候就要进行并发的控制,避免多线程的并发安全问题. 基于version的乐观锁,先获取要修改的信息的版本号,然后和自己的版本号对比,如果版本号不一样了,就意味着有别的线程已经修改过这个数据了,必须重新获取新的版本号再次尝试修改 然后是悲观锁,就是上来就先尝试给这个数据加锁,如果要是加锁成功了,这个时候这个数据只有当前这个线程可以操作,其他线程过来也尝试加锁,发现已经被上一个线程加锁了,就一直重新上锁,直到获得锁以后才可以对数据进行操作 全局锁是粒度最粗的 全局锁的实现就是直接锁掉整个要操作的index 首先,第一步,上锁12PUT /fs/lock/global/_create&#123;&#125; fs: 要上锁的index lock:就是指定一个对这个index上全局锁的一个type global: 就是这个全局锁对应的这个doc的id _create: 强制必须是创建,如果这个doc已经存在,就创建失败,报错 其实就是添加了一个空的document,语法是和添加document一样的 /index/type/id 创建完成后,如果另一个线程同时尝试上锁,也就是再执行一次这个请求12PUT /fs/lock/global/_create&#123;&#125; 返回值:12345678910111213141516171819&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;version_conflict_engine_exception&quot;, &quot;reason&quot;: &quot;[lock][global]: version conflict, document already exists (current version [1])&quot;, &quot;index_uuid&quot;: &quot;RWijfql1Qlqa_OPoor0Ubw&quot;, &quot;shard&quot;: &quot;2&quot;, &quot;index&quot;: &quot;fs&quot; &#125; ], &quot;type&quot;: &quot;version_conflict_engine_exception&quot;, &quot;reason&quot;: &quot;[lock][global]: version conflict, document already exists (current version [1])&quot;, &quot;index_uuid&quot;: &quot;RWijfql1Qlqa_OPoor0Ubw&quot;, &quot;shard&quot;: &quot;2&quot;, &quot;index&quot;: &quot;fs&quot; &#125;, &quot;status&quot;: 409&#125; 可以看到,锁已经存在的情况下,其他线程再尝试加锁就会报错,没获得锁的线程就一直重复尝试上锁,直到成功 此时,第一个成功上锁的线程就可以对document进行各种操作,比如更新文件名123456POST /fs/file/1/_update&#123; &quot;doc&quot;: &#123; &quot;name&quot;: &quot;README1.txt&quot; &#125;&#125; 返回值:123456789101112&#123; &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;file&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 2, &quot;result&quot;: &quot;updated&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;&#125; 修改完成,最后一步,释放锁,也就是删除/fs/lock/global这个document 1DELETE /fs/lock/global 第二个线程之前上锁失败,但是这时候第一个线程已经释放了锁,所以第二个线程上锁成功,然后他也可以对这个index里面的数据做各种操作,最后一步delete掉,释放锁就好了 全局锁的优缺点优点: 操作简单,使用容易,成本低缺点: 直接把整个index锁上了,这时候别的线程对index中所有的doc的操作都会被block住,导致整个系统的并发能力很低 适用场景上锁释放锁的操作不是很频繁,每次上锁之后,执行的操作耗时不会太长,可以使用这种方式, 方便]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-84-文件系统数据建模及搜索案例]]></title>
    <url>%2F2019%2F01%2F15%2FElasticsearch-84-%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E5%8F%8A%E6%90%9C%E7%B4%A2%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[数据建模对类似文件系统(filesystem)这种的 有多层级关系的数据进行建模,比如说路径这种多层级的数据 数据添加首先先自定义一个分词器.123456789101112PUT /fs&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;analyzer&quot;: &#123; &quot;paths&quot;: &#123; &quot;tokenizer&quot;: &quot;path_hierarchy&quot; &#125; &#125; &#125; &#125;&#125; ath_hierarchy tokenizer:举个例子,比如现在有一个路劲是a/b/c/d,经过path_hierarchy这种分词,会被拆分为a/b/c/d/a/b/c/a/b/a 分词器创建好之后,手动创建mapping 1234567891011121314151617PUT /fs/_mapping/file&#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;path&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;fields&quot;: &#123; &quot;tree&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;paths&quot; &#125; &#125; &#125; &#125;&#125; path这个field里面又创建了一个内置的filed,path.tree 索引创建完成,添加数据123456PUT /fs/file/1&#123; &quot;name&quot;: &quot;README.txt&quot;, &quot;path&quot;: &quot;/workspace/projects/helloworld&quot;, &quot;contents&quot;: &quot;这是我的第一个elasticsearch程序&quot;&#125; 需求一查找一份内容包括elasticsearch,在/workspace/projects/helloworld这个目录下的文件 搜索请求:1234567891011121314151617181920212223GET /fs/file/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;contents&quot;: &quot;elasticsearch&quot; &#125; &#125;, &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;path&quot;: &quot;/workspace/projects/helloworld&quot; &#125; &#125; &#125; &#125; ] &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526&#123; &quot;took&quot;: 1, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1.284885, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;fs&quot;, &quot;_type&quot;: &quot;file&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1.284885, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;README.txt&quot;, &quot;path&quot;: &quot;/workspace/projects/helloworld&quot;, &quot;contents&quot;: &quot;这是我的第一个elasticsearch程序&quot; &#125; &#125; ] &#125;&#125; 需求二搜索/workspace目录下,内容包含elasticsearch的所有的文件1234567891011121314151617181920212223GET /fs/file/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;contents&quot;: &quot;elasticsearch&quot; &#125; &#125;, &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;path.tree&quot;: &quot;/workspace&quot; &#125; &#125; &#125; &#125; ] &#125; &#125;&#125; 请求和上面基本是一样的,只是filter里面用之前创建的path.tree,因为path是不分词的]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-83-聚合分析案例]]></title>
    <url>%2F2019%2F01%2F15%2FElasticsearch-83-%E8%81%9A%E5%90%88%E5%88%86%E6%9E%90%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[需求对每个用户发表的博客进行分组 添加测试数据123456789101112131415161718192021222324252627282930313233PUT /website/users/3&#123; &quot;name&quot;: &quot;黄药师&quot;, &quot;email&quot;: &quot;huangyaoshi@sina.com&quot;, &quot;birthday&quot;: &quot;1970-10-24&quot;&#125;PUT /website/blogs/3&#123; &quot;title&quot;: &quot;我是黄药师&quot;, &quot;content&quot;: &quot;我是黄药师啊，各位同学们！！！&quot;, &quot;userInfo&quot;: &#123; &quot;userId&quot;: 1, &quot;username&quot;: &quot;黄药师&quot; &#125;&#125;PUT /website/users/2&#123; &quot;name&quot;: &quot;花无缺&quot;, &quot;email&quot;: &quot;huawuque@sina.com&quot;, &quot;birthday&quot;: &quot;1980-02-02&quot;&#125;PUT /website/blogs/4&#123; &quot;title&quot;: &quot;花无缺的身世揭秘&quot;, &quot;content&quot;: &quot;大家好，我是花无缺，所以我的身世是。。。&quot;, &quot;userInfo&quot;: &#123; &quot;userId&quot;: 2, &quot;username&quot;: &quot;花无缺&quot; &#125;&#125; 实现请求:123456789101112131415161718192021GET /website/blogs/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_username&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;userInfo.username.keyword&quot; &#125;, &quot;aggs&quot;: &#123; &quot;top_blogs&quot;: &#123; &quot;top_hits&quot;: &#123; &quot;_source&quot;: &#123; &quot;include&quot;: &quot;title&quot; &#125;, &quot;size&quot;: 5 &#125; &#125; &#125; &#125; &#125;&#125; 返回值:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485&#123; &quot;took&quot;: 32, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_username&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;小鱼儿&quot;, &quot;doc_count&quot;: 1, &quot;top_blogs&quot;: &#123; &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blogs&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;小鱼儿的第一篇博客&quot; &#125; &#125; ] &#125; &#125; &#125;, &#123; &quot;key&quot;: &quot;花无缺&quot;, &quot;doc_count&quot;: 1, &quot;top_blogs&quot;: &#123; &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blogs&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;花无缺的身世揭秘&quot; &#125; &#125; ] &#125; &#125; &#125;, &#123; &quot;key&quot;: &quot;黄药师&quot;, &quot;doc_count&quot;: 1, &quot;top_blogs&quot;: &#123; &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blogs&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;我是黄药师&quot; &#125; &#125; ] &#125; &#125; &#125; ] &#125; &#125;&#125; 根据用户的username进行分组,然后拿前五条数据,top_hits._source 是设置要显示的field.]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-82-应用层关联查询与通过冗余数据关联对比]]></title>
    <url>%2F2019%2F01%2F15%2FElasticsearch-82-%E5%BA%94%E7%94%A8%E5%B1%82%E5%85%B3%E8%81%94%E6%9F%A5%E8%AF%A2%E4%B8%8E%E9%80%9A%E8%BF%87%E5%86%97%E4%BD%99%E6%95%B0%E6%8D%AE%E5%85%B3%E8%81%94%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[关系型数据建模数据建模以一个博客网站作为案例背景,模拟用户发表各种博客,然后针对用户和博客之间的关系进行数据建模,同时针对建模好的数据执行各种搜索 + 聚合的操作 先以类似关系型数据库中的模型来构建数据,还是将有关联的数据分割为不同的实体 添加数据12345678910111213PUT /website/users/1 &#123; &quot;name&quot;: &quot;小鱼儿&quot;, &quot;email&quot;: &quot;xiaoyuer@sina.com&quot;, &quot;birthday&quot;: &quot;1980-01-01&quot;&#125;PUT /website/blogs/1&#123; &quot;title&quot;: &quot;我的第一篇博客&quot;, &quot;content&quot;: &quot;这是我的第一篇博客，开通啦！！！&quot;, &quot;userId&quot;: 1 &#125; 一个用户对应一个博客,一对多的关系 查询假设我们现在要查询小鱼儿发表的所有博客, 就应该先拿到小鱼儿的用户id,然后去博客表中去根据user_id去搜索,请求如下 先去搜索用户12345678910GET /website/users/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;name.keyword&quot;: &#123; &quot;value&quot;: &quot;小鱼儿&quot; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526&#123; &quot;took&quot;: 35, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.2876821, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;users&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.2876821, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;小鱼儿&quot;, &quot;email&quot;: &quot;xiaoyuer@sina.com&quot;, &quot;birthday&quot;: &quot;1980-01-01&quot; &#125; &#125; ] &#125;&#125; 拿到用户id之后,去博客表根据userID去搜索1234567891011121314GET /website/blogs/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;terms&quot;: &#123; &quot;userId&quot;: [ 1 ] &#125; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blogs&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;我的第一篇博客&quot;, &quot;content&quot;: &quot;这是我的第一篇博客，开通啦！！！&quot;, &quot;userId&quot;: 1 &#125; &#125; ] &#125;&#125; 这样就把小鱼儿所有的博客都搜出来了,这种搜索就属于是应用层的join关联,在应用层先查出一份数据,然后再查出一份数据,进行关联 这里思考一个问题.假设要查询的是1万个人的博客,那么第一次查询的时候会拿到1万个user_id,然后第二次查询要在terms中放入1万个user_id,这样的话性能是很差的 优缺点优点: 数据不冗余,维护方便缺点: 应用层的join,如果关联的数据过多,导致查询过大,性能差 document型数据建模数据建模下面使用冗余数据,采用document数据模型,进行数据建模,实现用户和博客的关联 添加数据12345678910111213141516PUT /website/users/1&#123; &quot;name&quot;: &quot;小鱼儿&quot;, &quot;email&quot;: &quot;xiaoyuer@sina.com&quot;, &quot;birthday&quot;: &quot;1980-01-01&quot;&#125;PUT /website/blogs/1&#123; &quot;title&quot;: &quot;小鱼儿的第一篇博客&quot;, &quot;content&quot;: &quot;大家好，我是小鱼儿。。。&quot;, &quot;userInfo&quot;: &#123; &quot;userId&quot;: 1, &quot;username&quot;: &quot;小鱼儿&quot; &#125;&#125; 冗余数据,就是说,将可能进行搜索的条件和要搜索的数据放在一个doc中 基于冗余用户数据搜索博客请求:12345678910GET /website/blogs/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;userInfo.username.keyword&quot;: &#123; &quot;value&quot;: &quot;小鱼儿&quot; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526272829&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.2876821, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blogs&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.2876821, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;小鱼儿的第一篇博客&quot;, &quot;content&quot;: &quot;大家好，我是小鱼儿。。。&quot;, &quot;userInfo&quot;: &#123; &quot;userId&quot;: 1, &quot;username&quot;: &quot;小鱼儿&quot; &#125; &#125; &#125; ] &#125;&#125; 这样的话就不需要再走应用层的join,先搜索一个数据,再去搜另一份数据,直接走一个有冗余的type即可,指定要搜索的条件,即可搜索出自己想要的数据 优缺点优点: 性能高,不需要执行多次搜索缺点: 数据冗余,维护成本高. 比如说一个username变了,要同时更新user type和blog type. 一般来说,NoSql类型的数据存储,都是冗余模式,所以一旦出现冗余数据的修改,必须记得将所有关联的数据全都更新]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-81-关系型数据库与document类型数据模型对比]]></title>
    <url>%2F2019%2F01%2F15%2FElasticsearch-81-%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8Edocument%E7%B1%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[Java类有如下两个类,部门类和员工类12345678public class Department &#123; private Integer deptId; private String name; private String desc; private List&lt;Employee&gt; employees;&#125; 123456789public class Employee &#123; private Integer empId; private String name; private Integer age; private String gender; private Department dept;&#125; 关系型数据库在关系型数据库中上面的java类可能对应的就是两张表1234department表: dept_idnamedesc 123456employee表:emp_id nameagegender dept_id 关系型数据库中,每个数据实体都拆分为一个独立的表,然后通过主外键将数据关联起来 document文档数据模型在非关系型数据库中,就是document文档数据模型,里面存放的数据可能就是这样的12345678910111213141516171819202122232425&#123; &quot;deptId&quot;: &quot;1&quot;, &quot;name&quot;: &quot;研发部门&quot;, &quot;desc&quot;: &quot;负责公司的所有研发项目&quot;, &quot;employees&quot;: [ &#123; &quot;empId&quot;: &quot;1&quot;, &quot;name&quot;: &quot;张三&quot;, &quot;age&quot;: 28, &quot;gender&quot;: &quot;男&quot; &#125;, &#123; &quot;empId&quot;: &quot;2&quot;, &quot;name&quot;: &quot;王兰&quot;, &quot;age&quot;: 25, &quot;gender&quot;: &quot;女&quot; &#125;, &#123; &quot;empId&quot;: &quot;3&quot;, &quot;name&quot;: &quot;李四&quot;, &quot;age&quot;: 34, &quot;gender&quot;: &quot;男&quot; &#125; ]&#125; 这种document数据模型,类似于面向对象的数据模型,将所有由关联关系的数据,放到一个doc json类型的数据中,整个数据的关系还有完整的数据 都放在了一起]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-80-海量bucket的优化机制]]></title>
    <url>%2F2019%2F01%2F15%2FElasticsearch-80-%E6%B5%B7%E9%87%8Fbucket%E7%9A%84%E4%BC%98%E5%8C%96%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[概述先来看一个需求 现在有一堆数据,是每个演员的每个电影的评论,现在我们要取到10个演员的评论数量排名前5的电影 看一下请求体12345678910111213141516171819&#123; &quot;aggs&quot; : &#123; &quot;actors&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;actors&quot;, &quot;size&quot; : 10, &quot;collect_mode&quot; : &quot;breadth_first&quot; &#125;, &quot;aggs&quot; : &#123; &quot;costars&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;films&quot;, &quot;size&quot; : 5 &#125; &#125; &#125; &#125; &#125;&#125; 第一个聚合,根据演员分组,就能拿到每个演员的评论的数量, 然后 size是10 取前10个,collect_mode先不管,然后是第二个按电影分组,拿到了每个演员每个电影的评论数量,取前5的电影,这个需求就ok了 接下来看一下深度优先和广度优先的两种聚合方式 深度优先的聚合方式假设每个演员对应的电影数据如下12 actor1 actor2 .... actorfilm1 film2 film3 film1 film2 film3 ...film 比如说,现在有10万个actor,其实最后只需要10个就好了 但是我们使用的是深度优先的方式,它已经构建了一颗完整的树出来了,假设每个actor有10个电影, 那就是10万actor + 100万film的数据量的树. 最后再裁剪掉10万个actor中的99990个actor和99990 10 个film,剩下10个actor, 每个actor再裁剪掉5个film,剩下的数据就是10 5 = 50个, 从110万的数据裁剪成5个. 先构建了大量的数据,最后裁掉99.99%的数据,浪费了 广度优先的方式执行先构建actor的数,如下1actor1 actor2 actor3 ..... n个actor 假设10万个actor,但是现在不去构建他下面的film数据, 数据量也就是10万个,然后裁掉99990个 剩下10个,这个时候再去构建film,裁剪掉5个 数据就是从10万到剩下的50个,比上面的深度优先性能高了10倍. 再看上面那个请求体,其中collect_mode参数值为breadth_first,就是表示广度优先]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-79-fielddata filter的细粒度内存加载控制以及预加载机制]]></title>
    <url>%2F2019%2F01%2F15%2FElasticsearch-79-fielddata-filter%E7%9A%84%E7%BB%86%E7%B2%92%E5%BA%A6%E5%86%85%E5%AD%98%E5%8A%A0%E8%BD%BD%E6%8E%A7%E5%88%B6%E4%BB%A5%E5%8F%8A%E9%A2%84%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[filter的细粒度内存加载控制12345678910111213141516POST /test_index/_mapping/my_type&#123; &quot;properties&quot;: &#123; &quot;my_field&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fielddata&quot;: &#123; &quot;filter&quot;: &#123; &quot;frequency&quot;: &#123; &quot;min&quot;: 0.01, &quot;min_segment_size&quot;: 500 &#125; &#125; &#125; &#125; &#125;&#125; 主要设置的就是两个值,一个是min,一个是min_segment_size min: 仅仅加载至少1%的doc中出现过的term对应的fielddata比如说,有一个词 hello, 总共有1000个doc,hello必须在10个doc中出现,那么这个hello对应的fielddata才会加载到内存中来 min_segment_size: 少于500个doc的segment不加载fielddata加载fielddata的时候,也是按照segment去进行加载的,某个segment里面的doc数量少于500个的话,这个segment的fielddata就不会加载 这个一般不用去设置,了解一下就好了 fielddata预加载机制如果真的要对分词的field执行聚合,那么每次都在query-time现场生成fielddata并加载到内存中来,这样的话速度是比较慢的, 我们可以预先生成加载fielddata到内存中来 fielddata预加载1234567891011POST /test_index/_mapping/test_type&#123; &quot;properties&quot;: &#123; &quot;test_field&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;fielddata&quot;: &#123; &quot;loading&quot; : &quot;eager&quot; &#125; &#125; &#125;&#125; 这样可以将fielddata的生成变为index-time,建立倒排索引的时候,就同步生成fielddata并且加载到内存中,这样的话对分词field的聚合性能会大幅度增强 序号标记预加载global ordinal 原理: 假设有如下几个doc,分别是1234doc1: status1doc2: status2doc3: status2doc4: status1 在这样有很多重复值的情况,会进行global ordinal标记 status1 –&gt; 0status2 –&gt; 1 标记完成后,如下1234doc1: 0doc2: 1doc3: 1doc4: 0 建立fielddata也会是这个样子的,这样的好处就是减少重复字符串的出现次数,减少内存的消耗 设置的语法如下:1234567891011POST /test_index/_mapping/test_type&#123; &quot;properties&quot;: &#123; &quot;test_field&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;fielddata&quot;: &#123; &quot;loading&quot; : &quot;eager_global_ordinals&quot; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-78-fielddata内存控制及circuit breaker短路器]]></title>
    <url>%2F2019%2F01%2F14%2FElasticsearch-78-fielddata%E5%86%85%E5%AD%98%E6%8E%A7%E5%88%B6%E5%8F%8Acircuit-breaker%E7%9F%AD%E8%B7%AF%E5%99%A8%2F</url>
    <content type="text"><![CDATA[fielddata核心原理fielddata加载到内存的过程是lazy加载的,对一个分词field执行聚合时,才会加载,field执行聚合时,才会加载(query-time),而且是field-level加载的一个index的一个field,所有doc都会被加载,而不是少数doc fielddata内存限制1indices.fielddata.cache.size: 20% 在elasticsearch安装目录的config目录下的elasticsearch.yml里面配置 通过这个配置可以限制fielddata占用的内存,超出限制的话,就清除内存中已有的fielddata数据, 默认是无限制的,如果限制的话,可能会导致大量的evict和reload,大量的IO性能损耗,以及内存碎片和GC,不建议使用 监控fielddata内存使用情况可以通过以下几个请求去查看1GET /_stats/fielddata?fields=* 1GET /_nodes/stats/indices/fielddata?fields=* 1GET /_nodes/stats/indices/fielddata?level=indices&amp;fields=* circuit breaker如果一次query 加载的fielddata超过总内存,就会oom(内存溢出) circuit breaker(短路器)会估算query要加载的fielddata的大小,如果超出总内存,就短路,query直接失败 短路器的配置也是在elasticsearch.yml里面去配置的 123456# fielddata的内存限制,默认60%indices.breaker.fielddata.limit：# 执行聚合的内存限制,默认40%indices.breaker.request.limit：# 综合上面两个,限制在70%以内indices.breaker.total.limit：]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-77-fielddata原理]]></title>
    <url>%2F2019%2F01%2F14%2FElasticsearch-77-fielddata%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[对于不分词的field执行聚合操作.先来看一个请求12345678910GET /test_index/test_type/_search&#123; &quot;aggs&quot;: &#123; &quot;group_by_test_field&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;test_field&quot; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [test_field] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.&quot; &#125; ], &quot;type&quot;: &quot;search_phase_execution_exception&quot;, &quot;reason&quot;: &quot;all shards failed&quot;, &quot;phase&quot;: &quot;query&quot;, &quot;grouped&quot;: true, &quot;failed_shards&quot;: [ &#123; &quot;shard&quot;: 0, &quot;index&quot;: &quot;test_index&quot;, &quot;node&quot;: &quot;f57uV91xS_GRTQS2Ho81rg&quot;, &quot;reason&quot;: &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [test_field] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.&quot; &#125; &#125; ], &quot;caused_by&quot;: &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [test_field] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.&quot; &#125; &#125;, &quot;status&quot;: 400&#125; 请求是根据test_field去做聚合分析的,执行的时候报错,大概意思就是说,你必须打开fielddata,然后将正排索引数据加载到内存中,才可以对分词的field执行聚合操作,而且会消耗很大的内存 所以,对于分词的field要做聚合操作的话,需要设置fielddata=true 设置fielddata为true直接设置就可以了,不需要删除索引,重新建立123456789POST /test_index/_mapping/test_type&#123; &quot;properties&quot;: &#123; &quot;test_field&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;fielddata&quot;: true &#125; &#125;&#125; 执行完毕,查询看一下1GET /test_index/_mapping/test_type 返回值:1234567891011121314151617181920&#123; &quot;test_index&quot;: &#123; &quot;mappings&quot;: &#123; &quot;test_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;test_field&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125;, &quot;fielddata&quot;: true &#125; &#125; &#125; &#125; &#125;&#125; 设置好了之后,再执行最开始那个聚合请求1234567891011GET /test_index/test_type/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_test_field&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;test_field&quot; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526&#123; &quot;took&quot;: 26, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_test_field&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;test&quot;, &quot;doc_count&quot;: 3 &#125; ] &#125; &#125;&#125; 已经是可以执行的了 使用内置field执行聚合操作看下这个索引的_mapping, test_field中有一个内置的field是test_field.keyword,这个内置的field是不会进行分词的,所以,也可以的在不设置fielddata=true的情况下,使用内置的keyword 来进行聚合操作 1234567891011GET /test_index/test_type/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_test_field&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;test_field.keyword&quot; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526&#123; &quot;took&quot;: 8, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_test_field&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;test&quot;, &quot;doc_count&quot;: 3 &#125; ] &#125; &#125;&#125; 这里需要注意一下,内置的keyword这个field还有一个ignore_above属性,默认是256,前面有说过,是只保留field值的前256个,所以要控制field值在256以内,或者也可以自己设置ignore_above的值 分词field+fileddata工作原理如果某个field设置的是不分词的,那么在index-time的时候,就会自动生成doc_value,针对这些不分词的field执行聚合操作的时候,自动就会用doc value来执行 对于分词的field,是没有doc_value的. 在index-time的时候,如果某个field是分词的,那么是不会给它建立doc_value的,因为分词后,占用的空间过于大,所以默认是不支持分词field进行聚合的 如果要对分词的field执行聚合的话,是必须打开和使用fielddata,完全存在纯内存中, 结构和doc_value是类似的,但是只会将fielddata加载到内存中来,然后基于内存中的fielddata执行分词field的聚合操作,最开始的那个请求的报错信息中也说了,可能会占用大量的内存 那为什么fielddata必须存在内存? 因为分词的字符串,需要按照term进行聚合,需要执行更加复杂的算法和操作,如果基于磁盘和os cache,那么性能会很差]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-76-基于doc value正排索引的聚合内部原理]]></title>
    <url>%2F2019%2F01%2F14%2FElasticsearch-76-%E5%9F%BA%E4%BA%8Edoc-value%E6%AD%A3%E6%8E%92%E7%B4%A2%E5%BC%95%E7%9A%84%E8%81%9A%E5%90%88%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[概述 聚合分析的内部原理是什么 aggs term avg max等执行一个聚合操作的时候,内部原理是怎样的 用了什么样的数据结构去执行聚合 是不是用的倒排索引 聚合分析原理先来看一个搜索 + 聚合的请求123456789101112131415GET /test_index/test_type/_search &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;search_field&quot;: &quot;苹果&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;group_by_agg_field&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125; &#125; &#125;&#125; 采用倒排索引实现聚合的弊端首先,es内部绝对不是用倒排索引来实现聚合分析的,先来看上面的搜索,比如说现在倒排索引中有如下数据 term doc list 苹果 doc1,doc2 小米 doc3,doc4 这时候,我们的query,只要搜索到”苹果”,拿到doc list,就停止了, 并不需要去扫描整个倒排索引.所以效率还是很高的. 然后接下来就是进行聚合,在颜色的倒排索引中去搜索,如果要找到所有的颜色的话,就必须要遍历所有的倒排索引,因为是要做分组的嘛,这个性能就十分辣鸡了 color doc list 红色 doc1 蓝色 doc2 红色 doc3 蓝色 doc4 … … 使用倒排索引和正排索引(doc_value)来实现首先还是和之前的query一样,从倒排索引中去搜索,拿到搜索结果之后,进行聚合,只要根据搜索的结果,一次在正排索引中去搜索,做聚合操作,比如正排索引如下: doc color doc1 红色 doc2 蓝色 doc3 红色 doc4 蓝色 … … 比如搜索结果是doc1,doc2, 这时候,去正排索引中去找这两个doc,找到之后就停了,不会再继续往下找了 假设正排索引中有100万条数据,搜索结果有100条,去正排索引中去找这100条doc,比如说找到第1000条的时候,结果中的100条doc都找到了,那就没必要再去往下找了,就停了 doc_value原理在PUT/POST数据的时候,就会生成doc_value数据,也就是正排索引 和倒排索引类似,doc_value也会写入到磁盘,先是os cache进行缓存,以提升访问doc value的性能,如果os cache内存大小不足够放的下整个doc_value,就会将doc_value的数据写入磁盘文件中 es官方建议是,es大量是基于os cache来进行缓存和提升性能的,不建议用jvm内存来进行缓存,那样会导致一定的gc开销和oom问题,给jvm更少的内存,给os cache更大的内存 ,一般64G的服务器,给jvm最多16G,剩下的给os cache,os cache可以提升doc value和倒排索引的缓存和查询效率 column压缩举个例子:有如下三个doc123doc1: 550doc2: 550doc3: 500 会把相同的值合并掉,如doc1,doc2的值相同,就只保留一个550的标识就可以了 所有值相同,直接保留单值 少于256个值,使用table encoding模式(一种压缩方式) 大于256个值,看有没有最大公约数,有就除以最大公约数,然后保留这个最大公约数,如果没有最大公约数,采用offset结合压缩的方式 如何禁用有些情况下,我们可能并不需要用到doc_value,这时候就可以通过下面的请求,把doc_value禁用掉,还可以减少磁盘占用12345678910111213PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;my_field&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &quot;doc_values&quot;: false &#125; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-75-percentiles百分比算法及优化]]></title>
    <url>%2F2019%2F01%2F12%2FElasticsearch-75-percentiles%E7%99%BE%E5%88%86%E6%AF%94%E7%AE%97%E6%B3%95%E5%8F%8A%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[需求背景有一个网站,记录了每次请求访问的耗时,需要统计tp50,tp90,tp99. tp50:50%的请求的耗时最长在多长时间 tp90:90%的请求的耗时最长在多长时间 tp99:99%的请求的耗时最长在多长时间 准备数据1DELETE /website 创建索引123456789101112131415161718PUT /website&#123; &quot;mappings&quot;: &#123; &quot;logs&quot;: &#123; &quot;properties&quot;: &#123; &quot;latency&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;province&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;timestamp&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125; &#125; &#125; &#125;&#125; 添加数据12345678910111213141516171819202122232425POST /website/logs/_bulk&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;latency&quot; : 105, &quot;province&quot; : &quot;江苏&quot;, &quot;timestamp&quot; : &quot;2016-10-28&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;latency&quot; : 83, &quot;province&quot; : &quot;江苏&quot;, &quot;timestamp&quot; : &quot;2016-10-29&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;latency&quot; : 92, &quot;province&quot; : &quot;江苏&quot;, &quot;timestamp&quot; : &quot;2016-10-29&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;latency&quot; : 112, &quot;province&quot; : &quot;江苏&quot;, &quot;timestamp&quot; : &quot;2016-10-28&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;latency&quot; : 68, &quot;province&quot; : &quot;江苏&quot;, &quot;timestamp&quot; : &quot;2016-10-28&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;latency&quot; : 76, &quot;province&quot; : &quot;江苏&quot;, &quot;timestamp&quot; : &quot;2016-10-29&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;latency&quot; : 101, &quot;province&quot; : &quot;新疆&quot;, &quot;timestamp&quot; : &quot;2016-10-28&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;latency&quot; : 275, &quot;province&quot; : &quot;新疆&quot;, &quot;timestamp&quot; : &quot;2016-10-29&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;latency&quot; : 166, &quot;province&quot; : &quot;新疆&quot;, &quot;timestamp&quot; : &quot;2016-10-29&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;latency&quot; : 654, &quot;province&quot; : &quot;新疆&quot;, &quot;timestamp&quot; : &quot;2016-10-28&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;latency&quot; : 389, &quot;province&quot; : &quot;新疆&quot;, &quot;timestamp&quot; : &quot;2016-10-28&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;latency&quot; : 302, &quot;province&quot; : &quot;新疆&quot;, &quot;timestamp&quot; : &quot;2016-10-29&quot; &#125; pencentiles12345678910111213141516GET /website/logs/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;latency_percentiles&quot;: &#123; &quot;percentiles&quot;: &#123; &quot;field&quot;: &quot;latency&quot;, &quot;percents&quot;: [ 50, 90, 99 ] &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223&#123; &quot;took&quot;: 10, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 12, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;latency_percentiles&quot;: &#123; &quot;values&quot;: &#123; &quot;50.0&quot;: 108.5, &quot;90.0&quot;: 380.3, &quot;99.0&quot;: 624.8500000000001 &#125; &#125; &#125;&#125; 从返回值来看, tp50是108.5,tp90是380.3,tp99是624.85 这个结果是es算的,他并不是完全准确的 案例先根据地区分组,然后计算上面的数据 请求:1234567891011121314151617181920212223GET /website/logs/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_province&quot;:&#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;province&quot; &#125;, &quot;aggs&quot;: &#123; &quot;latency_percentiles&quot;:&#123; &quot;percentiles&quot;: &#123; &quot;field&quot;: &quot;latency&quot;, &quot;percents&quot;: [ 50, 95, 99 ] &#125; &#125; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526272829303132333435363738394041424344&#123; &quot;took&quot;: 7, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 12, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_province&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;新疆&quot;, &quot;doc_count&quot;: 6, &quot;latency_percentiles&quot;: &#123; &quot;values&quot;: &#123; &quot;50.0&quot;: 288.5, &quot;95.0&quot;: 587.75, &quot;99.0&quot;: 640.75 &#125; &#125; &#125;, &#123; &quot;key&quot;: &quot;江苏&quot;, &quot;doc_count&quot;: 6, &quot;latency_percentiles&quot;: &#123; &quot;values&quot;: &#123; &quot;50.0&quot;: 87.5, &quot;95.0&quot;: 110.25, &quot;99.0&quot;: 111.65 &#125; &#125; &#125; ] &#125; &#125;&#125; percentiles rank比如现在要计算网站访问时延SLA统计SLA: 就是提供的服务标准,比如确保所有的请求都要在200ms以内 需求: 在200ms以内的有百分之多少,在1000ms以内的有百分之多少 这个时候就需要用到percentiles ranks了,percentiles ranks其实比pencentile还要常用 请求:12345678910111213141516171819202122GET /website/logs/_search &#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_province&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;province&quot; &#125;, &quot;aggs&quot;: &#123; &quot;latency_percentile_ranks&quot;: &#123; &quot;percentile_ranks&quot;: &#123; &quot;field&quot;: &quot;latency&quot;, &quot;values&quot;: [ 200, 1000 ] &#125; &#125; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142&#123; &quot;took&quot;: 7, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 12, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_province&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;新疆&quot;, &quot;doc_count&quot;: 6, &quot;latency_percentile_ranks&quot;: &#123; &quot;values&quot;: &#123; &quot;200.0&quot;: 29.40613026819923, &quot;1000.0&quot;: 100 &#125; &#125; &#125;, &#123; &quot;key&quot;: &quot;江苏&quot;, &quot;doc_count&quot;: 6, &quot;latency_percentile_ranks&quot;: &#123; &quot;values&quot;: &#123; &quot;200.0&quot;: 100, &quot;1000.0&quot;: 100 &#125; &#125; &#125; ] &#125; &#125;&#125; percentile的优化percentile是采用TDigest算法的,用很多节点来执行百分比的计算,近似估计,所以会有一定的误差,用到的节点越多,就越精准 compression参数默认值是100, 限制节点的数量最多 compression * 20 个去计算 设置的越大,占用的内存就越多,就越精准,但是性能也会越差, 一个节点占用32字节, 100node 20 32 = 64KB 所以,如果你想要percentile算法越精准,compression可以设置的越大]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-74-cartinality metric去重算法及性能优化]]></title>
    <url>%2F2019%2F01%2F11%2FElasticsearch-74-cartinality-metric%E5%8E%BB%E9%87%8D%E7%AE%97%E6%B3%95%E5%8F%8A%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[cartinality metriccartinality metric:对每个bucket中的指定的field进行去重,取去重之后的count,类似于count(distinct) 案例需求: 每月销售品牌的数量统计12345678910111213141516171819GET tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;month&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;sold_date&quot;, &quot;interval&quot;: &quot;month&quot; &#125;, &quot;aggs&quot;: &#123; &quot;distinct_brand&quot;: &#123; &quot;cardinality&quot;: &#123; &quot;field&quot;: &quot;brand&quot; &#125; &#125; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100&#123; &quot;took&quot;: 35, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;month&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key_as_string&quot;: &quot;2016-05-01T00:00:00.000Z&quot;, &quot;key&quot;: 1462060800000, &quot;doc_count&quot;: 1, &quot;distinct_brand&quot;: &#123; &quot;value&quot;: 1 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-06-01T00:00:00.000Z&quot;, &quot;key&quot;: 1464739200000, &quot;doc_count&quot;: 0, &quot;distinct_brand&quot;: &#123; &quot;value&quot;: 0 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-07-01T00:00:00.000Z&quot;, &quot;key&quot;: 1467331200000, &quot;doc_count&quot;: 1, &quot;distinct_brand&quot;: &#123; &quot;value&quot;: 1 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-08-01T00:00:00.000Z&quot;, &quot;key&quot;: 1470009600000, &quot;doc_count&quot;: 1, &quot;distinct_brand&quot;: &#123; &quot;value&quot;: 1 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-09-01T00:00:00.000Z&quot;, &quot;key&quot;: 1472688000000, &quot;doc_count&quot;: 0, &quot;distinct_brand&quot;: &#123; &quot;value&quot;: 0 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-10-01T00:00:00.000Z&quot;, &quot;key&quot;: 1475280000000, &quot;doc_count&quot;: 1, &quot;distinct_brand&quot;: &#123; &quot;value&quot;: 1 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-11-01T00:00:00.000Z&quot;, &quot;key&quot;: 1477958400000, &quot;doc_count&quot;: 2, &quot;distinct_brand&quot;: &#123; &quot;value&quot;: 1 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-12-01T00:00:00.000Z&quot;, &quot;key&quot;: 1480550400000, &quot;doc_count&quot;: 0, &quot;distinct_brand&quot;: &#123; &quot;value&quot;: 0 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-01-01T00:00:00.000Z&quot;, &quot;key&quot;: 1483228800000, &quot;doc_count&quot;: 1, &quot;distinct_brand&quot;: &#123; &quot;value&quot;: 1 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-02-01T00:00:00.000Z&quot;, &quot;key&quot;: 1485907200000, &quot;doc_count&quot;: 1, &quot;distinct_brand&quot;: &#123; &quot;value&quot;: 1 &#125; &#125; ] &#125; &#125;&#125; 先根据月进行分组,然后用cardinality对品牌去重,就ok了 优化准确率和内存开销precision_threshold先来看个请求123456789101112GET /tvs/sales/_search&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;distinct_brand&quot; : &#123; &quot;cardinality&quot; : &#123; &quot;field&quot; : &quot;brand&quot;, &quot;precision_threshold&quot; : 100 &#125; &#125; &#125;&#125; 请求中有一个参数 precision_threshold,值是100(默认的就是100),这个搜索请求是根据brand来去重的,precision_threshold的作用就是 如果brand的unique value(去重后的唯一值)在100个以内的话,几乎是100%的准确率的 precision_threshold:在多少个unique value以内,cardinality会保证几乎100%准确 内存开销cardinality算法会占用 precision_threshold 8byte 的内存消耗, 比如我们上面这个请求中设置的是100,那么他的内存消耗就是 100 8byte = 800byte ,占用的内存很小. 而且搜索结果中unique value 的数量如果的确在设置的precision_threshold值以内的话,是可以确保100%准确的 官方的说明是如果设置precision_threshold的值是100,实际的unique value有百万的话,错误率是5%以内 HyperLogLog++ (HLL)算法性能优化cardinality运算的底层算法是HLL算法,如果要优化的话也是去优化HLL算法的性能 会对所有的uqniue value取hash值,通过hash值近似去求distcint count,所以会有一定的误差 默认情况下,发送一个cardinality请求的时候,会动态地对所有的field value取hash值, 我们可以在建立索引的时候,就对字段去设置他的hash值, 就不用去动态的取了 示例1234567891011121314151617PUT /tvs/&#123; &quot;mappings&quot;: &#123; &quot;sales&quot;: &#123; &quot;properties&quot;: &#123; &quot;brand&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;hash&quot;: &#123; &quot;type&quot;: &quot;murmur3&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 索引建立好之后,查询的时候,就需要用brand.hash这个field123456789101112GET /tvs/sales/_search&#123; &quot;size&quot; : 0, &quot;aggs&quot; : &#123; &quot;distinct_brand&quot; : &#123; &quot;cardinality&quot; : &#123; &quot;field&quot; : &quot;brand.hash&quot;, &quot;precision_threshold&quot; : 100 &#125; &#125; &#125;&#125; 这种方式其实性能也不会提升多少,可用可不用]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-73-易并行聚合算法 三角选择原则 近似聚合算法]]></title>
    <url>%2F2019%2F01%2F11%2FElasticsearch-73-%E6%98%93%E5%B9%B6%E8%A1%8C%E8%81%9A%E5%90%88%E7%AE%97%E6%B3%95-%E4%B8%89%E8%A7%92%E9%80%89%E6%8B%A9%E5%8E%9F%E5%88%99-%E8%BF%91%E4%BC%BC%E8%81%9A%E5%90%88%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[易并行聚合算法有些聚合分析的算法是很容易并行的,比如说max函数,如图 比如说,现在一共有三个shard,一个查询最大值的搜索请求过来了,那么coordinate node(协调节点)会把请求打到这三个shard上 这个时候,3个shard把他们各自的最大值返回给coordinate node,然后coordinate node再取到这三个里面的最大值,返回给客户端就可以了 有些聚合分析的算法是不好并行的,比如说count(distinct) 找去重后的数据量,并不是说在所有shard上去重之后返回给coordinate node的,因为数据可能会有很多 如图,还是3个shard,每个shard去重后有100W条数据返回给coordinate node,这时候,就要占用大量的内存,最少也需要100W条数据的内存, 他不像上面的max一样,只要保留一个long值的内存,每个shard返回数据的时候对比一下,大于就替换这种, 他需要占用大量的内存, 那么还用这种算法就不是很合适了 es会采用近似聚合的方式,就是采用在每个node上进行近估计的方式,得到最终的结论,但是这个结果跟实际是有一定的偏差的,比如说count(distinct)去重后,是有100W的数据,但是es估计的值是105万或者95万,就有5%左右的错误率 三角选择原则 精准 实时 大数据 在这三个中只能选择两个,比如 精准+实时:没有大数据,适合数据量很小的情况,一般就是单机跑,随便怎么玩儿 精准+大数据:比如hadoop,批处理,非实时,可以处理海量数据,保证精准,但是可能会跑个几分钟几小时的 大数据+实时:比如es,不精准,近似估计,可能会有百分之几的错误率 近似聚合算法如果采取近似估计的算法,延迟大概在100ms左右,0.5%的错误率如果采用100%精准的算法,延时一般在几秒到几十秒甚至几十分钟几小时, 0%的错误率]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-72-聚合分析 自定义排序]]></title>
    <url>%2F2019%2F01%2F11%2FElasticsearch-72-%E8%81%9A%E5%90%88%E5%88%86%E6%9E%90-%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[之前全部的排序都是按照每个bucket的doc_count降序来排的,那么如何自定义排序呢 自定义排序需求: 按每种颜色的平均销售额降序排序123456789101112131415161718192021GET /tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_color&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot;, &quot;order&quot;: &#123; &quot;avg_price&quot;: &quot;desc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; 返回值:12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_color&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;红色&quot;, &quot;doc_count&quot;: 4, &quot;avg_price&quot;: &#123; &quot;value&quot;: 3250 &#125; &#125;, &#123; &quot;key&quot;: &quot;绿色&quot;, &quot;doc_count&quot;: 2, &quot;avg_price&quot;: &#123; &quot;value&quot;: 2100 &#125; &#125;, &#123; &quot;key&quot;: &quot;蓝色&quot;, &quot;doc_count&quot;: 2, &quot;avg_price&quot;: &#123; &quot;value&quot;: 2000 &#125; &#125; ] &#125; &#125;&#125; 已经是按照请求中指定的avg_price降序排列的了 下钻分析时,深层的metric排序需求: 颜色 + 品牌下钻分析,按最深层的metric排序 12345678910111213141516171819202122232425262728GET /tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_color&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125;, &quot;aggs&quot;: &#123; &quot;group_by_brand&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;brand&quot;, &quot;order&quot;: &#123; &quot;avg_price&quot;: &quot;desc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 返回值:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394&#123; &quot;took&quot;: 1, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_color&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;红色&quot;, &quot;doc_count&quot;: 4, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;三星&quot;, &quot;doc_count&quot;: 1, &quot;avg_price&quot;: &#123; &quot;value&quot;: 8000 &#125; &#125;, &#123; &quot;key&quot;: &quot;长虹&quot;, &quot;doc_count&quot;: 3, &quot;avg_price&quot;: &#123; &quot;value&quot;: 1666.6666666666667 &#125; &#125; ] &#125; &#125;, &#123; &quot;key&quot;: &quot;绿色&quot;, &quot;doc_count&quot;: 2, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;小米&quot;, &quot;doc_count&quot;: 1, &quot;avg_price&quot;: &#123; &quot;value&quot;: 3000 &#125; &#125;, &#123; &quot;key&quot;: &quot;TCL&quot;, &quot;doc_count&quot;: 1, &quot;avg_price&quot;: &#123; &quot;value&quot;: 1200 &#125; &#125; ] &#125; &#125;, &#123; &quot;key&quot;: &quot;蓝色&quot;, &quot;doc_count&quot;: 2, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;小米&quot;, &quot;doc_count&quot;: 1, &quot;avg_price&quot;: &#123; &quot;value&quot;: 2500 &#125; &#125;, &#123; &quot;key&quot;: &quot;TCL&quot;, &quot;doc_count&quot;: 1, &quot;avg_price&quot;: &#123; &quot;value&quot;: 1500 &#125; &#125; ] &#125; &#125; ] &#125; &#125;&#125; 看下请求和返回值,请求中,排序是加到了下面一层group_by_brand中, 然后搜索结果中,也是group_by_brand下面的bucket按照我们指定的字段排序了,在上层的group_by_color中,还是按照doc_count去排序的]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-71-过滤filter 聚合结合使用]]></title>
    <url>%2F2019%2F01%2F11%2FElasticsearch-71-%E8%BF%87%E6%BB%A4filter-%E8%81%9A%E5%90%88%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[filter过滤+聚合分析需求: 统计价格大于1200的电视的平均价格 请求:12345678910111213141516171819202122GET tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;price&quot;: &#123; &quot;gte&quot;: 1200 &#125; &#125; &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;avg_of_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125;&#125; 返回值:12345678910111213141516171819&#123; &quot;took&quot;: 15, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 7, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;avg_of_price&quot;: &#123; &quot;value&quot;: 2885.714285714286 &#125; &#125;&#125; 跟搜索聚合的结合使用其实是一样的,就是把match换成了filter bucket filter先来看一个请求:123456789101112131415161718192021222324252627282930313233343536373839404142434445GET tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;brand&quot;: &#123; &quot;value&quot;: &quot;长虹&quot; &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;recent_150d&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;sold_date&quot;: &#123; &quot;gte&quot;: &quot;now-150d&quot; &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;recent_150d_avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125;, &quot;recent_140d&quot;:&#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;sold_date&quot;: &#123; &quot;gte&quot;: &quot;now-140d&quot; &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;recent_140d_avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; 先是query去搜索数据,过滤出只是长虹牌的数据,然后下面的aggs就是针对搜索结果的聚合, 然后每一个聚合分析里面有一个filter和aggs, filter呢,是用来过滤数据的,他是只针对这一个聚合去过滤的,然后filter下面aggs是再对filter过滤后的数据进行聚合分析. 每组聚合分析里面的数据,都是在query的结果上去过滤的,互不影响]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-70-搜索 聚合分析结合使用]]></title>
    <url>%2F2019%2F01%2F10%2FElasticsearch-70-%E6%90%9C%E7%B4%A2-%E8%81%9A%E5%90%88%E5%88%86%E6%9E%90%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[之前的几个案例都是全部使用的聚合分析,接下来呢,使用搜索和聚合分析结合起来使用 案例需求: 统计指定品牌下每个颜色的销量 (小米) 请求:123456789101112131415161718GET tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;brand&quot;: &#123; &quot;value&quot;: &quot;小米&quot; &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;group_by_color&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930&#123; &quot;took&quot;: 35, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_color&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;绿色&quot;, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: &quot;蓝色&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125; &#125;&#125; es的aggregation scope:任何的聚合,都必须在搜索出来的结果数据中执行,搜索结果,就是聚合分析操作的scope global bucket如果我们需要聚合分析两组数据,一组是根据搜索出来的结果集进行聚合分析,一组是根据全部的数据进行聚合分析,这时候就需要用到global bucket了,先看一个案例 需求: 分析单个品牌和所有品牌的销售平均价格 请求:12345678910111213141516171819202122232425262728GET tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;brand&quot;: &#123; &quot;value&quot;: &quot;长虹&quot; &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;single_brand_avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;all&quot;:&#123; &quot;global&quot;: &#123;&#125;, &quot;aggs&quot;: &#123; &quot;all_brand_avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; 第一个aggs中single_brand_avg_price先计算了搜索返回结果的平均价格,然后在下面用了global,然后又一个aggs,计算全部的平均价格,global就是将所有的数据纳入聚合的scope,而不管之前的query 返回值:12345678910111213141516171819202122232425&#123; &quot;took&quot;: 33, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;all&quot;: &#123; &quot;doc_count&quot;: 8, &quot;all_brand_avg_price&quot;: &#123; &quot;value&quot;: 2650 &#125; &#125;, &quot;single_brand_avg_price&quot;: &#123; &quot;value&quot;: 1666.6666666666667 &#125; &#125;&#125; 返回值中single_brand_avg_price.value就是针对query执行的聚合结果, all.all_brand_avg_price是针对所有数据执行的聚合结果]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-69-深入聚合分析数据II]]></title>
    <url>%2F2019%2F01%2F09%2FElasticsearch-69-%E6%B7%B1%E5%85%A5%E8%81%9A%E5%90%88%E5%88%86%E6%9E%90%E6%95%B0%E6%8D%AEII%2F</url>
    <content type="text"><![CDATA[常用的几种metric操作上文中,用了avg和count这两个操作,一般来说,常用的metric操作就是以下几种 count: 计算数量,用terms操作来分组的话,就会自动有一个doc_count,就相当于是count avg: 求一个bucket内,指定field数据的平均值 max: 求一个bucket内,指定field数据的最大值 min: 求一个bucket内,指定field数据的最小值 sum: 求一个bucket内,指定field数据的和 示例需求: 统计每种颜色的电视的数量和价格的平均值,最大值,最小值,总和 请求体:123456789101112131415161718192021222324252627282930313233GET tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;color&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125;, &quot;aggs&quot;: &#123; &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;max_price&quot;:&#123; &quot;max&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;min_price&quot;:&#123; &quot;min&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;sum_price&quot;:&#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; 返回值:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970&#123; &quot;took&quot;: 6, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;color&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;红色&quot;, &quot;doc_count&quot;: 4, &quot;max_price&quot;: &#123; &quot;value&quot;: 8000 &#125;, &quot;min_price&quot;: &#123; &quot;value&quot;: 1000 &#125;, &quot;avg_price&quot;: &#123; &quot;value&quot;: 3250 &#125;, &quot;sum_price&quot;: &#123; &quot;value&quot;: 13000 &#125; &#125;, &#123; &quot;key&quot;: &quot;绿色&quot;, &quot;doc_count&quot;: 2, &quot;max_price&quot;: &#123; &quot;value&quot;: 3000 &#125;, &quot;min_price&quot;: &#123; &quot;value&quot;: 1200 &#125;, &quot;avg_price&quot;: &#123; &quot;value&quot;: 2100 &#125;, &quot;sum_price&quot;: &#123; &quot;value&quot;: 4200 &#125; &#125;, &#123; &quot;key&quot;: &quot;蓝色&quot;, &quot;doc_count&quot;: 2, &quot;max_price&quot;: &#123; &quot;value&quot;: 2500 &#125;, &quot;min_price&quot;: &#123; &quot;value&quot;: 1500 &#125;, &quot;avg_price&quot;: &#123; &quot;value&quot;: 2000 &#125;, &quot;sum_price&quot;: &#123; &quot;value&quot;: 4000 &#125; &#125; ] &#125; &#125;&#125; histogram上面的请求都是用的terms来分组的, terms其实就是把field的值相同的数据分到了一个bucket里面,而histogram呢是可以根据某一范围区间去划分的 比如现在有一个需求,按照价格区间来统计销量和销售额12345678910111213141516171819GET /tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_price&quot;: &#123; &quot;histogram&quot;: &#123; &quot;field&quot;: &quot;price&quot;, &quot;interval&quot;: 2000 &#125;, &quot;aggs&quot;: &#123; &quot;sum_price&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; 返回值:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&#123; &quot;took&quot;: 7, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_price&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: 0, &quot;doc_count&quot;: 3, &quot;sum_price&quot;: &#123; &quot;value&quot;: 3700 &#125; &#125;, &#123; &quot;key&quot;: 2000, &quot;doc_count&quot;: 4, &quot;sum_price&quot;: &#123; &quot;value&quot;: 9500 &#125; &#125;, &#123; &quot;key&quot;: 4000, &quot;doc_count&quot;: 0, &quot;sum_price&quot;: &#123; &quot;value&quot;: 0 &#125; &#125;, &#123; &quot;key&quot;: 6000, &quot;doc_count&quot;: 0, &quot;sum_price&quot;: &#123; &quot;value&quot;: 0 &#125; &#125;, &#123; &quot;key&quot;: 8000, &quot;doc_count&quot;: 1, &quot;sum_price&quot;: &#123; &quot;value&quot;: 8000 &#125; &#125; ] &#125; &#125;&#125; 详细看一下请求体中的1234&quot;histogram&quot;: &#123; &quot;field&quot;: &quot;price&quot;, &quot;interval&quot;: 2000&#125; 这个部分,histogram和term类似也是进行bucket分组操作的, 里面的field就是按照哪个field进行分组,interval划分范围,比如我们请求中的是2000,那会就会划分0-2000,2000-4000,4000-6000….等等区间,然后根据price的值,去决定分到哪个bucket中,bucket有了之后,对它进行metric操作,和之前是一样的 date histogram需求: 统计每个月的电视销量 date histogram,可以按照我们指定的某一个date类型的field,以及日期interval,按照一定的日期间隔,去划分bucket 请求:123456789101112131415161718GET tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;sale&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;sold_date&quot;, &quot;interval&quot;: &quot;month&quot;, &quot;format&quot;: &quot;yyyy-MM-dd&quot;, &quot;min_doc_count&quot;: 0, &quot;extended_bounds&quot;:&#123; &quot;min&quot;: &quot;2016-01-01&quot;, &quot;max&quot;: &quot;2017-12-31&quot; &#125; &#125; &#125; &#125;&#125; 看一下请求,interval是month,就是按照月去划分,比如说2017-01-01~2017-01-31就是一个bucket, 然后 去扫描每个数据的date_field的值,判断落在哪个bucket中 min_doc_count:设置为0,意思就是说,即使某个interval区间中,一条数据都没有,那么这个区间也还是要返回的,不然默认是会过滤掉这个区间的 extended_bounds:划分bucket的时候,会限定这个起始日期和截止日期 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140&#123; &quot;took&quot;: 28, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;sale&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key_as_string&quot;: &quot;2016-01-01&quot;, &quot;key&quot;: 1451606400000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-02-01&quot;, &quot;key&quot;: 1454284800000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-03-01&quot;, &quot;key&quot;: 1456790400000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-04-01&quot;, &quot;key&quot;: 1459468800000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-05-01&quot;, &quot;key&quot;: 1462060800000, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-06-01&quot;, &quot;key&quot;: 1464739200000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-07-01&quot;, &quot;key&quot;: 1467331200000, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-08-01&quot;, &quot;key&quot;: 1470009600000, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-09-01&quot;, &quot;key&quot;: 1472688000000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-10-01&quot;, &quot;key&quot;: 1475280000000, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-11-01&quot;, &quot;key&quot;: 1477958400000, &quot;doc_count&quot;: 2 &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-12-01&quot;, &quot;key&quot;: 1480550400000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-01-01&quot;, &quot;key&quot;: 1483228800000, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-02-01&quot;, &quot;key&quot;: 1485907200000, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-03-01&quot;, &quot;key&quot;: 1488326400000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-04-01&quot;, &quot;key&quot;: 1491004800000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-05-01&quot;, &quot;key&quot;: 1493596800000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-06-01&quot;, &quot;key&quot;: 1496275200000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-07-01&quot;, &quot;key&quot;: 1498867200000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-08-01&quot;, &quot;key&quot;: 1501545600000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-09-01&quot;, &quot;key&quot;: 1504224000000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-10-01&quot;, &quot;key&quot;: 1506816000000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-11-01&quot;, &quot;key&quot;: 1509494400000, &quot;doc_count&quot;: 0 &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-12-01&quot;, &quot;key&quot;: 1512086400000, &quot;doc_count&quot;: 0 &#125; ] &#125; &#125;&#125; 返回值中,key_as_string 就是日期,key是13位的时间戳,doc_count就是统计的数量 案例需求: 统计每个季度每个品牌的销售额 请求:12345678910111213141516171819202122232425262728293031323334353637GET /tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_quarter&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;sold_date&quot;, &quot;interval&quot;: &quot;quarter&quot;, &quot;format&quot;: &quot;yyyy-MM-dd&quot;, &quot;min_doc_count&quot;: 0, &quot;extended_bounds&quot;:&#123; &quot;min&quot;:&quot;2016-01-01&quot;, &quot;max&quot;:&quot;2017-12-31&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;group_by_brand&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;brand&quot; &#125;, &quot;aggs&quot;: &#123; &quot;sum_of_price&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125;, &quot;total_sum_price&quot;:&#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; 请求中,先按照季度来分组,分好之后下面的下钻分析中,第一个group_by_brand 按照品牌分组, 第二个是total_sum_price 计算每二个季度的销售额, 然后group_by_brand下面继续下钻分析,统计每个品牌的销售额 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163&#123; &quot;took&quot;: 4, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_quarter&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key_as_string&quot;: &quot;2016-01-01&quot;, &quot;key&quot;: 1451606400000, &quot;doc_count&quot;: 0, &quot;total_sum_price&quot;: &#123; &quot;value&quot;: 0 &#125;, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [] &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-04-01&quot;, &quot;key&quot;: 1459468800000, &quot;doc_count&quot;: 1, &quot;total_sum_price&quot;: &#123; &quot;value&quot;: 3000 &#125;, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;小米&quot;, &quot;doc_count&quot;: 1, &quot;sum_of_price&quot;: &#123; &quot;value&quot;: 3000 &#125; &#125; ] &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-07-01&quot;, &quot;key&quot;: 1467331200000, &quot;doc_count&quot;: 2, &quot;total_sum_price&quot;: &#123; &quot;value&quot;: 2700 &#125;, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;TCL&quot;, &quot;doc_count&quot;: 2, &quot;sum_of_price&quot;: &#123; &quot;value&quot;: 2700 &#125; &#125; ] &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2016-10-01&quot;, &quot;key&quot;: 1475280000000, &quot;doc_count&quot;: 3, &quot;total_sum_price&quot;: &#123; &quot;value&quot;: 5000 &#125;, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;长虹&quot;, &quot;doc_count&quot;: 3, &quot;sum_of_price&quot;: &#123; &quot;value&quot;: 5000 &#125; &#125; ] &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-01-01&quot;, &quot;key&quot;: 1483228800000, &quot;doc_count&quot;: 2, &quot;total_sum_price&quot;: &#123; &quot;value&quot;: 10500 &#125;, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;三星&quot;, &quot;doc_count&quot;: 1, &quot;sum_of_price&quot;: &#123; &quot;value&quot;: 8000 &#125; &#125;, &#123; &quot;key&quot;: &quot;小米&quot;, &quot;doc_count&quot;: 1, &quot;sum_of_price&quot;: &#123; &quot;value&quot;: 2500 &#125; &#125; ] &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-04-01&quot;, &quot;key&quot;: 1491004800000, &quot;doc_count&quot;: 0, &quot;total_sum_price&quot;: &#123; &quot;value&quot;: 0 &#125;, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [] &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-07-01&quot;, &quot;key&quot;: 1498867200000, &quot;doc_count&quot;: 0, &quot;total_sum_price&quot;: &#123; &quot;value&quot;: 0 &#125;, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [] &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2017-10-01&quot;, &quot;key&quot;: 1506816000000, &quot;doc_count&quot;: 0, &quot;total_sum_price&quot;: &#123; &quot;value&quot;: 0 &#125;, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [] &#125; &#125; ] &#125; &#125;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-68-深入聚合分析数据I]]></title>
    <url>%2F2019%2F01%2F07%2FElasticsearch-68-%E6%B7%B1%E5%85%A5%E8%81%9A%E5%90%88%E5%88%86%E6%9E%90%E6%95%B0%E6%8D%AEI%2F</url>
    <content type="text"><![CDATA[背景本文,将以一个家电卖场中的电视销售数据为背景,进行各种各样角度的聚合分析 准备数据创建索引tvs123456789101112131415161718192021PUT /tvs&#123; &quot;mappings&quot;: &#123; &quot;sales&quot;: &#123; &quot;properties&quot;: &#123; &quot;price&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;color&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;brand&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;sold_date&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125; &#125; &#125; &#125;&#125; 添加测试数据1234567891011121314151617POST /tvs/sales/_bulk&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 1000, &quot;color&quot; : &quot;红色&quot;, &quot;brand&quot; : &quot;长虹&quot;, &quot;sold_date&quot; : &quot;2016-10-28&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 2000, &quot;color&quot; : &quot;红色&quot;, &quot;brand&quot; : &quot;长虹&quot;, &quot;sold_date&quot; : &quot;2016-11-05&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 3000, &quot;color&quot; : &quot;绿色&quot;, &quot;brand&quot; : &quot;小米&quot;, &quot;sold_date&quot; : &quot;2016-05-18&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 1500, &quot;color&quot; : &quot;蓝色&quot;, &quot;brand&quot; : &quot;TCL&quot;, &quot;sold_date&quot; : &quot;2016-07-02&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 1200, &quot;color&quot; : &quot;绿色&quot;, &quot;brand&quot; : &quot;TCL&quot;, &quot;sold_date&quot; : &quot;2016-08-19&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 2000, &quot;color&quot; : &quot;红色&quot;, &quot;brand&quot; : &quot;长虹&quot;, &quot;sold_date&quot; : &quot;2016-11-05&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 8000, &quot;color&quot; : &quot;红色&quot;, &quot;brand&quot; : &quot;三星&quot;, &quot;sold_date&quot; : &quot;2017-01-01&quot; &#125;&#123; &quot;index&quot;: &#123;&#125;&#125;&#123; &quot;price&quot; : 2500, &quot;color&quot; : &quot;蓝色&quot;, &quot;brand&quot; : &quot;小米&quot;, &quot;sold_date&quot; : &quot;2017-02-12&quot; &#125; 根据颜色分类统计销量请求体1234567891011GET /tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;popular_colors&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125; &#125; &#125;&#125; 请求体重的各种参数: size:设置为0的话,只获取聚合结果,不会把原始数据返回回来 aggs:固定语法,要对一份数据执行分组聚合操作 popular_colors:需要对每个aggs取一个名字,名字是自定义的 terms:表示要根据字段的值进行分组 field:要根据那个字段进行分组 上面请求的返回值:12345678910111213141516171819202122232425262728293031323334&#123; &quot;took&quot;: 8, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;popular_colors&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;红色&quot;, &quot;doc_count&quot;: 4 &#125;, &#123; &quot;key&quot;: &quot;绿色&quot;, &quot;doc_count&quot;: 2 &#125;, &#123; &quot;key&quot;: &quot;蓝色&quot;, &quot;doc_count&quot;: 2 &#125; ] &#125; &#125;&#125; 返回结果中的数据 hits.hits: 我们指定了size是0,所以这里就是空的,否则会把执行聚合的原始数据返回回来 aggregations: 聚合结果 popular_color: 在查询时候指定的那个名称 buckets: 根据我们指定的field划分出来的buckets key: 每个bucket对应的那个值 doc_count: 这个bucket分组内,有多少个数据 默认是按照doc_count降序排序的 统计每种颜色的平均价格请求体:123456789101112131415161718GET /tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;colors&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125;, &quot;aggs&quot;: &#123; &quot;price_avg&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125;&#125; 还是和上面一样,按照color去分bucket,可以拿到每个color bucket中的数量(doc_count),这仅仅是一个bucket操作,doc_count的统计其实只是es的bucket操作默认执行的一个内置metric 上面请求中的计算平均值,就是对bucket执行的一个metric聚合统计操作 看一下请求体,在一个aggs执行的bucket操作(terms),同级下又加入了一个aggs,这第二个aggs内部,同样取了个名字,执行一个metric操作 avg,对之前的每个bucket中的数据的指定field, 求一个平均值 请求中的1234567&quot;aggs&quot;: &#123; &quot;price_avg&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; 就是一个metric操作,对分组后的每个bucket都要执行的一个操作 请求的返回值:12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;took&quot;: 1, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;colors&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;红色&quot;, &quot;doc_count&quot;: 4, &quot;price_avg&quot;: &#123; &quot;value&quot;: 3250 &#125; &#125;, &#123; &quot;key&quot;: &quot;绿色&quot;, &quot;doc_count&quot;: 2, &quot;price_avg&quot;: &#123; &quot;value&quot;: 2100 &#125; &#125;, &#123; &quot;key&quot;: &quot;蓝色&quot;, &quot;doc_count&quot;: 2, &quot;price_avg&quot;: &#123; &quot;value&quot;: 2000 &#125; &#125; ] &#125; &#125;&#125; 再来看一下返回值,buckets中除了key和doc_count还有 avg_price: 我们在发送请求时候,自己取的名字 value: metric计算的结果,每个bucket中的数据的price字段求平均值后的结果 这段请求,如果转成sql的话,就是1select avg(price) from tvs.sales group by color 下钻分析需求: 从颜色到品牌进行下钻分析, 分析每种颜色的平均价格,以及每个颜色中的每个品牌的平均价格 下钻的意思是,已经分了一个组了,然后还要对这个分组内的数据,再分组,比如上面这个案例中,颜色分组之后,还可以对品牌进行分组,最后对每个最小粒度的分组执行聚合分析的操作,就是下钻分析 搜索请求:123456789101112131415161718192021222324252627282930GET /tvs/sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_color&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot; &#125;, &quot;aggs&quot;: &#123; &quot;color_avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;group_by_brand&quot;:&#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;brand&quot; &#125;, &quot;aggs&quot;: &#123; &quot;brand_avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103&#123; &quot;took&quot;: 10, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 8, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_color&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;红色&quot;, &quot;doc_count&quot;: 4, &quot;color_avg_price&quot;: &#123; &quot;value&quot;: 3250 &#125;, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;长虹&quot;, &quot;doc_count&quot;: 3, &quot;brand_avg_price&quot;: &#123; &quot;value&quot;: 1666.6666666666667 &#125; &#125;, &#123; &quot;key&quot;: &quot;三星&quot;, &quot;doc_count&quot;: 1, &quot;brand_avg_price&quot;: &#123; &quot;value&quot;: 8000 &#125; &#125; ] &#125; &#125;, &#123; &quot;key&quot;: &quot;绿色&quot;, &quot;doc_count&quot;: 2, &quot;color_avg_price&quot;: &#123; &quot;value&quot;: 2100 &#125;, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;TCL&quot;, &quot;doc_count&quot;: 1, &quot;brand_avg_price&quot;: &#123; &quot;value&quot;: 1200 &#125; &#125;, &#123; &quot;key&quot;: &quot;小米&quot;, &quot;doc_count&quot;: 1, &quot;brand_avg_price&quot;: &#123; &quot;value&quot;: 3000 &#125; &#125; ] &#125; &#125;, &#123; &quot;key&quot;: &quot;蓝色&quot;, &quot;doc_count&quot;: 2, &quot;color_avg_price&quot;: &#123; &quot;value&quot;: 2000 &#125;, &quot;group_by_brand&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;TCL&quot;, &quot;doc_count&quot;: 1, &quot;brand_avg_price&quot;: &#123; &quot;value&quot;: 1500 &#125; &#125;, &#123; &quot;key&quot;: &quot;小米&quot;, &quot;doc_count&quot;: 1, &quot;brand_avg_price&quot;: &#123; &quot;value&quot;: 2500 &#125; &#125; ] &#125; &#125; ] &#125; &#125;&#125; 先看一下搜索请求,就是在计算完按颜色分组之后的平均值后,又分了一次组group_by_brand,按的是品牌,然后分组之后,再计算按颜色品牌的平均值 再看返回结果,结构基本和搜索请求是相同的,先是按颜色的分组,然后下面又套了一个按品牌的分组]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-67-聚合分析bucket和metric]]></title>
    <url>%2F2019%2F01%2F07%2FElasticsearch-67-%E8%81%9A%E5%90%88%E5%88%86%E6%9E%90bucket%E5%92%8Cmetric%2F</url>
    <content type="text"><![CDATA[核心概念bucket是一个数据的分组,metric就是对一个bucket执行的某种聚合分析的操作,比如说,求平均值,最大值,最小值等 举个例子,有这么一组数据 city name 北京 小李 北京 小王 上海 小张 上海 小丽 上海 小陈 上面的数据可以通过城市划分出来两个bucket,一个是北京bucket一个是上海bucket 北京bucket包含了两个人:小李,小王上海bucket包含了三个人:小张,小丽,小陈 就是说,按照某个字段进行bucket划分,那个字段的值相同的那些数据,就会被划分到一个bucket中 metric呢就是对这些bucket进行的聚合分析的操作 比如有这么一个sql1select count(1) from access_log group_by user_id 在这个sql中bucket就是 group_by user_id 那些user_id相同的数据,就是一个bucketmetric就是count(1),计算每个bucket中的总数这个操作]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-66-修改IK分词器源码基于mysql热更新词库]]></title>
    <url>%2F2019%2F01%2F05%2FElasticsearch-66-%E4%BF%AE%E6%94%B9IK%E5%88%86%E8%AF%8D%E5%99%A8%E6%BA%90%E7%A0%81%E5%9F%BA%E4%BA%8Emysql%E7%83%AD%E6%9B%B4%E6%96%B0%E8%AF%8D%E5%BA%93%2F</url>
    <content type="text"><![CDATA[上文中,我们如果要配置一个自定义的词语,或者停用词的时候,必须要手动添加到ik分词器的配置中,然后重启es节点,这样就很坑了,而且如果es集群中有上百个节点的话,那一个个的修改要疯了 通过修改ik分词器的源码,可以使用mysql作为词库,有词语更新的话,直接添加到mysql的表中就好了,不需要再去重启. 热更新方案第一种:修改ik分词器源码,然后手动支持从mysql中每隔一定时间,自动加载新的词库第二种:基于ik分词器原生支持的热更新方案,部署一个web服务器,提供一个http接口,通过modified和tag两个http响应头,来提供词语的热更新 第一种方案是比较常用的, 第二种呢ik git官方社区都不建议采用 源码下载从github上把源码拉下来1git clone https://github.com/medcl/elasticsearch-analysis-ik.git 我们的es是5.2.0版本的,ik分词器也切换到5.2.0版本的分支上面1git checkout v5.2.0 切换完成后,直接用idea打开就好了 源码修改第一步,在pom中加入mysql的依赖123456&lt;!-- mysql --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;6.0.6&lt;/version&gt;&lt;/dependency&gt; 第二步,配置mysql的连接,在config目录下创建一个.properties文件123456789jdbc.url=jdbc:mysql://localhost:3306/my_ik_word?allowMultiQueries=true&amp;autoReconnect=true&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;serverTimezone=GMTjdbc.user=rootjdbc.password=123456# 更新词库的语句jdbc.reload.sql=select word from hot_words# 更新停用词的语句jdbc.reload.stopword.sql=select stop_word as word from hot_stop_words# 隔多少时间去更新一次jdbc.reload.interval=1000 第三步,新建一个线程,run方法中调用Dictionary类的reLoadMainDict()方法,就是让他去重新加载词典123456789101112public class HotDicReloadThread implements Runnable &#123; private static final Logger logger = ESLoggerFactory.getLogger(HotDicReloadThread.class.getName()); @Override public void run() &#123; while (true)&#123; logger.info(&quot;-------reload hot dic from mysql--------&quot;); Dictionary.getSingleton().reLoadMainDict(); &#125; &#125;&#125; 第四步,Dictionary类中,加入mysql驱动类12345678910// prop用来获取上面的properties配置文件private static Properties prop = new Properties();static &#123; try &#123; Class.forName(&quot;com.mysql.jdbc.Driver&quot;); &#125; catch (ClassNotFoundException e) &#123; logger.error(&quot;error&quot;, e); &#125;&#125; 第五步,initial()方法中,启动刚刚创建的线程123456789101112131415161718192021222324252627282930313233343536373839/** * 词典初始化 由于IK Analyzer的词典采用Dictionary类的静态方法进行词典初始化 * 只有当Dictionary类被实际调用时，才会开始载入词典， 这将延长首次分词操作的时间 该方法提供了一个在应用加载阶段就初始化字典的手段 * * @return Dictionary */public static synchronized Dictionary initial(Configuration cfg) &#123; if (singleton == null) &#123; synchronized (Dictionary.class) &#123; if (singleton == null) &#123; singleton = new Dictionary(cfg); singleton.loadMainDict(); singleton.loadSurnameDict(); singleton.loadQuantifierDict(); singleton.loadSuffixDict(); singleton.loadPrepDict(); singleton.loadStopWordDict(); // 执行更新词库的线程 new Thread(new HotDicReloadThread()).start(); if(cfg.isEnableRemoteDict())&#123; // 建立监控线程 for (String location : singleton.getRemoteExtDictionarys()) &#123; // 10 秒是初始延迟可以修改的 60是间隔时间 单位秒 pool.scheduleAtFixedRate(new Monitor(location), 10, 60, TimeUnit.SECONDS); &#125; for (String location : singleton.getRemoteExtStopWordDictionarys()) &#123; pool.scheduleAtFixedRate(new Monitor(location), 10, 60, TimeUnit.SECONDS); &#125; &#125; return singleton; &#125; &#125; &#125; return singleton;&#125; 第六步,新添加一个loadMainDict()方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * 从mysql中加载热更新词典 */private void loadMySqlExtDict()&#123; Connection connection = null; Statement statement = null; ResultSet resultSet = null; try &#123; Path file = PathUtils.get(getDictRoot(),&quot;jdbc-reload.properties&quot;); prop.load(new FileInputStream(file.toFile())); logger.info(&quot;-------jdbc-reload.properties-------&quot;); for (Object key : prop.keySet()) &#123; logger.info(&quot;key:&#123;&#125;&quot;, prop.getProperty(String.valueOf(key))); &#125; logger.info(&quot;------- query hot dict from mysql, sql:&#123;&#125;-------&quot;, prop.getProperty(&quot;jdbc.reload.sql&quot;)); // 建立mysql连接 connection = DriverManager.getConnection( prop.getProperty(&quot;jdbc.url&quot;), prop.getProperty(&quot;jdbc.user&quot;), prop.getProperty(&quot;jdbc.password&quot;) ); // 执行查询 statement = connection.createStatement(); resultSet = statement.executeQuery(prop.getProperty(&quot;jdbc.reload.sql&quot;)); // 循环输出查询啊结果,添加到Main.dict中去 while (resultSet.next()) &#123; String theWord = resultSet.getString(&quot;word&quot;); logger.info(&quot;------hot word from mysql:&#123;&#125;------&quot;, theWord); // 加到mainDict里面 _MainDict.fillSegment(theWord.trim().toCharArray()); &#125; &#125; catch (Exception e) &#123; logger.error(&quot;error:&#123;&#125;&quot;, e); &#125; finally &#123; try &#123; if (resultSet != null) &#123; resultSet.close(); &#125; if (statement != null) &#123; statement.close(); &#125; if (connection != null) &#123; connection.close(); &#125; &#125; catch (SQLException e)&#123; logger.error(&quot;error&quot;, e); &#125; &#125;&#125; 第七步,在loadMainDict()方法最后,调用上面添加的这个方法12// 加载mysql词典this.loadMySqlExtDict(); 第八步,新添加loadMySqlStopwordDict()方法,用来从mysql中获取停用词12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * 从mysql中加载停用词 */private void loadMySqlStopwordDict()&#123; Connection conn = null; Statement stmt = null; ResultSet rs = null; try &#123; Path file = PathUtils.get(getDictRoot(), &quot;jdbc-reload.properties&quot;); prop.load(new FileInputStream(file.toFile())); logger.info(&quot;-------jdbc-reload.properties-------&quot;); for(Object key : prop.keySet()) &#123; logger.info(&quot;-------key:&#123;&#125;&quot;, prop.getProperty(String.valueOf(key))); &#125; logger.info(&quot;-------query hot stopword dict from mysql, sql:&#123;&#125;&quot;,props.getProperty(&quot;jdbc.reload.stopword.sql&quot;)); conn = DriverManager.getConnection( prop.getProperty(&quot;jdbc.url&quot;), prop.getProperty(&quot;jdbc.user&quot;), prop.getProperty(&quot;jdbc.password&quot;)); stmt = conn.createStatement(); rs = stmt.executeQuery(prop.getProperty(&quot;jdbc.reload.stopword.sql&quot;)); while(rs.next()) &#123; String theWord = rs.getString(&quot;word&quot;); logger.info(&quot;------- hot stopword from mysql: &#123;&#125;&quot;, theWord); _StopWords.fillSegment(theWord.trim().toCharArray()); &#125; Thread.sleep(Integer.valueOf(String.valueOf(prop.get(&quot;jdbc.reload.interval&quot;)))); &#125; catch (Exception e) &#123; logger.error(&quot;error&quot;, e); &#125; finally &#123; try &#123; if(rs != null) &#123; rs.close(); &#125; if(stmt != null) &#123; stmt.close(); &#125; if(conn != null) &#123; conn.close(); &#125; &#125; catch (SQLException e)&#123; logger.error(&quot;error:&#123;&#125;&quot;, e); &#125; &#125;&#125; 第九步,在loadStopWordDict()方法最后,调用上面的更新停用词的方法12// 从mysql中加载停用词this.loadMySqlStopwordDict(); 至此,源码修改完毕,数据库的两个表如下 词库表12345CREATE TABLE `hot_words` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `word` varchar(50) COLLATE utf8_unicode_ci DEFAULT NULL COMMENT &apos;词语&apos;, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_unicode_ci; 停用词库表12345CREATE TABLE `hot_stop_words` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `stop_word` varchar(50) COLLATE utf8_unicode_ci DEFAULT NULL COMMENT &apos;停用词&apos;, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8 COLLATE=utf8_unicode_ci; 这些做完以后,maven打包项目1mvn clean package -DskipTests 打包完成后,在项目目录的target\releases 路径下面有个压缩包,解压到es\plugins\ik目录下,然后将mysql的驱动包丢进去, 之后重启es就完成了. 测试在停用词的表中加入 “我”,然后去kibana中测试一下12345GET /_analyze&#123; &quot;text&quot;: &quot;我的&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;&#125; 返回值:123&#123; &quot;tokens&quot;: []&#125; 我, 这个停用词已经生效了,直接被干掉了. 修改后的项目修改后的代码也传到我的github上去了,可以直接clone下来切换分支使用1git clone https://github.com/zhouze-java/elasticsearch-analysis-ik.git 切换分支1git checkout ik_zhouze 然后打包一下1mvn clean package -DskipTests 其他操作和上面一样.记得把mysql的驱动包丢进去]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-65-IK分词器配置文件详解]]></title>
    <url>%2F2019%2F01%2F05%2FElasticsearch-65-IK%E5%88%86%E8%AF%8D%E5%99%A8%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[ik配置文件ik配置文件地址: es目录/plugins/ik/config main.dic:ik原生内置的中文词库,总共有27万多条,只要是这些单词,都会被分在一起 quantifier.dic:放了一些单位相关的词 suffix.dic:放了一些后缀 surname.dic:中国的姓氏 stopword.dic:英文停用词 IKAnalyzer.cfg.xml:用来配置自定义词库地址 ik原生中最要的两个配置文件就是main.dic和stopword.dic 停用词一般就是像 a the at 等等这些单词,停用词在分词的时候会直接被干掉,不会建立倒排索引 自定义词库每年都会有一些流行语,比如像蓝瘦香菇,网红等等这些词在ik原生的词典里面一般是没有的我们可以添加到custom目录下面的mydict.dic添加完成后,重启es,我们自己添加的这些词语就会生效 也可以自己建立停用词库,比如 了,的,啥,么,我们可能并不想让这些词去建立索引,就可以补充在custom/ext_stopword.dic中,然后重启es就可以生效了 或者说这些自定义词库,停用词,都可以自己创建一个dic文件,然后在IKAnalyzer.cfg.xml配置好文件的路径就可以了]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-64-IK中文分词器的安装和基本使用]]></title>
    <url>%2F2019%2F01%2F04%2FElasticsearch-64-IK%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%99%A8%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[ik中文分词器安装从github上拉取ik分词器1git clone https://github.com/medcl/elasticsearch-analysis-ik 切换分支1git checkout tags/v5.2.0 编译1mvn package 将target/releases/elasticsearch-analysis-ik-5.2.0.zip拷贝到es/plugins/ik目录下 在es/plugins/ik下对elasticsearch-analysis-ik-5.2.0.zip进行解压缩,然后删除压缩包 最后重启es 或者说直接这里下载压缩包即可 基础知识ik分词器中,包含了两种analyzer,可以根据自己的需要自己选,一般用ik_max_word ik_max_word会将文本做最细粒度的拆分,比如说会将”中华人民共和国国歌”拆分为”中华人民共和国,中华人民,中华,华人,人民共和国,人民,人,民,共和国,共和,和,国国,国歌”,会穷尽各种可能的组合 ik_smart会做最粗粒度的拆分,比如将”中华人民共和国国歌”拆分为”中华人民共和国,国歌” 基本的使用创建索引的时候指定使用ik分词器12345678910111213PUT /my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;text&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; &#125; &#125; &#125; &#125;&#125; 创建完成后添加几条测试数据12345678910111213POST /my_index/my_type/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;1&quot;&#125; &#125;&#123; &quot;text&quot;: &quot;男子偷上万元发红包求交女友 被抓获时仍然单身&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;2&quot;&#125; &#125;&#123; &quot;text&quot;: &quot;16岁少女为结婚“变”22岁 7年后想离婚被法院拒绝&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;3&quot;&#125; &#125;&#123; &quot;text&quot;: &quot;深圳女孩骑车逆行撞奔驰 遭索赔被吓哭(图)&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;4&quot;&#125; &#125;&#123; &quot;text&quot;: &quot;女人对护肤品比对男票好？网友神怼&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;5&quot;&#125; &#125;&#123; &quot;text&quot;: &quot;为什么国内的街道招牌用的都是红黄配？&quot; &#125;``` 来看下这个分词器的效果 GET my_index/_analyze{ “text”: “男子偷上万元发红包求交女友 被抓获时仍然单身”, “analyzer”: “ik_max_word”}1返回值: { “tokens”: [ { “token”: “男子”, “start_offset”: 0, “end_offset”: 2, “type”: “CN_WORD”, “position”: 0 }, { “token”: “偷上”, “start_offset”: 2, “end_offset”: 4, “type”: “CN_WORD”, “position”: 1 }, { “token”: “上万”, “start_offset”: 3, “end_offset”: 5, “type”: “CN_WORD”, “position”: 2 }, { “token”: “万元”, “start_offset”: 4, “end_offset”: 6, “type”: “CN_WORD”, “position”: 3 }, { “token”: “万”, “start_offset”: 4, “end_offset”: 5, “type”: “CN_WORD”, “position”: 4 }, { “token”: “元”, “start_offset”: 5, “end_offset”: 6, “type”: “CN_CHAR”, “position”: 5 }, { “token”: “发红包”, “start_offset”: 6, “end_offset”: 9, “type”: “CN_WORD”, “position”: 6 }, { “token”: “发红”, “start_offset”: 6, “end_offset”: 8, “type”: “CN_WORD”, “position”: 7 }, { “token”: “发”, “start_offset”: 6, “end_offset”: 7, “type”: “CN_WORD”, “position”: 8 }, { “token”: “红包”, “start_offset”: 7, “end_offset”: 9, “type”: “CN_WORD”, “position”: 9 }, { “token”: “求”, “start_offset”: 9, “end_offset”: 10, “type”: “CN_CHAR”, “position”: 10 }, { “token”: “交”, “start_offset”: 10, “end_offset”: 11, “type”: “CN_CHAR”, “position”: 11 }, { “token”: “女友”, “start_offset”: 11, “end_offset”: 13, “type”: “CN_WORD”, “position”: 12 }, { “token”: “抓获”, “start_offset”: 15, “end_offset”: 17, “type”: “CN_WORD”, “position”: 13 }, { “token”: “获”, “start_offset”: 16, “end_offset”: 17, “type”: “CN_WORD”, “position”: 14 }, { “token”: “时”, “start_offset”: 17, “end_offset”: 18, “type”: “CN_CHAR”, “position”: 15 }, { “token”: “仍然”, “start_offset”: 18, “end_offset”: 20, “type”: “CN_WORD”, “position”: 16 }, { “token”: “单身”, “start_offset”: 20, “end_offset”: 22, “type”: “CN_WORD”, “position”: 17 } ]}123他会把所有可能的词语都拆分出来 然后我们来测试搜索一下 GET /my_index/my_type/_search{ “query”: { “match”: { “text”: “16岁少女结婚好还是单身好？” } }}1返回: { “took”: 11, “timed_out”: false, “_shards”: { “total”: 5, “successful”: 5, “failed”: 0 }, “hits”: { “total”: 3, “max_score”: 3.603062, “hits”: [ { “_index”: “my_index”, “_type”: “my_type”, “_id”: “2”, “_score”: 3.603062, “_source”: { “text”: “16岁少女为结婚“变”22岁 7年后想离婚被法院拒绝” } }, { “_index”: “my_index”, “_type”: “my_type”, “_id”: “4”, “_score”: 1.3862944, “_source”: { “text”: “女人对护肤品比对男票好？网友神怼” } }, { “_index”: “my_index”, “_type”: “my_type”, “_id”: “1”, “_score”: 0.2699054, “_source”: { “text”: “男子偷上万元发红包求交女友 被抓获时仍然单身” } } ] }}`]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-63-误拼写时的fuzzy模糊搜索]]></title>
    <url>%2F2019%2F01%2F04%2FElasticsearch-63-%E8%AF%AF%E6%8B%BC%E5%86%99%E6%97%B6%E7%9A%84fuzzy%E6%A8%A1%E7%B3%8A%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[fuzzy搜索我们在搜索的时候,可能会出现单词误拼写的情况,举个例子,有两个documentdocument1: hello worlddocument2: hello java这时候搜索请求误写成了hallo world, 我们期望的结果是全查询出来,但是hallo world是匹配不上doc2的,那么用fuzzy技术,可以将拼写错误的搜索文本进行纠正,纠正以后去尝试匹配索引中的数据. 准备数据删除之前用的my_index,然后执行以下添加1234567POST /my_index/my_type/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 1 &#125;&#125;&#123; &quot;text&quot;: &quot;Surprise me!&quot;&#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 2 &#125;&#125;&#123; &quot;text&quot;: &quot;That was surprising.&quot;&#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 3 &#125;&#125;&#123; &quot;text&quot;: &quot;I wasn&apos;t surprised.&quot;&#125; 示例如果我们的搜索不用fuzzy,就用之前的match直接搜索surprize, 这样呢是搜索不出来的,然后使用fuzzy搜索1234567891011GET my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;fuzzy&quot;: &#123; &quot;text&quot;: &#123; &quot;value&quot;: &quot;surprize&quot;, &quot;fuzziness&quot;: 2 &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233&#123; &quot;took&quot;: 50, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 0.22585157, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.22585157, &quot;_source&quot;: &#123; &quot;text&quot;: &quot;Surprise me!&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.1898702, &quot;_source&quot;: &#123; &quot;text&quot;: &quot;I wasn&apos;t surprised.&quot; &#125; &#125; ] &#125;&#125; fuzziness,这个参数的意思是,你的搜索文本最多可以纠正几个字母去跟你的数据匹配,不设置的话,默认就是2 上面的结果中返回了两条数据第一条中,只要将surprize中的z换成s就匹配到了,只要纠正一个字母就可以了,在我们设置的fuzziness范围内的第二条中,需要将surprize中的z换成s,然后末尾加个d,纠正了两次,也在fuzziness范围内的 没有查询出来的内容是surprising,这个需要把z变成s,去掉e,再加上ing,需要5次才可以匹配到,所以没返回, 但是将fuzziness设置成5 之后,依然没用,是因为es中有最大纠正次数的限制 改进上面这种搜索是不常用的,常用的会直接在match中设置一个 fuzziness属性,值为AUTO就可以了,如下123456789101112GET my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;text&quot;: &#123; &quot;query&quot;: &quot;SURPIZE ME&quot;, &quot;operator&quot;: &quot;and&quot;, &quot;fuzziness&quot;: &quot;AUTO&quot; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324&#123; &quot;took&quot;: 15, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.44248468, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.44248468, &quot;_source&quot;: &#123; &quot;text&quot;: &quot;Surprise me!&quot; &#125; &#125; ] &#125;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-62-用function_score自定义相关度分数算法]]></title>
    <url>%2F2019%2F01%2F04%2FElasticsearch-62-%E7%94%A8function-score%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9B%B8%E5%85%B3%E5%BA%A6%E5%88%86%E6%95%B0%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[function_score我们可以自定义一个function_score函数,自己将某个field的值,跟es内置算出来的分数进行运算,然后由自己制定的field来进行分数的增强 准备测试数据给所有帖子增加follower数量1234567891011POST /forum/article/_bulk&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;1&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;follower_num&quot; : 5&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;2&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;follower_num&quot; : 10&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;3&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;follower_num&quot; : 25&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;4&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;follower_num&quot; : 3&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;5&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;follower_num&quot; : 60&#125; &#125; 案例将对帖子搜索得到的分数,和follower_num进行运算,由follower_num在一定程度上增强帖子的分数 比如我们有一个这样的搜索请求123456789GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;java spark&quot;, &quot;fields&quot;: [&quot;content&quot;,&quot;title&quot;] &#125; &#125;&#125; 然后现在用function_score来对分数进行增强1234567891011121314151617181920GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;function_score&quot;: &#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;java spark&quot;, &quot;fields&quot;: [&quot;content&quot;,&quot;title&quot;] &#125; &#125;, &quot;field_value_factor&quot;: &#123; &quot;field&quot;: &quot;follower_num&quot;, &quot;modifier&quot;: &quot;log1p&quot;, &quot;factor&quot; : 0.5 &#125;, &quot;boost_mode&quot;: &quot;sum&quot;, &quot;max_boost&quot;: 2 &#125; &#125;&#125; 先看一下这段搜索请求,是在之前的query的下一层包了一层function_score. 参数详解field_value_factorfield_value_factor中,如果只有field,那么会将每个doc的分数都乘以follower_num,如果有的doc的follower_num是0,那么分数也会变为0,效果不好,因此一般还会设置一个modifier属性,加一个log1p函数,加了这个函数以后,公式会变为: new_score = old_score log(1 + number_of_votes),这样算出来的分数是比较合理的后面还有个factor参数,可以进一步影响分数,公式会变为new_score = old_score log(1 + factor * number_of_votes) boost_mode可以决定分数与指定字段是如何计算的,默认的是乘法(multiply), 也可以指定 sum,min,max,replace max_boost限制计算出来的分数不要超过max_boost指定的值,但是在新版的es中作用不大]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-61-常见相关度分数优化方法]]></title>
    <url>%2F2019%2F01%2F03%2FElasticsearch-61-%E5%B8%B8%E8%A7%81%E7%9B%B8%E5%85%B3%E5%BA%A6%E5%88%86%E6%95%B0%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[上文中,说了整个es的相关度评分的算法,本文将使用四种常见的方法来优化相关度分数 query-time boost就是之前说过的给某一个查询增加权重,语法如下12345678910111213141516171819202122GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;java spark&quot;, &quot;boost&quot;:2 &#125; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java spark&quot; &#125; &#125; ] &#125; &#125;&#125; 给哪个match增加boost,哪个match的权重就越高,相关度评分就越高 重构查询结构比如说一个搜索是这样的1234567891011121314151617181920212223242526272829GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;spark&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;solution&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;beginner&quot; &#125; &#125; ] &#125; &#125;&#125; 这样的话,这四个match的权重是一样的,如下这样重构1234567891011121314151617181920212223242526272829303132333435GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;spark&quot; &#125; &#125;, &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;solution&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;beginner&quot; &#125; &#125; ] &#125; &#125; ] &#125; &#125;&#125; 把后面的两个查询放到了一个bool中,这样的话, 下面这个bool中的两个match的权重和上面的一个match的权重是一样的 在新版的es中,这样重构查询对分数的影响越来越小了,一般不用也可以 negative boost假如说我们要搜索包含java,不包含spark的时候, 有spark的内容是不会出来的,那么如果我们想要的结果是包含java的排在前面,包含spark的尽量排在后面,而不是直接排除掉来看一下语法123456789101112131415161718GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;boosting&quot;: &#123; &quot;positive&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java&quot; &#125; &#125;, &quot;negative&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;spark&quot; &#125; &#125;, &quot;negative_boost&quot;: 0.2 &#125; &#125;&#125; 这样的话包含了negative term的doc,分数会乘以negative_boost,使得分数降低 constant_score如果我们就不需要相关度评分的话,直接使用constant_score就可以了 所有的doc的分数都是1,就没有评分的概念了123456789101112131415161718192021222324252627GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;constant_score&quot;: &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;java&quot; &#125; &#125; &#125; &#125;, &#123; &quot;constant_score&quot;: &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;java&quot; &#125; &#125; &#125; &#125; ] &#125; &#125;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-60-深入揭秘lucene的相关度分数算法]]></title>
    <url>%2F2019%2F01%2F03%2FElasticsearch-60-%E6%B7%B1%E5%85%A5%E6%8F%AD%E7%A7%98lucene%E7%9A%84%E7%9B%B8%E5%85%B3%E5%BA%A6%E5%88%86%E6%95%B0%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[上文中说了TF/IDF算法,那么在底层的Lucene中TF/IDF算法的完整的公式是什么 Boolean model就是我们之前有说过的,一个match会被转换为bool的组合查询,比如说123&quot;match&quot;:&#123; &quot;query&quot;:&quot;hello world&quot;&#125; 会被转换为:1234567891011121314&quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;hello&quot; &#125; &#125;, &#123; &quot;natch&quot;: &#123; &quot;title&quot;: &quot;world&quot; &#125; &#125; ]&#125; 普通multivalue搜索,转换为bool搜索就是boolean model lucene practical scoring functionpractical scoring function,来计算一个query对一个doc的分数的公式,该函数会使用一个公式来计算123456789score(q,d) = queryNorm(q) · coord(q,d) · ∑ ( tf(t in d) · idf(t)2 · t.getBoost() · norm(t,d) ) (t in q) score(q,d) is the relevance score of document d for query q.这个公式的最终结果,就是说是一个query(叫做q),对一个doc(叫做d)的最终的总评分 queryNormqueryNorm(q) is the query normalization factor (new).queryNorm,是用来让一个doc的分数处于一个合理的区间内,不要太离谱,举个例子,一个doc分数是10000,一个doc分数是0.1,相差太大,不是很好 coordcoord(q,d) is the coordination factor (new).简单来说,就是对更加匹配的doc,进行一些分数上的成倍的奖励 ∑ (t in q)∑ :是求和的意思query中每个term对doc的分数,进行求和,多个term对一个doc的分数,组成一个vector space,就在这一步,要进行TF/IDF算法 tf(t in d)tf(t in d) is the term frequency for term t in document d.就是计算单个term对于doc的分数 idf(t)idf(t) is the inverse document frequency for term t.进行idf计算 norm(t,d)norm(t,d) is the field-length norm, combined with the index-time field-level boost, if any. (new). queryNorm详解queryNorm = 1 / √sumOfSquaredWeightssumOfSquaredWeights = 所有term的IDF分数之和,开一个平方根,然后做一个平方根分之1主要是为了将分数进行规范化, 开平方根,首先数据就变小了, 然后还用1去除以这个平方根,分数就会很小,比如 1.几 或者零点几分数就不会出现几万,几十万,那样的离谱的分数 coord详解奖励那些匹配更多字符的doc更多的分数举个例子:document1 包含 hello → score: 1.5document2 包含 hello world → score: 3.0document3 包含 hello world java → score: 4.5 把计算出来的总分数 * 匹配上的term数量 / 总的term数量,让匹配不同term/query数量的doc,分数之间拉开差距 document1 包含 hello → score: 1.5 1 / 3 = 0.5document2 包含 hello world → score: 3.0 2 / 3 = 2.0document3 包含 hello world java → score: 4.5 * 3 / 3 = 4.5 field level boost就是之前说过的搜索的权重计算]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-59-深入揭秘TF&IDF算法以及向量空间模型算法]]></title>
    <url>%2F2019%2F01%2F03%2FElasticsearch-59-%E6%B7%B1%E5%85%A5%E6%8F%AD%E7%A7%98TF-IDF%E7%AE%97%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[一个搜索请求过来,是怎样进行打分的呢 boolean model首先一个搜索请求过来的时候,会先进行过滤,过滤出包含指定term的doc,这个过程就是boolean model 举个例子,查询的是hello world,首先会先过滤出来包含hello和包含world和包含hello world的数据,过滤出来的这些document是不会去打分数的.为了减少后续计算的document的数量,提升性能 TF/IDF算法第二步进行TF/IDF计算,之前有详细说过这部分的计算,但是这两个是计算的单个term在doc里面的分数. 比如有两个documentdoc1:java is my favourite programming language, hello world !!!doc2:hello java, you are very good, oh hello world!!!还是搜索hello world 先计算hello对于doc1的分数TF(term frequency)算法:找到hello在doc1中出现了几次,会根据出现的次数给个分数,一个term在一个doc中出现的次数越多,给的相关度评分就越高 IDF(inversed document frequency)算法:找到hello在所有的doc中出现的次数,一个term在所有的doc中,出现的次数越多,评分越低. 然后是length norm,hello搜索的那个field的长度,filed长度越长,给的相关度评分越低,field长度越短,给的相关度评分越高. 最后,会将hello这个term对doc1的分数综合TF IDF length norm,计算出来一个综合的分数 计算world的方法同样的 上面我们可以看出计算的只是单个term对于doc的分数,但是最后需要给这个query对于doc的总的分数,就是第三步vector space model vector space model计算多个term对于一个doc的总分数比如说上面的hello world,es会根据hello world在所有的doc中的评分情况,计算出一个总的 query vector(query向量) 比如说,hello这个term,给的基于所有doc的一个评分就是2,world这个term,给的基于所有doc的一个评分就是5,那么这个向量就是[2,5] 再举个例子,现在有3个doc,分别是doc1:包含hellodoc2:包含worlddoc3:包含hello,world查询的还是hello world,会拿每个term计算出一个分数来,hello有一个分数,world有一个分数,再拿所有的term的分数组成一个doc vector那么doc1就是[2,0],doc2是[0,5],doc3是[2,5] 把这几个分数画在一个坐标中就是:然后会取每个doc vector对于query vector的弧度,给出每个doc对于多个term的总分数,弧度越大,分数越低,弧度越小,分数越高 如果term不止两个,是多个的话,就无法用图表示了,就是线性代数来计算]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-58-通过ngram分词机制实现index-time搜索联想]]></title>
    <url>%2F2019%2F01%2F03%2FElasticsearch-58-%E9%80%9A%E8%BF%87ngram%E5%88%86%E8%AF%8D%E6%9C%BA%E5%88%B6%E5%AE%9E%E7%8E%B0index-time%E6%90%9C%E7%B4%A2%E8%81%94%E6%83%B3%2F</url>
    <content type="text"><![CDATA[什么是ngram举个例子,现在有一个quick这个单词,在ngram的长度是1的时候,quick这个单词会被拆分为ngram length = 1的情况下被拆分为q u i c kngram length = 2的情况下被拆分为qu ui ic ckngram length = 3的情况下被拆分为qui uic ickngram length = 4的情况下被拆分为quic uickngram length = 5的情况下被拆分为quick 如上,被拆分出来的每一个词就是一个ngram. edge ngram本文将使用edge ngram,实现搜索联想功能那什么是edge ngram? 举例,还是quick这个单词,使用edge ngram的话,会被拆分为qququiquicquick 举例说明假设有一个document的值是hello world ,然后劲歌edge ngram拆分hhehelhellhello wwoworworlworld 然后我们去搜索 hello w的时候,会用hello 和 w分别去匹配然后返回. 这歌搜索联想跟我们之前的搜索联想不同,这里搜索的时候,不用再根据一个前缀去扫描整个倒排索引了,而是拿前缀去倒排索引里面去匹配即可,类似于match这种全文检索 实战案例删除之前的my_index,然后重新创建索引,需要设置一下分词器123456789101112131415161718192021222324PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;filter&quot;: &#123; &quot;autocomplete_filter&quot;:&#123; &quot;type&quot;:&quot;edge_ngram&quot;, &quot;min_gram&quot;:1, &quot;max_gram&quot;:20 &#125; &#125;, &quot;analyzer&quot;: &#123; &quot;autocomplete&quot;:&#123; &quot;type&quot;:&quot;custom&quot;, &quot;tokenizer&quot;: &quot;standard&quot;, &quot;filter&quot;: [ &quot;lowercase&quot;, &quot;autocomplete_filter&quot; ] &#125; &#125; &#125; &#125;&#125; 创建完成之后,测试一下这个分词器12345GET /my_index/_analyze&#123; &quot;analyzer&quot;: &quot;autocomplete&quot;, &quot;text&quot;: &quot;quick brown&quot;&#125; 返回值:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;q&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 5, &quot;type&quot;: &quot;word&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;qu&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 5, &quot;type&quot;: &quot;word&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;qui&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 5, &quot;type&quot;: &quot;word&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;quic&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 5, &quot;type&quot;: &quot;word&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;quick&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 5, &quot;type&quot;: &quot;word&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;b&quot;, &quot;start_offset&quot;: 6, &quot;end_offset&quot;: 11, &quot;type&quot;: &quot;word&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;br&quot;, &quot;start_offset&quot;: 6, &quot;end_offset&quot;: 11, &quot;type&quot;: &quot;word&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;bro&quot;, &quot;start_offset&quot;: 6, &quot;end_offset&quot;: 11, &quot;type&quot;: &quot;word&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;brow&quot;, &quot;start_offset&quot;: 6, &quot;end_offset&quot;: 11, &quot;type&quot;: &quot;word&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;brown&quot;, &quot;start_offset&quot;: 6, &quot;end_offset&quot;: 11, &quot;type&quot;: &quot;word&quot;, &quot;position&quot;: 1 &#125; ]&#125; 分词器没问题以后,手动设定mapping映射12345678910PUT /my_index/_mapping/my_type&#123; &quot;properties&quot;: &#123; &quot;title&quot;:&#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;autocomplete&quot;, &quot;search_analyzer&quot;: &quot;standard&quot; &#125; &#125;&#125; 然后往里加几条测试数据1234567891011121314151617181920212223242526272829POST /my_index/my_type/1&#123; &quot;title&quot;:&quot;hello world&quot;&#125;POST /my_index/my_type/2&#123; &quot;title&quot;:&quot;hello we&quot;&#125;POST /my_index/my_type/3&#123; &quot;title&quot;:&quot;hello win&quot;&#125;POST /my_index/my_type/4&#123; &quot;title&quot;:&quot;hello wind&quot;&#125;POST /my_index/my_type/5&#123; &quot;title&quot;:&quot;hello dog&quot;&#125;POST /my_index/my_type/6&#123; &quot;title&quot;:&quot;hello cat&quot;&#125; 最后来搜索测试一下,搜索hello w12345678GET /my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;title&quot;: &quot;hello w&quot; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&#123; &quot;took&quot;: 29, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 4, &quot;max_score&quot;: 0.8361317, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.8361317, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;hello we&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.8361317, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;hello wind&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.8271048, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;hello world&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.797104, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;hello win&quot; &#125; &#125; ] &#125;&#125; 这里如果使用的是match的话,只有hello的也会查询出来,全文检索,分数比较低. 推荐使用match_phrase,要求每个term都有,而且position刚好靠着1位,符合我们的期望]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-57-搜索联想]]></title>
    <url>%2F2019%2F01%2F02%2FElasticsearch-57-%E6%90%9C%E7%B4%A2%E8%81%94%E6%83%B3%2F</url>
    <content type="text"><![CDATA[准备工作删除之前的my_index1DELETE /my_index 然后再添加几条测试用的数据1234567891011121314151617181920212223242526272829POST /my_index/my_type/1&#123; &quot;title&quot;:&quot;hello world&quot;&#125;POST /my_index/my_type/2&#123; &quot;title&quot;:&quot;hello we&quot;&#125;POST /my_index/my_type/3&#123; &quot;title&quot;:&quot;hello win&quot;&#125;POST /my_index/my_type/4&#123; &quot;title&quot;:&quot;hello wind&quot;&#125;POST /my_index/my_type/5&#123; &quot;title&quot;:&quot;hello dog&quot;&#125;POST /my_index/my_type/6&#123; &quot;title&quot;:&quot;hello cat&quot;&#125; 搜索联想比如说我们在Google的搜索框中输入了elasti,搜索框下面可能会出来elasticsearch, elasticsearch权威指南等这些信息. 我们可以用match_phrase_prefix来实现这个效果,原理和match_phrase类似,唯一的区别就是会把最后一个term作为前缀去搜索. 比如我们搜索hello w, hello就是去进行match,搜索对应的document,最后一个term是w,w就会被作为前缀,去扫描整个倒排索引,找到所有的w开头的document,然后找到既包含hello又包含w开头的document,根据你的slop去计算,看在slop范围内,能不能让hello w,正好跟document中的hello和w开头的单词的position相匹配 指定slop的话,也是只会将最后一个term作为前缀 max_expansions默认情况下,前缀搜索要扫描所有倒排索引中的document,按时这样性能太差了,可以使用max_expansions指定prefix最多匹配多少个document,超过这个数量就不继续匹配了,限定性能 语法123456789101112GET /my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase_prefix&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;hello w&quot;, &quot;slop&quot;:10, &quot;max_expansions&quot;: 10 &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 4, &quot;max_score&quot;: 1.8798604, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1.8798604, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;hello we&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 1.8798604, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;hello wind&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.51623213, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;hello world&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.51623213, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;hello win&quot; &#125; &#125; ] &#125;&#125; 查询出来的都是包含hello和w是前缀的数据 在实际中,这种搜索也是尽量不要用,因为最后一个前缀始终要去扫描大量的索引,性能可能会很差]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-56-前缀搜索 通配符搜索 正则搜索]]></title>
    <url>%2F2019%2F01%2F02%2FElasticsearch-56-%E5%89%8D%E7%BC%80%E6%90%9C%E7%B4%A2-%E9%80%9A%E9%85%8D%E7%AC%A6%E6%90%9C%E7%B4%A2-%E6%AD%A3%E5%88%99%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[准备工作首先我们先手动建立一个index,再添加几条数据.创建index123456789101112PUT /my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;:&#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125;&#125; 添加几条测试数据1234567891011121314POST /my_index/my_type/1&#123; &quot;title&quot;:&quot;C3D0-KD345&quot;&#125;POST /my_index/my_type/2&#123; &quot;title&quot;:&quot;C3K5-DFG65&quot;&#125;POST /my_index/my_type/3&#123; &quot;title&quot;:&quot;C4I8-UI365&quot;&#125; 前缀搜索C3D0-KD345C3K5-DFG65C4I8-UI365上面我们添加了title是这几个的数据,然后我现在要搜索以C3开头的数据,那么就是要搜索id是1和2的这两条数据,然后看下语法12345678910GET /my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;prefix&quot;: &#123; &quot;title&quot;: &#123; // 要搜索的filed &quot;value&quot;: &quot;C3&quot; // 前缀的值 &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233&#123; &quot;took&quot;: 10, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;C3K5-DFG65&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;C3D0-KD345&quot; &#125; &#125; ] &#125;&#125; 返回结果就是id是1和2的两个document 前缀搜索的原理看下上面的返回值两个结果的_score都是1,prefix query是不计算relevance score的,与prefix filter唯一的区别就是,filter会cache bitset前缀搜索会去扫描整个倒排索引,前缀越短,要处理的doc就越多,性能越差,应该尽可能的用长前缀搜索 举个例子,现在有3个document,内容分别是:C3-D0-KD345C3-K5-DFG65C4-I8-UI365注意,和上面添加的数据是不一样的. 如果我们使用match全文检索的话,每个字符串是都需要被分词.结果是c3d0kd345k5dfg65c4i8ui365被分成了这几个词,这时候我们去查询c3, 去扫描倒排索引,一旦扫描到c3,就可以停了,已经拿到了所有包含c3的document list了,所以说match的性能往往是很高的 如果不分词,去使用前缀搜索呢,还是查询C3,先找到了C3-D0-KD345,然后还是要继续往下搜,因为后面还可能有其他的前缀是c3的字符串,扫描到了一个前缀匹配的term,不能停,必须继续搜索,直到扫描完整个的倒排索引,才能结束.所以,前缀搜索的性能是很差的 那为什么不使用match搜索,而用前缀搜索呢,因为实际的场景中,可能有些场景是全文检索解决不了的.再举一个例子,比如说,有以下3个document,值分别是:C3D0-KD345C3K5-DFG65C4I8-UI365这时候,分词的结果可能就是c3d0kd345… 这种情况下,用c3 match扫描整个倒排索引,是找不到的. 只能用prefix 前缀搜索 通配符搜索? 代表任意一个字符* 代表0个或任意多个字符 比如,表达式是C?K*5,就是以C开头后面可以有任意一个字符,然后接着是K,最后以5结尾看下语法:12345678910GET /my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;wildcard&quot;: &#123; &quot;title&quot;: &#123; &quot;value&quot;: &quot;C?K*5&quot; &#125; &#125; &#125;&#125; 跟前缀搜索类似,功能更强大,但是性能一样很差,也是需要扫描整个倒排索引 正则搜索[0-9] 代表指定范围内的数字[a-z] 代表指定范围内的字母. 代表一个字符+ 代表前面的正则表达式可以出现一次或多次 比如正则表达式是 C[0-9].+,就是以C开头后面有一个0-9以内的数字,然后后面可以有多个字符 语法12345678GET /my_index/my_type/_search&#123; &quot;query&quot;: &#123; &quot;regexp&quot;:&#123; &quot;title&quot;:&quot;C[0-9].+&quot; &#125; &#125;&#125; wildcard和regexp,与prefix原理一致,都会扫描整个索引,性能很差 在实际的应用中,这几种搜索能不用就尽量不用,因为性能很差]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-55-使用rescoring机制优化近似匹配的性能]]></title>
    <url>%2F2019%2F01%2F02%2FElasticsearch-55-%E4%BD%BF%E7%94%A8rescoring%E6%9C%BA%E5%88%B6%E4%BC%98%E5%8C%96%E8%BF%91%E4%BC%BC%E5%8C%B9%E9%85%8D%E7%9A%84%E6%80%A7%E8%83%BD%2F</url>
    <content type="text"><![CDATA[match 和 phrase match(proximity match)的区别match:只要简单的匹配到了一个term,就可以将term对应的doc返回. phrase match:首先扫描到所有term的document list,然后对每个document都计算term position,是否符合指定的范围,slop需要进行复杂的运算,来判断是否能通过slop移动,匹配一个document match query的性能比phrase match和proximity match要高很多,因为后两者都需要计算position的距离,match query比phrase match的性能要高10倍, 比proximity match的性能要高20倍.但是别太担心,因为es的性能一般都在毫秒级别,match query一般就在几毫秒,或者几十毫秒,而phrase match和proximity match 的性能在几十毫秒到几百毫秒之间,也是可以接受的 优化proximity match的性能主要思路就是:用match匹配先过滤出需要的数据,然后在用proximity match来根据距离提高doc的分数,同时proximity match只针对每个shard的分数排名的前n个document起作用,来重新调整他们的分数,这个过程称之为rescoring,重计分.因为一般的用户都会分页查询,只会看到前几页的数据,所以不需要对所有的结果都进行proximity match操作 举例比如说有一个查询,match出来了1000个document,默认的情况下,proximity match需要对全部的document都进行一次运算,判断slop移动是否能够匹配上,然后去贡献自己的分数,但是很多情况下,match出来的这1000个doc,用户是不会全部都看到的,可能最多只会看前五页,一页10条的话,我们只需要第前50个doc去进行slop移动匹配,去贡献自己的分数即可,不需要对全部的1000个doc都去进行计算和贡献分数. rescore重打分及其语法就上面的例子来说,match的1000个doc其实已经有了一个分数了,proximity match前50个doc进行rescore重打分即可,让前50个doc,term距离越近的,排在越前面 语法123456789101112131415161718192021GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java spark&quot; &#125; &#125;, &quot;rescore&quot;:&#123; &quot;window_size&quot;:50, &quot;query&quot;:&#123; &quot;rescore_query&quot;:&#123; &quot;match_phrase&quot;:&#123; &quot;content&quot;:&#123; &quot;query&quot;:&quot;java spark&quot;, &quot;slop&quot;:50 &#125; &#125; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-54-搜索实现召回率和精准度平衡]]></title>
    <url>%2F2018%2F12%2F27%2FElasticsearch-54-%E6%90%9C%E7%B4%A2%E5%AE%9E%E7%8E%B0%E5%8F%AC%E5%9B%9E%E7%8E%87%E5%92%8C%E7%B2%BE%E5%87%86%E5%BA%A6%E5%B9%B3%E8%A1%A1%2F</url>
    <content type="text"><![CDATA[首先需要了解两个概念,召回率和精准度 召回率比如搜索一个java spark,总共有100个document,能返回多少个document作为结果,这个就是召回率(recall) 精准度比如搜索一个java spark,能不能尽可能让包含java spark这个短语的,或者是java和spark离的很近的document,排在最前面,这个就是精准度(precision) 混合使用match和近似匹配直接使用match phrase短语搜索,会导致所有term都在document的filed中出现,而且距离要在slop规定的范围内,才能匹配的到 近似匹配的时候,召回率比较低,因为精准度太高了. 但是有时候我们可能希望是匹配到几个term中的部分,就可以作为返回结果返回回来,这样可以提高召回率,同时我们也希望用上match_phrase根据距离提升分数的功能,让几个term距离越近分数就越高,优先返回,也就是优先满足召回率 比如说,优先提升召回率就是:搜索java spark,包含java的也返回,包含spark的也返回,包含java和spark的也返回.同时兼顾精准度:就是说包含java和spark,同时java和spark离得越近的document排在前面 此时,我们可以使用bool组合match query和match_phrase query一起,来实现上述效果 实战案例构建一个搜索请求123456789101112131415161718192021222324GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java spark&quot; &#125; &#125; ], &quot;should&quot;: [ &#123; &quot;match_phrase&quot;: &#123; &quot;content&quot;: &#123; &quot;query&quot;: &quot;java spark&quot;, &quot;slop&quot;:50 &#125; &#125; &#125; ] &#125; &#125;&#125; 看一下上面这个请求,must里面可能返回的是包含java或spark或java spark,同时包含java spark的靠前,但是没法区分距离,也许距离很近但是排在了后面should里面呢,在slop以内,如果java spark能匹配上一个doc,那么就会对doc贡献自己的relevance score,如果java和spark靠的越近,那么分数就越高 先来试一下不加近似匹配的搜索1234567891011121314GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java spark&quot; &#125; &#125; ] &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263&#123; &quot;took&quot;: 31, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 0.68640786, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.68640786, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 50, &quot;title&quot;: &quot;this is java blog&quot;, &quot;content&quot;: &quot;i think java is the best programming language&quot;, &quot;sub_title&quot;: &quot;learned a lot of course&quot;, &quot;author_first_name&quot;: &quot;Smith&quot;, &quot;author_last_name&quot;: &quot;Williams&quot;, &quot;new_author_last_name&quot;: &quot;Williams&quot;, &quot;new_author_first_name&quot;: &quot;Smith&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;5&quot;, &quot;_score&quot;: 0.68324494, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot;: 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2018-12-03&quot;, &quot;tag&quot;: [ &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10, &quot;title&quot;: &quot;this is spark blog&quot;, &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java spark&quot;, &quot;sub_title&quot;: &quot;haha, hello world&quot;, &quot;author_first_name&quot;: &quot;Tonny&quot;, &quot;author_last_name&quot;: &quot;Peter Smith&quot;, &quot;new_author_last_name&quot;: &quot;Peter Smith&quot;, &quot;new_author_first_name&quot;: &quot;Tonny&quot; &#125; &#125; ] &#125;&#125; id是2的doc排在了id是5的前面.然后再加上近似匹配123456789101112131415161718192021222324GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java spark&quot; &#125; &#125; ], &quot;should&quot;: [ &#123; &quot;match_phrase&quot;: &#123; &quot;content&quot;: &#123; &quot;query&quot;: &quot;java spark&quot;, &quot;slop&quot;:50 &#125; &#125; &#125; ] &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1.258609, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;5&quot;, &quot;_score&quot;: 1.258609, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot;: 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2018-12-03&quot;, &quot;tag&quot;: [ &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10, &quot;title&quot;: &quot;this is spark blog&quot;, &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java spark&quot;, &quot;sub_title&quot;: &quot;haha, hello world&quot;, &quot;author_first_name&quot;: &quot;Tonny&quot;, &quot;author_last_name&quot;: &quot;Peter Smith&quot;, &quot;new_author_last_name&quot;: &quot;Peter Smith&quot;, &quot;new_author_first_name&quot;: &quot;Tonny&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.68640786, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 50, &quot;title&quot;: &quot;this is java blog&quot;, &quot;content&quot;: &quot;i think java is the best programming language&quot;, &quot;sub_title&quot;: &quot;learned a lot of course&quot;, &quot;author_first_name&quot;: &quot;Smith&quot;, &quot;author_last_name&quot;: &quot;Williams&quot;, &quot;new_author_last_name&quot;: &quot;Williams&quot;, &quot;new_author_first_name&quot;: &quot;Smith&quot; &#125; &#125; ] &#125;&#125; 可以看到id是5的doc排在了id是2的doc的前面]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-53-基于slop参数实现近似匹配]]></title>
    <url>%2F2018%2F12%2F27%2FElasticsearch-53-%E5%9F%BA%E4%BA%8Eslop%E5%8F%82%E6%95%B0%E5%AE%9E%E7%8E%B0%E8%BF%91%E4%BC%BC%E5%8C%B9%E9%85%8D%2F</url>
    <content type="text"><![CDATA[slop参数比如我们现在有一个搜索请求如下:1234567891011GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;java spark&quot;, &quot;slop&quot;:1 &#125; &#125; &#125;&#125; slop的作用是什么呢?query string 中的几个term,要经过几次移动才能与一个document匹配,移动的次数就是slop 举例说明现有一个document content的值是hello world, java is very good, spark is also very good.我们如果用之前说的match_phrase搜索java spark的话是搜索不到的 但是如果我们指定了slop,那么就允许java spark进行移动,来尝试与document进行匹配,比如就上面这个句子中要去匹配java spark如图,spark向后进行了三次移动后,就能匹配到了这个document了.slop的含义,不仅仅是说一个query string terms移动几次跟一个doc匹配上,而是说一个query string terms 最多可以移动几次去尝试跟一个doc匹配上就上面这个例子而言slop的值只要大于等于3 就可以匹配的到,如果设置的是2,是匹配不到的 再来看一个例子:1234567891011GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;content&quot;: &#123; &quot;query&quot;: &quot;spark data&quot;, &quot;slop&quot;: 3 &#125; &#125; &#125;&#125; 执行搜索,返回的这个document的content值是:spark is best big data solution based on scala ,an programming language similar to java spark搜索关键词是 spark data, content中spark 和 data中间有3个词, 所以也是只要移动3次就可以匹配的到,所以这个slop最小设置成3就可以匹配的到 那么如果是搜索的data spark 那要怎么移动呢1234567891011GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;content&quot;: &#123; &quot;query&quot;: &quot;data spark&quot;, &quot;slop&quot;: 5 &#125; &#125; &#125;&#125; 看下上面这个图,前两次移动是data和spark交换了位置,然后再进行3次移动后就匹配到了,所以这个请求的slop就是最小是5 slop搜索下,关键词离的越近,relevance score就会越高,再来看个案例.搜索关键词是java best1234567891011GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;content&quot;: &#123; &quot;query&quot;: &quot;java best&quot;, &quot;slop&quot;:15 &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 0.65380025, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.65380025, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 50, &quot;title&quot;: &quot;this is java blog&quot;, &quot;content&quot;: &quot;i think java is the best programming language&quot;, &quot;sub_title&quot;: &quot;learned a lot of course&quot;, &quot;author_first_name&quot;: &quot;Smith&quot;, &quot;author_last_name&quot;: &quot;Williams&quot;, &quot;new_author_last_name&quot;: &quot;Williams&quot;, &quot;new_author_first_name&quot;: &quot;Smith&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;5&quot;, &quot;_score&quot;: 0.07111243, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot;: 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2018-12-03&quot;, &quot;tag&quot;: [ &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10, &quot;title&quot;: &quot;this is spark blog&quot;, &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java spark&quot;, &quot;sub_title&quot;: &quot;haha, hello world&quot;, &quot;author_first_name&quot;: &quot;Tonny&quot;, &quot;author_last_name&quot;: &quot;Peter Smith&quot;, &quot;new_author_last_name&quot;: &quot;Peter Smith&quot;, &quot;new_author_first_name&quot;: &quot;Tonny&quot; &#125; &#125; ] &#125;&#125; 看先这两个的_score分数, 两个terms的距离越近,分数就越高 其实,加了slop的phrase match,就是proximity match,近似匹配]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-52-phrase match搜索及原理]]></title>
    <url>%2F2018%2F12%2F26%2FElasticsearch-52-phrase-match%E6%90%9C%E7%B4%A2%E5%8F%8A%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[近似匹配假设现在有两个document,他们的content的值分别是:java is my favourite programming language, and I also think spark is a very good big data system.java spark are very related, because scala is spark’s programming language and scala is also based on jvm like java. 用一个match query去搜索java spark12345&#123; &quot;match&quot;:&#123; &quot;content&quot;:&quot;java spark&quot; &#125;&#125; match query的话,只能搜索到包含java或者包含spark的document,但是不知道java和spark是不是离的很近 包含java或者包含spark的document都会被返回回来.我们其实并不知道哪个document中java和spark距离的比较近.如果我们是希望搜索java和spark,中间没有任何其他的字符,那么这时候用match匹配做全文检索肯定就不行了. 如果说我们要尽量让java和spark离的很近的document优先返回,要给他一个更高的relevance score,这就涉及到了proximity match 近似匹配. 如果现在有两个需求: java spark,要连在一起,中间没有任何字符 java spark,不需要连在一起,但是这两个单词靠的越近,doc的分数越高,排名越靠前 要实现上面两个需求,用match做全文检索是搞不定的. 必须得用proximity match,近似匹配 phrase match(短语匹配),proximity match(近似匹配)本文主要说的是 phrase match,就是仅仅搜索出java和spark靠在一起的那些doc,比如有个doc,是java use’d spark这样是不行的,必须是比如java spark are very good friends,是可以搜索出来的. phrase match: 就是将多个term作为一个短语,一起去搜索,只有包含这个短语的document才会作为返回结果. 案例先用match query全文检索搜索一下java spark12345678GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java spark&quot; &#125; &#125; &#125; 包含java和spark的都被返回来了,不是我们想要的结果,然后修改id是5的这个document的content,因为现在的数据没有符合我们要求的123456POST /forum/article/5/_update&#123; &quot;doc&quot;: &#123; &quot;content&quot;:&quot;spark is best big data solution based on scala ,an programming language similar to java spark&quot; &#125;&#125; 然后来用phrase match搜索12345678GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;content&quot;: &quot;java spark&quot; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;took&quot;: 17, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.5753642, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;5&quot;, &quot;_score&quot;: 0.5753642, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot;: 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2018-12-03&quot;, &quot;tag&quot;: [ &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10, &quot;title&quot;: &quot;this is spark blog&quot;, &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java spark&quot;, &quot;sub_title&quot;: &quot;haha, hello world&quot;, &quot;author_first_name&quot;: &quot;Tonny&quot;, &quot;author_last_name&quot;: &quot;Peter Smith&quot;, &quot;new_author_last_name&quot;: &quot;Peter Smith&quot;, &quot;new_author_first_name&quot;: &quot;Tonny&quot; &#125; &#125; ] &#125;&#125; 就是我们刚刚修改的那条数据,只有包含了java spark这个短语的document才返回了,其他的数据不会返回 原理term position比如现在有两个document的content值如下:document1: hello world, java sparkdocument2: hi, spark java 对上面的数据进行分词,然后会记录每个词在每个doc中出现的位置 word term position hello doc1(0) word doc1(1) java doc1(2) doc2(2) spark doc1(3) doc2(1) 我们可以用分词器来看一下12345GET _analyze&#123; &quot;text&quot;: &quot;hello world, java spark&quot;, &quot;analyzer&quot;: &quot;standard&quot;&#125; 返回值:1234567891011121314151617181920212223242526272829303132&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;hello&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 5, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;world&quot;, &quot;start_offset&quot;: 6, &quot;end_offset&quot;: 11, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;java&quot;, &quot;start_offset&quot;: 13, &quot;end_offset&quot;: 17, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 2 &#125;, &#123; &quot;token&quot;: &quot;spark&quot;, &quot;start_offset&quot;: 18, &quot;end_offset&quot;: 23, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 3 &#125; ]&#125; position就是每个词在句子中所在的位置 match_phrase搜索原理:用java spark在上面的两个document中进行搜索java对应的是doc1(2) doc2(2),spark对应的是doc1(3) doc2(1)要求一个doc,必须包含每个term,才能拿出来继续计算 首先看doc1:在document1中 spark的position比java的position大1,java的position是2,spark的position是3,满足条件然后看下doc2:在document2中 java position是2,spark position是1,spark position比java position小1,而不是大1,所以doc2不匹配]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-51-cross-fields搜索问题解决方案]]></title>
    <url>%2F2018%2F12%2F19%2FElasticsearch-51-cross-fields%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[解决方案一:使用copy_to用copy_to可以将多个field组合成一个field. 之前说的问题,其实就是出在了有多个field,那么我们只要把这些field合并成一个field即可,比如搜索一个人名,有first_name和last_name,将这两个field合并成一个full_name就可以解决了 示例首先,创建三个field: new_author_first_name , new_author_last_name , new_author_full_name12345678910111213141516PUT /forum/_mapping/article&#123; &quot;properties&quot;: &#123; &quot;new_author_first_name&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;copy_to&quot;: &quot;new_author_full_name&quot; &#125;, &quot;new_author_last_name&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;copy_to&quot;: &quot;new_author_full_name&quot; &#125;, &quot;new_author_full_name&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125; &#125;&#125; new_author_first_name和new_author_last_name都设置copy_to到new_author_full_name中去,用了这个copy_to语法之后,就可以将多个字段的值拷贝到一个字段中,并建立倒排索引 添加数据1234567891011POST /forum/article/_bulk&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;1&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;new_author_first_name&quot; : &quot;Peter&quot;, &quot;new_author_last_name&quot; : &quot;Smith&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;2&quot;&#125; &#125; &#123; &quot;doc&quot; : &#123;&quot;new_author_first_name&quot; : &quot;Smith&quot;, &quot;new_author_last_name&quot; : &quot;Williams&quot;&#125; &#125; &#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;3&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;new_author_first_name&quot; : &quot;Jack&quot;, &quot;new_author_last_name&quot; : &quot;Ma&quot;&#125; &#125; &#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;4&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;new_author_first_name&quot; : &quot;Robbin&quot;, &quot;new_author_last_name&quot; : &quot;Li&quot;&#125; &#125; &#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;5&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;new_author_first_name&quot; : &quot;Tonny&quot;, &quot;new_author_last_name&quot; : &quot;Peter Smith&quot;&#125; &#125; 添加完毕后,这时候可以去查询一下全部的数据,返现并没有new_author_full_name这个field,因为这个field就类似于之前有说过的 _all元数据,是不在_source中显示的 接着来查询名称是Peter Smith的数据12345678GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;new_author_full_name&quot;: &quot;Peter Smith&quot; &#125; &#125;&#125; 返回值:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 0.62191015, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.62191015, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 50, &quot;title&quot;: &quot;this is java blog&quot;, &quot;content&quot;: &quot;i think java is the best programming language&quot;, &quot;sub_title&quot;: &quot;learned a lot of course&quot;, &quot;author_first_name&quot;: &quot;Smith&quot;, &quot;author_last_name&quot;: &quot;Williams&quot;, &quot;new_author_last_name&quot;: &quot;Williams&quot;, &quot;new_author_first_name&quot;: &quot;Smith&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.51623213, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;hadoop&quot; ], &quot;tag_cnt&quot;: 2, &quot;view_cnt&quot;: 30, &quot;title&quot;: &quot;this is java and elasticsearch blog&quot;, &quot;content&quot;: &quot;i like to write best elasticsearch article&quot;, &quot;sub_title&quot;: &quot;learning more courses&quot;, &quot;author_first_name&quot;: &quot;Peter&quot;, &quot;author_last_name&quot;: &quot;Smith&quot;, &quot;new_author_last_name&quot;: &quot;Smith&quot;, &quot;new_author_first_name&quot;: &quot;Peter&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;5&quot;, &quot;_score&quot;: 0.5063205, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot;: 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2018-12-03&quot;, &quot;tag&quot;: [ &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10, &quot;title&quot;: &quot;this is spark blog&quot;, &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java&quot;, &quot;sub_title&quot;: &quot;haha, hello world&quot;, &quot;author_first_name&quot;: &quot;Tonny&quot;, &quot;author_last_name&quot;: &quot;Peter Smith&quot;, &quot;new_author_last_name&quot;: &quot;Peter Smith&quot;, &quot;new_author_first_name&quot;: &quot;Tonny&quot; &#125; &#125; ] &#125;&#125; 这里的搜索结果还是和之前一样的,因为es的算法的原因,没法实现这个场景,但是copy_to已经把前一节提到的问题解决了 之前的问题一被合并成一个field了,就不存在了,而且这里的查询可以使用minimum_should_match来去长尾,第三个问题Smith和Peter在一个field里面了,所以在所有document中出现的次数是均匀的,不会有极端的偏差 解决方案二:原生cross-fields语法1234567891011GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;Peter Smith&quot;, &quot;fields&quot;: [&quot;author_first_name&quot;,&quot;author_last_name&quot;], &quot;type&quot;: &quot;cross_fields&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125;&#125; 这种方法也可以解决上文提到的那三个问题 cross_fields是要求每个term都必须在任何一个field中出现 这就解决了第一个问题,举个例子:搜索条件还是Peter Smith,按照cross_fields来搜索的话要求Peter必须在author_first_name或author_last_name中出现要求Smith必须在author_first_name或author_last_name中出现Peter Smith可能是横跨在多个field中的,所以必须要求每个term都在某个field中出现,组合起来才能组成我们想要的标识,比如一个完整的人名 原来most-fields搜索的时候,可能像Smith Williams也可能会出现,因为most-fields要求只是任何一个field匹配了就可以,匹配的field越多,分数就越高 第二个问题是most-fields没办法去长尾的问题,用cross_fields的时候,每个term都要求出现,那长尾肯定被干掉了举个例子现在有一个搜索条件是java Hadoop spark 那么这三个term都必须在任何一个filed中出现了比如有的document中,只有一个field中包含一个java,那就被干掉了,作为长尾就没了. 第三个问题,在使用cross-fields查询的时候,es在计算IDF的时候会将每个query在每个filed中的IDF都取出来,取最小值,就不会出现极端情况下的最大值了 举个例子,还是查询Peter SmithSmith,在author_first_name这个field中,在所有document的这个field中,出现的频率很低,导致IDF分数很高;Smith在所有doc的author_last_name field中的频率算出一个IDF分数,因为一般来说last_name中的Smith频率都较高,所以IDF分数是正常的,不会太高;然后对于Smith来说,会取两个IDF分数中较小的那个分数.就不会出现IDF分过高的情况.]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-50-most_fields策略进行cross-fields搜索的弊端]]></title>
    <url>%2F2018%2F12%2F19%2FElasticsearch-50-most-fields%E7%AD%96%E7%95%A5%E8%BF%9B%E8%A1%8Ccross-fields%E6%90%9C%E7%B4%A2%E7%9A%84%E5%BC%8A%E7%AB%AF%2F</url>
    <content type="text"><![CDATA[cross-field搜索就是我们搜索一个唯一标识的时候跨越了多个field,比如一个人,标识是姓名,一个建筑的标识是地址. 姓名可以散落在多个field中,比如first_name和last_name中,地址可以散落在country,province,city中.跨多个field搜索一个标识,就是cross-fields搜索 这个情况下用most-fields搜索就比较合适了,因为best-fields是优先搜索单个field最匹配的结果,cross-fields本身就不是一个field的问题了 案例首先,准备数据1234567891011POST /forum/article/_bulk&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;1&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;author_first_name&quot; : &quot;Peter&quot;, &quot;author_last_name&quot; : &quot;Smith&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;2&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;author_first_name&quot; : &quot;Smith&quot;, &quot;author_last_name&quot; : &quot;Williams&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;3&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;author_first_name&quot; : &quot;Jack&quot;, &quot;author_last_name&quot; : &quot;Ma&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;4&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;author_first_name&quot; : &quot;Robbin&quot;, &quot;author_last_name&quot; : &quot;Li&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;5&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;author_first_name&quot; : &quot;Tonny&quot;, &quot;author_last_name&quot; : &quot;Peter Smith&quot;&#125; &#125; 然后来查询一下Peter Smith12345678910GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;Peter Smith&quot;, &quot;fields&quot;: [&quot;author_first_name&quot;,&quot;author_last_name&quot;], &quot;type&quot;: &quot;most_fields&quot; &#125; &#125;&#125; 返回值:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&#123; &quot;took&quot;: 28, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 0.6931472, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.6931472, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 50, &quot;title&quot;: &quot;this is java blog&quot;, &quot;content&quot;: &quot;i think java is the best programming language&quot;, &quot;sub_title&quot;: &quot;learned a lot of course&quot;, &quot;author_first_name&quot;: &quot;Smith&quot;, &quot;author_last_name&quot;: &quot;Williams&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.5753642, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;hadoop&quot; ], &quot;tag_cnt&quot;: 2, &quot;view_cnt&quot;: 30, &quot;title&quot;: &quot;this is java and elasticsearch blog&quot;, &quot;content&quot;: &quot;i like to write best elasticsearch article&quot;, &quot;sub_title&quot;: &quot;learning more courses&quot;, &quot;author_first_name&quot;: &quot;Peter&quot;, &quot;author_last_name&quot;: &quot;Smith&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;5&quot;, &quot;_score&quot;: 0.51623213, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot;: 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2018-12-03&quot;, &quot;tag&quot;: [ &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10, &quot;title&quot;: &quot;this is spark blog&quot;, &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java&quot;, &quot;sub_title&quot;: &quot;haha, hello world&quot;, &quot;author_first_name&quot;: &quot;Tonny&quot;, &quot;author_last_name&quot;: &quot;Peter Smith&quot; &#125; &#125; ] &#125;&#125; 来看一下返回值, id是2的document被排在了第一位,为什么?因为IDF分数高, document2的author_first_name 是Smith,在所有的doc中只出现过一次,出现的频率低再来看下document1 和 document5 这两个的author_last_name都出现了,所以导致document1的分数要比document2的分数要低 大概来说是这样的,es的算法很复杂,这些都是可能影响分数的. cross-fields问题 只是找到尽可能多的field匹配到的document,而不是某个field完全匹配的document most-fields没办法使用minimum_should_match去掉长尾数据,就是匹配特别少的结果 TF/IDF算法,比如上面搜索中的Peter Smith和Smith Williams,搜索Peter Smith的时候,由于first_name中很少有Smith的,所以在query中所有document中的频率很低,得到的分数很高,可能Smith Williams反而会排在Peter Smith的前面]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-49-实战案例-most-fields策略]]></title>
    <url>%2F2018%2F12%2F17%2FElasticsearch-49-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B-most-fields%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[对比之前我们写了best-fields策略,本文将使用most-fields来搜索,那么两者有什么区别呢?best-fields策略:主要是说,将某一个field匹配尽可能多的关键词document优先返回回来most-fields策略:主要是说将更多filed匹配到某个关键词的document优先返回回来 举例现在有两个document,如下:document1:1234&#123; &quot;title&quot;:&quot;China people&quot;, &quot;content&quot;:&quot;i am a good person&quot;&#125; document2:1234&#123; &quot;title&quot;:&quot;China person&quot;, &quot;content&quot;:&quot;i am a good people&quot;&#125; 一个搜索请求,搜索的关键字是China person,那么来看一下两种搜索策略的返回结果是怎样的 best-fields:会优先将document2返回回来,因为document2的title匹配了两个关键字most-fields:会优先将document1返回回来,因为document1匹配了两个field 实战案例先来准备数据,添加一个sub_title字段,手动创建mapping123456789101112131415POST /forum/_mapping/article&#123; &quot;properties&quot;: &#123; &quot;sub_title&quot;:&#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot;, &quot;fields&quot;: &#123; &quot;std&quot;:&#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;standard&quot; &#125; &#125; &#125; &#125;&#125; 添加数据1234567891011POST /forum/article/_bulk&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;1&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;sub_title&quot; : &quot;learning more courses&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;2&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;sub_title&quot; : &quot;learned a lot of course&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;3&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;sub_title&quot; : &quot;we have a lot of fun&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;4&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;sub_title&quot; : &quot;both of them are good&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;5&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;sub_title&quot; : &quot;haha, hello world&quot;&#125; &#125; 搜索查询sub_title中包含learning courses的document12345678GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;sub_title&quot;: &quot;learning courses&quot; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&#123; &quot;took&quot;: 4, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1.219939, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1.219939, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 50, &quot;title&quot;: &quot;this is java blog&quot;, &quot;content&quot;: &quot;i think java is the best programming language&quot;, &quot;sub_title&quot;: &quot;learned a lot of course&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.5063205, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;hadoop&quot; ], &quot;tag_cnt&quot;: 2, &quot;view_cnt&quot;: 30, &quot;title&quot;: &quot;this is java and elasticsearch blog&quot;, &quot;content&quot;: &quot;i like to write best elasticsearch article&quot;, &quot;sub_title&quot;: &quot;learning more courses&quot; &#125; &#125; ] &#125;&#125; 来看一下返回值,这里有个问题,为什么learned a lot of course 排在了 learning more courses 的前面? 在我们手动创建sub_title的mapping映射的时候,使用的是English分词器,所以会还原单词,将单词还原为其最基本的形态(stemmer),比如learning –&gt; learnlearned –&gt; learncourses –&gt; course 所以,我们的搜索条件也会变, learning courses –&gt; learn course,这时候去搜索对于这两个sub_title来说就是一样的,所以就会出现learned a lot of course 排在了 learning more courses 的前面这样的情况 most-fields搜索我们上面在sub_title中还创建了个子field sub_title.std,然后我们用这两个field来进行most-field搜索.请求12345678910GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;learning courses&quot;, &quot;type&quot;: &quot;most_fields&quot;, &quot;fields&quot;: [&quot;sub_title&quot;,&quot;sub_title.std&quot;] &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1.219939, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1.219939, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 50, &quot;title&quot;: &quot;this is java blog&quot;, &quot;content&quot;: &quot;i think java is the best programming language&quot;, &quot;sub_title&quot;: &quot;learned a lot of course&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1.012641, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;hadoop&quot; ], &quot;tag_cnt&quot;: 2, &quot;view_cnt&quot;: 30, &quot;title&quot;: &quot;this is java and elasticsearch blog&quot;, &quot;content&quot;: &quot;i like to write best elasticsearch article&quot;, &quot;sub_title&quot;: &quot;learning more courses&quot; &#125; &#125; ] &#125;&#125; 再来看一下返回值,依然是learned a lot of course排在了前面,但是learning more courses的分数有了大幅度的提高,可以对比一下第一个搜索时候的分数 区别和优缺点best-fields,是对多个field进行搜索,挑选某个filed匹配度最高的那个分数,同时在多个query最高分相同的情况下,在一定程度上考虑其他query的分数. 简单来说就是,对多个filed进行搜索,就想搜索到某一个field尽可能包含更多关键字的数据 优点:通过best_fields策略,以及综合考虑其他field,还有minimum_should_match支持,可以尽可能精准的将匹配的结果推送到最前面缺点:除了那些精准匹配的结果,其他差不多大的结果,排序结果不是太均匀,没有什么区分度了 most-fields,综合多个field一起进行搜索,尽可能多地让所有的field的query参与到总分数的计算中来,此时就会是个大杂烩,数显类似best_fields案例最开始的那个结果,结果不一定精准,某一个document的一个field包含更多的关键字,但是因为其他document有更多field匹配到了,所以排在前面, 所以就需要建立类似sub_title.std这样的field,尽可能让某一个field精准匹配query string,贡献更高的分数,将更精准匹配的数据排到前面 优点:将尽可能匹配更多的field的结果推送到前面,整个排序的结果都是比较均匀的缺点:可能那些精准匹配的结果无法排在最前面 实际的例子:wiki,明显的most_fields策略,搜索结果比较均匀,但是的确要翻好几页才能找到最匹配的结果]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-48-multi_match语法]]></title>
    <url>%2F2018%2F12%2F17%2FElasticsearch-48-multi-match%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[minimum_should_match作用我们先来看一个查询123456789101112131415161718192021222324252627GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;dis_max&quot;: &#123; &quot;queries&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;java beginner&quot;, &quot;minimum_should_match&quot;:&quot;50%&quot;, &quot;boost&quot;:2 &#125; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &#123; &quot;query&quot;: &quot;java beginner&quot;, &quot;minimum_should_match&quot;:&quot;30%&quot; &#125; &#125; &#125; ], &quot;tie_breaker&quot;: 0.3 &#125; &#125;&#125; 上面这个查询就是查询了title或者content中包含java beginner的内容,用了dis_max+tie_breaker查询,而且查询title的权重是2,还有一个搜索参数是 “minimum_should_match”,那么这个关键字是做什么用的呢 minimum_should_match: 去长尾,比如你搜素5个关键词,但是很多结果是只匹配一个关键词的,其实跟你想要的结果相差甚远,这些结果就是长尾minimum_should_match,可以控制搜索结果的精准度,只有匹配一定数量的关键词数据,才能返回 multi_match语法123456789101112GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;java beginner&quot;, &quot;type&quot;: &quot;best_fields&quot;, &quot;fields&quot;: [&quot;title^2&quot;,&quot;content&quot;], &quot;tie_breaker&quot;: 0.3, &quot;minimum_should_match&quot;:&quot;50%&quot; &#125; &#125;&#125; type: 默认就是best_fields查询title^2: 代表title的权重是2, 相当于上面的”boost”:2]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-47-实战案例-基于dis_max实现best fileds策略进行多字段搜索]]></title>
    <url>%2F2018%2F12%2F06%2FElasticsearch-47-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B-%E5%9F%BA%E4%BA%8Edis-max%E5%AE%9E%E7%8E%B0best-fileds%E7%AD%96%E7%95%A5%E8%BF%9B%E8%A1%8C%E5%A4%9A%E5%AD%97%E6%AE%B5%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[准备工作为帖子增加content字段1234567891011POST /forum/article/_bulk&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;1&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;content&quot; : &quot;i like to write best elasticsearch article&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;2&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;content&quot; : &quot;i think java is the best programming language&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;3&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;content&quot; : &quot;i am only an elasticsearch beginner&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;4&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;content&quot; : &quot;elasticsearch and hadoop are all very good solution, i am a beginner&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;5&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;content&quot; : &quot;spark is best big data solution based on scala ,an programming language similar to java&quot;&#125; &#125; 需求一搜索title或content中包含java或solution的帖子构建搜索条件12345678910111213141516171819GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;java solution&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java solution&quot; &#125; &#125; ] &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293&#123; &quot;took&quot;: 23, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 4, &quot;max_score&quot;: 0.8849759, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.8849759, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 50, &quot;title&quot;: &quot;this is java blog&quot;, &quot;content&quot;: &quot;i think java is the best programming language&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.7120095, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;QQPX-R-3956-#aD8&quot;, &quot;userID&quot;: 2, &quot;hidden&quot;: true, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 2, &quot;view_cnt&quot;: 80, &quot;title&quot;: &quot;this is java, elasticsearch, hadoop blog&quot;, &quot;content&quot;: &quot;elasticsearch and hadoop are all very good solution, i am a beginner&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;5&quot;, &quot;_score&quot;: 0.56008905, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot;: 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2018-12-03&quot;, &quot;tag&quot;: [ &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10, &quot;title&quot;: &quot;this is spark blog&quot;, &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.26742277, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;hadoop&quot; ], &quot;tag_cnt&quot;: 2, &quot;view_cnt&quot;: 30, &quot;title&quot;: &quot;this is java and elasticsearch blog&quot;, &quot;content&quot;: &quot;i like to write best elasticsearch article&quot; &#125; &#125; ] &#125;&#125; 我们来看一下返回值:排在第一位的是id是2的document,这个document中只有title包含了java,content也包含了java排在第二位的是id是4的document,这document中,是title中包含了java,content中包含了solution排在第三位的是id是5的document,这个document中,是content包含了java和solution 这样看来应该是id=5的document是相关度比id=4的高的,但是id=4的排在了前面,这是为什么呢? es的计算方式es在计算每个document的relevance score是每个query的分数的和,乘以matched query的数量,除以总query的数量对于每个query(就是上面should中的每个match),es都会计算一个数量, matched query 就是匹配到的条件的数量 我们来算一下id=4 的document的分数,查询中的两个条件{ “match”: { “title”: “java solution” }},针对document4 是有一个分数的,假设是1.1{ “match”: { “content”: “java solution” }}，针对document4,也是有一个分数的,假设是1.2query分数的和1.1 + 1.2 = 2.3,matched query的数量是2, 总共的query数量是2,所以计算出来就是2.3 * 2 / 2 = 2.3 我们再来算一下document 5 的分数,查询中的两个条件{ “match”: { “title”: “java solution” }},针对document5 是没有分数的,因为这个条件不匹配document5{ “match”: { “content”: “java solution” }}，针对document5,也是有一个分数的,假设是2.3这时候query分数的总和就是2.3,matched query的数量是1,总共的query数量是2,所以计算出来就是 2.3 * 1 / 2 = 1.15 2.3 &gt; 1.15 所以document4 排在了document5的前面 best fields策略 dis_maxbest fields策略: 就是说,搜索到的结果应该是某一个匹配到尽可能多的关键词的document被排在前面,而不是匹配到了少数的关键词还排在前面 搜索请求:12345678910111213141516171819GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;dis_max&quot;: &#123; &quot;queries&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;java solution&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java solution&quot; &#125; &#125; ] &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293&#123; &quot;took&quot;: 6, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 4, &quot;max_score&quot;: 0.68640786, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.68640786, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 50, &quot;title&quot;: &quot;this is java blog&quot;, &quot;content&quot;: &quot;i think java is the best programming language&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;5&quot;, &quot;_score&quot;: 0.56008905, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot;: 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2018-12-03&quot;, &quot;tag&quot;: [ &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10, &quot;title&quot;: &quot;this is spark blog&quot;, &quot;content&quot;: &quot;spark is best big data solution based on scala ,an programming language similar to java&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 0.5565415, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;QQPX-R-3956-#aD8&quot;, &quot;userID&quot;: 2, &quot;hidden&quot;: true, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 2, &quot;view_cnt&quot;: 80, &quot;title&quot;: &quot;this is java, elasticsearch, hadoop blog&quot;, &quot;content&quot;: &quot;elasticsearch and hadoop are all very good solution, i am a beginner&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.26742277, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;hadoop&quot; ], &quot;tag_cnt&quot;: 2, &quot;view_cnt&quot;: 30, &quot;title&quot;: &quot;this is java and elasticsearch blog&quot;, &quot;content&quot;: &quot;i like to write best elasticsearch article&quot; &#125; &#125; ] &#125;&#125; 可以看到,这次查询document5排在了document4的前面 dis_max语法,直接取多个query中,分数最高的那个query的分数即可,我们来分析一下:{ “match”: { “title”: “java solution” }},针对document4,是有一个分数的,比如1.1{ “match”: { “content”: “java solution” }},针对document4,也是有一个分数的,比如1.2取最大分数,1.2 { “match”: { “title”: “java solution” }},针对doc5,是没有分数的{ “match”: { “content”: “java solution” }}，针对doc5,是有一个分数的,比如2.3取最大分数,2.3 然后document4的分数 = 1.2 &lt; document5的分数 = 2.3,所以document5就可以排在更前面的地方,符合我们的需要 基于tie_breaker参数优化dis_max搜索效果场景搜索条件:搜索title或content中包含java beginner的帖子 假设我们现在有3个documentdocument1:title中包含java,content不包含 java beginner任何一个关键词document2:title中不包含任何一个关键词,content中包含beginnerdocument3:title中包含java,content中包含beginner 这时候执行搜索,可能出现的结果是document1和document2排在了document3的前面,而我们期望的是document3排在最前面 dis_max是只取一个query最大的分数,完全不考虑其他的query的分数 使用tie_breaker优化结果tie_breaker参数的意义在于,将其他的query分数,乘以tie_breaker,然后综合在一起计算,除了取最高分以外,还会考虑其他的query分数 tie_breaker的值在0-1之间 用法示例:1234567891011121314151617181920GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;dis_max&quot;: &#123; &quot;queries&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;java beginner&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java beginner&quot; &#125; &#125; ], &quot;tie_breaker&quot;:0.3 &#125; &#125;&#125; 跟queries是同级的, 可以去试一下加tie_breaker和不加时候查询的分数,对比一下就很清楚了,这里就不去演示了]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-46-多shard场景下relevance score不准确的问题]]></title>
    <url>%2F2018%2F12%2F05%2FElasticsearch-46-%E5%A4%9Ashard%E5%9C%BA%E6%99%AF%E4%B8%8Brelevance-score%E4%B8%8D%E5%87%86%E7%A1%AE%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[场景一个index的数据被分配到了多个shard上,每个shard都包含一部分这个index的数据如图所示,一个搜索请求条件是title中包含java,假如shard1 上面有10条符合条件的document,这个请求到达shard1上的时候,默认是在这个shard本地local去进行IDF计算在shard2中假如只有1个符合条件的数据,那shard2也会在local计算他的IDF,这时候这个分数就会算的很高 问题有时候导致出现的搜索结果,似乎不是你想要的结果,也许相关度高的document被排在了后面,很低的被排在了前面但是他的分数很高 解决方案生产环境生产环境中,数据量很大的话,在概率学的背景下,一般情况中es都是在多个shard中均匀的路由数据的,比如说有10个document,title都包含java,一共有5个shard,那么在概率学的背景下,如果负载均衡的话,其实每个shard都应该有2个document的title包含java如果说数据分布均匀的话,就没有上面说的问题了 测试环境测试环境下,可以将所有的primary shard个数设置为1,只有一个shard的话,所有的document都在这一个shard上面,就没有这个问题了 也可以在搜索时附带search_type=dfs_query_then_fetch参数计算一个doc的相关度分数的时候,就会将所有shard对的local IDF计算一下,获取出来,在本地进行global IDF分数的计算,会将所有shard的doc作为上下文来进行计算,也能确保准确性.但是production生产环境下,不推荐这个参数,因为性能很差]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-45-实战案例-基于boost的细粒度搜索条件权重控制]]></title>
    <url>%2F2018%2F12%2F05%2FElasticsearch-45-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B-%E5%9F%BA%E4%BA%8Eboost%E7%9A%84%E7%BB%86%E7%B2%92%E5%BA%A6%E6%90%9C%E7%B4%A2%E6%9D%A1%E4%BB%B6%E6%9D%83%E9%87%8D%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[场景我们来搜索一下标题必须包含 “blog” 的数据,然后可以包含 “java” “hadoop” “elasticsearch” “spark”的数据 实现组合搜索条件123456789101112131415161718192021222324252627282930313233343536GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;blog&quot; &#125; &#125; ], &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;java&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;elasticsearch&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;hadoop&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;spark&quot; &#125; &#125; ] &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107&#123; &quot;took&quot;: 43, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 5, &quot;max_score&quot;: 1.4930474, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 1.4930474, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;QQPX-R-3956-#aD8&quot;, &quot;userID&quot;: 2, &quot;hidden&quot;: true, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 2, &quot;view_cnt&quot;: 80, &quot;title&quot;: &quot;this is java, elasticsearch, hadoop blog&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.80226827, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;hadoop&quot; ], &quot;tag_cnt&quot;: 2, &quot;view_cnt&quot;: 30, &quot;title&quot;: &quot;this is java and elasticsearch blog&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;5&quot;, &quot;_score&quot;: 0.5753642, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot;: 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2018-12-03&quot;, &quot;tag&quot;: [ &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10, &quot;title&quot;: &quot;this is spark blog&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.5753642, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;JODL-X-1937-#pV7&quot;, &quot;userID&quot;: 2, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot;, &quot;tag&quot;: [ &quot;hadoop&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 100, &quot;title&quot;: &quot;this is elasticsearch blog&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.3971361, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 50, &quot;title&quot;: &quot;this is java blog&quot; &#125; &#125; ] &#125;&#125; 看一下返回结果, 5条数据都返回来了, 排在第一位的数据的title是 “this is java, elasticsearch, hadoop blog”,因为这条数据满足的条件最多,所以排在第一位 权重控制给每个条件一个权重值,boost, boost越大这个搜索条件的权重越大 现在 我们给”spark”,这个条件设置一个权重123456789101112131415161718192021222324252627282930313233343536373839GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;blog&quot; &#125; &#125; ], &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;java&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;elasticsearch&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;hadoop&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;spark&quot;, &quot;boost&quot;:5 &#125; &#125; &#125; ] &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107&#123; &quot;took&quot;: 8, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 5, &quot;max_score&quot;: 1.7260925, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;5&quot;, &quot;_score&quot;: 1.7260925, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot;: 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2018-12-03&quot;, &quot;tag&quot;: [ &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10, &quot;title&quot;: &quot;this is spark blog&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 1.4930474, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;QQPX-R-3956-#aD8&quot;, &quot;userID&quot;: 2, &quot;hidden&quot;: true, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 2, &quot;view_cnt&quot;: 80, &quot;title&quot;: &quot;this is java, elasticsearch, hadoop blog&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.80226827, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;hadoop&quot; ], &quot;tag_cnt&quot;: 2, &quot;view_cnt&quot;: 30, &quot;title&quot;: &quot;this is java and elasticsearch blog&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.5753642, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;JODL-X-1937-#pV7&quot;, &quot;userID&quot;: 2, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot;, &quot;tag&quot;: [ &quot;hadoop&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 100, &quot;title&quot;: &quot;this is elasticsearch blog&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.3971361, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 50, &quot;title&quot;: &quot;this is java blog&quot; &#125; &#125; ] &#125;&#125; 可以看到,搜索结果中包含spark的这个document排到了最前面, es在计算 relevance score 的时候,匹配权重更大的搜索条件的document,relevance score会更高,当然也会优先返回回来默认情况下,所有的搜索条件的权重都是1]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-44-基于term+bool实现的multi word搜索底层原理剖析]]></title>
    <url>%2F2018%2F12%2F04%2FElasticsearch-44-%E5%9F%BA%E4%BA%8Eterm-bool%E5%AE%9E%E7%8E%B0%E7%9A%84multi%20word%E6%90%9C%E7%B4%A2%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[在我们使用match这种查询的时候,在es底层其实会自动的转换成term+bool的这种查询 示例一原请求体:123&#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;java elasticsearch&quot;&#125;&#125; es转换后的请求体:12345678&#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;java&quot; &#125;&#125;, &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;elasticsearch&quot; &#125;&#125; ] &#125;&#125; 使用诸如上面的match query进行多值搜索的时候,es会在底层自动将这个match query转换为bool的语法 示例二原请求体:12345678&#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;java elasticsearch&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125;&#125; es转换后的请求体:12345678&#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;java&quot; &#125;&#125;, &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;elasticsearch&quot; &#125;&#125; ] &#125;&#125; 示例三原请求体:12345678&#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;java elasticsearch hadoop spark&quot;, &quot;minimum_should_match&quot;: &quot;75%&quot; &#125; &#125;&#125; es转换后的请求体:1234567891011&#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;java&quot; &#125;&#125;, &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;elasticsearch&quot; &#125;&#125;, &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;hadoop&quot; &#125;&#125;, &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;spark&quot; &#125;&#125; ], &quot;minimum_should_match&quot;: 3 &#125;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-43-实战案例-手动控制全文检索结果的精准度]]></title>
    <url>%2F2018%2F12%2F04%2FElasticsearch-43-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B-%E6%89%8B%E5%8A%A8%E6%8E%A7%E5%88%B6%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2%E7%BB%93%E6%9E%9C%E7%9A%84%E7%B2%BE%E5%87%86%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[准备工作为帖子增加标题字段1234567891011POST /forum/article/_bulk&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;1&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;title&quot; : &quot;this is java and elasticsearch blog&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;2&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;title&quot; : &quot;this is java blog&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;3&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;title&quot; : &quot;this is elasticsearch blog&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;4&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;title&quot; : &quot;this is java, elasticsearch, hadoop blog&quot;&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;5&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;title&quot; : &quot;this is spark blog&quot;&#125; &#125; 需求一搜索标题中包含java 或 elasticsearch的帖子12345678GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;java elasticsearch&quot; &#125; &#125;&#125; 就是只要标题中有java,elasticsearch其中的一个就可以作为返回结果,这个就和之前我们说的term query不一样了, term query是exact value,而这里的搜索是full text match query是负责全文检索的,当然如果要检索的field是not_analyzed不分词的,那么他的作用就和term query是一样的 需求二搜索标题中包含 java 和 elasticsearch的帖子1234567891011GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;:&#123; &quot;query&quot;: &quot;java elasticsearch&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125; &#125;&#125; 搜索结果精准度控制的第一步:灵活使用and关键字,如果希望所有的搜索关键字都要匹配,那么就用and,可以实现单纯match query无法实现的效果 需求三搜索包含java,elasticsearch,spark,hadoop,4个关键字中,至少3个的帖子12345678910111213141516GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;:&#123; &quot;query&quot;: &quot;java elasticsearch spark hadoop&quot;, &quot;minimum_should_match&quot;:&quot;75%&quot; &#125; &#125; &#125;&#125;``` 搜索结果精准度控制第二步:指定一些关键字中至少匹配到其中多少个关键字才能作为返回结果 #### bool组合搜索 GET forum/article/_search{ “query”: { “bool”: { “must”: [ { “match”: { “title”: “java” } } ], “must_not”: [ { “match”: { “title”: “spark” } } ], “should”: [ { “match”: { “title”: “hadoop” } }, { “match”: { “title”: “elasticsearch” } } ] } }}1234567891011121314151617看一下上面这个搜索请求, 就是搜索必须包含java,必须不包含spark,可以包含hadoop或elasticsearch的数据 #### bool组合多个搜索条件,计算relevance score的规则**must和should搜索对应的分数,加起来,除以must和should的总数** 在上面这个查询中: 排名第一:java,同时包含should中所有的关键字,hadoop,elasticsearch 排名第二:java,同时包含should中的elasticsearch 排名第三:java,不包含should中的任何关键字 should是会影响相关度分数的 must是确保说,谁必须有这个关键字,同时会根据这个must的条件去计算出document对这个搜索条件的relevance score 在满足must的基础之上,should中的条件,不匹配也可以,但是如果匹配的更多,那么document的relevance score就会更高 #### 用bool组合查询实现需求三搜索包含java,elasticsearch,spark,hadoop,4个关键字中,至少3个的帖子 GET /forum/article/_search{ “query”: { “bool”: { “should”: [ { “match”: { “title”: “java” } }, { “match”: { “title”: “elasticsearch” } }, { “match”: { “title”: “spark” } }, { “match”: { “title”: “hadoop” } } ], “minimum_number_should_match”: 3 } }}` 默认情况下should是可以不匹配任何一个的,但是如果没有must的话 should中必须匹配一个才可以,但是也可以通过我们上面请求用到的minimum_number_should_match 来控制必须满足几个才能作为返回结果 总结全文见检索的时候,进行多个值的检索,可以用match query 也可以空should搜过结果精准度控制: 用 and operator 或 minimum_number_should_match]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-42-实战案例-range filter进行范围过滤]]></title>
    <url>%2F2018%2F12%2F03%2FElasticsearch-42-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B-range-filter%E8%BF%9B%E8%A1%8C%E8%8C%83%E5%9B%B4%E8%BF%87%E6%BB%A4%2F</url>
    <content type="text"><![CDATA[准备工作为帖子增加浏览量的字段123456789POST /forum/article/_bulk&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;1&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;view_cnt&quot; : 30&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;2&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;view_cnt&quot; : 50&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;3&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;view_cnt&quot; : 100&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;4&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;view_cnt&quot; : 80&#125; &#125; 需求一搜索浏览量在30-60之间的帖子123456789101112131415GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;view_cnt&quot;: &#123; &quot;gt&quot;: 30, &quot;lt&quot;: 60 &#125; &#125; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526272829303132&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 50 &#125; &#125; ] &#125;&#125; lt: 小于 lte: 小于等于 gt: 大于 gte: 大于等于 需求二搜索发帖日期在最近1个月的帖子 先来添加一条最近一个月的帖子123POST /forum/article/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 5 &#125;&#125;&#123; &quot;articleID&quot; : &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot; : 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2018-12-03&quot;, &quot;tag&quot;: [&quot;elasticsearch&quot;], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10 &#125; 添加完成后搜索,一个月,也就是当前时间减去30天1234567891011121314GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;postDate&quot;: &#123; &quot;gt&quot;: &quot;now-30d&quot; &#125; &#125; &#125; &#125; &#125;&#125; 也可以是1234567891011121314GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;postDate&quot;: &#123; &quot;gt&quot;: &quot;2018-12-03||-30d&quot; &#125; &#125; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526272829303132&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;5&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;DHJK-B-1395-#Ky5&quot;, &quot;userID&quot;: 3, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2018-12-03&quot;, &quot;tag&quot;: [ &quot;elasticsearch&quot; ], &quot;tag_cnt&quot;: 1, &quot;view_cnt&quot;: 10 &#125; &#125; ] &#125;&#125; 总结 range 相当于sql中的between and 或者是 &gt;= , &lt;= range用来做范围过滤]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-41-实战案例-terms搜索多个值及搜索结果优化]]></title>
    <url>%2F2018%2F12%2F03%2FElasticsearch-41-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B-terms%E6%90%9C%E7%B4%A2%E5%A4%9A%E4%B8%AA%E5%80%BC%E5%8F%8A%E6%90%9C%E7%B4%A2%E7%BB%93%E6%9E%9C%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[之前的几个案例中都是用的term用来搜索. 本文使用terms来搜索数据 terms,就相当于sql中的in 准备工作为帖子添加tag字段123456789POST /forum/article/_bulk&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;1&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;tag&quot; : [&quot;java&quot;, &quot;hadoop&quot;]&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;2&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;tag&quot; : [&quot;java&quot;]&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;3&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;tag&quot; : [&quot;hadoop&quot;]&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;4&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;tag&quot; : [&quot;java&quot;, &quot;elasticsearch&quot;]&#125; &#125; terms搜索需求一搜索articleID为KDKE-B-9947-#kL5或QQPX-R-3956-#aD8的帖子 将需求转为sql就是:123SELECT * FROM forum.article whereid in (&apos;KDKE-B-9947-#kL5&apos;,&apos;QQPX-R-3956-#aD8&apos;) 然后在es中去构建搜索条件123456789101112131415GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;terms&quot;: &#123; &quot;articleID&quot;: [ &quot;KDKE-B-9947-#kL5&quot;, &quot;QQPX-R-3956-#aD8&quot; ] &#125; &#125; &#125; &#125;&#125; 需求二搜索tag中包含java的帖子1234567891011121314GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;terms&quot;: &#123; &quot;tag&quot;: [ &quot;java&quot; ] &#125; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ] &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;QQPX-R-3956-#aD8&quot;, &quot;userID&quot;: 2, &quot;hidden&quot;: true, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;elasticsearch&quot; ] &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot;, &quot;tag&quot;: [ &quot;java&quot;, &quot;hadoop&quot; ] &#125; &#125; ] &#125;&#125; 从结果上看的话,tag的值中只要包含了java就被搜索出来了 优化搜索结果上面一个搜索中只要包含了java的数据都被搜索出来了,现在我们想搜索只包含java的数据 首先,我们需要修改一下数据,增加一个tag_cnt的字段123456789POST /forum/article/_bulk&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;1&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;tag_cnt&quot; : 2&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;2&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;tag_cnt&quot; : 1&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;3&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;tag_cnt&quot; : 1&#125; &#125;&#123; &quot;update&quot;: &#123; &quot;_id&quot;: &quot;4&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;tag_cnt&quot; : 2&#125; &#125; 执行完毕后,再次来构建搜索条件1234567891011121314151617181920212223GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;:[ &#123; &quot;term&quot;:&#123; &quot;tag_cnt&quot;:1 &#125; &#125;, &#123; &quot;terms&quot;:&#123; &quot;tag&quot;:[&quot;java&quot;] &#125; &#125; ] &#125; &#125; &#125; &#125;&#125; 返回值:12345678910111213141516171819202122232425262728293031&#123; &quot;took&quot;: 4, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot;, &quot;tag&quot;: [ &quot;java&quot; ], &quot;tag_cnt&quot;: 1 &#125; &#125; ] &#125;&#125; 总结 掌握terms多值搜索 优化terms多值搜索结果 terms相当于sql中的in]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-40-实战案例-组合多个filter搜索]]></title>
    <url>%2F2018%2F12%2F03%2FElasticsearch-40-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B-%E7%BB%84%E5%90%88%E5%A4%9A%E4%B8%AAfilter%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[之前我们有写过用bool来组合多个query,同样的bool也可以组合多个filter来搜索 基于bool组合多个filter搜索数据需求一搜索发帖日期为2017-01-01,或者帖子ID为XHDK-A-1293-#fJ3的帖子,同时要求帖子的发帖日期绝对不为2017-01-02 这个需求如果写为SQL的话就是:1234SELECT * FROM forum.article where(postDate=&apos;2017-01-01&apos; or id =&apos;XHDK-A-1293-#fJ3&apos;)and postDate &lt;&gt; 2017-01-02 然后我们在es中组合一下搜索条件123456789101112131415161718192021222324GET forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;:[ &#123; &quot;term&quot;:&#123;&quot;postDate&quot;:&quot;2017-01-01&quot;&#125; &#125;, &#123; &quot;term&quot;:&#123;&quot;articleID&quot;:&quot;XHDK-A-1293-#fJ3&quot;&#125; &#125; ], &quot;must_not&quot;:[ &#123; &quot;term&quot;:&#123;&quot;postDate&quot;:&quot;2017-01-02&quot;&#125; &#125; ] &#125; &#125; &#125; &#125;&#125; 返回数据:123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;took&quot;: 61, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;JODL-X-1937-#pV7&quot;, &quot;userID&quot;: 2, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot; &#125; &#125; ] &#125;&#125; 需求二搜索帖子ID为XHDK-A-1293-#fJ3,或者是帖子ID为JODL-X-1937-#pV7而且发帖日期为2017-01-01的帖子 先来将需求转化为sql12345SELECT * FROM forum.article whereid = &apos;XHDK-A-1293-#fJ3&apos;or(id=&apos;JODL-X-1937-#pV7&apos; and postDate = &apos;2017-01-01&apos;) 然后在es中组合搜索条件 12345678910111213141516171819202122232425262728293031323334GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;:[ &#123; &quot;term&quot;:&#123; &quot;articleID&quot;:&quot;XHDK-A-1293-#fJ3&quot; &#125; &#125;, &#123; &quot;bool&quot;:&#123; &quot;must&quot;:[ &#123; &quot;term&quot;:&#123; &quot;articleID&quot;:&quot;JODL-X-1937-#pV7&quot; &#125; &#125;, &#123; &quot;term&quot;:&#123; &quot;postDate&quot;:&quot;2017-01-01&quot; &#125; &#125; ] &#125; &#125; ] &#125; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;took&quot;: 8, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;JODL-X-1937-#pV7&quot;, &quot;userID&quot;: 2, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot; &#125; &#125; ] &#125;&#125; 总结 should: 可以匹配其中任意一个 must: 必须匹配 must_not: 必须不匹配]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-39-filter原理深度剖析]]></title>
    <url>%2F2018%2F11%2F30%2FElasticsearch-39-filter%E5%8E%9F%E7%90%86%E6%B7%B1%E5%BA%A6%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[filter执行原理场景举个例子,假设有个字段是date类型的,在倒排索引中: word document1 document2 document3 2017-01-01 √ √ 2017-02-02 √ √ 2017-03-03 √ √ √ 在倒排索引中查找搜索串,获取document list这时候一个filter查询:2017-02-02,在倒排索引里面找,对应的document list是doc2,doc3 为每个在倒排索引中搜索到的结果构建一个bitset这点非常重要, 使用找到的document list构建一个bitset,一个二进制数组,数组每个元素都是0或1,用来标识一个doc对一个filter条件是否匹配,如果匹配就是1,不匹配就是0.上面的例子中,构建的bitset就是[0,1,1] 尽可能用简单的数据结构去实现复杂的功能,可以节省内存空间,提升性能 遍历每个过滤条件对应的bitset,优先从最稀疏的开始搜索,查找满足条件的所有document在一个search请求中,可以发出多个filter条件(这个后面再具体说),每个filter会对应一个bitset遍历每个filter条件对应的bitset,先从最稀疏的开始遍历. 怎么算稀疏呢?[0,0,0,1,0,0] – 比较稀疏[0,1,0,1,0,1]先遍历比较稀疏的bitset,可以过滤掉尽可能多的数据 比如现在有个请求 filter: postDate=2017-01,userID=1,然后构建的两个bitset分别是:[0,0,1,==1==,0,0][0,1,0,==1==,0,1]遍历玩两个bitset之后,找到匹配所有条件的document,就是第4个,这个时候就可以将符合结果document返回给客户端了 caching bitset 跟踪query对于在最近的256个query中超过一定次数的过滤条件,缓存其bitset.对于小segment(&lt;1000或&lt;3%)不缓存 举个例子,在最近的256次查询中,postDate=2017-02-02这个条件出现超过了一定的次数(不固定), 就会自动缓存这个filter对应的bitset filter对于小的segment中获取到的结果可以不缓存, segment中记录数小于1000的和segment大小小于index总大小的3%的 因为segment数据量很小的时候,扫描是很快的,而且我们之前有说过,segment会在后台自动合并的,小的segment很快会和其他小的segment合并,此时缓存也就没有什么意义了 大部分情况下 filter会在query之前执行filter先执行可以先过滤掉一部分数据,之前说过query是会计算相关度分数,然后去排序的,而filter是不计算分数,也不排序,所以先执行filter过滤掉尽可能多的数据 如果document有新增或修改,那么cached bitset会被自动更新举个例子,之前有个filter 过滤条件是postDate=2017-02-02,然后他的bitset是[0,0,0,1]这个时候如果新增了一条document进来 postDate也是 2017-02-02,id是5, 那么这个bitset会自动更新为[0,0,0,1,1]同理,如果id = 1的document的postDate更新为2017-02-02 那么bitset也会更新为[1,0,0,1,1] 以后只要是有相同的filter条件的，会直接来使用这个过滤条件对应的cached bitset]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-38-实战案例-term filter搜索]]></title>
    <url>%2F2018%2F11%2F30%2FElasticsearch-38-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B-term-filter%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[之前都是随便写的一些demo来测试es的api,本文及以后将会基于一个案例,来更加深入使用这些api,之后会再使用Java api来实现具体功能. 场景以一个IT论坛为背景,来置顶搜索需求,以及实现. 测试数据123456789POST /forum/article/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 1 &#125;&#125;&#123; &quot;articleID&quot; : &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot; : 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 2 &#125;&#125;&#123; &quot;articleID&quot; : &quot;KDKE-B-9947-#kL5&quot;, &quot;userID&quot; : 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-02&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 3 &#125;&#125;&#123; &quot;articleID&quot; : &quot;JODL-X-1937-#pV7&quot;, &quot;userID&quot; : 2, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 4 &#125;&#125;&#123; &quot;articleID&quot; : &quot;QQPX-R-3956-#aD8&quot;, &quot;userID&quot; : 2, &quot;hidden&quot;: true, &quot;postDate&quot;: &quot;2017-01-02&quot; &#125; 使用_bulk api来添加数据,目前我们只添加这几个field,articleID,userId,hidden 执行完毕以后,我们来查看一下dynamic mapping给我建立的mapping1GET /forum/_mapping/article 返回值:12345678910111213141516171819202122232425262728&#123; &quot;forum&quot;: &#123; &quot;mappings&quot;: &#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;articleID&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; // 1 &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;hidden&quot;: &#123; &quot;type&quot;: &quot;boolean&quot; &#125;, &quot;postDate&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;userID&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125; &#125; &#125;&#125; 这里我们看1处,”articleID”的类型是text,里面还有一个”articleID.keyword”,这个东西是干嘛的呢? 在新版es中,type=text的时候,默认会设置两个field,一个是field本身,比如”articleID”,他是分词的,还有一个就是field.keyword,比如”articleID.keyword”,默认是不分词的, keyword里面还有一个属性是”ignore_above”:256,意思就是最多会保留256个字符 term filter的使用term filter/query: 对搜索文本不分词,直接拿去倒排索引中去匹配,你输入的是什么,就去匹配什么 需求1:根据用户id来搜索帖子123456789101112GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;userID&quot;: 1 &#125; &#125; &#125; &#125;&#125; 需求2:搜索没有隐藏的帖子123456789101112GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;hidden&quot;: false &#125; &#125; &#125; &#125;&#125; 需求3:根据发帖日期搜索帖子123456789101112GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;postDate&quot;: &quot;2017-01-01&quot; &#125; &#125; &#125; &#125;&#125; 需求4:根据帖子id搜索帖子123456789101112GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot; &#125; &#125; &#125; &#125;&#125; 返回值:1234567891011121314&#123; &quot;took&quot;: 5, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 0, &quot;max_score&quot;: null, &quot;hits&quot;: [] &#125;&#125; 这里可以看到,一条结果也没有,但是应该是有这个数据的,为什么呢? 在添加数据的时候,字符串是默认会去分词,然后建立倒排索引的,而term是不去分词的,所以是查不到的 我们可以用上面es自动建立的keyword来进行搜索123456789101112GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;articleID.keyword&quot;: &quot;XHDK-A-1293-#fJ3&quot; &#125; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot; &#125; &#125; ] &#125;&#125; 这样就可以搜索到了,但是同时也有一个问题,就是keyword只会保留256个字符,如果这个字段太长的话那就还是搜索不到的.这时候,我们最好重建索引,手动设置mapping 删除索引1DELETE /forum 手动创建索引,指定articleID不分词123456789101112PUT /forum&#123; &quot;mappings&quot;: &#123; &quot;article&quot;:&#123; &quot;properties&quot;: &#123; &quot;articleID&quot;:&#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125;&#125; 然后把上面的数据重新添加进去.现在,再用articleID来进行查询123456789101112GET /forum/article/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot; &#125; &#125; &#125; &#125;&#125; 返回值:123456789101112131415161718192021222324252627&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;forum&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;articleID&quot;: &quot;XHDK-A-1293-#fJ3&quot;, &quot;userID&quot;: 1, &quot;hidden&quot;: false, &quot;postDate&quot;: &quot;2017-01-01&quot; &#125; &#125; ] &#125;&#125; 这时候就可以查询的到了 总结 term filter:根据exact value来进行搜索,数字,Boolean,date类型的天然支持 text类型的field需要在建立的索引的时候指定not_analyzed(新版中可以直接指定type为keyword),才可以使用term]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-37-Java API document 增删改查]]></title>
    <url>%2F2018%2F11%2F29%2FElasticsearch-37-Java-API-document-%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5%2F</url>
    <content type="text"><![CDATA[前文都是讲的理论知识,用restful API来做的测试. 本文将使用java API来操作索引,document. 添加依赖123456789101112&lt;!-- es依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;transport&lt;/artifactId&gt; &lt;version&gt;$&#123;elasticsearch.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;$&#123;elasticsearch.version&#125;&lt;/version&gt;&lt;/dependency&gt; yml配置1234567elasticsearch: ip: 127.0.0.1 port: 9300 pool: 5# 集群名称 cluster: name: elasticsearch 配置client12345678910111213141516171819202122232425262728293031323334353637383940414243@Configuration@Slf4jpublic class ElasticsearchConfig &#123; /** * ip地址 */ @Value(&quot;$&#123;elasticsearch.ip&#125;&quot;) private String hostName; @Value(&quot;$&#123;elasticsearch.port&#125;&quot;) private int port; @Value(&quot;$&#123;elasticsearch.pool&#125;&quot;) private int poolSize; @Value(&quot;$&#123;elasticsearch.cluster.name&#125;&quot;) private String clusterName; @Bean public TransportClient init()&#123; TransportClient transportClient = null; try &#123; // 配置 Settings settings = Settings.builder() .put(&quot;cluster.name&quot;, clusterName) // 集群嗅探机制,找到es集群 .put(&quot;client.transport.sniff&quot;, true) // 增加线程池个数 .put(&quot;thread_pool.search.size&quot;, poolSize) .build(); transportClient = new PreBuiltTransportClient(settings) // 设置地址端口号 .addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(hostName), port)); &#125; catch (Exception e)&#123; log.error(&quot;elasticsearch TransportClient init error,&#123;&#125;&quot;, e); &#125; return transportClient; &#125;&#125; 基础的配置已经完成了,接下来就是具体的方法 增删改查节点和索引Util工具类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471@Component@Slf4jpublic class ElasticsearchUtils &#123; @Autowired private TransportClient transportClient; private static TransportClient client; @PostConstruct public void init()&#123; client = this.transportClient; &#125; /** * 判断索引是否存在 * @param indexName 索引名称 * @return true/false */ public static boolean indexExist(String indexName)&#123; IndicesExistsResponse indicesExistsResponse = client.admin() .indices() .exists(new IndicesExistsRequest(indexName)) .actionGet(); if (indicesExistsResponse.isExists())&#123; log.info(&quot;Index [&apos;&#123;&#125;&apos;] is exists&quot;, indexName); &#125; else &#123; log.info(&quot;Index [&apos;&#123;&#125;&apos;] is not exists&quot;, indexName); &#125; return indicesExistsResponse.isExists(); &#125; /** * 创建索引 * @param indexName 索引名称 * @return isAcknowledged */ public static boolean createIndex(String indexName)&#123; if (!indexExist(indexName))&#123; log.info(&quot;Index is not exist&quot;); &#125; CreateIndexResponse response = client.admin() .indices() .prepareCreate(indexName) .execute() .actionGet(); return response.isAcknowledged(); &#125; /** * 删除索引 * @param indexName 索引名称 * @return isAcknowledged */ public static boolean deleteIndex(String indexName)&#123; if (!indexExist(indexName))&#123; log.info(&quot;Index is not exist&quot;); &#125; DeleteIndexResponse response = client.admin() .indices() .prepareDelete(indexName) .execute() .actionGet(); return response.isAcknowledged(); &#125; /** * 创建一个document,需要手动指定id * @param indexName 索引名称 * @param typeName 类型名称 * @param id id * @param xContentBuilder 数据(fields) * @return id */ public static String createDocument(String indexName, String typeName, String id, XContentBuilder xContentBuilder)&#123; IndexResponse response = client .prepareIndex(indexName, typeName, id) .setSource(xContentBuilder) .get(); log.info(&quot;add document response:&#123;&#125;&quot;, response.toString()); return response.getId(); &#125; /** * 创建一个document,不需要手动指定id * @param indexName 索引名称 * @param typeName 类型名称 * @param xContentBuilder 数据(fields) * @return id */ public static String createDocumentWithNoId(String indexName, String typeName, XContentBuilder xContentBuilder)&#123; IndexResponse response = client .prepareIndex(indexName, typeName) .setSource(xContentBuilder) .get(); log.info(&quot;add document response:&#123;&#125;&quot;, response.toString()); return response.getId(); &#125; /** * 更新document,partial update * @param indexName 索引名称 * @param typeName 类型名称 * @param id id * @param xContentBuilder 数据 * @return id */ public static String updateDocument(String indexName, String typeName, String id, XContentBuilder xContentBuilder)&#123; UpdateResponse updateResponse = client .prepareUpdate(indexName, typeName, id) .setDoc(xContentBuilder) .get(); log.info(&quot;update response:&#123;&#125;&quot;, updateResponse.toString()); return updateResponse.getId(); &#125; /** * 删除document * @param indexName 索引名称 * @param typeName 类型名称 * @param id id * @return id */ public static String deleteDocument(String indexName, String typeName, String id)&#123; DeleteResponse response = client .prepareDelete(indexName, typeName, id) .get(); log.info(&quot;delete response:&#123;&#125;&quot;, response.toString()); return response.getId(); &#125; /** * 根据id获取document * @param indexName 索引名称 * @param typeName 类型名称 * @param id id * @return _source数据 */ public static String getDocumentById(String indexName, String typeName, String id)&#123; GetResponse response = client .prepareGet(indexName, typeName, id) .get(); log.info(&quot;get response&quot;); return response.getSourceAsString(); &#125; /** * 只做查询,没有排序 * @param indexes 索引 * @param types 类型 * @param matchMap 搜索条件 * @param fields 要显示的fields,不传返回全部 * @return 结果集 */ public static List&lt;Map&lt;String,Object&gt;&gt; searchDocument(String indexes, String types, Map&lt;String,String&gt; matchMap, String fields)&#123; return searchDocument(indexes, types, 0, 0, matchMap, false, null, fields, null, null, null); &#125; /** * 查询/精准匹配,可以排序 * @param indexes 索引 * @param types 类型 * @param matchMap 查询条件 * @param fields 要显示的fields,不传返回全部 * @param matchPhrase true 使用短语精准匹配 * @param sortField 排序field * @param sortOrder 正序倒序(正序的话需要字段有正排索引) * @return 结果集 */ public static List&lt;Map&lt;String,Object&gt;&gt; searchDocument(String indexes, String types, Map&lt;String,String&gt; matchMap, String fields, boolean matchPhrase, String sortField, SortOrder sortOrder)&#123; return searchDocument(indexes, types, 0, 0, matchMap, matchPhrase, null, fields, sortField, sortOrder, null); &#125; /** * 查询/精准匹配,可以排序,高亮,文档大小限制 * @param indexes 索引 * @param types 类型 * @param matchMap 查询条件 * @param fields 要显示的fields,不传返回全部 * @param matchPhrase true 使用短语精准匹配 * @param sortField 排序field * @param sortOrder 正序倒序(正序的话需要字段有正排索引) * @param highlightField 高亮字段 * @param size 文档大小限制 * @return 结果集 */ public static List&lt;Map&lt;String,Object&gt;&gt; searchDocument(String indexes, String types, Map&lt;String,String&gt; matchMap, String fields, boolean matchPhrase, String sortField, SortOrder sortOrder, String highlightField, Integer size)&#123; return searchDocument(indexes, types, 0, 0, matchMap, matchPhrase, highlightField, fields, sortField, sortOrder, size); &#125; /** * 搜索document * @param indexes 索引名 * @param types 类型 * @param startTime 开始时间 * @param endTime 结束时间 * @param matchMap 查询条件(filed:value) * @param matchPhrase true 使用短语精准匹配 * @param highlightField 高亮显示的field * @param fields 要显示的fields,不传返回全部 * @param sortField 排序field * @param sortOrder 正序倒序(正序的话需要字段有正排索引) * @param size 文档大小限制 * @return 结果集 */ public static List&lt;Map&lt;String, Object&gt;&gt; searchDocument(String indexes, String types, long startTime, long endTime, Map&lt;String,String&gt; matchMap, boolean matchPhrase, String highlightField, String fields, String sortField, SortOrder sortOrder, Integer size)&#123; if (StringUtils.isEmpty(indexes))&#123; return null; &#125; // 构建查询的request body SearchRequestBuilder searchRequestBuilder = client.prepareSearch(indexes.split(&quot;,&quot;)); // 拆分type if (StringUtils.isNotEmpty(types))&#123; searchRequestBuilder.setTypes(types.split(&quot;,&quot;)); &#125; // 组合查询 bool BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery(); // 组装查询条件 boolQueryBuilder = boolQuery(boolQueryBuilder, startTime, endTime, matchMap, matchPhrase); // 设置高亮字段 searchRequestBuilder = setHighlightField(searchRequestBuilder, highlightField); // 搜索条件加到request中 searchRequestBuilder.setQuery(boolQueryBuilder); // 定制返回的fields if (StringUtils.isNotEmpty(fields))&#123; searchRequestBuilder.setFetchSource(fields.split(&quot;,&quot;), null); &#125; searchRequestBuilder.setFetchSource(true); // 设置排序 if (StringUtils.isNotEmpty(sortField))&#123; searchRequestBuilder.addSort(sortField, sortOrder); &#125; // 设置文档大小限制 if (size != null &amp;&amp; size &gt; 0)&#123; searchRequestBuilder.setSize(size); &#125; // 把请求体打印出来 log.info(&quot;查询请求体:&#123;&#125;&quot;, searchRequestBuilder); // 发送请求,执行查询 SearchResponse response = searchRequestBuilder .execute() .actionGet(); long totalHits = response.getHits().totalHits(); long length = response.getHits().getHits().length; log.info(&quot;共查询到[&#123;&#125;]条数据,处理数据条数[&#123;&#125;]&quot;, totalHits, length); if (response.status().getStatus() == 200)&#123; return setSearchResponse(response, highlightField); &#125; return null; &#125; /** * 分页查询 * @param indexes 索引 * @param types 类型 * @param pageNum 页码 * @param pageSize 每页显示数量 * @param startTime 开始时间 * @param endTime 结束时间 * @param fields 要显示的字段 * @param sortField 排序字段 * @param sortOrder 正序倒序(正序需要排序的字段有正排索引) * @param matchPhrase true 精准匹配 * @param highlightField 高亮子弹 * @param matchMap 查询条件 * @return PageVO */ public static PageVO searchDocumentPage(String indexes, String types, int pageNum, int pageSize, long startTime, long endTime, String fields, String sortField, SortOrder sortOrder, boolean matchPhrase, String highlightField, Map&lt;String,String&gt; matchMap)&#123; if (StringUtils.isEmpty(indexes))&#123; return null; &#125; SearchRequestBuilder searchRequestBuilder = client.prepareSearch(indexes.split(&quot;,&quot;)); if (StringUtils.isNotEmpty(types))&#123; searchRequestBuilder.setTypes(types.split(&quot;,&quot;)); &#125; searchRequestBuilder.setSearchType(SearchType.QUERY_THEN_FETCH); // 设置需要显示的字段 if (StringUtils.isNotEmpty(fields))&#123; searchRequestBuilder.setFetchSource(fields.split(&quot;,&quot;), null); &#125; // 设置排序字段 if (StringUtils.isNotEmpty(sortField))&#123; searchRequestBuilder.addSort(sortField, sortOrder); &#125; // 组合查询 bool BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery(); // 组装查询条件 boolQueryBuilder = boolQuery(boolQueryBuilder, startTime, endTime, matchMap, matchPhrase); // 设置高亮字段 searchRequestBuilder = setHighlightField(searchRequestBuilder, highlightField); // 搜索条件加到request中 searchRequestBuilder.setQuery(boolQueryBuilder); searchRequestBuilder.setQuery(QueryBuilders.matchAllQuery()); // 设置分页 searchRequestBuilder.setFrom(pageNum).setSize(pageSize); // 设置按照匹配度排序 searchRequestBuilder.setExplain(true); // 打印请求体 log.info(&quot;请求体:&#123;&#125;&quot;, searchRequestBuilder); // 发送请求,执行查询 SearchResponse response = searchRequestBuilder .execute() .actionGet(); long totalHits = response.getHits().totalHits(); long length = response.getHits().getHits().length; log.info(&quot;共查询到[&#123;&#125;]条数据,处理数据条数[&#123;&#125;]&quot;, totalHits, length); if (response.status().getStatus() == 200)&#123; // 解析查询对象 List&lt;Map&lt;String,Object&gt;&gt; rList = setSearchResponse(response, highlightField); return new PageVO(pageNum, pageSize, (int) totalHits, rList); &#125; return null; &#125; /** * 高亮结果集 特殊处理 * @param searchResponse 查询返回结果 * @param highlightField 高亮字段 * @return 结果 */ public static List&lt;Map&lt;String,Object&gt;&gt; setSearchResponse(SearchResponse searchResponse, String highlightField)&#123; List&lt;Map&lt;String,Object&gt;&gt; sourceList = new ArrayList&lt;&gt;(); StringBuilder stringBuilder = new StringBuilder(); // 循环查询结果 for (SearchHit searchHitFields : searchResponse.getHits().getHits()) &#123; // 把id放到_source里面去 searchHitFields.getSource().put(&quot;id&quot;, searchHitFields.getId()); // 有高亮字段的话做处理 if (StringUtils.isNotEmpty(highlightField))&#123; log.info(&quot;遍历高亮结果集,覆盖正常结果集...&#123;&#125;&quot;, searchHitFields.getSource()); Text[] texts = searchHitFields.getHighlightFields().get(highlightField).getFragments(); if (texts != null)&#123; for (Text text : texts) &#123; stringBuilder.append(text.toString()); &#125; // 遍历高亮结果集,覆盖正常结果集 searchHitFields.getSource().put(highlightField, stringBuilder.toString()); &#125; &#125; sourceList.add(searchHitFields.getSource()); &#125; return sourceList; &#125; /** * 封装 * @param boolQueryBuilder boolQueryBuilder * @param startTime 开始时间 * @param endTime 结束时间 * @param matchMap 查询条件 * @param matchPhrase true 使用精准匹配 * @return boolQueryBuilder */ public static BoolQueryBuilder boolQuery(BoolQueryBuilder boolQueryBuilder, long startTime, long endTime, Map&lt;String, String&gt; matchMap, boolean matchPhrase)&#123; // TODO 不清楚是做什么 if (startTime &gt; 0 &amp;&amp; endTime &gt; 0)&#123; boolQueryBuilder.must(QueryBuilders.rangeQuery(&quot;processTime&quot;) .format(&quot;epoch_millis&quot;) .from(startTime) .to(endTime) .includeLower(true) .includeUpper(true) ); &#125; // 搜索条件 if (!matchMap.isEmpty())&#123; for (Map.Entry&lt;String,String&gt; entry : matchMap.entrySet()) &#123; if (StringUtils.isNoneBlank(entry.getKey(),entry.getValue()))&#123; if (matchPhrase == Boolean.TRUE)&#123; // 精准匹配 boolQueryBuilder.must(QueryBuilders.matchPhraseQuery(entry.getKey(), entry.getValue())); &#125; else &#123; boolQueryBuilder.must(QueryBuilders.matchQuery(entry.getKey(), entry.getValue())); &#125; &#125; &#125; &#125; return boolQueryBuilder; &#125; /** * 封装设置高亮字段 * @param searchRequestBuilder searchRequestBuilder * @param highlightField 高亮字段 * @return searchRequestBuilder */ public static SearchRequestBuilder setHighlightField(SearchRequestBuilder searchRequestBuilder, String highlightField)&#123; // 高亮字段 if (StringUtils.isNotEmpty(highlightField))&#123; HighlightBuilder highlightBuilder = new HighlightBuilder(); // 设置前缀// highlightBuilder.preTags(&quot;&lt;span style=&apos;color:red&apos;&gt;&quot;); // 设置后缀// highlightBuilder.postTags(&quot;&lt;/span&gt;&quot;); // 设置高亮字段 highlightBuilder.field(highlightField); searchRequestBuilder.highlighter(highlightBuilder); &#125; return searchRequestBuilder; &#125;&#125; 分页model12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485/** * @author 周泽 * @date Create in 10:38 2018/11/29 * @Description 分页结果集 */@Getter@Setterpublic class PageVO &#123; /** * 当前页码 */ private Integer pageNum; /** * 一页显示数量 */ private Integer pageSize; /** * 总数量 */ private Integer total; /** * 结果集合 */ private List&lt;Map&lt;String,Object&gt;&gt; rList; /** * 共有多少页 */ private Integer pageCount; /** * 页码列表的开始索引(包含) */ private Integer beginPageIndex; /** * 码列表的结束索引(包含) */ private Integer endPageIndex; /** * 只接受前4个必要的属性，会自动的计算出其他3个属性的值 * @param pageNum 当前页码 * @param pageSize 每页显示条数 * @param total 总条数 * @param rList 结果集合 */ public PageVO(int pageNum, int pageSize, int total, List&lt;Map&lt;String, Object&gt;&gt; rList) &#123; this.pageNum = pageNum; this.pageSize = pageSize; this.total = total; this.rList = rList; // 计算总页码 pageCount = (total + pageSize - 1) / pageSize; // 计算 beginPageIndex 和 endPageIndex // &gt;&gt; 总页数不多于10页，则全部显示 if (pageCount &lt;= 10) &#123; beginPageIndex = 1; endPageIndex = pageCount; &#125; else &#123; // &gt;&gt; 总页数多于10页，则显示当前页附近的共10个页码 // 当前页附近的共10个页码（前4个 + 当前页 + 后5个） beginPageIndex = pageNum - 4; endPageIndex = pageNum + 5; // 当前面的页码不足4个时，则显示前10个页码 if (beginPageIndex &lt; 1) &#123; beginPageIndex = 1; endPageIndex = 10; &#125; // 当后面的页码不足5个时，则显示后10个页码 if (endPageIndex &gt; pageCount) &#123; endPageIndex = pageCount; beginPageIndex = pageCount - 10 + 1; &#125; &#125; &#125;&#125; 单元测试代码在源码里.源码地址]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-36-深度解析document增删改原理及优化过程]]></title>
    <url>%2F2018%2F11%2F27%2FElasticsearch-36-%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90document%E5%A2%9E%E5%88%A0%E6%94%B9%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BC%98%E5%8C%96%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[document写入原理在es底层,用的是Lucene,Lucene底层的index是分为多个segment的,每个segment都会存放部分数据 图中,客户端写入一个document的时候: 先写到了操作系统中的buffer缓存中 然后进行commit point buffer中的数据写入了新的index segment 然后写入操作系统的缓存中 缓存中的index segment被fsync强制刷新到磁盘上 同时新的index segment被打开,供搜索使用 将buffer缓存清空 更新删除原理如果是更新操作,实际上是将现有的document标记为deleted,然后将新的document写入新的index segment中,下次search过来的时候,也许会匹配到一个document的多个版本,但是之前的版本已经被标记为deleted了,所以会被过滤掉,不会作为搜索结果返回,删除操作同理. 每次commit point时,会有一个.del文件,标记了哪些segment中的哪些document被标记为deleted了搜索的时候回依次查询所有的segment,从旧的到新的,比如被修改过的document,在旧的segment中,会被标记为deleted,在新的segment中会有其新的数据 问题如果按照上面的流程的话,每次都必须等待fsync将segment刷入磁盘,才能将segment打开供search使用,这样的话,从一个document写入,到它可以被搜索,可能会超过1分钟,这就不是近实时的搜索了, 主要瓶颈在于fsync实际发生磁盘IO写数据进磁盘是很耗时的. 流程改进 数据写入buffer中 每隔一定的时间(默认是1s),buffer中的数据被写入新的segment文件,然后写入os cache中 只要segment写入到了os cache中了,那就直接打开index segment 供使用,不立即commit. 最后把buffer清空 数据写入os cache并被打开供搜索的过程,叫做refresh,默认是每隔1s refresh一次,就是说每隔一秒,就会将buffer中的数据写入一个新的index segment文件,先写入os cache中.所以es是近实时的,数据写入到可以搜索,默认是1秒. 我们也可以手动去设置refresh的间隔时间,比如时效性要求较低,写入数据一分钟后被搜索到就可以了123456PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;refresh_interval&quot;: &quot;60s&quot; &#125;&#125; 问题数据不及时写入到磁盘中,而是在缓存中,如果宕机的话,数据就会丢失,就不可靠了 再次优化写入流程 写入document的时候,数据同时写入buffer缓冲和translog日志文件 每隔1秒中,buffer中的数据被写入新的segment file,并进入os cache中,此时segment被打开并供search使用 buffer被清空 重复1-3,新的segment不断添加,buffer不断被清空,而translog中的数据不断累加 当translog长度达到一定的程度的时候,commit操作发生.5.1. buffer中所有数据写入一个新的segment中,并写入os cache 打开供使用.5.2. buffer被清空5.3. 一个commit point被写入磁盘,标明了所有的index segment5.4. os cache中的所有 数据被fsync强行刷到磁盘上去5.5. 现有的translog被清空,创建一个新的translog 基于translog和commit point进行数据恢复磁盘上存储的是上次commit point为止,所有的segment file,那么translog中存储的就是上一次flush(commit point)知道现在最近的数据变更记录如果说 os cache中已经囤积了一些数据,没有被刷到磁盘上,这个时候宕机了, 这时候机器重启,此时会将translog文件中的变更记录进行回放,重新执行之前的各种操作,等待下次commit即可 每次flush 会自动清空translog,默认每隔30分钟flush一次,或者当translog过大的时候,也会flush.我们也可以手动flush,POST /my_index/_flush,一般来说别手动flush,让它自动执行就可以了 translog,也是先放在缓存中的,每隔5秒被fsync一次到磁盘上.一般是在一次增删改操作之后. 如果说在一次增删改操作的时候正好要fsync translog到磁盘上,那么会等待primary shard和replica shard都成功之后,这次增删改操作才会成功 但是这种在一次增删改时强行fsync translog可能会导致部分操作比较耗时如果可以允许部分数据丢失,可以设置异步fsync translog 12345PUT /my_index/_settings&#123; &quot;index.translog.durability&quot;: &quot;async&quot;, &quot;index.translog.sync_interval&quot;: &quot;5s&quot;&#125; 终极优化上面说的,每秒生成一个segment文件,文件会越来越多,而且每次search都要搜索所有的segment,很耗时es会默认在后台执行合并的操作,在merge的时候,被标记为deleted的document 也会被彻底物理删除 每次merge的流程是 选择一些有相似大小的segment,merge成一个大的segment 将新的segment flush到磁盘上去 写一个新的commit point,包括了新的segment,并排除旧的那些segment 将新的segment打开供搜索 将旧的segment删除 也可以通过 POST /my_index/_optimize?max_num_segments=1 来手动合并,但是尽量不要手动执行]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-35-使用scroll+bulk+索引别名实现零停机重建索引]]></title>
    <url>%2F2018%2F11%2F27%2FElasticsearch-35-%E4%BD%BF%E7%94%A8scroll-bulk-%E7%B4%A2%E5%BC%95%E5%88%AB%E5%90%8D%E5%AE%9E%E7%8E%B0%E9%9B%B6%E5%81%9C%E6%9C%BA%E9%87%8D%E5%BB%BA%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[场景如果我们一开始新建了一个索引,并且依靠dynamic mapping,这个时候插入一条数据是2018-01-01这种格式的,这field就会被自动映射成了date类型,但是其实他应该是个string类型的,这时候应该怎么做呢? 解决方案一个field的设置是不能被修改的,如果要修改一个field,那么应该重新按照新的mapping来创建一个index,然后将旧的index中的数据查询出来,用_bulk api批量插入到新的索引中去 批量查询的时候,建议采用scroll api,采用多线程并发的方式来reindex数据. 案例我们先插入一条数据如下: 1234PUT /old_my_index/my_type/1&#123; &quot;title&quot;:&quot;2017-01-01&quot;&#125; 然后获取这个index的mapping 12345678910111213&#123; &quot;old_my_index&quot;: &#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125; &#125; &#125; &#125; &#125;&#125; 可以看到title已经被映射成了date类型,这时 如果我们在添加一个字符串的值是添加不进去的. 而且如果想修改这个field的类型也是不可能的 此时唯一的办法就是进行reindex,也就是说重新建立一个索引,将旧索引中的数据查询出来,导入新索引 这里可能会有一个问题,旧的索引名称是old_my_index,假如新的索引名称是new_my_index, 这时候已经有一个java应用在使用old_my_index在操作了, 那么这时候是不是要先停止应用,修改索引,然后重启呢? 这样的话会导致java应用停机,降低可用性 针对上面的问题呢,我们可以先给java应用一个旧索引的别名, java用的只是一个别名,指向旧的索引 1PUT /old_my_index/_alias/my_index 执行上面的代码,就是给了old_my_index一个别名(my_index),然后我们新建一个索引,将title这个field调整为string类型的 123456789101112PUT /new_my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;:&#123; &quot;properties&quot;: &#123; &quot;title&quot;:&#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; &#125;&#125; 新建完成以后,用scroll api从旧的索引中查询数据12345678GET old_my_index/my_type/_search?scroll=1m&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [&quot;_doc&quot;], &quot;size&quot;: 1&#125; 返回值:12345678910111213141516171819202122232425262728&#123; &quot;_scroll_id&quot;: &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAAHVFmY1N3VWOTF4U19HUlRRUzJIbzgxcmcAAAAAAAAB1xZmNTd1VjkxeFNfR1JUUVMySG84MXJnAAAAAAAAAdQWZjU3dVY5MXhTX0dSVFFTMkhvODFyZwAAAAAAAAHYFmY1N3VWOTF4U19HUlRRUzJIbzgxcmcAAAAAAAAB1hZmNTd1VjkxeFNfR1JUUVMySG84MXJn&quot;, &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: null, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;old_my_index&quot;, &quot;_type&quot;: &quot;my_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;2017-01-01&quot; &#125;, &quot;sort&quot;: [ 0 ] &#125; ] &#125;&#125; 查询出来以后用bulk api将scroll查出来的一批数据,批量写入新的索引. 123POST /_bulk&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;new_my_index&quot;,&quot;_type&quot;:&quot;my_type&quot;,&quot;_id&quot;:&quot;1&quot;&#125;&#125;&#123;&quot;title&quot;:&quot;2017-01-01&quot;&#125; 重复循环scroll查询和bulk批量插入,直到所有的数据都添加到了新的索引中 添加完成后,将别名切换到新的索引上去,这样的话java应用就直接通过别名使用新的索引中的数据了,不需要停机重启,高可用 1234567891011121314151617POST /_aliases&#123; &quot;actions&quot;: [ &#123; &quot;remove&quot;: &#123; // 把别名从旧的索引上先移除 &quot;index&quot;: &quot;old_my_index&quot;, &quot;alias&quot;: &quot;my_index&quot; &#125; &#125;, &#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;new_my_index&quot;, // 将别名指向新的索引 &quot;alias&quot;: &quot;my_index&quot; &#125; &#125; ]&#125; 总结总体来说,就是最开始就给索引一个别名去让客户端去使用,然后如果要切换索引的话,就先建一个索引,然后查询旧的索引数据,将数据插入到新的索引中,完成后将别名指向新的索引,就实现了零停机重建索引]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-34-定制自己的dynamic mapping策略]]></title>
    <url>%2F2018%2F11%2F26%2FElasticsearch-34-%E5%AE%9A%E5%88%B6%E8%87%AA%E5%B7%B1%E7%9A%84dynamic-mapping%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[定制dynamic策略true: 遇到陌生字段,就进行dynamic mappingfalse: 遇到陌生字段,就忽略strict: 遇到陌生字段,就报错 示例我们现在来新建一个index.1234567891011121314151617PUT /my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;dynamic&quot;:&quot;strict&quot;, // 1 &quot;properties&quot;: &#123; &quot;title&quot;:&#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;address&quot;:&#123; &quot;type&quot;: &quot;object&quot;, &quot;dynamic&quot;:&quot;true&quot; // 2 &#125; &#125; &#125; &#125;&#125; 1处,我们设置了这个my_type的dynamic是strict,就是遇到陌生的字段,就报错2处,设置了address这个object类型的filed的dynamic是true, 遇到陌生字段就进行dynamic mapping 先来放一条数据进去123456789PUT /my_index/my_type/1&#123; &quot;title&quot;:&quot;my title&quot;, &quot;content&quot;:&quot;test content&quot;, // 1 &quot;address&quot;:&#123; &quot;province&quot;:&quot;zhejiang&quot;, // 2 &quot;city&quot;:&quot;hangzhou&quot; // 3 &#125;&#125; 1处,content这个field 我们创建索引时,并没有设置设置这个content, dynamic是strict,遇到陌生字段应该报错2,3处的province和city,我们也没有设置, address的dynamic策略应该是遇到陌生字段就进行dynamic mapping 运行上面代码,返回值:12345678910111213&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;strict_dynamic_mapping_exception&quot;, &quot;reason&quot;: &quot;mapping set to strict, dynamic introduction of [content] within [my_type] is not allowed&quot; &#125; ], &quot;type&quot;: &quot;strict_dynamic_mapping_exception&quot;, &quot;reason&quot;: &quot;mapping set to strict, dynamic introduction of [content] within [my_type] is not allowed&quot; &#125;, &quot;status&quot;: 400&#125; 报错了因为我们在my_type中设置的dynamic是strict 然后把content这个field删掉12345678PUT /my_index/my_type/1&#123; &quot;title&quot;:&quot;my title&quot;, &quot;address&quot;:&#123; &quot;province&quot;:&quot;zhejiang&quot;, &quot;city&quot;:&quot;hangzhou&quot; &#125;&#125; 执行后,添加成功. 然后我们来查询一下这个type的mapping1GET /my_index/_mapping/my_type 返回值:12345678910111213141516171819202122232425262728293031323334353637&#123; &quot;my_index&quot;: &#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; &quot;dynamic&quot;: &quot;strict&quot;, &quot;properties&quot;: &#123; &quot;address&quot;: &#123; &quot;dynamic&quot;: &quot;true&quot;, &quot;properties&quot;: &#123; &quot;city&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;province&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125;, &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; &#125; &#125;&#125; 可以看到address中的province和city这两个字段已经被自动dynamic mapping了 定制dynamic mapping策略date_detectiones会默认按照一定的格式识别date类型,比如yyyy-MM-dd,但是如果某个field先过来一个2018-01-01的值,就会被自动dynamic mapping 成 date类型,后面如果再来一个”hello word”之类的值,就会报错.我们可以手动关闭某个type的date_detection,如果有需要,自己手动指定某个field为date类型.1234PUT /index/_mapping/type&#123; &quot;date_detection&quot;: false&#125; 定制自己的dynamic mapping template (type级别)首先我们需要在建索引的时候添加一个模板. 12345678910111213141516171819PUT /my_index&#123; &quot;mappings&quot;: &#123; &quot;my_type&quot;: &#123; // type名称 &quot;dynamic_templates&quot;:[ &#123; &quot;en&quot;:&#123; // 模板名称,自定义的 &quot;match&quot;:&quot;*_en&quot;, // 通配符匹配_en结尾的field &quot;match_mapping_type&quot;:&quot;string&quot;, &quot;mapping&quot;:&#123; &quot;type&quot;:&quot;string&quot;, &quot;analyzer&quot;:&quot;english&quot; // english分词器 &#125; &#125; &#125; ] &#125; &#125;&#125; 上面这段代码就是说 field名称是_en结尾的话,就是string类型的,分词器是english分词器 我们来添加两条数据,然后查询测试一下 123456789PUT /my_index/my_type/1&#123; &quot;title&quot;: &quot;this is my first article&quot;&#125;PUT /my_index/my_type/2&#123; &quot;title_en&quot;: &quot;this is my first article&quot;&#125; 分别用title 和 title_en去匹配 is这个词.会发现 用title_en是匹配不到的 title没有匹配到任何的dynamic模板,默认就是standard分词器,不会过滤停用词,is会进入倒排索引,用is来搜索是可以搜索到的title_en匹配到了dynamic模板,就是english分词器,会过滤停用词,is这种停用词就会被过滤掉,用is来搜索就搜索不到了 定制自己的dynamic mapping template (index级别)例: 1234567891011PUT /my_index&#123; &quot;mappings&quot;: &#123; &quot;_default_&quot;: &#123; &quot;_all&quot;: &#123; &quot;enabled&quot;: false &#125; &#125;, &quot;blog&quot;: &#123; &quot;_all&quot;: &#123; &quot;enabled&quot;: true &#125; &#125; &#125;&#125; 就是说默认的type的_all是禁用的, blog这个type的_all是启用的.]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-33-_mapping root object深入剖析]]></title>
    <url>%2F2018%2F11%2F26%2FElasticsearch-33-mapping-root-object%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[root object就是某个type对应的mapping json,包括了properties,metadata(_id,_source,_type), settings(analyzer),其他settings(比如include_in_all) 12345678PUT /index&#123; &quot;mappings&quot;: &#123; &quot;type&quot;: &#123; // 这里面的json就是这个type的 root object &#125; &#125;&#125; properties主要包括了各个field的数据类型,分不分词,用哪个分词器等. _source_source的好处: 查询的时候,可以直接拿到完整的document,不需要先拿到document id,再发送一起请求拿document. partial update是基于_source实现的. reindex时,直接基于_source实现,不需要从数据库(或者其他外部存储)查询数据再修改. 可以基于_source定制返回field. debug query更容易,因为可以直接看到_source. 如果不需要用到上面这些的话,可以禁用_source1234PUT /index/_mapping/type&#123; &quot;_source&quot;: &#123;&quot;enabled&quot;: false&#125;&#125; _all_all我们之前有详细介绍过,就是将所有field打包在一起,作为一个_all field,建立索引,没指定任何field进行搜索的时候,就是使用_all field在搜索.当然如果不需要用到的话也可以设置关闭1234PUT /index/_mapping/type&#123; &quot;_all&quot;: &#123;&quot;enabled&quot;: false&#125;&#125; 也可以在field级别设置include_in_all field,设置是否将filed的值包含在_all中123456789PUT /index/_mapping/type&#123; &quot;properties&quot;: &#123; &quot;field&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;include_in_all&quot;: false &#125; &#125;&#125; 标识性metadata包括_index,_type,_id]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-32-type底层数据结构]]></title>
    <url>%2F2018%2F11%2F26%2FElasticsearch-32-type%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[type的底层数据结构type是一个index中用来区分类似的数据的,类似的数据有可能有不同的field,而且有不同的属性来控制索引的建立 es是基于Lucene的,在es中每个field都有自己的数据类型,比如date,text等,但在底层的Lucene建立索引的时候,全部是opaque bytes类型,不区分类型的. Lucene是没有type的概念的,在document中,实际上是将type作为document的一个field类存储,即_type,es通过_type来进行过滤和筛选 一个index中的多个type,实际上是放在一起存储的,因此一个index下,不能有多个type重名. 举例说明现在,在ecommerce这个index下,有两个type,一个是elactronic_goods,另一个是fresh_goods,如下:1234567891011121314151617181920212223242526272829303132&#123; &quot;ecommerce&quot;: &#123; &quot;mappings&quot;: &#123; &quot;elactronic_goods&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &#125;, &quot;price&quot;: &#123; &quot;type&quot;: &quot;double&quot; &#125;, &quot;service_period&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125; &#125; &#125;, &quot;fresh_goods&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &#125;, &quot;price&quot;: &#123; &quot;type&quot;: &quot;double&quot; &#125;, &quot;eat_period&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125; &#125; &#125; &#125; &#125;&#125; 我们现在有两个document,分别是两个type下的数据,如下123456// type是elactronic_goods&#123; &quot;name&quot;: &quot;geli kongtiao&quot;, &quot;price&quot;: 1999.0, &quot;service_period&quot;: &quot;one year&quot;&#125; 123456// type是fresh_goods&#123; &quot;name&quot;: &quot;aozhou dalongxia&quot;, &quot;price&quot;: 199.0, &quot;eat_period&quot;: &quot;one week&quot;&#125; 这个index的底层存储是这样的:12345678910111213141516171819202122&#123; &quot;ecommerce&quot;: &#123; &quot;mappings&quot;: &#123; &quot;_type&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125; &quot;price&quot;: &#123; &quot;type&quot;: &quot;double&quot; &#125; &quot;service_period&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125; &quot;eat_period&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125; &#125; &#125;&#125; 可以看到type被当做了一个属性放到了document中, elactronic_goods,fresh_goods这两个type中不同的属性也被放到了一起 这两个document在底层的存储是:1234567&#123; &quot;_type&quot;: &quot;elactronic_goods&quot;, &quot;name&quot;: &quot;geli kongtiao&quot;, &quot;price&quot;: 1999.0, &quot;service_period&quot;: &quot;one year&quot;, &quot;eat_period&quot;: &quot;&quot;&#125; 1234567&#123; &quot;_type&quot;: &quot;fresh_goods&quot;, &quot;name&quot;: &quot;aozhou dalongxia&quot;, &quot;price&quot;: 199.0, &quot;service_period&quot;: &quot;&quot;, &quot;eat_period&quot;: &quot;one week&quot;&#125; 所以说,将类似结构的type放在一个index下,这些type应该有多个field是相同的. 假如说,两个type的field完全不同,放在一个index下,那么每条数据的很多field在底层的Lucene中是空置,会有严重的性能问题]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-31-手动创建索引以及定制分词器]]></title>
    <url>%2F2018%2F11%2F24%2FElasticsearch-31-%E6%89%8B%E5%8A%A8%E5%88%9B%E5%BB%BA%E7%B4%A2%E5%BC%95%E4%BB%A5%E5%8F%8A%E5%AE%9A%E5%88%B6%E5%88%86%E8%AF%8D%E5%99%A8%2F</url>
    <content type="text"><![CDATA[索引创建索引语法:1234567891011121314PUT /index&#123; &quot;settings&quot;:&#123; // any settings... &#125;, &quot;mappings&quot;:&#123; type1:&#123; // any settings... &#125;, type2:&#123; // any settings... &#125; &#125;&#125; 示例: 12345678910111213141516PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1, // primary shard的数量 &quot;number_of_replicas&quot;: 0 // replica shard 的数量 &#125;, &quot;mappings&quot;: &#123; &quot;my_type&quot;:&#123; &quot;properties&quot;: &#123; &quot;field1&quot;:&#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; &#125;&#125; 修改索引语法:1234PUT /index/_settings&#123; // any settings&#125; 示例:1234PUT /my_index/_settings&#123; &quot;number_of_replicas&quot;: 1 // 修改replica shard 的数量&#125; 删除索引1234DELETE /index DELETE /index1,index2DELETE /index_* // 通配符删除DELETE /_all // 删除全部 在elasticsearch.yml中设置action.destructive_requires_name: true,以后就不能使用 _all删除全部了 分词器修改分词器之前我们说过,es默认的分词器就是standard,他做了以下几件事:standard tokenizer:以单词边界进行切分standard token filter:什么都不做lowercase token filter:将所有字母转换为小写stop token filer(默认被禁用):移除停用词,比如a the it等等 我们先来新建一个索引,并启用english stop token filer12345678910111213PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; // 分词器相关 &quot;analyzer&quot;: &#123; // 分词器 &quot;es_std&quot;:&#123; // 自定义名称 &quot;type&quot;:&quot;standard&quot;, // 分词器类型 &quot;stopwords&quot;:&quot;_english_&quot; &#125; &#125; &#125; &#125;&#125; 执行成功后我们用之前说的测试分词器的方法来测试一下 12345GET /my_index/_analyze&#123; &quot;analyzer&quot;: &quot;es_std&quot;, // 我们上面定义的分词器名称 &quot;text&quot;: &quot;a dog is in the house&quot;&#125; 返回结果:123456789101112131415161718&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;dog&quot;, &quot;start_offset&quot;: 2, &quot;end_offset&quot;: 5, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;house&quot;, &quot;start_offset&quot;: 16, &quot;end_offset&quot;: 21, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 5 &#125; ]&#125; 可以看到停用词已经被去掉了 定制自己的分词器我们先把创建的这个索引删除掉1DELETE /my_index 然后手动定制分词器12345678910111213141516171819202122232425262728PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;char_filter&quot;: &#123; // 字符转换 &quot;&amp;_to_and&quot;:&#123; &quot;type&quot;:&quot;mapping&quot;, &quot;mappings&quot;:[&quot;&amp; =&gt; and &quot;] //&amp; 转成 and &#125; &#125;, &quot;filter&quot;: &#123; &quot;my_stop_words&quot;:&#123; // 自定义停用词过滤 &quot;type&quot;:&quot;stop&quot;, &quot;stopwords&quot;:[&quot;the&quot;,&quot;a&quot;] // 要过滤的词 &#125; &#125;, &quot;analyzer&quot;: &#123; &quot;my_analyzer&quot;:&#123; // 自定义名称 &quot;type&quot;:&quot;custom&quot;, &quot;char_filter&quot;:[&quot;html_strip&quot;,&quot;&amp;_to_and&quot;], // html脚本过滤和上面定义的&amp;_to_and &quot;tokenizer&quot;:&quot;standard&quot;, &quot;filter&quot;:[&quot;lowercase&quot;,&quot;my_stop_words&quot;] // 大小写转换 和 上面定义的停用词过滤 &#125; &#125; &#125; &#125;&#125; 执行完毕后来测试一下 12345GET /my_index/_analyze&#123; &quot;analyzer&quot;: &quot;my_analyzer&quot;, &quot;text&quot;: &quot;tom&amp;jerry are a friend in the house, &lt;a&gt;, HAHA!!&quot;&#125; 返回值:12345678910111213141516171819202122232425262728293031323334353637383940414243444546&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;tomandjerry&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 9, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;are&quot;, &quot;start_offset&quot;: 10, &quot;end_offset&quot;: 13, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;friend&quot;, &quot;start_offset&quot;: 16, &quot;end_offset&quot;: 22, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 3 &#125;, &#123; &quot;token&quot;: &quot;in&quot;, &quot;start_offset&quot;: 23, &quot;end_offset&quot;: 25, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 4 &#125;, &#123; &quot;token&quot;: &quot;house&quot;, &quot;start_offset&quot;: 30, &quot;end_offset&quot;: 35, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 6 &#125;, &#123; &quot;token&quot;: &quot;haha&quot;, &quot;start_offset&quot;: 42, &quot;end_offset&quot;: 46, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 7 &#125; ]&#125; a the 这两个停用词被去掉了,&amp;也转为and了,a标签被过滤掉,最后的大写也转成了小写 使用自定义分词器上面我们自定义的分词器已经可以使用了,那么如何让type中的某个filed来使用我们自定义的分词器123456789PUT /my_index/_mapping/my_type &#123; &quot;properties&quot;: &#123; &quot;content&quot;:&#123; // field名称 &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;my_analyzer&quot; // 分词器名称 &#125; &#125;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-30-scroll滚动查询]]></title>
    <url>%2F2018%2F11%2F24%2FElasticsearch-30-scroll%E6%BB%9A%E5%8A%A8%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[scroll查询如果我们要一次性查询10万条数据,那么性能会很差,此时一般会采用scroll滚动查询,一批一批的查,直到所有的数据都查询处理完成. 使用scroll滚动搜索,可以先搜索一批数据,然后下次再搜索一批数据,以此类推,直到搜索出全部的数据来 scroll搜索会在第一次搜索的时候,保存一个当前识图的快照,之后只会基于该旧的视图快照提供数据搜索,如果这个期间数据变更是不会让用户看到的. scroll搜索一般不会用_score相关分数去排序, 采用基于 _doc进行排序,性能比较高. 每次发送scroll请求,我们还需要指定一个scroll参数,指定一个时间窗口,每次搜索请求只要在这个时间窗口内能完成就可以了 示例在test_index/test_type下一共有5条数据,然后使用scroll滚动搜索,每次查询2条12345678910GET test_index/test_type/_search?scroll=1m&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &quot;_doc&quot; ], &quot;size&quot;: 2&#125; 返回值:1234567891011121314151617181920212223242526272829303132333435363738394041&#123; &quot;_scroll_id&quot;: &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAAJcFmY1N3VWOTF4U19HUlRRUzJIbzgxcmcAAAAAAAACWBZmNTd1VjkxeFNfR1JUUVMySG84MXJnAAAAAAAAAlsWZjU3dVY5MXhTX0dSVFFTMkhvODFyZwAAAAAAAAJaFmY1N3VWOTF4U19HUlRRUzJIbzgxcmcAAAAAAAACWRZmNTd1VjkxeFNfR1JUUVMySG84MXJn&quot;, &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 5, &quot;max_score&quot;: null, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;AWccvc7blcpuqacodv57&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;test_content&quot;: &quot;test1&quot;, &quot;test_title&quot;: &quot;test2&quot; &#125;, &quot;sort&quot;: [ 0 ] &#125;, &#123; &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;AWcctb8Zlcpuqacodv55&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;test_content&quot;: &quot;test1&quot; &#125;, &quot;sort&quot;: [ 0 ] &#125; ] &#125;&#125; 可以看到,返回值中有一个scroll_id,下次请求时要把这个scroll_id传过去,而且要在上次查询传过去的时间窗口scroll=1m,这个时间内进行第二次查询 GET /_search/scroll { &quot;scroll&quot;:&quot;1m&quot;, &quot;scroll_id&quot;:&quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAAJcFmY1N3VWOTF4U19HUlRRUzJIbzgxcmcAAAAAAAACWBZmNTd1VjkxeFNfR1JUUVMySG84MXJnAAAAAAAAAlsWZjU3dVY5MXhTX0dSVFFTMkhvODFyZwAAAAAAAAJaFmY1N3VWOTF4U19HUlRRUzJIbzgxcmcAAAAAAAACWRZmNTd1VjkxeFNfR1JUUVMySG84MXJn&quot; } scroll查询看起来挺像分页的,但是其实使用场景不一样,分页主要是用来一页一页搜索,给用户看的,scroll查询主要是用来一批一批检索数据的,让系统进行处理]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-29-搜索原理内核解析]]></title>
    <url>%2F2018%2F11%2F24%2FElasticsearch-29-%E6%90%9C%E7%B4%A2%E5%8E%9F%E7%90%86%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[query phase假设我们有一个index里面的数据分布在3个primary shard上(对应的replica也有),现在总共有7个shard,我们现在要搜索这个index中的数据的第10000条到10010条.如图所示 请求发送给某一个shard时,这个shard就是coordinate node, coordinate node会构建一个 priority queue,队列长度是查询时的from和size的和,默认是0 + 10 = 10; 我们要查询的是10000-10010条数据,所以请求的from = 9999,size = 10,这个时候coordinate node会在它本地建立一个长度是 9999 + 10 = 10009 的 priority queue, 然后coordinate node将请求打到其他的shard上去 接收到请求的每个shard,也会在本地建立一个 from + size大小的priority queue,每个shard将自己下标是0 - 10009的数据放到这个队列中, 也就是10010条数据,返回给coordinate node. coordinate node 将返回的所有数据进行合并,合并成一份from * size大小的priority queue,全局排序后,放到自己队列中去 最后在自己的队列中取出当前要获取的那一页的数据. 这里也可以看出我们之前提到过的deep paging问题,就是说,from * size分页太深,那么每个shard都要返回大量的数据给coordinate node,消耗大量的带宽,内存, CPU fetch phase在上面的query phase的工作处理完成之后,coordinate node 在priority queue里面找到了需要的数据, 但是其实这个队列时存的document的id, 这个时候,coordinate node就发送mget请求(批量查询)到所有shard上去获取对应的document 然后各个shard将document返回给coordinate node, coordinate node将合并后的document结果返回给客户端 bouncing results问题比如说有两个document,field值相同;但是分布在不同的shard上面,在不同的shard上可能排序也不相同, 每次请求轮询打到不同的shard上去,页面上看到的搜索结果的排序可能都不一样, 这就是 bouncing results,也就是跳跃的结果. preferencepreference 决定了哪些shard会执行搜索请求. bouncing results问题解决将preference设置为一个字符串,比如说user_id,让每个user每次搜索的时候,都使用同一个shard去执行,就不会看到bouncing results了]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-28-doc values初步了解]]></title>
    <url>%2F2018%2F11%2F23%2FElasticsearch-28-doc-values%E5%88%9D%E6%AD%A5%E4%BA%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[doc value搜索的时候,要依靠倒排索引去搜索,但是在排序的时候需要依靠正排索引,找到每个document的每个field,然后进行排序,所谓的正排索引,其实就是doc values es在建立索引的时候,一方面会建立倒排索引,以供搜索使用;一方面还会建立正排索引,也就是doc values,以供排序,聚合,过滤等操作使用 doc values是被保存在磁盘上的,此时如果内存足够,os会自动将其缓存在内存中,性能还是很高的,如果内存不够,os会将其写入到磁盘上 举例比如有两个document,数据如下document1: { “name”: “jack”, “age”: 27 }document2: { “name”: “tom”, “age”: 30 } 在es建立正排索引的时候就是这样子的 document name age document1 jack 27 document2 ton 30 这样在排序的时候es直接拿到正排索引里面的某一列去排序就好了]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-27-搜索相关度TF&IDF算法]]></title>
    <url>%2F2018%2F11%2F23%2FElasticsearch-27-%E6%90%9C%E7%B4%A2%E7%9B%B8%E5%85%B3%E5%BA%A6TF-IDF%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[算法介绍relevance score算法,简单来说就是计算出一个索引中的文本,与搜索文本,他们之间的关联匹配程度 Elasticsearch使用的是 term frequency/inverse document frequency算法，简称为TF/IDF算法 TF算法(Term frequency)Term frequency:搜索文本中的各个词条在field文本中出现了多少次,出现次数越多就越相关 举个例子:搜索请求是: hello worlddocument1:hello you, and world is very gooddocument2:hello, how are you hello和world这两个词在document1中出现了两次,document2中出现了一次,所以document更相关 IDF算法(inverse document frequency)inverse document frequency: 搜索文本中的各个词条在整个索引的所有document中出现了多少次,出现的次数越多,就越不相关 举例:搜索请求是:hello worlddocument1:hello, today is very gooddocument2:hi world, how are you 看起来hello和world是每个document都出先一次,但是这个应该是document2更相关 比如说在index中现在有一万条document,hello这个单词在所有的document中出现了1000次,world这个单词在所有的document中出现了100次,所以document2就更相关 Field-length normField-length norm: field的值长度越长,相关度越弱 举例:搜索请求:hello worlddocument1: { “title”: “hello article”, “content”: “babaaba…..(1万个单词)” }document2: { “title”: “my article”, “content”: “blablabala…. (1万个单词),hi world” }这个时候hello 和 world这两个词在整个index中出现的次数是一样多的,但是document1更相关,因为title这个filed中的数据短 查询_score是如何被计算出来的语法:12345678GET /index/type/_search?explain&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;field&quot;: &quot;text&quot; &#125; &#125;&#125; 分析一个document是如何被匹配上的语法:12345678GET /index/type/id/_explain&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;field&quot;: &quot;text&quot; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-26-字符串排序问题及解决方案]]></title>
    <url>%2F2018%2F11%2F23%2FElasticsearch-26-%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%8E%92%E5%BA%8F%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[字符串排序问题如果对一个string类型的field进行排序,结果往往不准确,因为string类型的field要进行分词,分词后是多个单词,再排序就不是我们想要的结果了 如何解决通常解决方式是,将一个string类型的field建立两次索引,一个分词用来进行搜索,一个不分词用来排序 示例我们之前建立过一个website的索引,先把它删除掉1DELETE /website 然后重新建立索引并手动创建mapping.12345678910111213141516171819202122232425262728PUT /website&#123; &quot;mappings&quot;: &#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; // 这里是重点,title里面在建立一个 string类型的field &quot;raw&quot;:&#123; // 名称 &quot;type&quot;: &quot;string&quot;, // 数据类型,不分词只能是string &quot;index&quot;: &quot;not_analyzed&quot; // 指定不分词 &#125; &#125;, &quot;fielddata&quot;: true // 建立正排索引,这个后面详细说 &#125;, &quot;content&quot;:&#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;post_date&quot;:&#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;author_id&quot;:&#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125; &#125;&#125; 建立好之后,往里面添加点数据 1234567891011121314151617181920212223PUT /website/article/1&#123; &quot;title&quot;: &quot;second article&quot;, &quot;content&quot;: &quot;this is my second article&quot;, &quot;post_date&quot;: &quot;2017-02-01&quot;, &quot;author_id&quot;: 110&#125;PUT /website/article/2&#123; &quot;title&quot;: &quot;first article&quot;, &quot;content&quot;: &quot;this is my frist article&quot;, &quot;post_date&quot;: &quot;2017-01-01&quot;, &quot;author_id&quot;: 110&#125;PUT /website/article/3&#123; &quot;title&quot;: &quot;third article&quot;, &quot;content&quot;: &quot;this is my third article&quot;, &quot;post_date&quot;: &quot;2017-03-01&quot;, &quot;author_id&quot;: 110&#125; 数据添加完成,我们来查询按照title排序一下12345678910111213GET /website/article/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;title&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ]&#125; 返回结果:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&#123; &quot;took&quot;: 1, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: null, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;third article&quot;, &quot;content&quot;: &quot;this is my third article&quot;, &quot;post_date&quot;: &quot;2017-03-01&quot;, &quot;author_id&quot;: 110 &#125;, &quot;sort&quot;: [ &quot;third&quot; ] &#125;, &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;second article&quot;, &quot;content&quot;: &quot;this is my second article&quot;, &quot;post_date&quot;: &quot;2017-02-01&quot;, &quot;author_id&quot;: 110 &#125;, &quot;sort&quot;: [ &quot;second&quot; ] &#125;, &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;first article&quot;, &quot;content&quot;: &quot;this is my frist article&quot;, &quot;post_date&quot;: &quot;2017-01-01&quot;, &quot;author_id&quot;: 110 &#125;, &quot;sort&quot;: [ &quot;first&quot; ] &#125; ] &#125;&#125; 可以看到 返回值中的sort这一列,是按照分词之后进行排序的,然后用我们上面创建出来title.raw来进行排序看下效果12345678910111213GET /website/article/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;title.raw&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ]&#125; 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&#123; &quot;took&quot;: 1, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: null, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;third article&quot;, &quot;content&quot;: &quot;this is my third article&quot;, &quot;post_date&quot;: &quot;2017-03-01&quot;, &quot;author_id&quot;: 110 &#125;, &quot;sort&quot;: [ &quot;third article&quot; ] &#125;, &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;second article&quot;, &quot;content&quot;: &quot;this is my second article&quot;, &quot;post_date&quot;: &quot;2017-02-01&quot;, &quot;author_id&quot;: 110 &#125;, &quot;sort&quot;: [ &quot;second article&quot; ] &#125;, &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;first article&quot;, &quot;content&quot;: &quot;this is my frist article&quot;, &quot;post_date&quot;: &quot;2017-01-01&quot;, &quot;author_id&quot;: 110 &#125;, &quot;sort&quot;: [ &quot;first article&quot; ] &#125; ] &#125;&#125; 再来看一下返回值中的sort ,这样排序就没有分词而是直接去排序的]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-25-Query DSL常用查询]]></title>
    <url>%2F2018%2F11%2F23%2FElasticsearch-25-Query-DSL%E5%B8%B8%E7%94%A8%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[Query DSL的常用的几种查询语法match all查询查询全部123456GET /index/type/_search&#123; &quot;query&quot;:&#123; &quot;match_all&quot;:&#123;&#125; &#125;&#125; match查询指定field搜索条件查询, 搜索的关键词会被分词12345678GET /_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;field&quot;: &quot;text&quot; &#125; &#125;&#125; multi match搜索条件在多个field上进行查询, 搜索条件也会被分词123456789GET /_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;text&quot;, &quot;fields&quot;: [&quot;field1&quot;,&quot;field2&quot;] &#125; &#125;&#125; range query在区间范围内查询1234567891011GET /_search&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;field&quot;: &#123; &quot;gte&quot;: 0, &quot;lte&quot;: 10 &#125; &#125; &#125;&#125; term query搜索条件不去进行分词查询,同样的,只能查询设置为不分词的field12345678GET /_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;field&quot;: &quot;text&quot; &#125; &#125;&#125; terms query一个field 去匹配多个值1234567891011GET /_search&#123; &quot;query&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: [ &quot;text1&quot;, &quot;text2&quot; ] &#125; &#125;&#125; filter之前有说过filter是不计算相关度分数的,就是直接把符合条件的数据筛选出来如果只用filter过滤的话 需要加”constant_score” 例:123456789101112131415GET _search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;field&quot;: &#123; &quot;gte&quot;: 10, &quot;lte&quot;: 20 &#125; &#125; &#125; &#125; &#125;&#125; 定位搜索不合法的原因语法:1234GET index/type/_validate/query?explain&#123; // 搜索条件&#125; 示例我们先来写一个错误的查询来试一下12345678GET test_index/test_type/_validate/query?explain&#123; &quot;query&quot;: &#123; &quot;math&quot;: &#123; // 应该是match 写成了 math &quot;test_field&quot;: &quot;text&quot; &#125; &#125;&#125; 返回值:1234&#123; &quot;valid&quot;: false, &quot;error&quot;: &quot;org.elasticsearch.common.ParsingException: no [query] registered for [math]&quot;&#125; 再来试一个正确的12345678GET test_index/test_type/_validate/query?explain&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;test_field&quot;: &quot;text&quot; &#125; &#125;&#125; 返回值:123456789101112131415&#123; &quot;valid&quot;: true, &quot;_shards&quot;: &#123; &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;explanations&quot;: [ &#123; &quot;index&quot;: &quot;test_index&quot;, &quot;valid&quot;: true, &quot;explanation&quot;: &quot;+test_field:text #(#_type:test_type)&quot; &#125; ]&#125; 一般用在那种特别复杂庞大的搜索下，比如你一下子写了上百行的搜索，这个时候可以先用validate api去验证一下，搜索是否合法 定制排序规则默认情况下,es是按照_score去排序的,然后某些情况下,可能没有有用的 _score,比如说filter ,那么我们如何使用自己的排序规则呢? 语法:12345678910111213GET /index/type/_search&#123; &quot;query&quot;:&#123; ... &#125;, &quot;sort&quot;:[ &#123; &quot;field&quot;:&#123; &quot;order&quot;:&quot;desc&quot; &#125; &#125; ]&#125; 就是在query后面加一个sort来进行排序,指定用哪一个field,和升序还是降序]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-24-search api和Query DSL基本语法]]></title>
    <url>%2F2018%2F11%2F22%2FElasticsearch-24-search-api%E5%92%8CQuery-DSL%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[search api 的基本语法12GET /_search&#125;&#123; 12345GET /_search&#123; &quot;from&quot;:0, &quot;size&quot;:10&#125; 1GET /_search?from=0&amp;size=10 可以直接将参数拼接在url请求上,也可以放在request body中 在HTTP协议中,一般不允许GET请求带上reques body,但是因为GET请求更加适合描述查询数据的操作,因此还是这么用了,很多浏览器,或者是服务器,也都支持GET+request body模式,如果遇到不支持的场景，也可以用POST请求12345POST /_search&#123; &quot;from&quot;:0, &quot;size&quot;:10&#125; Query DSL基本语法:123456789101112131415&#123; QUERY_NAME: &#123; ARGUMENT: VALUE, ARGUMENT: VALUE,... &#125;&#125;&#123; QUERY_NAME: &#123; FIELD_NAME: &#123; ARGUMENT: VALUE, ARGUMENT: VALUE,... &#125; &#125;&#125; 示例12345678GET /test_index/test_type/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; // 查询条件 &quot;test_field&quot;: &quot;test&quot; &#125; &#125;&#125; 组合多个搜索条件示例我们先来添加几个document 用来进行搜索123456789101112131415161718192021PUT /query_index/query_type/1&#123; &quot;title&quot;: &quot;my elasticsearch article&quot;, &quot;content&quot;: &quot;es is very bad&quot;, &quot;author_id&quot;: 110&#125;PUT /query_index/query_type/2&#123; &quot;title&quot;: &quot;my hadoop article&quot;, &quot;content&quot;: &quot;hadoop is very bad&quot;, &quot;author_id&quot;: 111&#125;PUT /query_index/query_type/3&#123; &quot;title&quot;: &quot;my elasticsearch article&quot;, &quot;content&quot;: &quot;es is very goods&quot;, &quot;author_id&quot;: 111&#125; 然后我们制定一个搜索条件,比如 我们要查询 title必须包含 elasticsearch ,content 可以包含 elasticsearch 也可以不包含,author_id必须不为111 我们先来看一下数据: title必须包含 elasticsearch : id是2和3的数据都符合 content 可以包含 elasticsearch 也可以不包含: 2和3中都没有包含, author_id必须不为111: 3的id是111根据这几个条件来看搜索结果就是id为1的那一条数据,然后我们来组合搜索条件进行搜索 12345678910111213141516171819202122232425262728GET /query_index/query_type/_search&#123; &quot;query&quot;: &#123; // 查询 &quot;bool&quot;: &#123; // 组合查询条件 &quot;must&quot;: [ // 必须符合的条件 &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;elasticsearch&quot; &#125; &#125; ], &quot;should&quot;: [ // 可以符合,也可以不符合的条件 &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;elasticsearch&quot; &#125; &#125; ], &quot;must_not&quot;: [ // 必须不符合的条件 &#123; &quot;match&quot;: &#123; &quot;author_id&quot;: 111 &#125; &#125; ] &#125; &#125;&#125; 执行后的结果: 1234567891011121314151617181920212223242526&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.25316024, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;query_index&quot;, &quot;_type&quot;: &quot;query_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.25316024, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;my elasticsearch article&quot;, &quot;content&quot;: &quot;es is very bad&quot;, &quot;author_id&quot;: 110 &#125; &#125; ] &#125;&#125; 只返回了id是1的数据 query 与 filter示例我们现在有三条数据,如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&#123; &quot;_index&quot;: &quot;company&quot;, &quot;_type&quot;: &quot;emp&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;address&quot;: &#123; &quot;country&quot;: &quot;china&quot;, &quot;province&quot;: &quot;jiangsu&quot;, &quot;city&quot;: &quot;nanjing&quot; &#125;, &quot;name&quot;: &quot;tom&quot;, &quot;age&quot;: 30, &quot;join_date&quot;: &quot;2016-01-01&quot; &#125;&#125;,&#123; &quot;_index&quot;: &quot;company&quot;, &quot;_type&quot;: &quot;emp&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;jack&quot;, &quot;age&quot;: 27, &quot;join_date&quot;: &quot;2017-01-01&quot;, &quot;address&quot;: &#123; &quot;country&quot;: &quot;china&quot;, &quot;province&quot;: &quot;zhejiang&quot;, &quot;city&quot;: &quot;hangzhou&quot; &#125; &#125;&#125;,&#123; &quot;_index&quot;: &quot;company&quot;, &quot;_type&quot;: &quot;emp&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;address&quot;: &#123; &quot;country&quot;: &quot;china&quot;, &quot;province&quot;: &quot;shanxi&quot;, &quot;city&quot;: &quot;xian&quot; &#125;, &quot;name&quot;: &quot;marry&quot;, &quot;age&quot;: 35, &quot;join_date&quot;: &quot;2015-01-01&quot; &#125;&#125; 现在有一个搜索请求, 搜索年龄必须大于等于30,同时join_date必须是2016-01-01 我们来构造一个包含query和filter的搜索请求123456789101112131415161718192021GET /company/emp/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; // 组合搜索 &quot;must&quot;: [ // 必须满足的条件 &#123; &quot;match&quot;: &#123; &quot;join_date&quot;: &quot;2016-01-01&quot; &#125; &#125; ], &quot;filter&quot;: &#123; // 过滤器 &quot;range&quot;: &#123; &quot;age&quot;: &#123; &quot;gte&quot;: 30 &#125; &#125; &#125; &#125; &#125;&#125; 返回值: 12345678910111213141516171819202122232425262728293031&#123; &quot;took&quot;: 16, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;company&quot;, &quot;_type&quot;: &quot;emp&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;address&quot;: &#123; &quot;country&quot;: &quot;china&quot;, &quot;province&quot;: &quot;jiangsu&quot;, &quot;city&quot;: &quot;nanjing&quot; &#125;, &quot;name&quot;: &quot;tom&quot;, &quot;age&quot;: 30, &quot;join_date&quot;: &quot;2016-01-01&quot; &#125; &#125; ] &#125;&#125; 可以看到搜到了一条满足条件的数据. query 与 filter 对比 filter:仅仅只是按照搜索条件过滤出需要的数据而已,不计算任何相关度分数,对相关度没有任何影响. query: 会去计算每个document相对于搜索条件的相关度,并按照相关度进行排序. 一般来说,我们在搜索的时候需要将最匹配的数据先返回的时候,就用query,如果只是需要根据条件筛选出一些数据,不关注其相关度,就用filter 除非你的这些搜索条件,你希望越符合这些搜索条件的document越排在前面,那么这些搜索条件要放到query中去.如果你不希望一些搜索条件来影响你的document排序的话,那么就放在filter中即可 query 与 filter 性能filter不需要计算相关度分数进行排序,同时还有内置的cache,自动缓存最常使用的filter数据query相反,要计算相关度分数,按照分数进行排序,而且无法cache结果]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-23-_mapping复杂数据类型和object类型底层数据存储]]></title>
    <url>%2F2018%2F11%2F22%2FElasticsearch-23-mapping%E5%A4%8D%E6%9D%82%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%92%8Cobject%E7%B1%BB%E5%9E%8B%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[几种复杂的数据类型multivalue field比如数据是1&#123;&quot;tags&quot;:[&quot;tag1&quot;,&quot;tag2&quot;]&#125; 这种数据建立索引时,与string类型是一样的, 数组中的数据是不能混的,要放字符串都放字符串. empty field比如 null, [], [null]这样的数据 object field我们先来添加一个document1234567891011PUT /company/emp/1&#123; &quot;name&quot;:&quot;jack&quot;, &quot;age&quot;:27, &quot;join_date&quot;:&quot;2017-01-01&quot;, &quot;address&quot;:&#123; &quot;country&quot;:&quot;china&quot;, &quot;province&quot;:&quot;zhejiang&quot;, &quot;city&quot;:&quot;hangzhou&quot; &#125;&#125; 像上面这个address就是object类型的, 我们来看一下这个type的_mapping1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&#123; &quot;company&quot;: &#123; &quot;mappings&quot;: &#123; &quot;emp&quot;: &#123; &quot;properties&quot;: &#123; &quot;address&quot;: &#123; &quot;properties&quot;: &#123; &quot;city&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;country&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;province&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125;, &quot;age&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;join_date&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 可以看到 address中的每个field都有对应的type等 其实像我们刚才添加的这个document,它的数据在es底层是像这样存储的12345678&#123; &quot;name&quot;:[jack], &quot;age&quot;:[27], &quot;join_date&quot;:[2017-01-01], &quot;address.country&quot;:[china], &quot;address.province&quot;:[zhejiang], &quot;address.city&quot;:[hangzhou]&#125; 再比如有更复杂的数据,比如下面我们再加一个,数据是1234567&#123; &quot;authors&quot;: [ &#123; &quot;age&quot;: 26, &quot;name&quot;: &quot;Jack White&quot;&#125;, &#123; &quot;age&quot;: 55, &quot;name&quot;: &quot;Tom Jones&quot;&#125;, &#123; &quot;age&quot;: 39, &quot;name&quot;: &quot;Kitty Smith&quot;&#125; ]&#125; 像这种包含json数组的数据,底层会从横向转为列式存储,就像这样1234&#123; &quot;authors.age&quot;: [26, 55, 39], &quot;authors.name&quot;: [jack, white, tom, jones, kitty, smith]&#125; 就是一列存在一起]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-22-mapping详解]]></title>
    <url>%2F2018%2F11%2F22%2FElasticsearch-22-mapping%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[什么是mapping往es里面直接插入数据,es会自动建立索引,同时建立type以及对应的mapping. mapping中就定义了每个field的数据类型 不同的数据类型,可能有的是精确搜索(exact value),有的是全文检索(full text). exact value在建立倒排索引的时候,分词是将整个值一起作为一个关键词建立到倒排索引中的;而full text是会经过各种处理的,分词 normalization 才会建立到倒排索引中. 一个搜索过来的时候对exact value field或者是full text field进行搜索的行为也是不一样的,会跟建立倒排索引的行为保持一致;比如说exact value搜索的时候,就是直接按照整个值进行匹配;full text query string,也会进行分词和normalization再去倒排索引中去搜索 可以用es的dynamic mapping 让其自动建立mapping,包括自动设置数据类型,也可以提前手动创建index的type的mapping,自己对各自的field进行设置,包括数据类型,索引行为,分词器等等. 总结: mapping,就是index的type的元数据,每个type都有一个自己的mapping,决定了数据类型,建立倒排索引的行为,还有进行搜索的行为 核心数据类型mapping 下的核心数据类型有: 字符串: String整型: byte, short, integer, long浮点型: float, double布尔型: boolean日期类型: date dynamic mapping 数据类型映射 数据 映射后的数据类型 true/fasle boolean 123 long 123.45 double 2017-01-01 date “hello world” string/text 查询mapping语法:1GET /index/_mapping/type 手动建立mapping只能在创建index的时候手动建立mapping,或者新增field mapping, 不能修改 filed mapping 之间我们建立过一个website的index,我们先删掉.1DELETE /website 现在来手动建立这个索引,并手动创建mapping1234567891011121314151617181920212223242526PUT /website&#123; &quot;mappings&quot;: &#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;author_id&quot;:&#123; // field &quot;type&quot;: &quot;long&quot; // 类型 &#125;, &quot;title&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;english&quot; // 指定分词器 &#125;, &quot;content&quot;:&#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;post_date&quot;:&#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;publisher_id&quot;:&#123; &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; // 不分词,就是 exact value ,上面的类型一定要写string,否则不生效 &#125; &#125; &#125; &#125;&#125; analyzed:分词not_analyzed:不分词no:直接不建立到倒排索引里,也就是说 搜索不到 好了,创建完成,然后我们来尝试修改一下这个mapping.123456789101112PUT /website&#123; &quot;mappings&quot;: &#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;author_id&quot;:&#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; &#125;&#125; 返回值:1234567891011121314151617&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;index_already_exists_exception&quot;, &quot;reason&quot;: &quot;index [website/-6NKQPj3TPWDrrlxhalkmw] already exists&quot;, &quot;index_uuid&quot;: &quot;-6NKQPj3TPWDrrlxhalkmw&quot;, &quot;index&quot;: &quot;website&quot; &#125; ], &quot;type&quot;: &quot;index_already_exists_exception&quot;, &quot;reason&quot;: &quot;index [website/-6NKQPj3TPWDrrlxhalkmw] already exists&quot;, &quot;index_uuid&quot;: &quot;-6NKQPj3TPWDrrlxhalkmw&quot;, &quot;index&quot;: &quot;website&quot; &#125;, &quot;status&quot;: 400&#125; 运行后发现,报错了,原因就是建立好的field mapping是不能去修改的,但是我们可以新增一个field,并指定type等123456789PUT /website/_mapping/article&#123; &quot;properties&quot;: &#123; &quot;new_field&quot;:&#123; &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125; &#125;&#125; 返回值:123&#123; &quot;acknowledged&quot;: true&#125; 可以看到已经新增成功了. 测试mapping完成后我们测试一下分词的效果, content这个field是普通的text类型,我们来测试一下12345GET /website/_analyze&#123; &quot;field&quot;: &quot;content&quot;, &quot;text&quot;: &quot;my-dogs&quot;&#125; 返回值:123456789101112131415161718&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;my&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;dogs&quot;, &quot;start_offset&quot;: 3, &quot;end_offset&quot;: 7, &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;: 1 &#125; ]&#125; 可以看到my-dogs 被拆分为 my dogs,去掉了 - ,没有进行单复数转换等,因为默认的分词器就是standard analyzer(标准分词器) 再来试一下new_field,我们在设置的时候这个filed是不能分词的12345GET website/_analyze&#123; &quot;field&quot;: &quot;new_field&quot;, &quot;text&quot;: &quot;my dogs&quot;&#125; 返回值:12345678910111213&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;remote_transport_exception&quot;, &quot;reason&quot;: &quot;[f57uV91][127.0.0.1:9300][indices:admin/analyze[s]]&quot; &#125; ], &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Can&apos;t process field [new_field], Analysis requests are only supported on tokenized fields&quot; &#125;, &quot;status&quot;: 400&#125; 报错了,因为这个field是不能分词的.]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-21-query string分词和mapping案例遗留问题揭秘]]></title>
    <url>%2F2018%2F11%2F22%2FElasticsearch-21-query-string%E5%88%86%E8%AF%8D%E5%92%8Cmapping%E6%A1%88%E4%BE%8B%E9%81%97%E7%95%99%E9%97%AE%E9%A2%98%E6%8F%AD%E7%A7%98%2F</url>
    <content type="text"><![CDATA[query string 分词query string必须以和index建立时相同的analyzer进行分词. 比如,我们有一个document,其中有一个field,它的值是:hello you and me,建立倒排索引.我们要搜索这个document对应的index,搜索文本是hello me ,搜索请求就是:1GET /index/type/_search?q=field:hello me “hello me”就是query string,默认情况下,es会使用它对应的field建立倒排索引时相同的分词器进行分词和normalization,只有这样,才能实现正确的搜索. 举个例子,document在建立倒排索引的时候,会把dogs转为dog,然后我们在搜索的时候传一个dogs过去,就找不到了,所以搜索传过去的dogs也必须变为dog才能实现正确的搜索. mapping引入案例遗留问题揭秘这里有一个知识点: 不同类型的field,可能有的就是full text(全文检索),有的就是exact value(精确搜索) 在初始mapping中,我们引入了一个小案例,当时的查询结果是:1234GET /website/article/_search?q=2017 3条结果 GET /website/article/_search?q=2017-01-01 3条结果GET /website/article/_search?q=post_date:2017-01-01 1条结果GET /website/article/_search?q=post_date:2017 1条结果 首先看第一个查询,我们没有指定用哪一个field进行查询,那默认的就是 _all 查询,之前有说过 _all的话会把document中的所有field的值当做字符串拼接, _all搜索的时候是full text,要分词进行normalization后查询 我们来看一下第一个document中的数据:123456&#123; &quot;post_date&quot;: &quot;2017-01-01&quot;, &quot;title&quot;: &quot;my first article&quot;, &quot;content&quot;: &quot;this is my first article in this website&quot;, &quot;author_id&quot;: 11400&#125; 它的_all就是 “2017-01-01 my first article this is my first article in this website 11400” 三个document的 _all中分别有 2017-01-01 2017-01-02 2017-01-03 这个建立倒排索引就是 word document1 document2 document3 2017 √ √ √ 01 √ 02 √ 03 √ 这时候第一个搜索 _all 查询2017 肯定能查到3条第二个搜索请求的query string 会被分为 2017,01,01, 所以也能查到3条数据 然后是第三个请求,是指定post_date这个filed去查询, post_date 是个date类型的,而不是字符串类型, date类型的数据会按照exact value去建立索引 word document1 document2 document3 2017-01-01 √ 2017-01-02 √ 2017-01-03 √ 所以搜索第三个请求时可以搜索到1条结果. 按照上面的说法的话,第4个请求应该是搜索不到结果的,但是实际上有一条结果,这个在这里不讲解,因为是es 5.2以后做的一个优化 分词器测试12345GET /_analyze&#123; &quot;analyzer&quot;: &quot;standard&quot;, // 指定分词器 &quot;text&quot;: &quot;Text to analyze&quot; // 要拆分的文本&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-20-分词器详解]]></title>
    <url>%2F2018%2F11%2F22%2FElasticsearch-20-%E5%88%86%E8%AF%8D%E5%99%A8%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[什么是分词器作用: 拆分词语,进行normalization操作(提升recall召回率) 比如说,有一个句子,然后将这个句子拆分成一个一个的单词,同时对每个单词进行normalization(时态转换,单复数转换等等). recall召回率:简单来说就是搜索的时候,增加能够搜索到的结果的数量. 分词器一共做了三件事:character filter:在一段文本进行分词之前,先进行预处理,比如说过滤html标签(&lt;span&gt;123&lt;/span&gt; 转换为123), &amp; 转换为 andtokenizer:分词,比如 hello me 分为 hello 和 metoken filter:进行大小写转换,停用词去掉,近义词转换等normalization操作,比如 dogs –&gt; dog, liked –&gt; like, Tom –&gt; tom, a/an/the去掉,等等 分词器很重要,讲一段文本进行各种处理,最后处理好的结果才会拿去建立倒排索引. 内置分词器的介绍比如某个document中的某一个field的值是: Set the shape to semi-transparent by calling set_trans(5) standard analyzer(标准分词器) :将句子拆分为set, the, shape, to, semi, transparent, by, calling, set_trans, 5,做了大写转小写,-去除,()去除等操作 simple analyzer(简单分词器):拆分为set,the,shape,to,semi,transparent,by,calling,set,trans,可以看到做了大写转小写，-去除，(5)去除，_去除 等操作 whitespace analyzer(空格分词器)：Set,the,shape,semi-transparent,by,calling,set_trans(5), 简单的按照空格进行分词 language analyzer(语言分词器，比如说英语分词器)：set,shape,semi,transpar,call,set_tran,5,可以看到大写转小写,the没有任何含义,被干掉了,以及一些拆分的,transparent转换成了transpar,calling转化时态call,等等 默认的分词器是standard analyzer(标准分词器)]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-19-倒排索引核心原理]]></title>
    <url>%2F2018%2F11%2F21%2FElasticsearch-19-%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[场景假设我们现在有两个document. document1: I really liked my small dogs, and I think my mom also liked them. document2: He never liked any dogs, so I hope that my mom will not expect me to liked him. 第一步 分词,初步建立倒排索引两个document中的数据将会被分词,比如分成这样 word document1 document2 I √ √ really √ liked √ √ my √ √ small √ dogs √ and √ think √ mom √ √ also √ them √ He √ never √ any √ so √ hope √ that √ will √ not √ expect √ me √ to √ him √ 这个时候我们如果搜索 mother like little dog 的时候,不会有任何结果的先回对搜索条件拆词,拆分为motherlikelittledog 这个时候去上面的倒排索引去匹配,发现没有一个词是可以匹配的到的. 这显然不是我门想要的搜索结果 其实建立倒排索引的时候,还会做一件事,就是进行normalization标准化,包括时态转换，复数，同义词，大小写等,对拆出的各个单词进行相应的处理,以便后面搜索的时候能够搜索到相关联document的概率 进行normalization后的倒排索引: word document1 document2 normalization I √ √ really √ like √ √ liked – &gt;like my √ √ little √ small –&gt; little dog √ √ dogs –&gt; dog and √ think √ mom √ √ also √ them √ He √ never √ any √ so √ hope √ that √ will √ not √ expect √ me √ to √ him √ 这时候再按上面的搜索条件 mother like little dog 搜索,将搜索条件分词,进行normalization后mother –&gt; momlike –&gt; likelittle –&gt; littledog –&gt; dog 这时候拿关键词去匹配上面的倒排索引,就能把document1和document2都搜索出来]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-18-精确匹配与全文检索]]></title>
    <url>%2F2018%2F11%2F21%2FElasticsearch-18-%E7%B2%BE%E7%A1%AE%E5%8C%B9%E9%85%8D%E4%B8%8E%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[精确匹配(exact value)比如我们在之前的例子,通过精确匹配搜索 2017-01-01的时候, field中必须包含2017-01-01 才能搜索出来,如果只搜索个01,这样是搜不出来的 全文检索 (full text)不单纯的只是匹配一个完整的值,而是可以对值进行分词后进行匹配,还可以通过缩写 时态 大小写 同义词等进行匹配 缩写搜索比如查询 cn 可以将 China 搜索出来 格式转换比如查询 likes 可以将 like liked 搜索出来 大小写转换比如查询 tom 可以将 Tom 搜索出来 同义词搜索比如查询 love 它的同义词 like 也可以搜索出来]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-17-初识mapping]]></title>
    <url>%2F2018%2F11%2F21%2FElasticsearch-17-%E5%88%9D%E8%AF%86mapping%2F</url>
    <content type="text"><![CDATA[首先,我们先插入几条数据,让ES为我们自动建立一个索引1234567891011121314151617181920212223PUT /website/article/1&#123; &quot;post_date&quot;: &quot;2017-01-01&quot;, &quot;title&quot;: &quot;my first article&quot;, &quot;content&quot;: &quot;this is my first article in this website&quot;, &quot;author_id&quot;: 11400&#125;PUT /website/article/2&#123; &quot;post_date&quot;: &quot;2017-01-02&quot;, &quot;title&quot;: &quot;my second article&quot;, &quot;content&quot;: &quot;this is my second article in this website&quot;, &quot;author_id&quot;: 11400&#125;PUT /website/article/3&#123; &quot;post_date&quot;: &quot;2017-01-03&quot;, &quot;title&quot;: &quot;my third article&quot;, &quot;content&quot;: &quot;this is my third article in this website&quot;, &quot;author_id&quot;: 11400&#125; 对这些数据进行几次搜索_all 搜索 20171GET /website/article/_search?q=2017 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 0.28004453, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.28004453, &quot;_source&quot;: &#123; &quot;post_date&quot;: &quot;2017-01-02&quot;, &quot;title&quot;: &quot;my second article&quot;, &quot;content&quot;: &quot;this is my second article in this website&quot;, &quot;author_id&quot;: 11400 &#125; &#125;, &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.28004453, &quot;_source&quot;: &#123; &quot;post_date&quot;: &quot;2017-01-01&quot;, &quot;title&quot;: &quot;my first article&quot;, &quot;content&quot;: &quot;this is my first article in this website&quot;, &quot;author_id&quot;: 11400 &#125; &#125;, &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.28004453, &quot;_source&quot;: &#123; &quot;post_date&quot;: &quot;2017-01-03&quot;, &quot;title&quot;: &quot;my third article&quot;, &quot;content&quot;: &quot;this is my third article in this website&quot;, &quot;author_id&quot;: 11400 &#125; &#125; ] &#125;&#125; 一共查询出来3条结果 指定 post_date 搜索20171GET /website/article/_search?q=post_date:2017 返回值:123456789101112131415161718192021222324252627&#123; &quot;took&quot;: 1, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;post_date&quot;: &quot;2017-01-01&quot;, &quot;title&quot;: &quot;my first article&quot;, &quot;content&quot;: &quot;this is my first article in this website&quot;, &quot;author_id&quot;: 11400 &#125; &#125; ] &#125;&#125; 一共查询出来1条结果 _all 搜索 2017-01-011GET /website/article/_search?q=2017-01-01 返回值:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&#123; &quot;took&quot;: 9, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 3, &quot;max_score&quot;: 1.0566096, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1.0566096, &quot;_source&quot;: &#123; &quot;post_date&quot;: &quot;2017-01-01&quot;, &quot;title&quot;: &quot;my first article&quot;, &quot;content&quot;: &quot;this is my first article in this website&quot;, &quot;author_id&quot;: 11400 &#125; &#125;, &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.84013355, &quot;_source&quot;: &#123; &quot;post_date&quot;: &quot;2017-01-02&quot;, &quot;title&quot;: &quot;my second article&quot;, &quot;content&quot;: &quot;this is my second article in this website&quot;, &quot;author_id&quot;: 11400 &#125; &#125;, &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.84013355, &quot;_source&quot;: &#123; &quot;post_date&quot;: &quot;2017-01-03&quot;, &quot;title&quot;: &quot;my third article&quot;, &quot;content&quot;: &quot;this is my third article in this website&quot;, &quot;author_id&quot;: 11400 &#125; &#125; ] &#125;&#125; 一共查询出来3条结果 指定 post_date 搜索2017-01-011GET /website/article/_search?q=post_date:2017-01-01 返回值:123456789101112131415161718192021222324252627&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;post_date&quot;: &quot;2017-01-01&quot;, &quot;title&quot;: &quot;my first article&quot;, &quot;content&quot;: &quot;this is my first article in this website&quot;, &quot;author_id&quot;: 11400 &#125; &#125; ] &#125;&#125; 一共查询出来1条结果 总体结果:1234GET /website/article/_search?q=2017 3条结果 GET /website/article/_search?q=2017-01-01 3条结果GET /website/article/_search?q=post_date:2017-01-01 1条结果GET /website/article/_search?q=post_date:2017 1条结果 mapping 概念自动或手动为index中的type建立的一种数据结构相关的配置,简称为mappingdynamic mapping自动为我们建立index,创建type,以及type对应的mapping,mapping中包含了每个field对应的数据类型,以及如何分词等设置 查询mapping查询语法:1GET /index/_mapping/type 比如查询我们上面例子中的mapping1GET /website/_mapping/article 返回值:12345678910111213141516171819202122232425262728293031323334&#123; &quot;website&quot;: &#123; &quot;mappings&quot;: &#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;author_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;post_date&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 可以看到 里面包含了我们每个field的数据类型等信息. 搜索结果为什么不一致?因为es自动建立mapping的时候,设置了各个filed的数据类型,不同的数据类型的分词 搜索等行为是不一样的,所以出现了_all 搜索和指定field搜索时的数据不一致的情况]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-16-_search搜索详解]]></title>
    <url>%2F2018%2F11%2F21%2FElasticsearch-16-_search%E6%90%9C%E7%B4%A2%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[multi-index和multi-type搜索模式就是一次性搜索多个index和type下的数据 示例 搜索所有index,所有type下的所有数据 1GET /_search 指定一个index,搜索其下所有的type的数据 1GET /index/_search 查询某个index下指定的type的数据 1GET /index/type/_search 同时搜索多个index下的所有数据 1GET /index1,index2,index3,.../_search 按照通配符去匹配多个index 12GET /*1,*2/_search# 查询以 1 和 2 结尾的index 搜索一个index下指定的多个type的数据 1GET /index/type1,type2/_search 搜索多个index下的多个type的数据 1GET /index1,index2/type1,type2/_search 搜索所有index下的指定type的数据 1GET /_all/type1,type2/_search 搜索原理客户端发送一个请求,会把请求打到所有的primary shard上去执行,因为每个shard都包含部分数据,所以每个shard上都可能包含搜索请求的结果但是如果primary shard有replica shard,那么请求也可以打到replica shard上面 分页搜索查询时传入参数 size 和 from 即可 示例比如我们要查询movies/movie下的数据一共是6条,分三页查询12345678# 查询第一页GET /movies/movie/_search?size=2&amp;from=0# 查询第二页GET /movies/movie/_search?size=2&amp;from=2# 查询第三页GET /movies/movie/_search?size=2&amp;from=4 from 是从0开始的 deep paging问题什么是deep paging问题? 为什么会产生这个问题? 他的底层原理是什么? 场景,比如我们现在有4个shard 一共有60000条数据,在其中3个shard中,每个有20000条数据,这个时候要进行搜索第1000页的数据,每页显示10条,实际上这里拿到的是第10001~10010条数据 假设这个请求先打到一个不包含这些数据的节点上去,那么这个节点就是一个协调节点(coordinate node),然后这个协调节点会将请求转发到包含数据的节点上去,如图: 查询60000条数据中的第1000页,实际上每个shard都要将内部的20000条数据中的1000页,也就是10001~10010条的数据拿出来, 这时候实际上不是只返回这10条数据 是返回第一条到10010条数据, 3个shard都返回10010条数据给coordinate node,coordinate node 总共会受到30030条数据,然后进行排序,在这30030条数据中取到第10页,也就是这些数据中的第10001~10010条数据返回. 总的来说就是先要把所有shard上的数据集中起来排序后再去分页. 搜索过深的时候,就要在coordinate node上保存大量的数据,还要进行排序,排序之后,再取出对应的那一页,所以这个过程,既耗费网络带宽,耗费内存,还耗费CPU,影响性能,我们应该尽量避免出现这种deep paging的操作 query string其实就是在http请求中,把一些搜索的参数做为query string附加到url上面. 示例查询 /movies/movie 下title包含kill这个词的数据1GET /movies/movie/_search?q=title:kill 查询 /movies/movie 下title必须包含kill这个词的数据1GET /movies/movie/_search?q=+title:kill 查询/movies/movie 下title不包含kill这个词的数据1GET /movies/movie/_search?q=-title:kill 其实第一个和第二个的作用是差不多的,主要是 “+” 和 “-“ 的区别 一个是必须包含,一个是不包含 _all metadata 原理和作用查询所有filed下包含kill的数据1GET /movies/movie/_search?q=kill 上面这个查询中并没有指定具体是哪个field包含kill这个词,是直接搜索的所有field的.当我们添加一个document的时候,它里面包含了多个field,此时es会自动将多个field的值,用字符串的方式串联起来,变成一个长字符串,作为_all 的值,同时对 _all进行分词建立索引.当我们搜索没有指定具体哪一个field的时候,就默认对_all 进行搜索,其实它里面就包含了所有field的值 举例说明我们新添加一个document,内容是123456&#123; &quot;name&quot;:&quot;jack&quot;, &quot;age&quot;:26, &quot;email&quot; : &quot;jack@sina.com&quot;, &quot;address&quot;:&quot;hangzhou&quot;&#125; “jack 26 jack@sina.com hangzhou”,就作为这一条document的_all元数据的值,同时进行分词后建立对应的倒排索引]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-15-写一致性原理及quorum机制]]></title>
    <url>%2F2018%2F11%2F20%2FElasticsearch-15-%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7%E5%8E%9F%E7%90%86%E5%8F%8Aquorum%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[概念我们在发送任何一个增删改操作的时候,都可以带上一个consistency参数,指明我们想要的写一致性是什么,比如:1PUT /index/type/id?consistency= consistency的值可以有三个: one:要求这个写操作,只要有一个primary shard是active活跃可用的状态就可以执行. all:要求这个写操作,必须所有的primary shard和replica shard都是active活跃的,才可以执行 quorum:默认的值,要求所有的shard中,必须大部分的shard都是active活跃的,才可以执行. 那么怎么算大部分shard都是活跃的呢,es有一个计算的公式 quorum机制前置条件当replica shard 的数量大于1的时候才会生效 计算公式12quorum = int((number_of_priamry_shard + number_of_replica_shard) / 2) + 1# 这里的number_of_replica_shard是相对于 primary shard的数量 举例说明如果现在有3个primary shard, number_of_replica_shard = 3, 也就是说一共有3 + 3 * 3 = 12个shard,根据公式, int((3 + 3) / 2) + 1 = 4 意思就是说,在所有的12个shard中必须是有4个shard是active状态的才可以执行写操作 如果节点的数量少于quorum数量,可能导致quorum不齐全,进而导致无法执行任何写操作 特殊场景处理如果说我们就一个primary shard,replica = 1,此时总共就两个shard,按照公式int((1 + 1) / 2) + 1 = 2,此时要求两个shard都是活跃的才行,但是我们可能就有一个节点,只能有一个primary shard,那么这个情况下是不是就无法写入了呢? es提供了一种特殊的处理场景,就是说当number_of_replicas &gt; 1的时候才生效,如果没有这种特殊处理的话,单节点集群就无法正常工作. 超时等待当quorum不齐全时, 会默认等待一分钟,等待期间,期望活跃的shard数量可以增加,时间到了还不能添加的话就超时.我们也可以在写操作的时候,加一个timeout的参数,比如1PUT /index/type/id?timeout=30ms 就是说我们自己去设定当quorum不齐全的时候es的timeout时长.]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-14-document增删改查原理]]></title>
    <url>%2F2018%2F11%2F20%2FElasticsearch-14-document%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[熟悉了es路由规则以后,再来看一下document的增删改查的原理. 场景现在有3个es节点在一个集群内, 有3个primary shard 对应的replica数量是1 就是说有3个 primary shard 和3个replica,总共6个shard, 现在有个客户端要创建一个document到es中 增删改操作客户端随便选择一个node,然后将请求发送到node上去, 因为任意一个node都知道document应该存在哪个shard上,所以请求发给哪一个都是一样的. 比如图中,请求节点到达node1了,那node就成为了coordinate node(协调节点),协调节点通过我们之前说到的路由公式,来计算这个document应该在哪个shard上面,然后将请求发送到这个节点上去, 比如应该放到shard2 中, shard2在自己本地创建document,创建倒排索引,创建完毕后会将数据同步到他对应的replica上去 写入完毕 同步数据到replica完成后,通知协调节点,然后协调节点返回响应给客户端 查询操作客户端先发送一个查询请求到任意的一个node上去 如图,请求发到node1 上去后node1就成为了coordinate node(协调节点) ,协调节点对document进行路由,路由之后就知道document在哪个primary shard上了,对于读请求,不一定就将请求转发到primary shard上去,因为replica shard也可以承担读请求的 es采用round-robin随机轮询算法在primary shard以及其所有replica中随机选择一个，让读请求负载均衡. 然后对应的shard去查询,将查询结果返回给协调节点,协调节点最后响应给客户端 特殊情况如果查询的document还在建立索引的过程中,这个时候只有primary shard上有,任何一个replica shard上都没有,此时如果请求路由到replica shard上可能会导致无法读取到document,但是document完成索引建立 将数据同步到replica shard上之后,就都有了]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-13-document路由原理]]></title>
    <url>%2F2018%2F11%2F20%2FElasticsearch-13-document%E8%B7%AF%E7%94%B1%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[概念我们知道,一个index的数据会被分为多片,每片都放在一个shard中.所以,一个document只能存在于一个shard中.当客户端创建document的时候,es此时就要决定这个document是要放在哪一个shard中. 这个过程就被是document routing 数据路由 路由算法12shard = hash(routing) % number_of_primary_shards# routing的哈希值 除以 primary shard的数量,然后取余. 每次增删改查一个document的时候,都会带过来一个routing number,默认的就是这个document的id(可以是手动指定,也可以是自动生成). 举个例子,现在有一个index,有3个shard P0 P1 P2,假设routing = _id, _id = 1, es会将这个routing值传入一个hash函数中,返回一个routing值的hash值, 假如hash(routing) = 21, 然后将hash函数产出的值对这个index的primary shard的数量求余数, 21 % 3 = 0,就决定了这个document应该放在P0上面. 决定一个document在哪个shard上,最重要的一个值就是routing值,默认是id也可以手动指定,相同的routing值,每次过来,从hash函数中产出的hash值一定是相同的 无论hash值是多少,无论是什么数字,对number_of_primary_shard求余数,结果一定是在0~number_of_primary_shard-1 这个范围之内的. 再来想想我们之前说过,primary shard的数量在创建完index之后是不能去修改的,还是上面那个例子,数据被放到了P0中,如果现在加了一个primary shard的数量会怎么样呢.我们去查询这个document的时候,_id = 1, hash值还是21, 21 % 4 = 1,计算结果是数据在P1上面,但是实际在P0上,就会间接导致数据的丢失. routing值的作用和如何手动指定默认的routing就是_id,也可以在发送请求的时候,手动指定一个routing ,如下:1PUT /index/type/id?routing=user_id 手动指定routing是很有用的,可以保证说,某一类的document一定被路由到一个shard上去,那么在后续进行应用级别的负载均衡,以及提升批量读取的性能的时候,是很有帮助的]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-12-批量增删改查操作]]></title>
    <url>%2F2018%2F11%2F19%2FElasticsearch-12-%E6%89%B9%E9%87%8F%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[批量查询批量查询的优点: 如果用 GET /index_name/type_name/id 这样的查询去查询100条数据的话,就要发送100次网络请求,开销比较大,如果是批量查询的话,查询100条数据只需要用1次网络请求,网络请求的性能开销会很少. _mget批量查询比如我们要查询test_index/test_type下面 id是1和2的两条数据执行代码:123456789101112131415GET /_mget&#123; &quot;docs&quot;:[ &#123; &quot;_index&quot;:&quot;test_index&quot;, // 索引名 &quot;_type&quot;:&quot;test_type&quot;, // 类型名 &quot;_id&quot;:1 // id &#125;, &#123; &quot;_index&quot;:&quot;test_index&quot;, &quot;_type&quot;:&quot;test_type&quot;, &quot;_id&quot;:2 &#125; ] &#125; 返回值:12345678910111213141516171819202122232425&#123; &quot;docs&quot;: [ &#123; &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;test_content&quot;: &quot;test content&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_version&quot;: 2, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;num&quot;: 1, &quot;tags&quot;: [] &#125; &#125; ]&#125; GET请求后面直接跟_mget端点, 将参数封装在一个docs数组里面,数组中的一个元素就是一个请求条件, 执行后返回值也封装在了一个docs数组中,一个元素对应一个返回结果. 如果说查询的是同一个index下的不同type的数据,直接将请求跟在url上就可以了,请求体中值需要指明type 和 id即可12345678910111213GET /test_index/_mget&#123; &quot;docs&quot;:[ &#123; &quot;_type&quot;:&quot;test_type&quot;, // 类型名称 &quot;_id&quot;:1 // id &#125;, &#123; &quot;_type&quot;:&quot;test_type&quot;, &quot;_id&quot;:2 &#125; ] &#125; 如果查询的是同一个index下的同一个type下的内容,那么index和type都写在url上,请求体中只需要一个ids的数组即可1234GET /test_index/test_type/_mget&#123; &quot;ids&quot;:[1,2]&#125; 可以看出来ids不需要包在docs中, 但是返回值还是会包在docs数组中. 总结:一般来说,在进行查询的时候,如果一次要查询多条数据的话,一定要用batch批量操作的api,尽可能减少网络请求的开销,可以大大提升性能. _bulk批量增删改_bulk 可以操作批量增删改,语法如下:12&#123;&quot;action&quot;:&#123;&quot;metadata&quot;&#125;&#125;&#123;&quot;data&quot;&#125; bulk api对json的语法有严格的要求,每个json不能换行,只能放一行,同时一个json和另一个json之间必须换行,每个操作需要两个json字符串,删除操作只需要一个. 举例,比如现在要创建一个document,放到bulk里面是这样的12&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;test_index&quot;,&quot;_type&quot;:&quot;test_type&quot;,&quot;_id&quot;:1&#125;&#125;&#123;&quot;test_field&quot;:&quot;test1&quot;,&quot;test_field2&quot;:&quot;test2&quot;&#125; 那么有哪些类型的操作可以执行呢? delete:,删除一个document create:相当于 PUT /index/type/id/_create,强制创建 index:普通的PUT操作,可以是创建,也可以是全量替换 update:相当于 partial update操作 示例:12345678POST /_bulk&#123;&quot;delete&quot;:&#123;&quot;_index&quot;:&quot;test_index&quot;,&quot;_type&quot;:&quot;test_type&quot;,&quot;_id&quot;:3&#125;&#125;&#123;&quot;create&quot;:&#123;&quot;_index&quot;:&quot;test_index&quot;,&quot;_type&quot;:&quot;test_type&quot;,&quot;_id&quot;:2&#125;&#125;&#123;&quot;test_field&quot;:&quot;already exists&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;test_index&quot;,&quot;_type&quot;:&quot;test_type&quot;,&quot;_id&quot;:1&#125;&#125;&#123;&quot;test_field&quot;:&quot;replace all&quot;&#125;&#123;&quot;update&quot;:&#123;&quot;_index&quot;:&quot;test_index&quot;,&quot;_type&quot;:&quot;test_type&quot;,&quot;_id&quot;:4,&quot;_retry_on_conflict&quot;:3&#125;&#125;&#123;&quot;doc&quot;:&#123;&quot;test_content&quot;:&quot;update id is4&quot;&#125;&#125; 上面这个请求中: 删除了id为4document 强制创建一个document,id为2 (id为2的已经存在了) 将id为1的document全量替换 partial update更新id为4的document,更新失败的时候重试3次 执行返回结果:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&#123; &quot;took&quot;: 79, &quot;errors&quot;: true, &quot;items&quot;: [ &#123; &quot;delete&quot;: &#123; &quot;found&quot;: true, &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_version&quot;: 2, &quot;result&quot;: &quot;deleted&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;status&quot;: 200 &#125; &#125;, &#123; &quot;create&quot;: &#123; &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;status&quot;: 409, &quot;error&quot;: &#123; &quot;type&quot;: &quot;version_conflict_engine_exception&quot;, &quot;reason&quot;: &quot;[test_type][2]: version conflict, document already exists (current version [2])&quot;, &quot;index_uuid&quot;: &quot;qFba3qxtTO6YxlO3m7qtug&quot;, &quot;shard&quot;: &quot;2&quot;, &quot;index&quot;: &quot;test_index&quot; &#125; &#125; &#125;, &#123; &quot;index&quot;: &#123; &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 2, &quot;result&quot;: &quot;updated&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: false, &quot;status&quot;: 200 &#125; &#125;, &#123; &quot;update&quot;: &#123; &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_version&quot;: 2, &quot;result&quot;: &quot;updated&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;status&quot;: 200 &#125; &#125; ]&#125; 可以看到,第2个操作失败了,原因是已经存在了,但是其他操作仍然执行成功了,所以bulk操作中，任意一个操作失败，是不会影响其他的操作的，但是在返回结果里，会告诉你异常日志 bulk size最佳大小bulk request会加载到内存里面,如果太大的话,性能反而会下降,因此需要反复尝试一个最佳的bulk size. 一般从1000~5000 条数据开始,或者数据大小在5~15MB之间,尝试逐渐增加 奇特的json格式详解上面有说过bulk的请求体 每个json不能换行,只能放一行,同时一个json和另一个jso之间必须换行,那为什么要这样设计呢? 第一点,bulk中的操作都可能要转发到不同的node的shard去执行.第二,如果允许任意换行,es拿到json后就得进行如下处理: 将json数组解析为JSONArray对象,这个时候,整个数据就会在内存中出现两份,一份是json文本,一份是JSONArray对象 解析json数组里的每个json,对每个请求中的document进行路由 为路由到一个shard上的多个请求,创建一个请求数组 将这个请求数组进行序列化 将序列化后的请求数组发送到对应的节点上去 所以按照这种方式的话,会耗费更多的内存,更多的jvm gc开销上文中有说到过bulk size的最佳大小,如果说现在有100个bulk请求,每个请求10MB,那就是 1000MB的数据,每个请求的json都copy一份为JSONArray对象,此时内存中的占用就会翻倍,会占用到2000MB+的内存,占用更多的内存就可能积压其他请求的内存使用量,就可能导致其他请求的性能急速下降. 另外,占用更多的内存,gc回收次数更多,每次要回收的垃圾对象更多,耗费的时间更多,会导致es的java虚拟机停止工作线程的时间更多 使用这种不换行的格式的话 不需要转为json对象,不会在内存中copy数据,直接按照换行符切割即可 对每两个一组的json,读取metadata,进行document路由. 直接将对应的json发送到node上去 最大的优势在于,不需要将json数组解析为JSONArray对象,不需要浪费内存空间,尽可能保证性能]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-11-基于groovy脚本进行partial update]]></title>
    <url>%2F2018%2F11%2F19%2FElasticsearch-11-%E5%9F%BA%E4%BA%8Egroovy%E8%84%9A%E6%9C%AC%E8%BF%9B%E8%A1%8Cpartial-update%2F</url>
    <content type="text"><![CDATA[基于groovy脚本进行partial update首先添加一条数据到es中12345PUT test_index/test_type/2&#123; &quot;num&quot;:0, &quot;tags&quot;:[]&#125; 内置脚本1234POST test_index/test_type/2/_update&#123; &quot;script&quot;: &quot;ctx._source.num+=1&quot;&#125; 通过ctx._source.filed 去操作filed的值 执行完毕后去查询一下1GET test_index/test_type/2 返回值:1234567891011&#123; &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_version&quot;: 2, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;num&quot;: 1, &quot;tags&quot;: [] &#125;&#125; 可以看到num已经变成了1 外部脚本用外部脚本进行partial update首先要在es的config/scripts目录下新建groovy脚本,比如我们要在tags下加一个值,新建脚本test-add-tags.groovy,文件内容是1ctx._source.tags+=new_tag 然后用外部脚本进行更新12345678910POST test_index/test_type/2/_update&#123; &quot;script&quot;: &#123; &quot;lang&quot;: &quot;groovy&quot;, // 指定脚本语言 &quot;file&quot;: &quot;test-add-tags&quot;, // 文件名称(不包含文件后缀名) &quot;params&quot;: &#123; // 参数的集合 &quot;new_tag&quot;:&quot;tag1&quot; // 参数名和值 &#125; &#125;&#125; 用外部脚本删除document同样需要在config/scripts目录下新建groovy脚本, 脚本名称:test-delete-document,脚本内容是:1ctx.op=ctx._source.num == count ? &apos;delete&apos; : &apos;none&apos; 就是num的值跟count相等的时候才删除,如果不相等,什么都不做.然后执行partial update12345678910POST test_index/test_type/2/_update&#123; &quot;script&quot;: &#123; &quot;lang&quot;: &quot;groovy&quot;, &quot;file&quot;: &quot;test-delete-document&quot;, &quot;params&quot;: &#123; &quot;count&quot;:1 &#125; &#125;&#125; 传入的参数count是1,这个时候document中的num的值也是1,所以这document会被删除掉 upsert操作此时这个id是2的document已经被删除,如果这个时候直接去进行partial update,会报错,因为这个document不存在这时候可以使用upsert去进行初始化操作执行:12345678POST /test_index/test_type/2/_update&#123; &quot;script&quot;: &quot;ctx._source.num+=1&quot;, &quot;upsert&quot;: &#123; &quot;num&quot;:0, &quot;tags&quot;:[] &#125;&#125; 执行完成后,查询发现num是0 tags是[],并没有去执行script中的脚本.再次运行上面的代码, 查询后发现num已经更新为1 upsert操作:如果指定的document不存在,就执行upsert中的初始化操作,如果指定的document存在,就执行doc或者script中指定的partial update操作.]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-10-并发控制方案]]></title>
    <url>%2F2018%2F11%2F17%2FElasticsearch-10-%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[并发场景假设现在是矮子电商场景下,用户需要去购买商品,程序的工作流程是: 读取商品信息,比如商品名称价格等. 用户下单去购买商品 更新商品库存es的工作流程: 先查询document数据,商品信息等,显示到页面,同时在内存中缓存该document的数据 当网页发生购买以后,直接基于内存中的数据进行计算和操作 将计算后的结果返回到es中 现在有两个线程同时去执行这个流程,比如购买一台电脑库存是100,线程A 线程B并发执行,两个线程查询到的库存都是100,这个时候用户去下单购买,线程A 将商品库存减一是99件,线程B也将库存减一也是99件,然后A先更新es的中的商品库存,更新为99,此时线程B也去更新商品库存为99,这个时候一共是发生了两次购买操作,正常结果是库存应该减少两件,但其实只减少了一件,这就导致了并发冲突,使得数据结果不正确. 有些场景下,在不管数据正不正确的情况下这样操作是无所谓,比如我们只管将数据写入es,不管数据是什么样的,或者是说算错了也没关系的这些情况,但是一般情况下我们是需要做并发控制的防止数据错乱 并发控制方案悲观锁并发控制方案还是上面的这个场景,使用悲观锁控制并发,就是线程A去读取商品信息,读取的时候给这个商品信息加锁,然后进行一系列的操作,比如减少库存等,将库存减一后,再更新到商品信息中去,然后释放锁,在这个过程中,线程B如果去请求这个商品数据是请求不到的,在线程A释放锁之前,B是获取不到的,当A释放锁之后,这个时候商品库存已经变化了,B拿到的数据就是A已经操作完成后的数据,同时线程B也会对数据加锁,操作完成之后再释放. 悲观锁的并发控制方案,就是在各种情况下都上锁,上锁之后,就只有一个线程可以操作这条数据了,当然,不同场景下上的锁也不同,比如行级锁 表级锁 读锁 写锁等. 乐观锁并发控制方案es就是使用的乐观锁,使用乐观锁控制并发的时候,并不会对数据进行加锁,而是通过版本号控制(version),这点和zookeeper相似,还是上面的场景,线程A B 同时去请求商品信息,然后拿到的都是一样的,比如库存是100, 这时候比如数据的version=1, 假设线程A先把库存-1然后更新到es中,这时候es会拿版本号和线程A中的数据的版本号进行对比,这时候version都是1写入成功,库存更新为99 es中数据的版本号变更为2,这时候线程B也去进行更新,发现es中数据的版本号是2,但是线程B的版本号还是1,版本号不同的情况下,线程B重新请求es的数据 然后再将库存-1,版本号也变为2,然后再去更新es中的数据,这个时候库存变为98 版本号变为3. es内部基于乐观锁控制并发的原理假设现在有两个节点一个shard一个replica,里面有一条document 数据是test1 version也是1 现在有两个请求同时去修改document数据,我们期待的结果是1先修改为test2,然后2再修改为test3 但是这个两个请求时并发的所以第2个请求可能先到达es先进性修改, 如果没有乐观锁并发控制的话,第2个请求先到达将内容修改为test3,第1个请求后到,再去修改为test2,这个时候数据就变成了test,跟我们预期的结果就不一样了,因为按照顺序来说是先修改为test2 再修改为test3 同理shard去将数据同步到replica的时候也会多线程异步执行的,如果没有乐观锁的控制,数据也可能发生错误但是在有乐观锁的情况下,基于version去控制,如果说第二个请求先到先修改了数据 version已经加1了比如变为2,replica先同步了第二个请求的数据,第一个请求再去同步的时候去比对一下版本号,version还是1,那么es就会直接把这条数据扔掉,这个时候数据就会保持一个正确的状态 基于external version进行乐观锁并发控制es提供了一个feature,就是说你可以不用他提供的内部版本号来进行并发控制,可以基于你自己维护的一个版本号来进行并发控制.举个例子,假如你的数据在mysql里面也有一份,然后你的应用本身就维护了一个版本号,无论是自己生成的或者是程序控制的.这个时候进行乐观锁并发控制的时候可能并不是想要用内部的_version来进行控制,而是用你自己维护的那个版本号来控制. 基于内部的_version进行控制时,只需要在请求后面加 _version=版本号即可,基于自己的version控制的话需要在请求后面加?_version=版本号&amp;version_type=external这两种版本控制的唯一区别就在于 基于内部_version控制时,只有当你提供的version和es中的version一毛一样的时候才能修改,只要不一样就报错,而基于自己的版本号控制时,只有当你提供的version比es中的_version大的时候才能完成修改 两种锁的优缺点 悲观锁 优点:方便,直接加锁,对应用程序来说是透明的,不需要做额外的操作 缺点:并发能力很低,同一时间只能有一个线程访问操作数据 乐观锁 优点:并发能力高,不给数据加锁,可以有大量线程并发操作 缺点:麻烦,每次更新的时候都要先去比对版本号,然后版本号不一致时,可能需要重新加载数据,再次修改然后再写,而且这个过程可能要重复好几次]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-9-document核心元数据解析]]></title>
    <url>%2F2018%2F11%2F16%2FElasticsearch-9-document%E6%A0%B8%E5%BF%83%E5%85%83%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[本文主要写document中的_index,_type,_id,_source这几个元数据先随便添加一个document1234PUT /test_index/test_type/1&#123; &quot;test_content&quot;:&quot;test content&quot;&#125; 查询1GET /test_index/test_type/1 返回值12345678910&#123; &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;test_content&quot;: &quot;test content&quot; &#125;&#125; 返回值详解_index元数据 代表document存放在哪一个index中 类似的数据放在一个索引中,非类似的数据应该放到不同的索引中,什么意思呢,比如现在有一个商品类的数据(product)和一个商品销售的数据(sale),那么应该把product的数据放到一个index里面,sale的数据放到另一个index里面, 如果把所有的数据都放到一个index里面,这样做是不合适的. index中包含了很多类似的document,类似就是指这些document的fields很大一部分是相同的, 比如说现在有3个document但是每个document里面的fields是完全不一样的,这就是不类似的,就不太适合放到同一个index里面了 索引的命名规则,必须是小写的 不能用大写,第二 不能用下划线开头,第三 不能包含逗号 详细来说一下上面的第2点,为什么说应该把类似的数据放到同一个index里面, 比如我们现在把product的数据和sale的数据都放到了同一个index下, 这些数据将会被分配到不同的shard上面,每个shard上面既有product数据也有sale数据, 现在网站后台需要做sale的聚合分析,而且sale的数据量很大,这个时候所有的shard都会去执行这次聚合分析,耗费大量的资源,与此同时在网站前端用户需要去搜索product中的数据,因为这些shard都在做后台的聚合分析占用了大量的资源,导致前端用户搜索product数据时会变慢,影响用户体验,所以相同类型的数据放到一个index上,在自己独立的shard中与其他数据不在一个shard中就不会互相影响 _type元数据 代表document属于index中的哪个类别 一个索引通常会划分为多个type,逻辑上对index中有些许不同的数据进行分类,因为相同类型的数据,会有很多相同的fields,但是也可能有些许的差别,比如商品数据放到了一个index中,但是商品下面还划分为电子商品 生鲜商品 等等. type命名 名称是可以大小写的,但是不能用下划线开头也不能包含逗号 _id元数据 代表document的唯一标识,与index和type一起,可以定位一个document 我们可以手动指定document的id,也可以不指定 es会为我们自动创建一个id Q:什么时候适合使用手动生成id,什么时候使用自动生成的id?A:一般来说是从某些其他的系统中导入数据到es时,会采取这种方式,就是系统中已经有数据的唯一标识了,比如我们现在有一个商品的数据库,要把里面的数据导入到es里面做搜索,数据库里面肯定会有一个数据的主键primary key,这时将数据导入到es中的时候就适合用数据库中已有的primary key做id自动生成的id,我们先来看一下,是什么样的先添加一个数据,不指定id1234POST /test_index/test_type&#123; &quot;test_content&quot;:&quot;test1&quot;&#125; 返回值:12345678910111213&#123; &quot;_index&quot;: &quot;test_index&quot;, &quot;_type&quot;: &quot;test_type&quot;, &quot;_id&quot;: &quot;AWcctb8Zlcpuqacodv55&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true&#125; 可以看到id是一个字符串,长度是固定为20位的,URL安全,base64编码,使用的是GUID生成策略,分布式系统并行生成时不可能发生冲突 _source元数据就是我们在创建document的时候,在request中写入的json数据,当我们get的时候,会把这个json数据原封不动的返回到_source中来.如果我们发送的有多个field 在返回时只想要指定的field 只需要在get请求后面加?_source=field1,field2…即可]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-8-容错机制]]></title>
    <url>%2F2018%2F11%2F16%2FElasticsearch-8-%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[容错机制假设场景,现在一共有9个shard,其中3个shard 6个replica,一共有三个es节点,node1是master节点,具体如下图: 如果下载master节点挂掉,shard1,replica2-1,replica3-1 节点会丢失,在master节点挂掉的一瞬间 shard1就没了,此时shard1就不是active状态了,集群中不是所有的primary shard都是active状态了,所以集群的状态会在这一瞬间变为red 容错第一步集群会自动选举另外一个node成为新的master,比如node2,承担起master的责任来 容错第二步新的master会将丢失掉的primary shard的某个replica shard提升为primary shard,此时集群的状态是yellow了,因为所有的primary shard都变成了活跃状态, 但是因为少了一个replica shard 所以不是所有的replica shard都是active状态了 容错第三步新的master重启之前宕机的节点,将丢失的副本都拷贝到该node上去,而且该node会使用宕机之前已有的shard的数据,只是同步一下宕机后发生的修改.此时集群的状态变为green,因为所有的primary shard 和 replica shard都是active状态了]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-7-扩容机制]]></title>
    <url>%2F2018%2F11%2F16%2FElasticsearch-7-%E6%89%A9%E5%AE%B9%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[横向扩容上文中有提到有两个es节点的环境下shard和replica的分配 两个node环境下replica和shard的分配目前集群中有两个es节点,创建一个index,设置有3个shard每个shard对应一个replica,如下图: 新添加一个节点到集群中前面有说到过es集群会自动做负载均衡,如果我们现在加一个es节点到集群中来的话,es会按照一定的规则(一个shard和它对应的replica不会被分配到同一个节点上去)将部分shard分配到新的节点上去,如图: 横向扩容的好处横向扩容后,每台节点上的shard会减少,就意味着每个shard可以占用节点上的更多资源比如IO/CPU/Memory,整个系统的性能会更好 扩容的极限上面我们一共有6个shard(3个shard 3个replica),最多扩容到6个节点,就是说每个节点上都有一个shard,这个时候每个shard可以占用他所在服务器的所有资源,性能是最好的 突破扩容瓶颈如何超出系统的扩容瓶颈呢, 比如现在我们有6个shard但是要扩容到9个节点,同时想让系统的性能更好,这个时候我们可以增加replica的数量,因为primary shard的数量是不能变的,我们只能改变replica的数量,比如有3个primary shard,设置replica的数量为2, 这个时候总共有 3 + 3 x 2 = 9个shard 就可以分布到9个节点上去,因为 replica 也可以处理读请求 所以整个集群的性能会更好. 集群容错性比如集群中有3个节点,一共6个shard,这个时候 如果某一个节点宕机,比如node3宕机了,如下图: 这个时候shard3 和 replica1已经丢失了,剩下其他两个节点上的shard1 replica2 shard2 replica3 ,replica3 因为replica3 是shard3的副本,所以node3 宕机是是不会造成数据的丢失的, 所以3个shard 3个replica分布在3个节点上任意一个节点宕机都不会造成数据的丢失]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-6-shard&replica机制]]></title>
    <url>%2F2018%2F11%2F16%2FElasticsearch-6-shard-replica%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[shard&amp;replica机制梳理 一个index可以包含多个shard, index中的数据会均匀的分配到每个shard中,就是es分片的机制. 每个shard都是一个最小的工作单元,承载部分数据,es是基于Lucene去开发的,其实每个shard就是Lucene的实例,有完整的建立索引和处理请求的能力 增减节点的时候shard会自动在nodes中负载均衡,比如一共有6个es节点,但是有7个shard 这个时候其中的一个es节点会有两个shard,如果这时候集群中再加进来一个es节点,那么承载两个shard的节点会分配一个shard到新加的节点上去 每个document肯定只会存在于某一个primary shard中以及其对应的replica shard中,不可能同时存在于两个 primary shard中 replica shard是primary shard的副本,负责容错,以及承担读请求负载. primary shard的数量在创建索引的时候就固定了,replica shard的数量可以随时修改 一个index中primary shard的默认数量是5,replica shard默认是1(就是每个primary shard都有1个 replica),默认有10个shard 5个primary shard和5个replica shard primary shard不能和自己的replica shard放在同一个节点上,如果放在同一个节点上 当这个节点宕机的时候primary shard和replica shard都丢失了,就起不到容错的作用, 同一个节点上一个放其他节点的replica shard 单个es节点环境下创建index比如我们现在创建一个index 设置有3个primary shard 每个 shard对应一个replica创建代码如下:1234567PUT /test_index&#123; &quot;settings&quot; : &#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 1 &#125;&#125; 这时候es集群的状态是yellow,而且只会将3个primary shard分配到这个es节点上去,对应的3个replica是不会被创建的,就是上面提到的 shard自己的replica是不能跟自己放在一起的,此时集群可以正常工作,但是这个节点一旦宕机的话,数据就会全部丢失,而且集群不可用,无法承接任何请求 两个es节点环境下shard的分配在上面单节点的环境下如果在添加一个es节点到集群中, 此时新添加进来的节点会承载之前节点的primary shard的replica shard, primary shard 和 replica shard中的数据是一样的, replica 和 primary shard 同样可以处理读请求.]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-5-基础分布式架构]]></title>
    <url>%2F2018%2F11%2F16%2FElasticsearch-5-%E5%9F%BA%E7%A1%80%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[上文中写了Elasticsearch的聚合分析,下钻分析,嵌套聚合等, 本文主要是写Elasticsearch的分布式机制, 扩容策略等 Elasticsearch对分布式机制的透明隐藏特性Elasticsearch是一套分布式的系统,分布式是为了应对大数据量 分片机制: 往ES插入数据的时候ES集群会自动分配到某一shard上,比如 之前用rest API插入数据时,我们并没有关心数据是怎么分配的,是分配到哪个shard上的 cluster discovery:集群发现机制,就是新启动一个es节点时,集群会自动发现节点并加入集群. shard负载均衡:比如我们现在有3个es节点,总共要有25个shard要分配到3个节点上去,es会自动进行均匀分配,以保持每个节点均衡的读写负载请求 shard副本 请求路由:将请求自动路由到对应处理的shard上 集群扩容 shard重分配 我们在实际使用中并不需要关心这些,Elasticsearch会自动处理 Elasticsearch的垂直扩容与水平扩容假设现在有6台服务器,每台服务器容量是1T,马上数据量要增长到8T,这个时候有两种方案: 垂直扩容:重新购置两台服务器,每台服务器的容量是2T,替换两台老的服务器,那么现在服务器总容量就是 4x1T + 2x2T =8T 水平扩容:新购置两台服务器,每台容量1T,直接加到集群中去,那么现在总容量就是8 x 1T = 8T 业界经常采用的方案，采购越来越多的普通服务器，性能比较一般，但是很多普通服务器组织在一起，就能构成强大的计算和存储能力 master节点 管理es集群的元数据,比如说索引的创建和删除,维护索引元数据,节点的增加和移除,维护集群的元数据 默认情况下,会自动选择出一台节点,作为master节点 节点对等的分布式架构每个es节点都能接收请求,接收请求后自动路由到可以处理该请求的shard,并收集返回数据.]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-4-聚合分析]]></title>
    <url>%2F2018%2F11%2F16%2FElasticsearch-4-%E8%81%9A%E5%90%88%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[上文中,添加了6个电影的document,接下来做这些document的聚合分析,统计等.上文添加的6个电影数据中都包含有genres 的一个数组 统计每个genres下的电影数量1234567891011GET /movies/movie/_search&#123; &quot;size&quot;: 0, // size不设置的话 hits中会把对进行聚合的所有数据返回. &quot;aggs&quot;: &#123; &quot;group_by_genres&quot;: &#123; // group_by_genres 是自定义的名字 &quot;terms&quot;: &#123; &quot;field&quot;: &quot;genres&quot; // 要做聚合的field &#125; &#125; &#125;&#125; 运行一下,会发现报错 如下:123456789101112131415161718192021222324252627282930&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [genres] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.&quot; &#125; ], &quot;type&quot;: &quot;search_phase_execution_exception&quot;, &quot;reason&quot;: &quot;all shards failed&quot;, &quot;phase&quot;: &quot;query&quot;, &quot;grouped&quot;: true, &quot;failed_shards&quot;: [ &#123; &quot;shard&quot;: 0, &quot;index&quot;: &quot;movies&quot;, &quot;node&quot;: &quot;f57uV91xS_GRTQS2Ho81rg&quot;, &quot;reason&quot;: &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [genres] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.&quot; &#125; &#125; ], &quot;caused_by&quot;: &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Fielddata is disabled on text fields by default. Set fielddata=true on [genres] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.&quot; &#125; &#125;, &quot;status&quot;: 400&#125; 错误原因是:默认情况下，在文本字段上禁用Fielddata。在[genres]上设置fielddata=true，以便通过反转索引来加载内存中的fielddata。请注意，这可能会占用大量内存 这里我们需要将文本field的fielddata属性设置为true,具体原因之后再说.123456789PUT /movies/_mapping/movie&#123; &quot;properties&quot;: &#123; &quot;genres&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;fielddata&quot;: true &#125; &#125;&#125; 然后再执行上面的查询, 返回结果为:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&#123; &quot;took&quot;: 5, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 6, &quot;max_score&quot;: 0, &quot;hits&quot;: [] &#125;, &quot;aggregations&quot;: &#123; &quot;group_by_genres&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 0, &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;drama&quot;, &quot;doc_count&quot;: 4 &#125;, &#123; &quot;key&quot;: &quot;crime&quot;, &quot;doc_count&quot;: 3 &#125;, &#123; &quot;key&quot;: &quot;biography&quot;, &quot;doc_count&quot;: 2 &#125;, &#123; &quot;key&quot;: &quot;action&quot;, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: &quot;adventure&quot;, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: &quot;cirme&quot;, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: &quot;drame&quot;, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: &quot;mystery&quot;, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: &quot;thriller&quot;, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: &quot;war&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125; &#125;&#125; 具体的聚合结果返回到了 aggregations 下的 buckets 下, key为每个genres下的元素, doc_count 是包含该key的电影数量. 对名称中包含kill的电影，计算每个genres下的电影数量12345678910111213141516GET /movies/movie/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;kill&quot; &#125; &#125;, &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_genres&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;genres&quot; &#125; &#125; &#125;&#125; 其实就是在上个查询的基础上加了一个query条件,先查询query,将返回的结果再进行聚合分析 先按genres分组,然后计算每个genres下的电影的年份的平均值123456789101112131415161718GET movies/movie/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_genres&quot;: &#123; // 自定义分组名称 &quot;terms&quot;: &#123; &quot;field&quot;: &quot;genres&quot; // 聚合genres &#125;, &quot;aggs&quot;: &#123; &quot;avg_year&quot;: &#123; // 在上面分组的基础上 在进行聚合分析 &quot;avg&quot;: &#123; // 计算平均值 &quot;field&quot;: &quot;year&quot; &#125; &#125; &#125; &#125; &#125;&#125; 平均值计算是按照每组里面的数据进行平均 计算每个genres下的电影的平均年份，并且按照平均年份降序排序123456789101112131415161718192021GET movies/movie/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_genres&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;genres&quot;, &quot;order&quot;: &#123; &quot;avg_year&quot;: &quot;desc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;avg_year&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;year&quot; &#125; &#125; &#125; &#125; &#125;&#125; 在上一个分组计算平均值的基础上 在上层的terms里面加一个order 要排序的字段就是下面一层聚合计算平均值的名称”avg_year” 按照指定的年份范围区间进行分组，然后在每组内再按照genres进行分组，最后再计算每组的平均年份上文中添加的数据 电影年份有 1962 1972 1979 2007 2003, 用range来进行分组 分为1960-1970 1970-1980 2000-2010 三组12345678910111213141516171819202122232425GET /movies/movie/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_year&quot;: &#123; &quot;range&quot;: &#123; &quot;field&quot;: &quot;year&quot;, &quot;ranges&quot;: [ &#123; &quot;from&quot;: 1960, &quot;to&quot;: 1970 &#125;, &#123; &quot;from&quot;: 1970, &quot;to&quot;: 1980 &#125;, &#123; &quot;from&quot;: 2000, &quot;to&quot;: 2010 &#125; ] &#125; &#125; &#125;&#125; 三组年份的电影分好以后,再往下一层按genres分一层,分好之后再往下聚合,用来计算平均值123456789101112131415161718192021222324252627282930313233343536373839GET /movies/movie/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_year&quot;: &#123; &quot;range&quot;: &#123; &quot;field&quot;: &quot;year&quot;, &quot;ranges&quot;: [ &#123; &quot;from&quot;: 1960, &quot;to&quot;: 1970 &#125;, &#123; &quot;from&quot;: 1970, &quot;to&quot;: 1980 &#125;, &#123; &quot;from&quot;: 2000, &quot;to&quot;: 2010 &#125; ] &#125;, &quot;aggs&quot;: &#123; &quot;group_by_genres&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;genres&quot; &#125;, &quot;aggs&quot;: &#123; &quot;avg_year&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;year&quot; &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-3-花式查询]]></title>
    <url>%2F2018%2F11%2F16%2FElasticsearch-3-%E8%8A%B1%E5%BC%8F%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[新增语法:12345678910111213PUT indexName/typeName/id&#123; json数据&#125;或POST indexName/typeName&#123; json数据&#125;# 如果不指定id的话 es会自动分配一个id 示例下面添加了6个电影信息,索引名称是movies,类型名称是movie1234567891011121314151617181920212223242526272829303132333435363738394041424344454647PUT movies/movie/1&#123; &quot;title&quot;:&quot;The Godfather&quot;, &quot;director&quot;:&quot;Francis Ford Coppola&quot;, &quot;year&quot;:1972, &quot;genres&quot;: [&quot;Cirme&quot;,&quot;Drame&quot;]&#125;PUT movies/movie/2&#123; &quot;title&quot;: &quot;Lawrence of Arabia&quot;, &quot;director&quot;: &quot;David Lean&quot;, &quot;year&quot;: 1962, &quot;genres&quot;: [&quot;Adventure&quot;, &quot;Biography&quot;, &quot;Drama&quot;]&#125;PUT movies/movie/3&#123; &quot;title&quot;: &quot;To Kill a Mockingbird&quot;, &quot;director&quot;: &quot;Robert Mulligan&quot;, &quot;year&quot;: 1962, &quot;genres&quot;: [&quot;Crime&quot;, &quot;Drama&quot;, &quot;Mystery&quot;]&#125;PUT movies/movie/4&#123; &quot;title&quot;: &quot;Apocalypse Now&quot;, &quot;director&quot;: &quot;Francis Ford Coppola&quot;, &quot;year&quot;: 1979, &quot;genres&quot;: [&quot;Drama&quot;, &quot;War&quot;]&#125;PUT movies/movie/5&#123; &quot;title&quot;: &quot;Kill Bill: Vol. 1&quot;, &quot;director&quot;: &quot;Quentin Tarantino&quot;, &quot;year&quot;: 2003, &quot;genres&quot;: [&quot;Action&quot;, &quot;Crime&quot;, &quot;Thriller&quot;]&#125;PUT movies/movie/6&#123; &quot;title&quot;: &quot;The Assassination of Jesse James by the Coward Robert Ford&quot;, &quot;director&quot;: &quot;Andrew Dominik&quot;, &quot;year&quot;: 2007, &quot;genres&quot;: [&quot;Biography&quot;, &quot;Crime&quot;, &quot;Drama&quot;]&#125; 添加完毕之后,通过搜索查询一下这几部电影 (使用_search端点)语法:12GET [index_name]/[type_name]/_search # index_name和type_name都是可选的 示例123GET _search //搜索所有索引和所有类型。GET movies/_search //在电影索引中搜索所有类型GET movies/movie/_search //在电影索引中显式搜索电影类型的文档 query DSLDSL: Domain Specified Language ,特定领域的语言http request body: 请求体,可以用json格式来构建查询语法,比较方便,可以构建各种复杂语法. 查询所有的电影1234GET movies/movie/_search&#123; &quot;query&quot;: &#123;&quot;match_all&quot;: &#123;&#125;&#125;&#125; 查询名称包含kill的电影,同时年份按照降序排序12345678910111213GET movies/movie/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;kill&quot; &#125; &#125;, &quot;sort&quot;: [ &#123; &quot;year&quot;: &quot;desc&quot; &#125; ]&#125; 分页查询,假设每页显示一条数据,现在查询第二页123456GET movies/movie/_search&#123; &quot;query&quot;: &#123;&quot;match_all&quot;: &#123;&#125;&#125;, &quot;from&quot;: 1, &quot;size&quot;: 1&#125; 指定查询出来的电影只要名称和年份12345GET movies/movie/_search&#123; &quot;query&quot;: &#123;&quot;match_all&quot;: &#123;&#125;&#125;, &quot;_source&quot;: [&quot;title&quot;,&quot;year&quot;]&#125; query filter 过滤器查询电影名称包含kill,而且年份大于2000年的电影12345678910111213141516171819GET /movies/movie/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;:&#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;kill&quot; &#125; &#125;, &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;year&quot;: &#123; &quot;gt&quot;: 2000 &#125; &#125; &#125; &#125; &#125;&#125; full-text search (全文检索)123456GET /movies/movie/_search&#123; &quot;query&quot;: &#123;&quot;match&quot;: &#123; &quot;title&quot;: &quot;Kill Bill Lawrence&quot; &#125;&#125;&#125; 首先 将 /movies/movie/下面的所有数据的title的值拆分,建立倒排索引,然后搜索关键字会被拆分为Kill和Bill和Lawrence 然后在倒排索引中检索对应的数据 全文检索会将输入的关键字拆解开来，去倒排索引里面去一一匹配，只要能匹配上任意一个拆解后的单词，就可以作为结果返回 phrase search 短语搜索12345678GET /movies/movie/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;title&quot;: &quot;kill&quot; &#125; &#125;&#125; 跟全文检索相反,要求输入关键字 必须在指定的字段文本中,完全包含一模一样的,才可以算匹配,才能做为返回结果.比如 查询kill 会返回 “Kill Bill: Vol. 1” 和 “To Kill a Mockingbird” 查询kill bill 只会返回 “Kill Bill: Vol. 1” highlight search 高亮搜索结果GET /movies/movie/_search{ “query”: { “match”: { “title”: “kill” } }, “highlight”: { “fields”: { “title”:{} } }} 返回会在 highlight &gt; title 中 将关键字加 &lt;em&gt;&lt;/em&gt; 标签 搜索结果说明 took：耗费了几毫秒 timed_out：是否超时 _shards：一个搜索请求会达到一个index的所有primary shard上,当然 每个primary shard都可能有一个或多个replica shard,所以请求也可以到primary shard的其中一个replica shard上去,shards fail的条件(primary和replica全部挂掉)，不影响其他shard. hits.total：查询结果的数量 hits.max_score：本次搜索的所有结果中，最大的相关度分数是多少，每一条document对于search的相关度，越相关，_score分数越大，排位越靠前 hits.hits：默认查询前10条数据,完整数据,_score 降序排序 time_out机制详解查询默认是没有timeout的,可以手动指定1GET /_search?timeout=10ms 意思就是在timeout的时间范围内,将搜索到的所有结果直接返回给客户端,不需要等数据全部查到后返回,确保一次搜索请求可以在指定的时间内完成,为一些时间敏感的搜索应用提供良好的支持 代码地址 代码地址]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-2-集群状态检查和CRUD操作]]></title>
    <url>%2F2018%2F11%2F16%2FElasticsearch-2-%E9%9B%86%E7%BE%A4%E7%8A%B6%E6%80%81%E6%A3%80%E6%9F%A5%E5%92%8CCRUD%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[检查集群的健康状态Kibana中1GET _cat/health?v 返回值12epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1537168122 15:08:42 elasticsearch yellow 1 1 1 1 0 0 1 0 - 50.0% status:集群的健康状况green:每个索引的primary shard和replica shard都是active状态的yellow:每个缩影的primary shard都是active状态的,但是部分replica shard不是active状态,处于不可用的状态.red:不是所有索引的primary shard都是active状态,部分索引有数据丢失了 当只启动一个es的时候 集群状态是yellow的,，就启动了一个es进程，相当于就只有一个node。现在es中有一个index，就是kibana自己内置建立的index。由于默认的配置是给每个index分配5个primary shard和5个replica shard，而且primary shard和replica shard不能在同一台机器上（为了容错）。现在kibana自己建立的index是1个primary shard和1个replica shard。当前就一个node，所以只有1个primary shard被分配了和启动了，但是一个replica shard没有第二台机器去启动。此时只要启动第二个es进程，就会在es集群中有2个node，然后那1个replica shard就会自动分配过去，然后cluster status就会变成green状态。 索引管理快速查看集群中有哪些索引:1GET /_cat/indices?v 返回值12health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizeyellow open .kibana rUm9n9wMRQCCrRDEhqneBg 1 1 1 0 3.1kb 3.1kb 创建索引1PUT /test_index?pretty 删除索引1DELETE /test_index?pretty document的CRUD操作新增document 建立索引12345678910111213141516171819PUT /index_name/type_name/id&#123; json数据&#125;// 返回值&#123; &quot;_index&quot;: &quot;index_name&quot;, &quot;_type&quot;: &quot;type_name&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;created&quot;: true&#125; es会自动建立index和type，不需要提前创建，而且es默认会对document每个field都建立倒排索引，让其可以被搜索 检索/查询文档1234567891011121314151617181920GET index_name/type_name/id// 返回值&#123; &quot;_index&quot;: &quot;index_name&quot;, &quot;_type&quot;: &quot;type_name&quot;, &quot;_id&quot;: &quot;id&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;gaolujie yagao&quot;, &quot;desc&quot;: &quot;gaoxiao meibai&quot;, &quot;price&quot;: 30, &quot;producer&quot;: &quot;gaolujie producer&quot;, &quot;tags&quot;: [ &quot;meibai&quot;, &quot;fangzhu&quot; ] &#125;&#125; 更新document全量替换123456PUT /index_name/type_name/id&#123; json数据&#125;// 不做更新的filed也需要传过去 上面这个操作是全量替换,其实是es先把旧的数据标记为deleted,不做物理删除,然后再创建一个新的document出来,id也和原来的一样,当es数据量增大时,es才会去删除被标记为deleted的数据,来释放空间 那么如何强制创建呢,就是上面这个操作不想去更新,就是想去再创建一个新的document,那么这时候可以在请求后面加一个参数 ?_create,但是执行时候发现这样会报错,因为这个id已经存在了引起冲突,所以会报错 partial update12345678POST /index_name/type_name/id/_update&#123; &quot;doc&quot;:&#123; &quot;需要更新的列&quot;:&quot;值&quot; &#125;&#125;// 上面这个更新看起来就比较方便了,每次只需要传递少数的发生修改的filed过去即可,不需要将全量的document数据发送过去. 两者对比我们使用全量替换的时候,当我们只需要修改document中其中某一个字段的时候必须知道其他字段的值,这时候我们需要进行一次查询,然后将不需要修改的字段原封不动的写入到语句中去,可以很容易的看出进行一次数据修改,需要用于先去查询,然后更新再写回到es中去,耗费的时间相对较长,而且在写入数据的时候,可能别人已经对数据进行修改了,容易产生并发冲突 然后我们来看一下partial update,其实partial update的执行和全量替换是一样的,也是先获取document,然后将传过来的filed更新到document中,将老的document标记为deleted,用新的数据创建新的document, 但是相较于全量替换,partial update的所有查询修改都是发生在es内部的,操作基本都是毫秒级别的,耗费的时间相对较小,并发冲突的可能性也就相对较小. 总结partial update与全量替换的执行步骤其实是一致的,但是partial update所有的查询和修改操作都是在es内部的shard中进行的,避免了网络数据传输的开销,提升 了性能. 第二减少了查询和修改中的时间间隔,有效减少了并发冲突. 删除document1DELETE /index_name/type_name/id 这里的删除也是做的逻辑删除,es把要删除的document标记为deleted状态,也是在存储空间不够的时候es才会去做物理删除来释放空间]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-1-Windows下安装Es]]></title>
    <url>%2F2018%2F11%2F16%2FElasticsearch-1-Windows%E4%B8%8B%E5%AE%89%E8%A3%85Es%2F</url>
    <content type="text"><![CDATA[下载elasticsearchelasticsearch-5.2.0.zip 解压 cmd进入bin目录,启动1elasticsearch.bat 检查是否启动成功1http://localhost:9200/?pretty 返回值12345678910111213&#123; &quot;name&quot; : &quot;f57uV91&quot;, // 节点名称 &quot;cluster_name&quot; : &quot;elasticsearch&quot;, // 所在集群名称 &quot;cluster_uuid&quot; : &quot;uyiZiUqnSSaV-eazKkv_sg&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;5.2.0&quot;, // 版本号 &quot;build_hash&quot; : &quot;24e05b9&quot;, &quot;build_date&quot; : &quot;2017-01-24T19:52:35.800Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;6.4.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 下载Kibanakibana-5.2.0-windows-x86.zip 解压 进入bin目录,启动kibana.bat localhost:5601/ 进入Dev Tools]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-0-核心概念]]></title>
    <url>%2F2018%2F11%2F16%2FElasticsearch-0-%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[全文检索把要搜索的内容 进行分词储存为倒排索引, 搜索的时候,去扫描的是内容中的所有关键词而不是去扫描内容本身内容中能拆分出来多少词 倒排索引中就会有多少行 先遍历倒排索引中的关键词,然后去数据源中找到数据返回 Lucene封装了建立倒排索引,以及搜索的代码,包括各种算法, 提供了java使用的api,用Lucene可以将已有的数据建立索引,Lucene会在本地磁盘上组织索引的数据结构,另外可以用Lucene提供的提供的功能和api对磁盘上的索引数据进行搜索 ElasticsearchLucene索引是建立在磁盘上的,当数据量大的时候,建立的索引数据占用的空间也会越来越大, 可能会需要多台机器来存放索引数据,这时前端搜索关键字的时候,就需要去判断去哪台机器上的索引搜索,而且服务器宕机的情况下索引数据可能会丢失等.这个时候就需要Elasticsearch,ES底层封装了Lucene,可以配置多个ES节点优点: 自动维护数据的分布到多个节点的索引建立,还有搜索请求分布到多个节点的执行 自动维护数据的冗余副本,保证某些机器宕机之后不会丢失数据 封装了更多的高级功能,便于快速开发应用,提供更复杂的搜索功能,聚合分析的功能,基于地理位置的搜索等 功能: 分布式的搜索引擎和数据分析引擎 数据分析:比如查询一个电商网站中,某一类商品销量前10的店铺;某类商品中一个月内访问量最高的商品等 全文检索,结构化检索,数据分析 结构化检索:比如搜索某一类的商品有哪些 对海量的数据进行实时的处理 ES可以自动将海量的数据分散到多台服务器上存储和检索 进实时是指秒级别的数据搜索和分析 特点: 可以作为一个大型分布式集群技术,处理PB级别的数据, 也可以运行在单机上 将全文检索和数据分析合并在了一起 对用户而言 开箱即用 对传统数据库的补充,提供了很多数据库不能提供的功能,如全文检索,同义词处理,相关度排名,复杂数据分析,海量数据的近实时处理等 核心概念Near RealTime(NRT): 近实时, 从数据写入到可以被搜索到大概有1s的延迟; 基于ES执行搜索和分析可以达到秒级 Cluster: 集群,包含多个节点,节点可以通过配置集群名称来标识是属于哪个集群的 Node:节点 集群中的一个节点, 节点也有名称(默认是随机分配的), 默认节点会被加入到名为”elasticsearch”的集群 Document&amp;field: 文档,ES中的最小数据单元,一个document可以是任意一条数据, 数据结构通常是JSON 一个document中会有多个field,每个field就是json中的一个字段 Index:索引,包含一堆相似结构的文档数据, 比如有一个订单索引,商品分类索引,索引会有一个名称,一个index代表了一类类似的或者相同的document,index可以包含多个document,比如创建一个product index(商品索引),里面就可能存放了所有的商品数据 Type:类型,每个索引里可以有一个或多个type, type是index中的一个逻辑分类,一个type下的document都有相同的filed shard:单机无法存储大量数据,ES可以将一个索引中的数据拆分成多个shard,分布在多台服务器上存储.有了shard就可以横向扩展,当数据量增加的时候,直接再增加一个es节点就可以了,搜索和分析的操作分布到多台服务器上执行可以提高吞吐量和性能. replica: 服务器可能随时故障或宕机,此时shard节点会丢失,因此可以为每个shard创建多个replica副本, replica可以在shard宕机的情况下提供备用服务 同时可以提升搜索操作的吞吐量和性能.primary shard(建立索引时一次设置,不能修改,默认5个)replica shard(随时修改数量,默认1个&lt;对应每个primary shard的1个&gt;), 默认每个索引10个shard,5个 primary shard,5个replica shard 最小的高可用配置是两台服务器 ES对应到数据库document —&gt; 行type —&gt; 表index —&gt; 库]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
</search>
